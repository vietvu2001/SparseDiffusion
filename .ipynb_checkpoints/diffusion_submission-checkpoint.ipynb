{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac099f71-dc19-40ba-a045-1fe46a99e3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "# Set default data type\n",
    "torch.set_default_dtype(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e150d4cf-00c9-4271-8072-394e8b6be944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change working directory\n",
    "if os.getcwd() == \"/home/groups/montanar/vietvu01\":\n",
    "    new_wd = os.path.join(os.getcwd(), 'SparseDiffusion')\n",
    "    os.chdir(new_wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d668bb4-7db4-472b-8811-2fcdf5304503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: NVIDIA A100-SXM4-80GB\n",
      "Total memory: 84.97 GB\n",
      "Reserved memory: 0.01 GB\n",
      "Allocated memory: 0.01 GB\n",
      "Free memory: 84.94 GB\n"
     ]
    }
   ],
   "source": [
    "# Call GPU device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # Use the first GPU (index 0)\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available, using CPU\")\n",
    "\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    torch.cuda.empty_cache()\n",
    "    total_memory = torch.cuda.get_device_properties(device).total_memory\n",
    "    reserved_memory = torch.cuda.memory_reserved(device)\n",
    "    allocated_memory = torch.cuda.memory_allocated(device)\n",
    "    free_memory = total_memory - reserved_memory - allocated_memory\n",
    "    print(f\"Total memory: {total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Reserved memory: {reserved_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Allocated memory: {allocated_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Free memory: {free_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7f14a24f-a64d-4b8f-907f-dad39d8b0ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functions from helper files\n",
    "from sample_data import sample_data, SubmatrixDataset\n",
    "from loss import mc_loss_batch_simul, mc_loss_batch_fixed\n",
    "from denoiser import TopKStraightThrough, TestMPNN_3\n",
    "from polytime_algs import optimal_polynomial_estimator, evaluate_estimator\n",
    "from generation import generation\n",
    "from train import train, set_global_seed, train_part_1, train_part_2, train_part_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b162963-e8fc-4846-a0f6-57348d047a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function loss.mc_loss_batch_fixed(model, batch, t, n, k, predict, device)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_loss_batch_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "171dc190-190b-4a36-b112-77a22bd32cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e925d01-1ab9-4e43-8593-efe53259b072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0ec3935-2b48-401d-99ca-7e675fba8296",
   "metadata": {},
   "source": [
    "# 3. Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70200141-1fce-42aa-b543-b87948427d3b",
   "metadata": {},
   "source": [
    "**3.1: Stability testing for training pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b581e8f-a070-402a-b6d3-f8645fd4b9c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 7\n",
      "Iteration 0, Batch: 0, Loss: 0.526398241519928\n",
      "Iteration 0, Batch: 1, Loss: 0.5352771878242493\n",
      "Iteration 0, Batch: 2, Loss: 0.5423979163169861\n",
      "Iteration 0, Batch: 3, Loss: 0.5858768224716187\n",
      "Iteration 0, Batch: 4, Loss: 0.525317907333374\n",
      "Iteration 0, Batch: 5, Loss: 0.48988914489746094\n",
      "Iteration 0, Batch: 6, Loss: 0.5047929286956787\n",
      "Iteration 0, Batch: 7, Loss: 0.5648434162139893\n",
      "Iteration 0, Batch: 8, Loss: 0.4948868155479431\n",
      "Iteration 0, Batch: 9, Loss: 0.49493053555488586\n",
      "Iteration 0, Batch: 10, Loss: 0.4999651312828064\n",
      "Iteration 0, Batch: 11, Loss: 0.5499891042709351\n",
      "Iteration 0, Batch: 12, Loss: 0.4149986207485199\n",
      "Iteration 0, Batch: 13, Loss: 0.484998881816864\n",
      "Iteration 0, Batch: 14, Loss: 0.5299948453903198\n",
      "Iteration 0, Batch: 15, Loss: 0.4599864184856415\n",
      "Iteration 0, Batch: 16, Loss: 0.4299752712249756\n",
      "Iteration 0, Batch: 17, Loss: 0.5349417924880981\n",
      "Iteration 0, Batch: 18, Loss: 0.49994632601737976\n",
      "Iteration 0, Batch: 19, Loss: 0.49992015957832336\n",
      "Iteration 0, Batch: 20, Loss: 0.5249089002609253\n",
      "Iteration 0, Batch: 21, Loss: 0.4248834252357483\n",
      "Iteration 0, Batch: 22, Loss: 0.5198926329612732\n",
      "Iteration 0, Batch: 23, Loss: 0.4848407506942749\n",
      "Iteration 0, Batch: 24, Loss: 0.41488736867904663\n",
      "Iteration 0, Batch: 25, Loss: 0.5248055458068848\n",
      "Iteration 0, Batch: 26, Loss: 0.5047374367713928\n",
      "Iteration 0, Batch: 27, Loss: 0.5048272609710693\n",
      "Iteration 0, Batch: 28, Loss: 0.5647989511489868\n",
      "Iteration 0, Batch: 29, Loss: 0.5197904706001282\n",
      "Iteration 0, Batch: 30, Loss: 0.5249574780464172\n",
      "Iteration 0, Batch: 31, Loss: 0.5249745845794678\n",
      "Iteration 0, Batch: 32, Loss: 0.5647594332695007\n",
      "Iteration 0, Batch: 33, Loss: 0.5895723104476929\n",
      "Iteration 0, Batch: 34, Loss: 0.4997299015522003\n",
      "Iteration 0, Batch: 35, Loss: 0.5649484395980835\n",
      "Iteration 0, Batch: 36, Loss: 0.49984052777290344\n",
      "Iteration 0, Batch: 37, Loss: 0.4450046718120575\n",
      "Iteration 0, Batch: 38, Loss: 0.5198002457618713\n",
      "Iteration 0, Batch: 39, Loss: 0.4999752342700958\n",
      "Iteration 0, Batch: 40, Loss: 0.4699036180973053\n",
      "Iteration 0, Batch: 41, Loss: 0.4849335253238678\n",
      "Iteration 0, Batch: 42, Loss: 0.4847891926765442\n",
      "Iteration 0, Batch: 43, Loss: 0.4998203217983246\n",
      "Iteration 0, Batch: 44, Loss: 0.5197816491127014\n",
      "Iteration 0, Batch: 45, Loss: 0.46485358476638794\n",
      "Iteration 0, Batch: 46, Loss: 0.5297637581825256\n",
      "Iteration 0, Batch: 47, Loss: 0.46987369656562805\n",
      "Iteration 0, Batch: 48, Loss: 0.459878534078598\n",
      "Iteration 0, Batch: 49, Loss: 0.43991896510124207\n",
      "Avg loss: 0.5042955976724625\n",
      "Iteration 1, Batch: 0, Loss: 0.5347943902015686\n",
      "Iteration 1, Batch: 1, Loss: 0.5147537589073181\n",
      "Iteration 1, Batch: 2, Loss: 0.4648820459842682\n",
      "Iteration 1, Batch: 3, Loss: 0.4848823547363281\n",
      "Iteration 1, Batch: 4, Loss: 0.4548155963420868\n",
      "Iteration 1, Batch: 5, Loss: 0.5098944306373596\n",
      "Iteration 1, Batch: 6, Loss: 0.46491390466690063\n",
      "Iteration 1, Batch: 7, Loss: 0.5048615336418152\n",
      "Iteration 1, Batch: 8, Loss: 0.5147612690925598\n",
      "Iteration 1, Batch: 9, Loss: 0.5398746132850647\n",
      "Iteration 1, Batch: 10, Loss: 0.5398005843162537\n",
      "Iteration 1, Batch: 11, Loss: 0.55477374792099\n",
      "Iteration 1, Batch: 12, Loss: 0.44479888677597046\n",
      "Iteration 1, Batch: 13, Loss: 0.4248526692390442\n",
      "Iteration 1, Batch: 14, Loss: 0.4848203957080841\n",
      "Iteration 1, Batch: 15, Loss: 0.5198931694030762\n",
      "Iteration 1, Batch: 16, Loss: 0.5048465132713318\n",
      "Iteration 1, Batch: 17, Loss: 0.44991201162338257\n",
      "Iteration 1, Batch: 18, Loss: 0.5199083685874939\n",
      "Iteration 1, Batch: 19, Loss: 0.4847991168498993\n",
      "Iteration 1, Batch: 20, Loss: 0.4797798693180084\n",
      "Iteration 1, Batch: 21, Loss: 0.5548330545425415\n",
      "Iteration 1, Batch: 22, Loss: 0.5298143625259399\n",
      "Iteration 1, Batch: 23, Loss: 0.5248240828514099\n",
      "Iteration 1, Batch: 24, Loss: 0.5547304153442383\n",
      "Iteration 1, Batch: 25, Loss: 0.4799005687236786\n",
      "Iteration 1, Batch: 26, Loss: 0.5197554230690002\n",
      "Iteration 1, Batch: 27, Loss: 0.5547572374343872\n",
      "Iteration 1, Batch: 28, Loss: 0.5298022627830505\n",
      "Iteration 1, Batch: 29, Loss: 0.5098590850830078\n",
      "Iteration 1, Batch: 30, Loss: 0.5597081780433655\n",
      "Iteration 1, Batch: 31, Loss: 0.4948287904262543\n",
      "Iteration 1, Batch: 32, Loss: 0.5396525263786316\n",
      "Iteration 1, Batch: 33, Loss: 0.5147346258163452\n",
      "Iteration 1, Batch: 34, Loss: 0.46490275859832764\n",
      "Iteration 1, Batch: 35, Loss: 0.4847951829433441\n",
      "Iteration 1, Batch: 36, Loss: 0.4548846185207367\n",
      "Iteration 1, Batch: 37, Loss: 0.4498368799686432\n",
      "Iteration 1, Batch: 38, Loss: 0.42497488856315613\n",
      "Iteration 1, Batch: 39, Loss: 0.5297858119010925\n",
      "Iteration 1, Batch: 40, Loss: 0.5048848390579224\n",
      "Iteration 1, Batch: 41, Loss: 0.4748815894126892\n",
      "Iteration 1, Batch: 42, Loss: 0.499856173992157\n",
      "Iteration 1, Batch: 43, Loss: 0.5347105860710144\n",
      "Iteration 1, Batch: 44, Loss: 0.464872807264328\n",
      "Iteration 1, Batch: 45, Loss: 0.5048614740371704\n",
      "Iteration 1, Batch: 46, Loss: 0.5848274230957031\n",
      "Iteration 1, Batch: 47, Loss: 0.4848397672176361\n",
      "Iteration 1, Batch: 48, Loss: 0.48490530252456665\n",
      "Iteration 1, Batch: 49, Loss: 0.5797693729400635\n",
      "Avg loss: 0.5038281863927841\n",
      "Iteration 2, Batch: 0, Loss: 0.45482298731803894\n",
      "Iteration 2, Batch: 1, Loss: 0.46990224719047546\n",
      "Iteration 2, Batch: 2, Loss: 0.49483928084373474\n",
      "Iteration 2, Batch: 3, Loss: 0.4998418986797333\n",
      "Iteration 2, Batch: 4, Loss: 0.47480446100234985\n",
      "Iteration 2, Batch: 5, Loss: 0.549807608127594\n",
      "Iteration 2, Batch: 6, Loss: 0.5797393918037415\n",
      "Iteration 2, Batch: 7, Loss: 0.534791111946106\n",
      "Iteration 2, Batch: 8, Loss: 0.4998425245285034\n",
      "Iteration 2, Batch: 9, Loss: 0.5398432612419128\n",
      "Iteration 2, Batch: 10, Loss: 0.5148111581802368\n",
      "Iteration 2, Batch: 11, Loss: 0.5049024224281311\n",
      "Iteration 2, Batch: 12, Loss: 0.479907363653183\n",
      "Iteration 2, Batch: 13, Loss: 0.4898485541343689\n",
      "Iteration 2, Batch: 14, Loss: 0.5497162342071533\n",
      "Iteration 2, Batch: 15, Loss: 0.5247921347618103\n",
      "Iteration 2, Batch: 16, Loss: 0.5198137760162354\n",
      "Iteration 2, Batch: 17, Loss: 0.5248114466667175\n",
      "Iteration 2, Batch: 18, Loss: 0.5047826170921326\n",
      "Iteration 2, Batch: 19, Loss: 0.5496610999107361\n",
      "Iteration 2, Batch: 20, Loss: 0.48982128500938416\n",
      "Iteration 2, Batch: 21, Loss: 0.4948616623878479\n",
      "Iteration 2, Batch: 22, Loss: 0.5048484206199646\n",
      "Iteration 2, Batch: 23, Loss: 0.5047589540481567\n",
      "Iteration 2, Batch: 24, Loss: 0.5198546648025513\n",
      "Iteration 2, Batch: 25, Loss: 0.5346333384513855\n",
      "Iteration 2, Batch: 26, Loss: 0.5248481035232544\n",
      "Iteration 2, Batch: 27, Loss: 0.40985411405563354\n",
      "Iteration 2, Batch: 28, Loss: 0.4498107135295868\n",
      "Iteration 2, Batch: 29, Loss: 0.48482662439346313\n",
      "Iteration 2, Batch: 30, Loss: 0.46478018164634705\n",
      "Iteration 2, Batch: 31, Loss: 0.5547283291816711\n",
      "Iteration 2, Batch: 32, Loss: 0.47985512018203735\n",
      "Iteration 2, Batch: 33, Loss: 0.5098356604576111\n",
      "Iteration 2, Batch: 34, Loss: 0.5049206614494324\n",
      "Iteration 2, Batch: 35, Loss: 0.41996684670448303\n",
      "Iteration 2, Batch: 36, Loss: 0.5497963428497314\n",
      "Iteration 2, Batch: 37, Loss: 0.529837965965271\n",
      "Iteration 2, Batch: 38, Loss: 0.49973219633102417\n",
      "Iteration 2, Batch: 39, Loss: 0.5349128246307373\n",
      "Iteration 2, Batch: 40, Loss: 0.5448269248008728\n",
      "Iteration 2, Batch: 41, Loss: 0.4847746193408966\n",
      "Iteration 2, Batch: 42, Loss: 0.48984938859939575\n",
      "Iteration 2, Batch: 43, Loss: 0.524754524230957\n",
      "Iteration 2, Batch: 44, Loss: 0.47484511137008667\n",
      "Iteration 2, Batch: 45, Loss: 0.5347726941108704\n",
      "Iteration 2, Batch: 46, Loss: 0.4598076641559601\n",
      "Iteration 2, Batch: 47, Loss: 0.45990169048309326\n",
      "Iteration 2, Batch: 48, Loss: 0.4398839473724365\n",
      "Iteration 2, Batch: 49, Loss: 0.5498371124267578\n",
      "Avg loss: 0.5038203853368759\n",
      "Iteration 3, Batch: 0, Loss: 0.4848465621471405\n",
      "Iteration 3, Batch: 1, Loss: 0.449868381023407\n",
      "Iteration 3, Batch: 2, Loss: 0.47979140281677246\n",
      "Iteration 3, Batch: 3, Loss: 0.5447007417678833\n",
      "Iteration 3, Batch: 4, Loss: 0.5249015092849731\n",
      "Iteration 3, Batch: 5, Loss: 0.5098198056221008\n",
      "Iteration 3, Batch: 6, Loss: 0.544861912727356\n",
      "Iteration 3, Batch: 7, Loss: 0.5099102854728699\n",
      "Iteration 3, Batch: 8, Loss: 0.5247161984443665\n",
      "Iteration 3, Batch: 9, Loss: 0.5148035883903503\n",
      "Iteration 3, Batch: 10, Loss: 0.5048142075538635\n",
      "Iteration 3, Batch: 11, Loss: 0.5047730207443237\n",
      "Iteration 3, Batch: 12, Loss: 0.5197557210922241\n",
      "Iteration 3, Batch: 13, Loss: 0.5198290348052979\n",
      "Iteration 3, Batch: 14, Loss: 0.4949077367782593\n",
      "Iteration 3, Batch: 15, Loss: 0.49473699927330017\n",
      "Iteration 3, Batch: 16, Loss: 0.5198173522949219\n",
      "Iteration 3, Batch: 17, Loss: 0.479908287525177\n",
      "Iteration 3, Batch: 18, Loss: 0.44988763332366943\n",
      "Iteration 3, Batch: 19, Loss: 0.4998120665550232\n",
      "Iteration 3, Batch: 20, Loss: 0.5198226571083069\n",
      "Iteration 3, Batch: 21, Loss: 0.5048709511756897\n",
      "Iteration 3, Batch: 22, Loss: 0.5197062492370605\n",
      "Iteration 3, Batch: 23, Loss: 0.46980658173561096\n",
      "Iteration 3, Batch: 24, Loss: 0.5098950862884521\n",
      "Iteration 3, Batch: 25, Loss: 0.4398835301399231\n",
      "Iteration 3, Batch: 26, Loss: 0.4697430729866028\n",
      "Iteration 3, Batch: 27, Loss: 0.5598717927932739\n",
      "Iteration 3, Batch: 28, Loss: 0.5048491358757019\n",
      "Iteration 3, Batch: 29, Loss: 0.5148645043373108\n",
      "Iteration 3, Batch: 30, Loss: 0.5348809361457825\n",
      "Iteration 3, Batch: 31, Loss: 0.43971559405326843\n",
      "Iteration 3, Batch: 32, Loss: 0.4897904098033905\n",
      "Iteration 3, Batch: 33, Loss: 0.4599846303462982\n",
      "Iteration 3, Batch: 34, Loss: 0.4798476994037628\n",
      "Iteration 3, Batch: 35, Loss: 0.5247538089752197\n",
      "Iteration 3, Batch: 36, Loss: 0.49485668540000916\n",
      "Iteration 3, Batch: 37, Loss: 0.5298008322715759\n",
      "Iteration 3, Batch: 38, Loss: 0.5047462582588196\n",
      "Iteration 3, Batch: 39, Loss: 0.4897516667842865\n",
      "Iteration 3, Batch: 40, Loss: 0.46988433599472046\n",
      "Iteration 3, Batch: 41, Loss: 0.5048316121101379\n",
      "Iteration 3, Batch: 42, Loss: 0.5449268817901611\n",
      "Iteration 3, Batch: 43, Loss: 0.474822074174881\n",
      "Iteration 3, Batch: 44, Loss: 0.5298543572425842\n",
      "Iteration 3, Batch: 45, Loss: 0.5197904706001282\n",
      "Iteration 3, Batch: 46, Loss: 0.4848603308200836\n",
      "Iteration 3, Batch: 47, Loss: 0.5447967648506165\n",
      "Iteration 3, Batch: 48, Loss: 0.5247149467468262\n",
      "Iteration 3, Batch: 49, Loss: 0.5548171997070312\n",
      "Avg loss: 0.5038240700960159\n",
      "Iteration 4, Batch: 0, Loss: 0.5348532795906067\n",
      "Iteration 4, Batch: 1, Loss: 0.4898386299610138\n",
      "Iteration 4, Batch: 2, Loss: 0.5096977353096008\n",
      "Iteration 4, Batch: 3, Loss: 0.5347709655761719\n",
      "Iteration 4, Batch: 4, Loss: 0.5198196172714233\n",
      "Iteration 4, Batch: 5, Loss: 0.5198135375976562\n",
      "Iteration 4, Batch: 6, Loss: 0.4848636984825134\n",
      "Iteration 4, Batch: 7, Loss: 0.45493847131729126\n",
      "Iteration 4, Batch: 8, Loss: 0.5347119569778442\n",
      "Iteration 4, Batch: 9, Loss: 0.5248264074325562\n",
      "Iteration 4, Batch: 10, Loss: 0.4648009240627289\n",
      "Iteration 4, Batch: 11, Loss: 0.5047077536582947\n",
      "Iteration 4, Batch: 12, Loss: 0.4949001967906952\n",
      "Iteration 4, Batch: 13, Loss: 0.49482405185699463\n",
      "Iteration 4, Batch: 14, Loss: 0.454943984746933\n",
      "Iteration 4, Batch: 15, Loss: 0.47984522581100464\n",
      "Iteration 4, Batch: 16, Loss: 0.4998724162578583\n",
      "Iteration 4, Batch: 17, Loss: 0.5497372150421143\n",
      "Iteration 4, Batch: 18, Loss: 0.4899410903453827\n",
      "Iteration 4, Batch: 19, Loss: 0.3999268710613251\n",
      "Iteration 4, Batch: 20, Loss: 0.5398432016372681\n",
      "Iteration 4, Batch: 21, Loss: 0.41987472772598267\n",
      "Iteration 4, Batch: 22, Loss: 0.5497865676879883\n",
      "Iteration 4, Batch: 23, Loss: 0.47489383816719055\n",
      "Iteration 4, Batch: 24, Loss: 0.5297675728797913\n",
      "Iteration 4, Batch: 25, Loss: 0.4798930287361145\n",
      "Iteration 4, Batch: 26, Loss: 0.5748437643051147\n",
      "Iteration 4, Batch: 27, Loss: 0.4848397672176361\n",
      "Iteration 4, Batch: 28, Loss: 0.5497783422470093\n",
      "Iteration 4, Batch: 29, Loss: 0.5247536897659302\n",
      "Iteration 4, Batch: 30, Loss: 0.4848878085613251\n",
      "Iteration 4, Batch: 31, Loss: 0.5747390389442444\n",
      "Iteration 4, Batch: 32, Loss: 0.4449295997619629\n",
      "Iteration 4, Batch: 33, Loss: 0.5148782134056091\n",
      "Iteration 4, Batch: 34, Loss: 0.5497323870658875\n",
      "Iteration 4, Batch: 35, Loss: 0.5198521018028259\n",
      "Iteration 4, Batch: 36, Loss: 0.5098516941070557\n",
      "Iteration 4, Batch: 37, Loss: 0.5048446655273438\n",
      "Iteration 4, Batch: 38, Loss: 0.4948357343673706\n",
      "Iteration 4, Batch: 39, Loss: 0.534846305847168\n",
      "Iteration 4, Batch: 40, Loss: 0.5397647619247437\n",
      "Iteration 4, Batch: 41, Loss: 0.4747704267501831\n",
      "Iteration 4, Batch: 42, Loss: 0.5197145938873291\n",
      "Iteration 4, Batch: 43, Loss: 0.4798903167247772\n",
      "Iteration 4, Batch: 44, Loss: 0.5097807049751282\n",
      "Iteration 4, Batch: 45, Loss: 0.529739499092102\n",
      "Iteration 4, Batch: 46, Loss: 0.5097455382347107\n",
      "Iteration 4, Batch: 47, Loss: 0.4348561763763428\n",
      "Iteration 4, Batch: 48, Loss: 0.4898165166378021\n",
      "Iteration 4, Batch: 49, Loss: 0.49990853667259216\n",
      "Avg loss: 0.5038258630037308\n",
      "Iteration 5, Batch: 0, Loss: 0.5398504734039307\n",
      "Iteration 5, Batch: 1, Loss: 0.49480459094047546\n",
      "Iteration 5, Batch: 2, Loss: 0.4398881196975708\n",
      "Iteration 5, Batch: 3, Loss: 0.46979594230651855\n",
      "Iteration 5, Batch: 4, Loss: 0.5448655486106873\n",
      "Iteration 5, Batch: 5, Loss: 0.5148770213127136\n",
      "Iteration 5, Batch: 6, Loss: 0.484891802072525\n",
      "Iteration 5, Batch: 7, Loss: 0.48978129029273987\n",
      "Iteration 5, Batch: 8, Loss: 0.514811635017395\n",
      "Iteration 5, Batch: 9, Loss: 0.5498474836349487\n",
      "Iteration 5, Batch: 10, Loss: 0.5347381234169006\n",
      "Iteration 5, Batch: 11, Loss: 0.44490140676498413\n",
      "Iteration 5, Batch: 12, Loss: 0.5048564672470093\n",
      "Iteration 5, Batch: 13, Loss: 0.48490041494369507\n",
      "Iteration 5, Batch: 14, Loss: 0.49991026520729065\n",
      "Iteration 5, Batch: 15, Loss: 0.4947983920574188\n",
      "Iteration 5, Batch: 16, Loss: 0.48483139276504517\n",
      "Iteration 5, Batch: 17, Loss: 0.5598784685134888\n",
      "Iteration 5, Batch: 18, Loss: 0.4847666025161743\n",
      "Iteration 5, Batch: 19, Loss: 0.5148179531097412\n",
      "Iteration 5, Batch: 20, Loss: 0.4998682737350464\n",
      "Iteration 5, Batch: 21, Loss: 0.4997963607311249\n",
      "Iteration 5, Batch: 22, Loss: 0.48977693915367126\n",
      "Iteration 5, Batch: 23, Loss: 0.564784824848175\n",
      "Iteration 5, Batch: 24, Loss: 0.46495673060417175\n",
      "Iteration 5, Batch: 25, Loss: 0.4998360276222229\n",
      "Iteration 5, Batch: 26, Loss: 0.509783923625946\n",
      "Iteration 5, Batch: 27, Loss: 0.509807288646698\n",
      "Iteration 5, Batch: 28, Loss: 0.4748595952987671\n",
      "Iteration 5, Batch: 29, Loss: 0.5098530650138855\n",
      "Iteration 5, Batch: 30, Loss: 0.5148929953575134\n",
      "Iteration 5, Batch: 31, Loss: 0.5048691630363464\n",
      "Iteration 5, Batch: 32, Loss: 0.5248965620994568\n",
      "Iteration 5, Batch: 33, Loss: 0.5248216390609741\n",
      "Iteration 5, Batch: 34, Loss: 0.48477908968925476\n",
      "Iteration 5, Batch: 35, Loss: 0.4498756229877472\n",
      "Iteration 5, Batch: 36, Loss: 0.5197952389717102\n",
      "Iteration 5, Batch: 37, Loss: 0.5198783874511719\n",
      "Iteration 5, Batch: 38, Loss: 0.46978139877319336\n",
      "Iteration 5, Batch: 39, Loss: 0.4797563850879669\n",
      "Iteration 5, Batch: 40, Loss: 0.5348644852638245\n",
      "Iteration 5, Batch: 41, Loss: 0.48480284214019775\n",
      "Iteration 5, Batch: 42, Loss: 0.5347320437431335\n",
      "Iteration 5, Batch: 43, Loss: 0.5648144483566284\n",
      "Iteration 5, Batch: 44, Loss: 0.4897395074367523\n",
      "Iteration 5, Batch: 45, Loss: 0.5047674775123596\n",
      "Iteration 5, Batch: 46, Loss: 0.48986437916755676\n",
      "Iteration 5, Batch: 47, Loss: 0.5096327662467957\n",
      "Iteration 5, Batch: 48, Loss: 0.49480581283569336\n",
      "Iteration 5, Batch: 49, Loss: 0.514714777469635\n",
      "Avg loss: 0.5038244289159775\n",
      "Iteration 6, Batch: 0, Loss: 0.4749040901660919\n",
      "Iteration 6, Batch: 1, Loss: 0.5248349905014038\n",
      "Iteration 6, Batch: 2, Loss: 0.5047986507415771\n",
      "Iteration 6, Batch: 3, Loss: 0.4998740553855896\n",
      "Iteration 6, Batch: 4, Loss: 0.5647055506706238\n",
      "Iteration 6, Batch: 5, Loss: 0.5049169659614563\n",
      "Iteration 6, Batch: 6, Loss: 0.5247430205345154\n",
      "Iteration 6, Batch: 7, Loss: 0.48988255858421326\n",
      "Iteration 6, Batch: 8, Loss: 0.4948846995830536\n",
      "Iteration 6, Batch: 9, Loss: 0.49985456466674805\n",
      "Iteration 6, Batch: 10, Loss: 0.4548323452472687\n",
      "Iteration 6, Batch: 11, Loss: 0.49480894207954407\n",
      "Iteration 6, Batch: 12, Loss: 0.47996604442596436\n",
      "Iteration 6, Batch: 13, Loss: 0.5049116611480713\n",
      "Iteration 6, Batch: 14, Loss: 0.44994911551475525\n",
      "Iteration 6, Batch: 15, Loss: 0.4349011778831482\n",
      "Iteration 6, Batch: 16, Loss: 0.44983986020088196\n",
      "Iteration 6, Batch: 17, Loss: 0.4398719370365143\n",
      "Iteration 6, Batch: 18, Loss: 0.43978026509284973\n",
      "Iteration 6, Batch: 19, Loss: 0.4598931074142456\n",
      "Iteration 6, Batch: 20, Loss: 0.5197800993919373\n",
      "Iteration 6, Batch: 21, Loss: 0.5148733258247375\n",
      "Iteration 6, Batch: 22, Loss: 0.494852215051651\n",
      "Iteration 6, Batch: 23, Loss: 0.5697413682937622\n",
      "Iteration 6, Batch: 24, Loss: 0.5198597311973572\n",
      "Iteration 6, Batch: 25, Loss: 0.5697011947631836\n",
      "Iteration 6, Batch: 26, Loss: 0.5347393155097961\n",
      "Iteration 6, Batch: 27, Loss: 0.5699293613433838\n",
      "Iteration 6, Batch: 28, Loss: 0.5247790217399597\n",
      "Iteration 6, Batch: 29, Loss: 0.47984835505485535\n",
      "Iteration 6, Batch: 30, Loss: 0.43991783261299133\n",
      "Iteration 6, Batch: 31, Loss: 0.4848509728908539\n",
      "Iteration 6, Batch: 32, Loss: 0.4798891246318817\n",
      "Iteration 6, Batch: 33, Loss: 0.5049364566802979\n",
      "Iteration 6, Batch: 34, Loss: 0.5097161531448364\n",
      "Iteration 6, Batch: 35, Loss: 0.4696752727031708\n",
      "Iteration 6, Batch: 36, Loss: 0.5298122763633728\n",
      "Iteration 6, Batch: 37, Loss: 0.4848194122314453\n",
      "Iteration 6, Batch: 38, Loss: 0.4598289430141449\n",
      "Iteration 6, Batch: 39, Loss: 0.4947161078453064\n",
      "Iteration 6, Batch: 40, Loss: 0.5497578978538513\n",
      "Iteration 6, Batch: 41, Loss: 0.5048497915267944\n",
      "Iteration 6, Batch: 42, Loss: 0.5546852946281433\n",
      "Iteration 6, Batch: 43, Loss: 0.6096981763839722\n",
      "Iteration 6, Batch: 44, Loss: 0.5198575258255005\n",
      "Iteration 6, Batch: 45, Loss: 0.534837007522583\n",
      "Iteration 6, Batch: 46, Loss: 0.4896884560585022\n",
      "Iteration 6, Batch: 47, Loss: 0.44989141821861267\n",
      "Iteration 6, Batch: 48, Loss: 0.5597657561302185\n",
      "Iteration 6, Batch: 49, Loss: 0.5697727203369141\n",
      "Avg loss: 0.5038244837522506\n",
      "Iteration 7, Batch: 0, Loss: 0.5147296786308289\n",
      "Iteration 7, Batch: 1, Loss: 0.4898770749568939\n",
      "Iteration 7, Batch: 2, Loss: 0.5099232196807861\n",
      "Iteration 7, Batch: 3, Loss: 0.4698636829853058\n",
      "Iteration 7, Batch: 4, Loss: 0.45472458004951477\n",
      "Iteration 7, Batch: 5, Loss: 0.494857519865036\n",
      "Iteration 7, Batch: 6, Loss: 0.41980817914009094\n",
      "Iteration 7, Batch: 7, Loss: 0.4848910868167877\n",
      "Iteration 7, Batch: 8, Loss: 0.4548605978488922\n",
      "Iteration 7, Batch: 9, Loss: 0.4947127401828766\n",
      "Iteration 7, Batch: 10, Loss: 0.5148332118988037\n",
      "Iteration 7, Batch: 11, Loss: 0.4698415696620941\n",
      "Iteration 7, Batch: 12, Loss: 0.4897807240486145\n",
      "Iteration 7, Batch: 13, Loss: 0.4898034930229187\n",
      "Iteration 7, Batch: 14, Loss: 0.5097646713256836\n",
      "Iteration 7, Batch: 15, Loss: 0.5198328495025635\n",
      "Iteration 7, Batch: 16, Loss: 0.46491721272468567\n",
      "Iteration 7, Batch: 17, Loss: 0.5346868634223938\n",
      "Iteration 7, Batch: 18, Loss: 0.5347129106521606\n",
      "Iteration 7, Batch: 19, Loss: 0.5048252940177917\n",
      "Iteration 7, Batch: 20, Loss: 0.47482484579086304\n",
      "Iteration 7, Batch: 21, Loss: 0.4549541473388672\n",
      "Iteration 7, Batch: 22, Loss: 0.6048234701156616\n",
      "Iteration 7, Batch: 23, Loss: 0.574576199054718\n",
      "Iteration 7, Batch: 24, Loss: 0.49983230233192444\n",
      "Iteration 7, Batch: 25, Loss: 0.5496610403060913\n",
      "Iteration 7, Batch: 26, Loss: 0.489783376455307\n",
      "Iteration 7, Batch: 27, Loss: 0.46485835313796997\n",
      "Iteration 7, Batch: 28, Loss: 0.47988176345825195\n",
      "Iteration 7, Batch: 29, Loss: 0.5347725749015808\n",
      "Iteration 7, Batch: 30, Loss: 0.569749653339386\n",
      "Iteration 7, Batch: 31, Loss: 0.5099287629127502\n",
      "Iteration 7, Batch: 32, Loss: 0.5848583579063416\n",
      "Iteration 7, Batch: 33, Loss: 0.47986671328544617\n",
      "Iteration 7, Batch: 34, Loss: 0.44998839497566223\n",
      "Iteration 7, Batch: 35, Loss: 0.4798585772514343\n",
      "Iteration 7, Batch: 36, Loss: 0.5198726654052734\n",
      "Iteration 7, Batch: 37, Loss: 0.5397699475288391\n",
      "Iteration 7, Batch: 38, Loss: 0.4949040114879608\n",
      "Iteration 7, Batch: 39, Loss: 0.46988344192504883\n",
      "Iteration 7, Batch: 40, Loss: 0.5248430371284485\n",
      "Iteration 7, Batch: 41, Loss: 0.5198222994804382\n",
      "Iteration 7, Batch: 42, Loss: 0.48978132009506226\n",
      "Iteration 7, Batch: 43, Loss: 0.49976226687431335\n",
      "Iteration 7, Batch: 44, Loss: 0.5448423027992249\n",
      "Iteration 7, Batch: 45, Loss: 0.51984703540802\n",
      "Iteration 7, Batch: 46, Loss: 0.5147943496704102\n",
      "Iteration 7, Batch: 47, Loss: 0.514787495136261\n",
      "Iteration 7, Batch: 48, Loss: 0.46480855345726013\n",
      "Iteration 7, Batch: 49, Loss: 0.5497878789901733\n",
      "Avg loss: 0.5038194459676743\n",
      "Iteration 8, Batch: 0, Loss: 0.4748864769935608\n",
      "Iteration 8, Batch: 1, Loss: 0.5249189734458923\n",
      "Iteration 8, Batch: 2, Loss: 0.5399911403656006\n",
      "Iteration 8, Batch: 3, Loss: 0.524806022644043\n",
      "Iteration 8, Batch: 4, Loss: 0.5048608183860779\n",
      "Iteration 8, Batch: 5, Loss: 0.454802006483078\n",
      "Iteration 8, Batch: 6, Loss: 0.5647774934768677\n",
      "Iteration 8, Batch: 7, Loss: 0.4948470890522003\n",
      "Iteration 8, Batch: 8, Loss: 0.5447373986244202\n",
      "Iteration 8, Batch: 9, Loss: 0.5099466443061829\n",
      "Iteration 8, Batch: 10, Loss: 0.509857177734375\n",
      "Iteration 8, Batch: 11, Loss: 0.4847797155380249\n",
      "Iteration 8, Batch: 12, Loss: 0.5597629547119141\n",
      "Iteration 8, Batch: 13, Loss: 0.534890353679657\n",
      "Iteration 8, Batch: 14, Loss: 0.5448773503303528\n",
      "Iteration 8, Batch: 15, Loss: 0.5148309469223022\n",
      "Iteration 8, Batch: 16, Loss: 0.5598251819610596\n",
      "Iteration 8, Batch: 17, Loss: 0.4999038279056549\n",
      "Iteration 8, Batch: 18, Loss: 0.5197171568870544\n",
      "Iteration 8, Batch: 19, Loss: 0.46978628635406494\n",
      "Iteration 8, Batch: 20, Loss: 0.48482710123062134\n",
      "Iteration 8, Batch: 21, Loss: 0.5097203254699707\n",
      "Iteration 8, Batch: 22, Loss: 0.43987104296684265\n",
      "Iteration 8, Batch: 23, Loss: 0.48973238468170166\n",
      "Iteration 8, Batch: 24, Loss: 0.4749174416065216\n",
      "Iteration 8, Batch: 25, Loss: 0.47488275170326233\n",
      "Iteration 8, Batch: 26, Loss: 0.4449803829193115\n",
      "Iteration 8, Batch: 27, Loss: 0.5248879790306091\n",
      "Iteration 8, Batch: 28, Loss: 0.4896887540817261\n",
      "Iteration 8, Batch: 29, Loss: 0.5748186111450195\n",
      "Iteration 8, Batch: 30, Loss: 0.4648095667362213\n",
      "Iteration 8, Batch: 31, Loss: 0.4548051357269287\n",
      "Iteration 8, Batch: 32, Loss: 0.5397717952728271\n",
      "Iteration 8, Batch: 33, Loss: 0.4948243498802185\n",
      "Iteration 8, Batch: 34, Loss: 0.539768636226654\n",
      "Iteration 8, Batch: 35, Loss: 0.4749560058116913\n",
      "Iteration 8, Batch: 36, Loss: 0.44993311166763306\n",
      "Iteration 8, Batch: 37, Loss: 0.4699256718158722\n",
      "Iteration 8, Batch: 38, Loss: 0.5448963642120361\n",
      "Iteration 8, Batch: 39, Loss: 0.5497775673866272\n",
      "Iteration 8, Batch: 40, Loss: 0.4448152780532837\n",
      "Iteration 8, Batch: 41, Loss: 0.4898793697357178\n",
      "Iteration 8, Batch: 42, Loss: 0.46479323506355286\n",
      "Iteration 8, Batch: 43, Loss: 0.4848538041114807\n",
      "Iteration 8, Batch: 44, Loss: 0.5148482918739319\n",
      "Iteration 8, Batch: 45, Loss: 0.4747926592826843\n",
      "Iteration 8, Batch: 46, Loss: 0.5198530554771423\n",
      "Iteration 8, Batch: 47, Loss: 0.5597336292266846\n",
      "Iteration 8, Batch: 48, Loss: 0.5197324156761169\n",
      "Iteration 8, Batch: 49, Loss: 0.4898907244205475\n",
      "Avg loss: 0.5038358491659164\n",
      "Iteration 9, Batch: 0, Loss: 0.49492427706718445\n",
      "Iteration 9, Batch: 1, Loss: 0.4999043941497803\n",
      "Iteration 9, Batch: 2, Loss: 0.41495048999786377\n",
      "Iteration 9, Batch: 3, Loss: 0.5947551727294922\n",
      "Iteration 9, Batch: 4, Loss: 0.4847067892551422\n",
      "Iteration 9, Batch: 5, Loss: 0.5747884511947632\n",
      "Iteration 9, Batch: 6, Loss: 0.47976207733154297\n",
      "Iteration 9, Batch: 7, Loss: 0.5346962213516235\n",
      "Iteration 9, Batch: 8, Loss: 0.4899406433105469\n",
      "Iteration 9, Batch: 9, Loss: 0.47488531470298767\n",
      "Iteration 9, Batch: 10, Loss: 0.5397417545318604\n",
      "Iteration 9, Batch: 11, Loss: 0.47487184405326843\n",
      "Iteration 9, Batch: 12, Loss: 0.5498249530792236\n",
      "Iteration 9, Batch: 13, Loss: 0.4747873544692993\n",
      "Iteration 9, Batch: 14, Loss: 0.4549102783203125\n",
      "Iteration 9, Batch: 15, Loss: 0.49479666352272034\n",
      "Iteration 9, Batch: 16, Loss: 0.5048605799674988\n",
      "Iteration 9, Batch: 17, Loss: 0.46993952989578247\n",
      "Iteration 9, Batch: 18, Loss: 0.4248593747615814\n",
      "Iteration 9, Batch: 19, Loss: 0.48484647274017334\n",
      "Iteration 9, Batch: 20, Loss: 0.5398275852203369\n",
      "Iteration 9, Batch: 21, Loss: 0.49987170100212097\n",
      "Iteration 9, Batch: 22, Loss: 0.48483455181121826\n",
      "Iteration 9, Batch: 23, Loss: 0.5697171092033386\n",
      "Iteration 9, Batch: 24, Loss: 0.4597965478897095\n",
      "Iteration 9, Batch: 25, Loss: 0.42988112568855286\n",
      "Iteration 9, Batch: 26, Loss: 0.5097774863243103\n",
      "Iteration 9, Batch: 27, Loss: 0.5797111988067627\n",
      "Iteration 9, Batch: 28, Loss: 0.4449753165245056\n",
      "Iteration 9, Batch: 29, Loss: 0.5348283052444458\n",
      "Iteration 9, Batch: 30, Loss: 0.4598563015460968\n",
      "Iteration 9, Batch: 31, Loss: 0.5397676229476929\n",
      "Iteration 9, Batch: 32, Loss: 0.5447579622268677\n",
      "Iteration 9, Batch: 33, Loss: 0.579771876335144\n",
      "Iteration 9, Batch: 34, Loss: 0.5146934390068054\n",
      "Iteration 9, Batch: 35, Loss: 0.4648686945438385\n",
      "Iteration 9, Batch: 36, Loss: 0.5147712230682373\n",
      "Iteration 9, Batch: 37, Loss: 0.5896850228309631\n",
      "Iteration 9, Batch: 38, Loss: 0.529914140701294\n",
      "Iteration 9, Batch: 39, Loss: 0.46982648968696594\n",
      "Iteration 9, Batch: 40, Loss: 0.48987555503845215\n",
      "Iteration 9, Batch: 41, Loss: 0.5246790051460266\n",
      "Iteration 9, Batch: 42, Loss: 0.5697153806686401\n",
      "Iteration 9, Batch: 43, Loss: 0.4546758830547333\n",
      "Iteration 9, Batch: 44, Loss: 0.44475114345550537\n",
      "Iteration 9, Batch: 45, Loss: 0.5396420359611511\n",
      "Iteration 9, Batch: 46, Loss: 0.45984530448913574\n",
      "Iteration 9, Batch: 47, Loss: 0.47490042448043823\n",
      "Iteration 9, Batch: 48, Loss: 0.5548214316368103\n",
      "Iteration 9, Batch: 49, Loss: 0.4997599422931671\n",
      "Avg loss: 0.5038110488653182\n",
      "Iteration 10, Batch: 0, Loss: 0.5247232913970947\n",
      "Iteration 10, Batch: 1, Loss: 0.5347741842269897\n",
      "Iteration 10, Batch: 2, Loss: 0.49479198455810547\n",
      "Iteration 10, Batch: 3, Loss: 0.49499937891960144\n",
      "Iteration 10, Batch: 4, Loss: 0.5797500610351562\n",
      "Iteration 10, Batch: 5, Loss: 0.5298725962638855\n",
      "Iteration 10, Batch: 6, Loss: 0.4498220980167389\n",
      "Iteration 10, Batch: 7, Loss: 0.4648668169975281\n",
      "Iteration 10, Batch: 8, Loss: 0.5098624229431152\n",
      "Iteration 10, Batch: 9, Loss: 0.5198462605476379\n",
      "Iteration 10, Batch: 10, Loss: 0.4748581647872925\n",
      "Iteration 10, Batch: 11, Loss: 0.4949333071708679\n",
      "Iteration 10, Batch: 12, Loss: 0.474882572889328\n",
      "Iteration 10, Batch: 13, Loss: 0.5497905611991882\n",
      "Iteration 10, Batch: 14, Loss: 0.5747830867767334\n",
      "Iteration 10, Batch: 15, Loss: 0.5298172235488892\n",
      "Iteration 10, Batch: 16, Loss: 0.4348427355289459\n",
      "Iteration 10, Batch: 17, Loss: 0.47482526302337646\n",
      "Iteration 10, Batch: 18, Loss: 0.5247467160224915\n",
      "Iteration 10, Batch: 19, Loss: 0.4997195303440094\n",
      "Iteration 10, Batch: 20, Loss: 0.5098686814308167\n",
      "Iteration 10, Batch: 21, Loss: 0.5348044633865356\n",
      "Iteration 10, Batch: 22, Loss: 0.5498355031013489\n",
      "Iteration 10, Batch: 23, Loss: 0.5347514152526855\n",
      "Iteration 10, Batch: 24, Loss: 0.5198357701301575\n",
      "Iteration 10, Batch: 25, Loss: 0.5049425959587097\n",
      "Iteration 10, Batch: 26, Loss: 0.5097471475601196\n",
      "Iteration 10, Batch: 27, Loss: 0.5446833968162537\n",
      "Iteration 10, Batch: 28, Loss: 0.48989465832710266\n",
      "Iteration 10, Batch: 29, Loss: 0.5148577690124512\n",
      "Iteration 10, Batch: 30, Loss: 0.43993261456489563\n",
      "Iteration 10, Batch: 31, Loss: 0.43979933857917786\n",
      "Iteration 10, Batch: 32, Loss: 0.5148655772209167\n",
      "Iteration 10, Batch: 33, Loss: 0.4748319089412689\n",
      "Iteration 10, Batch: 34, Loss: 0.47484126687049866\n",
      "Iteration 10, Batch: 35, Loss: 0.5297813415527344\n",
      "Iteration 10, Batch: 36, Loss: 0.4398876130580902\n",
      "Iteration 10, Batch: 37, Loss: 0.4549006521701813\n",
      "Iteration 10, Batch: 38, Loss: 0.5197119116783142\n",
      "Iteration 10, Batch: 39, Loss: 0.5097169876098633\n",
      "Iteration 10, Batch: 40, Loss: 0.46480241417884827\n",
      "Iteration 10, Batch: 41, Loss: 0.47482970356941223\n",
      "Iteration 10, Batch: 42, Loss: 0.49479520320892334\n",
      "Iteration 10, Batch: 43, Loss: 0.5298100709915161\n",
      "Iteration 10, Batch: 44, Loss: 0.5097523331642151\n",
      "Iteration 10, Batch: 45, Loss: 0.494841992855072\n",
      "Iteration 10, Batch: 46, Loss: 0.4748474061489105\n",
      "Iteration 10, Batch: 47, Loss: 0.4848203957080841\n",
      "Iteration 10, Batch: 48, Loss: 0.5847747921943665\n",
      "Iteration 10, Batch: 49, Loss: 0.5298311114311218\n",
      "Avg loss: 0.5038220858573914\n",
      "Iteration 11, Batch: 0, Loss: 0.5398173928260803\n",
      "Iteration 11, Batch: 1, Loss: 0.4848192036151886\n",
      "Iteration 11, Batch: 2, Loss: 0.48987215757369995\n",
      "Iteration 11, Batch: 3, Loss: 0.5348752737045288\n",
      "Iteration 11, Batch: 4, Loss: 0.5198095440864563\n",
      "Iteration 11, Batch: 5, Loss: 0.49484774470329285\n",
      "Iteration 11, Batch: 6, Loss: 0.47493159770965576\n",
      "Iteration 11, Batch: 7, Loss: 0.5347896814346313\n",
      "Iteration 11, Batch: 8, Loss: 0.4797283113002777\n",
      "Iteration 11, Batch: 9, Loss: 0.494933158159256\n",
      "Iteration 11, Batch: 10, Loss: 0.4698171019554138\n",
      "Iteration 11, Batch: 11, Loss: 0.4298616349697113\n",
      "Iteration 11, Batch: 12, Loss: 0.46483099460601807\n",
      "Iteration 11, Batch: 13, Loss: 0.44485172629356384\n",
      "Iteration 11, Batch: 14, Loss: 0.5098612308502197\n",
      "Iteration 11, Batch: 15, Loss: 0.5148400664329529\n",
      "Iteration 11, Batch: 16, Loss: 0.5495935082435608\n",
      "Iteration 11, Batch: 17, Loss: 0.4998532831668854\n",
      "Iteration 11, Batch: 18, Loss: 0.5448402166366577\n",
      "Iteration 11, Batch: 19, Loss: 0.499728798866272\n",
      "Iteration 11, Batch: 20, Loss: 0.4848191738128662\n",
      "Iteration 11, Batch: 21, Loss: 0.5348951816558838\n",
      "Iteration 11, Batch: 22, Loss: 0.4698275327682495\n",
      "Iteration 11, Batch: 23, Loss: 0.49977827072143555\n",
      "Iteration 11, Batch: 24, Loss: 0.4549059271812439\n",
      "Iteration 11, Batch: 25, Loss: 0.5598198771476746\n",
      "Iteration 11, Batch: 26, Loss: 0.5398796796798706\n",
      "Iteration 11, Batch: 27, Loss: 0.5248185396194458\n",
      "Iteration 11, Batch: 28, Loss: 0.5198423862457275\n",
      "Iteration 11, Batch: 29, Loss: 0.4997553527355194\n",
      "Iteration 11, Batch: 30, Loss: 0.5148345232009888\n",
      "Iteration 11, Batch: 31, Loss: 0.5446333289146423\n",
      "Iteration 11, Batch: 32, Loss: 0.48488402366638184\n",
      "Iteration 11, Batch: 33, Loss: 0.5297679305076599\n",
      "Iteration 11, Batch: 34, Loss: 0.5348811149597168\n",
      "Iteration 11, Batch: 35, Loss: 0.5447893142700195\n",
      "Iteration 11, Batch: 36, Loss: 0.5347367525100708\n",
      "Iteration 11, Batch: 37, Loss: 0.4747738540172577\n",
      "Iteration 11, Batch: 38, Loss: 0.554709792137146\n",
      "Iteration 11, Batch: 39, Loss: 0.4548746347427368\n",
      "Iteration 11, Batch: 40, Loss: 0.5399142503738403\n",
      "Iteration 11, Batch: 41, Loss: 0.4949721693992615\n",
      "Iteration 11, Batch: 42, Loss: 0.5198654532432556\n",
      "Iteration 11, Batch: 43, Loss: 0.42489898204803467\n",
      "Iteration 11, Batch: 44, Loss: 0.5098633170127869\n",
      "Iteration 11, Batch: 45, Loss: 0.5248041152954102\n",
      "Iteration 11, Batch: 46, Loss: 0.5047786235809326\n",
      "Iteration 11, Batch: 47, Loss: 0.4947715997695923\n",
      "Iteration 11, Batch: 48, Loss: 0.4598686397075653\n",
      "Iteration 11, Batch: 49, Loss: 0.47983333468437195\n",
      "Avg loss: 0.5038260060548783\n",
      "Iteration 12, Batch: 0, Loss: 0.4698094129562378\n",
      "Iteration 12, Batch: 1, Loss: 0.5247471332550049\n",
      "Iteration 12, Batch: 2, Loss: 0.4997614324092865\n",
      "Iteration 12, Batch: 3, Loss: 0.5547469258308411\n",
      "Iteration 12, Batch: 4, Loss: 0.5197830200195312\n",
      "Iteration 12, Batch: 5, Loss: 0.4298107922077179\n",
      "Iteration 12, Batch: 6, Loss: 0.49984651803970337\n",
      "Iteration 12, Batch: 7, Loss: 0.514843761920929\n",
      "Iteration 12, Batch: 8, Loss: 0.5746540427207947\n",
      "Iteration 12, Batch: 9, Loss: 0.5548712015151978\n",
      "Iteration 12, Batch: 10, Loss: 0.4798326790332794\n",
      "Iteration 12, Batch: 11, Loss: 0.4148521423339844\n",
      "Iteration 12, Batch: 12, Loss: 0.5098482966423035\n",
      "Iteration 12, Batch: 13, Loss: 0.5096632838249207\n",
      "Iteration 12, Batch: 14, Loss: 0.46983465552330017\n",
      "Iteration 12, Batch: 15, Loss: 0.5047191977500916\n",
      "Iteration 12, Batch: 16, Loss: 0.5097972750663757\n",
      "Iteration 12, Batch: 17, Loss: 0.4998023808002472\n",
      "Iteration 12, Batch: 18, Loss: 0.4447331130504608\n",
      "Iteration 12, Batch: 19, Loss: 0.4945726692676544\n",
      "Iteration 12, Batch: 20, Loss: 0.5347493290901184\n",
      "Iteration 12, Batch: 21, Loss: 0.5246071219444275\n",
      "Iteration 12, Batch: 22, Loss: 0.5397207736968994\n",
      "Iteration 12, Batch: 23, Loss: 0.47489455342292786\n",
      "Iteration 12, Batch: 24, Loss: 0.5047532320022583\n",
      "Iteration 12, Batch: 25, Loss: 0.5496349930763245\n",
      "Iteration 12, Batch: 26, Loss: 0.5147722363471985\n",
      "Iteration 12, Batch: 27, Loss: 0.4597429931163788\n",
      "Iteration 12, Batch: 28, Loss: 0.5248603820800781\n",
      "Iteration 12, Batch: 29, Loss: 0.5497112274169922\n",
      "Iteration 12, Batch: 30, Loss: 0.5397192239761353\n",
      "Iteration 12, Batch: 31, Loss: 0.48987463116645813\n",
      "Iteration 12, Batch: 32, Loss: 0.4496942162513733\n",
      "Iteration 12, Batch: 33, Loss: 0.5197185277938843\n",
      "Iteration 12, Batch: 34, Loss: 0.5247546434402466\n",
      "Iteration 12, Batch: 35, Loss: 0.524694561958313\n",
      "Iteration 12, Batch: 36, Loss: 0.45465362071990967\n",
      "Iteration 12, Batch: 37, Loss: 0.5296304821968079\n",
      "Iteration 12, Batch: 38, Loss: 0.47968390583992004\n",
      "Iteration 12, Batch: 39, Loss: 0.4943227171897888\n",
      "Iteration 12, Batch: 40, Loss: 0.5545368194580078\n",
      "Iteration 12, Batch: 41, Loss: 0.4800901710987091\n",
      "Iteration 12, Batch: 42, Loss: 0.4345651865005493\n",
      "Iteration 12, Batch: 43, Loss: 0.5495234727859497\n",
      "Iteration 12, Batch: 44, Loss: 0.5094736218452454\n",
      "Iteration 12, Batch: 45, Loss: 0.5143700242042542\n",
      "Iteration 12, Batch: 46, Loss: 0.499274879693985\n",
      "Iteration 12, Batch: 47, Loss: 0.4695010185241699\n",
      "Iteration 12, Batch: 48, Loss: 0.4498593807220459\n",
      "Iteration 12, Batch: 49, Loss: 0.5588839054107666\n",
      "Avg loss: 0.5036960357427597\n",
      "Iteration 13, Batch: 0, Loss: 0.529431164264679\n",
      "Iteration 13, Batch: 1, Loss: 0.4947286546230316\n",
      "Iteration 13, Batch: 2, Loss: 0.4741727411746979\n",
      "Iteration 13, Batch: 3, Loss: 0.5196564197540283\n",
      "Iteration 13, Batch: 4, Loss: 0.5089640021324158\n",
      "Iteration 13, Batch: 5, Loss: 0.4893821179866791\n",
      "Iteration 13, Batch: 6, Loss: 0.5049198865890503\n",
      "Iteration 13, Batch: 7, Loss: 0.5189017653465271\n",
      "Iteration 13, Batch: 8, Loss: 0.5299423336982727\n",
      "Iteration 13, Batch: 9, Loss: 0.5144902467727661\n",
      "Iteration 13, Batch: 10, Loss: 0.5485543608665466\n",
      "Iteration 13, Batch: 11, Loss: 0.5145413279533386\n",
      "Iteration 13, Batch: 12, Loss: 0.50368732213974\n",
      "Iteration 13, Batch: 13, Loss: 0.45501193404197693\n",
      "Iteration 13, Batch: 14, Loss: 0.4793507158756256\n",
      "Iteration 13, Batch: 15, Loss: 0.4896286725997925\n",
      "Iteration 13, Batch: 16, Loss: 0.46938827633857727\n",
      "Iteration 13, Batch: 17, Loss: 0.4742456376552582\n",
      "Iteration 13, Batch: 18, Loss: 0.4994955360889435\n",
      "Iteration 13, Batch: 19, Loss: 0.5097493529319763\n",
      "Iteration 13, Batch: 20, Loss: 0.5091280937194824\n",
      "Iteration 13, Batch: 21, Loss: 0.4692981243133545\n",
      "Iteration 13, Batch: 22, Loss: 0.524461567401886\n",
      "Iteration 13, Batch: 23, Loss: 0.5198549032211304\n",
      "Iteration 13, Batch: 24, Loss: 0.5496453046798706\n",
      "Iteration 13, Batch: 25, Loss: 0.5796394348144531\n",
      "Iteration 13, Batch: 26, Loss: 0.45957159996032715\n",
      "Iteration 13, Batch: 27, Loss: 0.4843989312648773\n",
      "Iteration 13, Batch: 28, Loss: 0.4939925968647003\n",
      "Iteration 13, Batch: 29, Loss: 0.5792365074157715\n",
      "Iteration 13, Batch: 30, Loss: 0.48414725065231323\n",
      "Iteration 13, Batch: 31, Loss: 0.5136510133743286\n",
      "Iteration 13, Batch: 32, Loss: 0.5445960164070129\n",
      "Iteration 13, Batch: 33, Loss: 0.5150851011276245\n",
      "Iteration 13, Batch: 34, Loss: 0.4699220657348633\n",
      "Iteration 13, Batch: 35, Loss: 0.5140541195869446\n",
      "Iteration 13, Batch: 36, Loss: 0.45395734906196594\n",
      "Iteration 13, Batch: 37, Loss: 0.49481281638145447\n",
      "Iteration 13, Batch: 38, Loss: 0.5347575545310974\n",
      "Iteration 13, Batch: 39, Loss: 0.4945756793022156\n",
      "Iteration 13, Batch: 40, Loss: 0.5095582604408264\n",
      "Iteration 13, Batch: 41, Loss: 0.48940131068229675\n",
      "Iteration 13, Batch: 42, Loss: 0.47441473603248596\n",
      "Iteration 13, Batch: 43, Loss: 0.509608805179596\n",
      "Iteration 13, Batch: 44, Loss: 0.4349444508552551\n",
      "Iteration 13, Batch: 45, Loss: 0.529495120048523\n",
      "Iteration 13, Batch: 46, Loss: 0.5041490793228149\n",
      "Iteration 13, Batch: 47, Loss: 0.5197147727012634\n",
      "Iteration 13, Batch: 48, Loss: 0.46477827429771423\n",
      "Iteration 13, Batch: 49, Loss: 0.5194891095161438\n",
      "Avg loss: 0.5034516483545304\n",
      "Iteration 14, Batch: 0, Loss: 0.5444455742835999\n",
      "Iteration 14, Batch: 1, Loss: 0.4844925105571747\n",
      "Iteration 14, Batch: 2, Loss: 0.43463727831840515\n",
      "Iteration 14, Batch: 3, Loss: 0.6094682812690735\n",
      "Iteration 14, Batch: 4, Loss: 0.509289562702179\n",
      "Iteration 14, Batch: 5, Loss: 0.5247543454170227\n",
      "Iteration 14, Batch: 6, Loss: 0.5394626259803772\n",
      "Iteration 14, Batch: 7, Loss: 0.47973334789276123\n",
      "Iteration 14, Batch: 8, Loss: 0.49987348914146423\n",
      "Iteration 14, Batch: 9, Loss: 0.5192632079124451\n",
      "Iteration 14, Batch: 10, Loss: 0.48928308486938477\n",
      "Iteration 14, Batch: 11, Loss: 0.4944644570350647\n",
      "Iteration 14, Batch: 12, Loss: 0.5596884489059448\n",
      "Iteration 14, Batch: 13, Loss: 0.499773770570755\n",
      "Iteration 14, Batch: 14, Loss: 0.49965521693229675\n",
      "Iteration 14, Batch: 15, Loss: 0.4293064773082733\n",
      "Iteration 14, Batch: 16, Loss: 0.5296707153320312\n",
      "Iteration 14, Batch: 17, Loss: 0.5392778515815735\n",
      "Iteration 14, Batch: 18, Loss: 0.5342448949813843\n",
      "Iteration 14, Batch: 19, Loss: 0.4989296495914459\n",
      "Iteration 14, Batch: 20, Loss: 0.4743640720844269\n",
      "Iteration 14, Batch: 21, Loss: 0.4594654440879822\n",
      "Iteration 14, Batch: 22, Loss: 0.43479830026626587\n",
      "Iteration 14, Batch: 23, Loss: 0.5146267414093018\n",
      "Iteration 14, Batch: 24, Loss: 0.48439696431159973\n",
      "Iteration 14, Batch: 25, Loss: 0.5197532773017883\n",
      "Iteration 14, Batch: 26, Loss: 0.4396897554397583\n",
      "Iteration 14, Batch: 27, Loss: 0.5843407511711121\n",
      "Iteration 14, Batch: 28, Loss: 0.5545376539230347\n",
      "Iteration 14, Batch: 29, Loss: 0.5039823055267334\n",
      "Iteration 14, Batch: 30, Loss: 0.49961623549461365\n",
      "Iteration 14, Batch: 31, Loss: 0.4837081730365753\n",
      "Iteration 14, Batch: 32, Loss: 0.4746055603027344\n",
      "Iteration 14, Batch: 33, Loss: 0.5295306444168091\n",
      "Iteration 14, Batch: 34, Loss: 0.5143539309501648\n",
      "Iteration 14, Batch: 35, Loss: 0.5391961336135864\n",
      "Iteration 14, Batch: 36, Loss: 0.49934789538383484\n",
      "Iteration 14, Batch: 37, Loss: 0.5147050023078918\n",
      "Iteration 14, Batch: 38, Loss: 0.5689877271652222\n",
      "Iteration 14, Batch: 39, Loss: 0.5744399428367615\n",
      "Iteration 14, Batch: 40, Loss: 0.49988263845443726\n",
      "Iteration 14, Batch: 41, Loss: 0.4348309636116028\n",
      "Iteration 14, Batch: 42, Loss: 0.489437997341156\n",
      "Iteration 14, Batch: 43, Loss: 0.4693628251552582\n",
      "Iteration 14, Batch: 44, Loss: 0.45929381251335144\n",
      "Iteration 14, Batch: 45, Loss: 0.5495566725730896\n",
      "Iteration 14, Batch: 46, Loss: 0.5342039465904236\n",
      "Iteration 14, Batch: 47, Loss: 0.46488556265830994\n",
      "Iteration 14, Batch: 48, Loss: 0.4498424828052521\n",
      "Iteration 14, Batch: 49, Loss: 0.43490347266197205\n",
      "Avg loss: 0.5034872335195542\n",
      "Iteration 15, Batch: 0, Loss: 0.554603099822998\n",
      "Iteration 15, Batch: 1, Loss: 0.4992547035217285\n",
      "Iteration 15, Batch: 2, Loss: 0.47447195649147034\n",
      "Iteration 15, Batch: 3, Loss: 0.5294352769851685\n",
      "Iteration 15, Batch: 4, Loss: 0.4693528115749359\n",
      "Iteration 15, Batch: 5, Loss: 0.5634377002716064\n",
      "Iteration 15, Batch: 6, Loss: 0.4995955526828766\n",
      "Iteration 15, Batch: 7, Loss: 0.5092930793762207\n",
      "Iteration 15, Batch: 8, Loss: 0.5141198039054871\n",
      "Iteration 15, Batch: 9, Loss: 0.47924941778182983\n",
      "Iteration 15, Batch: 10, Loss: 0.5095106363296509\n",
      "Iteration 15, Batch: 11, Loss: 0.554637610912323\n",
      "Iteration 15, Batch: 12, Loss: 0.48395124077796936\n",
      "Iteration 15, Batch: 13, Loss: 0.47434622049331665\n",
      "Iteration 15, Batch: 14, Loss: 0.5345363020896912\n",
      "Iteration 15, Batch: 15, Loss: 0.5292580127716064\n",
      "Iteration 15, Batch: 16, Loss: 0.48441535234451294\n",
      "Iteration 15, Batch: 17, Loss: 0.5046277642250061\n",
      "Iteration 15, Batch: 18, Loss: 0.5196762084960938\n",
      "Iteration 15, Batch: 19, Loss: 0.5244981050491333\n",
      "Iteration 15, Batch: 20, Loss: 0.4644901156425476\n",
      "Iteration 15, Batch: 21, Loss: 0.45953047275543213\n",
      "Iteration 15, Batch: 22, Loss: 0.494445264339447\n",
      "Iteration 15, Batch: 23, Loss: 0.49422797560691833\n",
      "Iteration 15, Batch: 24, Loss: 0.5091947913169861\n",
      "Iteration 15, Batch: 25, Loss: 0.5245359539985657\n",
      "Iteration 15, Batch: 26, Loss: 0.4894641041755676\n",
      "Iteration 15, Batch: 27, Loss: 0.5094434022903442\n",
      "Iteration 15, Batch: 28, Loss: 0.5045874118804932\n",
      "Iteration 15, Batch: 29, Loss: 0.4994121491909027\n",
      "Iteration 15, Batch: 30, Loss: 0.4789411723613739\n",
      "Iteration 15, Batch: 31, Loss: 0.5091536045074463\n",
      "Iteration 15, Batch: 32, Loss: 0.5094481110572815\n",
      "Iteration 15, Batch: 33, Loss: 0.4946357309818268\n",
      "Iteration 15, Batch: 34, Loss: 0.48484882712364197\n",
      "Iteration 15, Batch: 35, Loss: 0.5197446942329407\n",
      "Iteration 15, Batch: 36, Loss: 0.5144177079200745\n",
      "Iteration 15, Batch: 37, Loss: 0.5295574069023132\n",
      "Iteration 15, Batch: 38, Loss: 0.5143982768058777\n",
      "Iteration 15, Batch: 39, Loss: 0.5247148275375366\n",
      "Iteration 15, Batch: 40, Loss: 0.49947842955589294\n",
      "Iteration 15, Batch: 41, Loss: 0.45977094769477844\n",
      "Iteration 15, Batch: 42, Loss: 0.45964208245277405\n",
      "Iteration 15, Batch: 43, Loss: 0.4647761881351471\n",
      "Iteration 15, Batch: 44, Loss: 0.459610253572464\n",
      "Iteration 15, Batch: 45, Loss: 0.4948194921016693\n",
      "Iteration 15, Batch: 46, Loss: 0.49918004870414734\n",
      "Iteration 15, Batch: 47, Loss: 0.5193958878517151\n",
      "Iteration 15, Batch: 48, Loss: 0.5598219037055969\n",
      "Iteration 15, Batch: 49, Loss: 0.5146914720535278\n",
      "Avg loss: 0.5034529912471771\n",
      "Iteration 16, Batch: 0, Loss: 0.49984750151634216\n",
      "Iteration 16, Batch: 1, Loss: 0.5089756846427917\n",
      "Iteration 16, Batch: 2, Loss: 0.47969648241996765\n",
      "Iteration 16, Batch: 3, Loss: 0.5294982194900513\n",
      "Iteration 16, Batch: 4, Loss: 0.47935596108436584\n",
      "Iteration 16, Batch: 5, Loss: 0.519416093826294\n",
      "Iteration 16, Batch: 6, Loss: 0.46442747116088867\n",
      "Iteration 16, Batch: 7, Loss: 0.5748623013496399\n",
      "Iteration 16, Batch: 8, Loss: 0.4996373653411865\n",
      "Iteration 16, Batch: 9, Loss: 0.5194973349571228\n",
      "Iteration 16, Batch: 10, Loss: 0.5793637633323669\n",
      "Iteration 16, Batch: 11, Loss: 0.5539718270301819\n",
      "Iteration 16, Batch: 12, Loss: 0.5291366577148438\n",
      "Iteration 16, Batch: 13, Loss: 0.4790751039981842\n",
      "Iteration 16, Batch: 14, Loss: 0.5097105503082275\n",
      "Iteration 16, Batch: 15, Loss: 0.5388390421867371\n",
      "Iteration 16, Batch: 16, Loss: 0.4996340870857239\n",
      "Iteration 16, Batch: 17, Loss: 0.4687495827674866\n",
      "Iteration 16, Batch: 18, Loss: 0.5143648386001587\n",
      "Iteration 16, Batch: 19, Loss: 0.5241029858589172\n",
      "Iteration 16, Batch: 20, Loss: 0.5740311145782471\n",
      "Iteration 16, Batch: 21, Loss: 0.5341930389404297\n",
      "Iteration 16, Batch: 22, Loss: 0.49450406432151794\n",
      "Iteration 16, Batch: 23, Loss: 0.459268182516098\n",
      "Iteration 16, Batch: 24, Loss: 0.5288252830505371\n",
      "Iteration 16, Batch: 25, Loss: 0.5287238955497742\n",
      "Iteration 16, Batch: 26, Loss: 0.5142703056335449\n",
      "Iteration 16, Batch: 27, Loss: 0.5495736598968506\n",
      "Iteration 16, Batch: 28, Loss: 0.5198748707771301\n",
      "Iteration 16, Batch: 29, Loss: 0.46920695900917053\n",
      "Iteration 16, Batch: 30, Loss: 0.444821298122406\n",
      "Iteration 16, Batch: 31, Loss: 0.49945905804634094\n",
      "Iteration 16, Batch: 32, Loss: 0.5092495679855347\n",
      "Iteration 16, Batch: 33, Loss: 0.4945729672908783\n",
      "Iteration 16, Batch: 34, Loss: 0.4946502447128296\n",
      "Iteration 16, Batch: 35, Loss: 0.46435561776161194\n",
      "Iteration 16, Batch: 36, Loss: 0.4889194369316101\n",
      "Iteration 16, Batch: 37, Loss: 0.5444937944412231\n",
      "Iteration 16, Batch: 38, Loss: 0.44966116547584534\n",
      "Iteration 16, Batch: 39, Loss: 0.5390574336051941\n",
      "Iteration 16, Batch: 40, Loss: 0.47967344522476196\n",
      "Iteration 16, Batch: 41, Loss: 0.47440633177757263\n",
      "Iteration 16, Batch: 42, Loss: 0.4591432511806488\n",
      "Iteration 16, Batch: 43, Loss: 0.4341577887535095\n",
      "Iteration 16, Batch: 44, Loss: 0.48434290289878845\n",
      "Iteration 16, Batch: 45, Loss: 0.48968183994293213\n",
      "Iteration 16, Batch: 46, Loss: 0.5043079853057861\n",
      "Iteration 16, Batch: 47, Loss: 0.5392606258392334\n",
      "Iteration 16, Batch: 48, Loss: 0.45339712500572205\n",
      "Iteration 16, Batch: 49, Loss: 0.4745962917804718\n",
      "Avg loss: 0.5033368480205536\n",
      "Iteration 17, Batch: 0, Loss: 0.572861909866333\n",
      "Iteration 17, Batch: 1, Loss: 0.5827147364616394\n",
      "Iteration 17, Batch: 2, Loss: 0.5304290056228638\n",
      "Iteration 17, Batch: 3, Loss: 0.5037326216697693\n",
      "Iteration 17, Batch: 4, Loss: 0.47890812158584595\n",
      "Iteration 17, Batch: 5, Loss: 0.5093154311180115\n",
      "Iteration 17, Batch: 6, Loss: 0.48843738436698914\n",
      "Iteration 17, Batch: 7, Loss: 0.5387558341026306\n",
      "Iteration 17, Batch: 8, Loss: 0.5194434523582458\n",
      "Iteration 17, Batch: 9, Loss: 0.4434141516685486\n",
      "Iteration 17, Batch: 10, Loss: 0.42946726083755493\n",
      "Iteration 17, Batch: 11, Loss: 0.47469016909599304\n",
      "Iteration 17, Batch: 12, Loss: 0.48875394463539124\n",
      "Iteration 17, Batch: 13, Loss: 0.43398597836494446\n",
      "Iteration 17, Batch: 14, Loss: 0.5874934196472168\n",
      "Iteration 17, Batch: 15, Loss: 0.4740409851074219\n",
      "Iteration 17, Batch: 16, Loss: 0.5081311464309692\n",
      "Iteration 17, Batch: 17, Loss: 0.5090920925140381\n",
      "Iteration 17, Batch: 18, Loss: 0.48151087760925293\n",
      "Iteration 17, Batch: 19, Loss: 0.46534299850463867\n",
      "Iteration 17, Batch: 20, Loss: 0.518881618976593\n",
      "Iteration 17, Batch: 21, Loss: 0.488992840051651\n",
      "Iteration 17, Batch: 22, Loss: 0.45875975489616394\n",
      "Iteration 17, Batch: 23, Loss: 0.47419363260269165\n",
      "Iteration 17, Batch: 24, Loss: 0.4543282985687256\n",
      "Iteration 17, Batch: 25, Loss: 0.6083650588989258\n",
      "Iteration 17, Batch: 26, Loss: 0.4689783453941345\n",
      "Iteration 17, Batch: 27, Loss: 0.4890408217906952\n",
      "Iteration 17, Batch: 28, Loss: 0.4478187561035156\n",
      "Iteration 17, Batch: 29, Loss: 0.44898155331611633\n",
      "Iteration 17, Batch: 30, Loss: 0.5039608478546143\n",
      "Iteration 17, Batch: 31, Loss: 0.4839976727962494\n",
      "Iteration 17, Batch: 32, Loss: 0.4775611460208893\n",
      "Iteration 17, Batch: 33, Loss: 0.4843122661113739\n",
      "Iteration 17, Batch: 34, Loss: 0.4741538166999817\n",
      "Iteration 17, Batch: 35, Loss: 0.5283862352371216\n",
      "Iteration 17, Batch: 36, Loss: 0.49415549635887146\n",
      "Iteration 17, Batch: 37, Loss: 0.5496110320091248\n",
      "Iteration 17, Batch: 38, Loss: 0.5088384747505188\n",
      "Iteration 17, Batch: 39, Loss: 0.5432060360908508\n",
      "Iteration 17, Batch: 40, Loss: 0.5380491614341736\n",
      "Iteration 17, Batch: 41, Loss: 0.5537449717521667\n",
      "Iteration 17, Batch: 42, Loss: 0.5434804558753967\n",
      "Iteration 17, Batch: 43, Loss: 0.5480483174324036\n",
      "Iteration 17, Batch: 44, Loss: 0.4789518713951111\n",
      "Iteration 17, Batch: 45, Loss: 0.4590950012207031\n",
      "Iteration 17, Batch: 46, Loss: 0.5125157237052917\n",
      "Iteration 17, Batch: 47, Loss: 0.5042654275894165\n",
      "Iteration 17, Batch: 48, Loss: 0.5085052847862244\n",
      "Iteration 17, Batch: 49, Loss: 0.5624227523803711\n",
      "Avg loss: 0.5027224838733673\n",
      "Iteration 18, Batch: 0, Loss: 0.4873172640800476\n",
      "Iteration 18, Batch: 1, Loss: 0.5175012946128845\n",
      "Iteration 18, Batch: 2, Loss: 0.44452011585235596\n",
      "Iteration 18, Batch: 3, Loss: 0.45430082082748413\n",
      "Iteration 18, Batch: 4, Loss: 0.5285532474517822\n",
      "Iteration 18, Batch: 5, Loss: 0.5139535665512085\n",
      "Iteration 18, Batch: 6, Loss: 0.44411155581474304\n",
      "Iteration 18, Batch: 7, Loss: 0.4290400743484497\n",
      "Iteration 18, Batch: 8, Loss: 0.4988655745983124\n",
      "Iteration 18, Batch: 9, Loss: 0.5141590237617493\n",
      "Iteration 18, Batch: 10, Loss: 0.5291622281074524\n",
      "Iteration 18, Batch: 11, Loss: 0.528264582157135\n",
      "Iteration 18, Batch: 12, Loss: 0.5188312530517578\n",
      "Iteration 18, Batch: 13, Loss: 0.5187613368034363\n",
      "Iteration 18, Batch: 14, Loss: 0.4686570167541504\n",
      "Iteration 18, Batch: 15, Loss: 0.5147863626480103\n",
      "Iteration 18, Batch: 16, Loss: 0.5031642317771912\n",
      "Iteration 18, Batch: 17, Loss: 0.5090753436088562\n",
      "Iteration 18, Batch: 18, Loss: 0.5444772839546204\n",
      "Iteration 18, Batch: 19, Loss: 0.5172380805015564\n",
      "Iteration 18, Batch: 20, Loss: 0.4740068018436432\n",
      "Iteration 18, Batch: 21, Loss: 0.5237259268760681\n",
      "Iteration 18, Batch: 22, Loss: 0.518419623374939\n",
      "Iteration 18, Batch: 23, Loss: 0.48361361026763916\n",
      "Iteration 18, Batch: 24, Loss: 0.5099266171455383\n",
      "Iteration 18, Batch: 25, Loss: 0.5280967354774475\n",
      "Iteration 18, Batch: 26, Loss: 0.5391425490379333\n",
      "Iteration 18, Batch: 27, Loss: 0.5485023260116577\n",
      "Iteration 18, Batch: 28, Loss: 0.5692646503448486\n",
      "Iteration 18, Batch: 29, Loss: 0.5040444731712341\n",
      "Iteration 18, Batch: 30, Loss: 0.46844038367271423\n",
      "Iteration 18, Batch: 31, Loss: 0.4735194742679596\n",
      "Iteration 18, Batch: 32, Loss: 0.5189738869667053\n",
      "Iteration 18, Batch: 33, Loss: 0.5242958068847656\n",
      "Iteration 18, Batch: 34, Loss: 0.5478950142860413\n",
      "Iteration 18, Batch: 35, Loss: 0.497951865196228\n",
      "Iteration 18, Batch: 36, Loss: 0.4981158673763275\n",
      "Iteration 18, Batch: 37, Loss: 0.5386751294136047\n",
      "Iteration 18, Batch: 38, Loss: 0.5265831351280212\n",
      "Iteration 18, Batch: 39, Loss: 0.4918522834777832\n",
      "Iteration 18, Batch: 40, Loss: 0.5127150416374207\n",
      "Iteration 18, Batch: 41, Loss: 0.47567084431648254\n",
      "Iteration 18, Batch: 42, Loss: 0.4990045428276062\n",
      "Iteration 18, Batch: 43, Loss: 0.49312278628349304\n",
      "Iteration 18, Batch: 44, Loss: 0.5225317478179932\n",
      "Iteration 18, Batch: 45, Loss: 0.438992977142334\n",
      "Iteration 18, Batch: 46, Loss: 0.5092259645462036\n",
      "Iteration 18, Batch: 47, Loss: 0.3990972340106964\n",
      "Iteration 18, Batch: 48, Loss: 0.5045654773712158\n",
      "Iteration 18, Batch: 49, Loss: 0.5083922743797302\n",
      "Avg loss: 0.5026621061563492\n",
      "Iteration 19, Batch: 0, Loss: 0.5336909294128418\n",
      "Iteration 19, Batch: 1, Loss: 0.5134245753288269\n",
      "Iteration 19, Batch: 2, Loss: 0.5190004706382751\n",
      "Iteration 19, Batch: 3, Loss: 0.498680055141449\n",
      "Iteration 19, Batch: 4, Loss: 0.5175457000732422\n",
      "Iteration 19, Batch: 5, Loss: 0.5079537034034729\n",
      "Iteration 19, Batch: 6, Loss: 0.43311285972595215\n",
      "Iteration 19, Batch: 7, Loss: 0.4945501685142517\n",
      "Iteration 19, Batch: 8, Loss: 0.4787903428077698\n",
      "Iteration 19, Batch: 9, Loss: 0.5376089811325073\n",
      "Iteration 19, Batch: 10, Loss: 0.5031061172485352\n",
      "Iteration 19, Batch: 11, Loss: 0.4680078327655792\n",
      "Iteration 19, Batch: 12, Loss: 0.5290046334266663\n",
      "Iteration 19, Batch: 13, Loss: 0.5226992964744568\n",
      "Iteration 19, Batch: 14, Loss: 0.5926990509033203\n",
      "Iteration 19, Batch: 15, Loss: 0.47683218121528625\n",
      "Iteration 19, Batch: 16, Loss: 0.4630047380924225\n",
      "Iteration 19, Batch: 17, Loss: 0.44458433985710144\n",
      "Iteration 19, Batch: 18, Loss: 0.45433539152145386\n",
      "Iteration 19, Batch: 19, Loss: 0.5342655181884766\n",
      "Iteration 19, Batch: 20, Loss: 0.45854395627975464\n",
      "Iteration 19, Batch: 21, Loss: 0.49783027172088623\n",
      "Iteration 19, Batch: 22, Loss: 0.5426353216171265\n",
      "Iteration 19, Batch: 23, Loss: 0.5532196760177612\n",
      "Iteration 19, Batch: 24, Loss: 0.4972643256187439\n",
      "Iteration 19, Batch: 25, Loss: 0.5043375492095947\n",
      "Iteration 19, Batch: 26, Loss: 0.4738701283931732\n",
      "Iteration 19, Batch: 27, Loss: 0.5438472628593445\n",
      "Iteration 19, Batch: 28, Loss: 0.5223555564880371\n",
      "Iteration 19, Batch: 29, Loss: 0.46357735991477966\n",
      "Iteration 19, Batch: 30, Loss: 0.5349745154380798\n",
      "Iteration 19, Batch: 31, Loss: 0.5035825371742249\n",
      "Iteration 19, Batch: 32, Loss: 0.5148813128471375\n",
      "Iteration 19, Batch: 33, Loss: 0.46315279603004456\n",
      "Iteration 19, Batch: 34, Loss: 0.4945899546146393\n",
      "Iteration 19, Batch: 35, Loss: 0.508864164352417\n",
      "Iteration 19, Batch: 36, Loss: 0.443412721157074\n",
      "Iteration 19, Batch: 37, Loss: 0.5588773488998413\n",
      "Iteration 19, Batch: 38, Loss: 0.5130357146263123\n",
      "Iteration 19, Batch: 39, Loss: 0.4825476109981537\n",
      "Iteration 19, Batch: 40, Loss: 0.4729207754135132\n",
      "Iteration 19, Batch: 41, Loss: 0.5113710165023804\n",
      "Iteration 19, Batch: 42, Loss: 0.42165035009384155\n",
      "Iteration 19, Batch: 43, Loss: 0.4940743148326874\n",
      "Iteration 19, Batch: 44, Loss: 0.5388835668563843\n",
      "Iteration 19, Batch: 45, Loss: 0.5199079513549805\n",
      "Iteration 19, Batch: 46, Loss: 0.5824739933013916\n",
      "Iteration 19, Batch: 47, Loss: 0.4777282476425171\n",
      "Iteration 19, Batch: 48, Loss: 0.5475202202796936\n",
      "Iteration 19, Batch: 49, Loss: 0.4538556933403015\n",
      "Avg loss: 0.502373661994934\n",
      "Iteration 20, Batch: 0, Loss: 0.5337674021720886\n",
      "Iteration 20, Batch: 1, Loss: 0.48826003074645996\n",
      "Iteration 20, Batch: 2, Loss: 0.4893675148487091\n",
      "Iteration 20, Batch: 3, Loss: 0.5073492527008057\n",
      "Iteration 20, Batch: 4, Loss: 0.4831310212612152\n",
      "Iteration 20, Batch: 5, Loss: 0.46343547105789185\n",
      "Iteration 20, Batch: 6, Loss: 0.5087114572525024\n",
      "Iteration 20, Batch: 7, Loss: 0.5237070322036743\n",
      "Iteration 20, Batch: 8, Loss: 0.5105322003364563\n",
      "Iteration 20, Batch: 9, Loss: 0.49530431628227234\n",
      "Iteration 20, Batch: 10, Loss: 0.4929749667644501\n",
      "Iteration 20, Batch: 11, Loss: 0.5136681795120239\n",
      "Iteration 20, Batch: 12, Loss: 0.5080575346946716\n",
      "Iteration 20, Batch: 13, Loss: 0.448608934879303\n",
      "Iteration 20, Batch: 14, Loss: 0.533128559589386\n",
      "Iteration 20, Batch: 15, Loss: 0.5643874406814575\n",
      "Iteration 20, Batch: 16, Loss: 0.4724227786064148\n",
      "Iteration 20, Batch: 17, Loss: 0.5038082003593445\n",
      "Iteration 20, Batch: 18, Loss: 0.5045791268348694\n",
      "Iteration 20, Batch: 19, Loss: 0.48200082778930664\n",
      "Iteration 20, Batch: 20, Loss: 0.511154294013977\n",
      "Iteration 20, Batch: 21, Loss: 0.4742165505886078\n",
      "Iteration 20, Batch: 22, Loss: 0.4897301495075226\n",
      "Iteration 20, Batch: 23, Loss: 0.5214563012123108\n",
      "Iteration 20, Batch: 24, Loss: 0.5014545917510986\n",
      "Iteration 20, Batch: 25, Loss: 0.5391815900802612\n",
      "Iteration 20, Batch: 26, Loss: 0.41498759388923645\n",
      "Iteration 20, Batch: 27, Loss: 0.4831799864768982\n",
      "Iteration 20, Batch: 28, Loss: 0.5075331330299377\n",
      "Iteration 20, Batch: 29, Loss: 0.4995149075984955\n",
      "Iteration 20, Batch: 30, Loss: 0.5140999555587769\n",
      "Iteration 20, Batch: 31, Loss: 0.49220865964889526\n",
      "Iteration 20, Batch: 32, Loss: 0.5373382568359375\n",
      "Iteration 20, Batch: 33, Loss: 0.43815940618515015\n",
      "Iteration 20, Batch: 34, Loss: 0.47816917300224304\n",
      "Iteration 20, Batch: 35, Loss: 0.5333924889564514\n",
      "Iteration 20, Batch: 36, Loss: 0.4772772192955017\n",
      "Iteration 20, Batch: 37, Loss: 0.5807435512542725\n",
      "Iteration 20, Batch: 38, Loss: 0.5043899416923523\n",
      "Iteration 20, Batch: 39, Loss: 0.5440973043441772\n",
      "Iteration 20, Batch: 40, Loss: 0.48690202832221985\n",
      "Iteration 20, Batch: 41, Loss: 0.4590695798397064\n",
      "Iteration 20, Batch: 42, Loss: 0.4888629913330078\n",
      "Iteration 20, Batch: 43, Loss: 0.5478880405426025\n",
      "Iteration 20, Batch: 44, Loss: 0.5129541158676147\n",
      "Iteration 20, Batch: 45, Loss: 0.5038765668869019\n",
      "Iteration 20, Batch: 46, Loss: 0.5031375288963318\n",
      "Iteration 20, Batch: 47, Loss: 0.49378347396850586\n",
      "Iteration 20, Batch: 48, Loss: 0.5086977481842041\n",
      "Iteration 20, Batch: 49, Loss: 0.5285217761993408\n",
      "Avg loss: 0.5020636230707168\n",
      "Iteration 21, Batch: 0, Loss: 0.4831593334674835\n",
      "Iteration 21, Batch: 1, Loss: 0.4827788174152374\n",
      "Iteration 21, Batch: 2, Loss: 0.4991609752178192\n",
      "Iteration 21, Batch: 3, Loss: 0.5033311247825623\n",
      "Iteration 21, Batch: 4, Loss: 0.523230254650116\n",
      "Iteration 21, Batch: 5, Loss: 0.5133405327796936\n",
      "Iteration 21, Batch: 6, Loss: 0.4870671033859253\n",
      "Iteration 21, Batch: 7, Loss: 0.4515753388404846\n",
      "Iteration 21, Batch: 8, Loss: 0.4796438217163086\n",
      "Iteration 21, Batch: 9, Loss: 0.5017920732498169\n",
      "Iteration 21, Batch: 10, Loss: 0.46177756786346436\n",
      "Iteration 21, Batch: 11, Loss: 0.5717368721961975\n",
      "Iteration 21, Batch: 12, Loss: 0.5033354163169861\n",
      "Iteration 21, Batch: 13, Loss: 0.5022938251495361\n",
      "Iteration 21, Batch: 14, Loss: 0.5028814077377319\n",
      "Iteration 21, Batch: 15, Loss: 0.4884364604949951\n",
      "Iteration 21, Batch: 16, Loss: 0.5417351126670837\n",
      "Iteration 21, Batch: 17, Loss: 0.4868146479129791\n",
      "Iteration 21, Batch: 18, Loss: 0.5420560240745544\n",
      "Iteration 21, Batch: 19, Loss: 0.5029813051223755\n",
      "Iteration 21, Batch: 20, Loss: 0.46722063422203064\n",
      "Iteration 21, Batch: 21, Loss: 0.5218826532363892\n",
      "Iteration 21, Batch: 22, Loss: 0.4704807996749878\n",
      "Iteration 21, Batch: 23, Loss: 0.4776938557624817\n",
      "Iteration 21, Batch: 24, Loss: 0.4622182250022888\n",
      "Iteration 21, Batch: 25, Loss: 0.48121175169944763\n",
      "Iteration 21, Batch: 26, Loss: 0.5119851231575012\n",
      "Iteration 21, Batch: 27, Loss: 0.5462429523468018\n",
      "Iteration 21, Batch: 28, Loss: 0.4709933400154114\n",
      "Iteration 21, Batch: 29, Loss: 0.4875553548336029\n",
      "Iteration 21, Batch: 30, Loss: 0.5252418518066406\n",
      "Iteration 21, Batch: 31, Loss: 0.4400090277194977\n",
      "Iteration 21, Batch: 32, Loss: 0.49803629517555237\n",
      "Iteration 21, Batch: 33, Loss: 0.5358024835586548\n",
      "Iteration 21, Batch: 34, Loss: 0.4995620548725128\n",
      "Iteration 21, Batch: 35, Loss: 0.4931999146938324\n",
      "Iteration 21, Batch: 36, Loss: 0.4579496681690216\n",
      "Iteration 21, Batch: 37, Loss: 0.5224162936210632\n",
      "Iteration 21, Batch: 38, Loss: 0.48482367396354675\n",
      "Iteration 21, Batch: 39, Loss: 0.5229148864746094\n",
      "Iteration 21, Batch: 40, Loss: 0.487923264503479\n",
      "Iteration 21, Batch: 41, Loss: 0.5469049215316772\n",
      "Iteration 21, Batch: 42, Loss: 0.48625701665878296\n",
      "Iteration 21, Batch: 43, Loss: 0.5233644247055054\n",
      "Iteration 21, Batch: 44, Loss: 0.52367103099823\n",
      "Iteration 21, Batch: 45, Loss: 0.5174262523651123\n",
      "Iteration 21, Batch: 46, Loss: 0.480232298374176\n",
      "Iteration 21, Batch: 47, Loss: 0.5317570567131042\n",
      "Iteration 21, Batch: 48, Loss: 0.5005590319633484\n",
      "Iteration 21, Batch: 49, Loss: 0.5019341111183167\n",
      "Avg loss: 0.5001319652795791\n",
      "Iteration 22, Batch: 0, Loss: 0.5116952061653137\n",
      "Iteration 22, Batch: 1, Loss: 0.4834107756614685\n",
      "Iteration 22, Batch: 2, Loss: 0.4446522891521454\n",
      "Iteration 22, Batch: 3, Loss: 0.5245707035064697\n",
      "Iteration 22, Batch: 4, Loss: 0.5448386073112488\n",
      "Iteration 22, Batch: 5, Loss: 0.48417147994041443\n",
      "Iteration 22, Batch: 6, Loss: 0.4650709331035614\n",
      "Iteration 22, Batch: 7, Loss: 0.5047619342803955\n",
      "Iteration 22, Batch: 8, Loss: 0.434869647026062\n",
      "Iteration 22, Batch: 9, Loss: 0.5342236161231995\n",
      "Iteration 22, Batch: 10, Loss: 0.45477503538131714\n",
      "Iteration 22, Batch: 11, Loss: 0.4996652901172638\n",
      "Iteration 22, Batch: 12, Loss: 0.4196156859397888\n",
      "Iteration 22, Batch: 13, Loss: 0.48466163873672485\n",
      "Iteration 22, Batch: 14, Loss: 0.5298188924789429\n",
      "Iteration 22, Batch: 15, Loss: 0.54449063539505\n",
      "Iteration 22, Batch: 16, Loss: 0.4596242904663086\n",
      "Iteration 22, Batch: 17, Loss: 0.5543047785758972\n",
      "Iteration 22, Batch: 18, Loss: 0.4794454872608185\n",
      "Iteration 22, Batch: 19, Loss: 0.514565646648407\n",
      "Iteration 22, Batch: 20, Loss: 0.5093762278556824\n",
      "Iteration 22, Batch: 21, Loss: 0.49975430965423584\n",
      "Iteration 22, Batch: 22, Loss: 0.509835958480835\n",
      "Iteration 22, Batch: 23, Loss: 0.5342147350311279\n",
      "Iteration 22, Batch: 24, Loss: 0.5093821883201599\n",
      "Iteration 22, Batch: 25, Loss: 0.4647737145423889\n",
      "Iteration 22, Batch: 26, Loss: 0.4643080532550812\n",
      "Iteration 22, Batch: 27, Loss: 0.48421260714530945\n",
      "Iteration 22, Batch: 28, Loss: 0.45939573645591736\n",
      "Iteration 22, Batch: 29, Loss: 0.48922547698020935\n",
      "Iteration 22, Batch: 30, Loss: 0.5089592337608337\n",
      "Iteration 22, Batch: 31, Loss: 0.5536956787109375\n",
      "Iteration 22, Batch: 32, Loss: 0.5085062384605408\n",
      "Iteration 22, Batch: 33, Loss: 0.4587841033935547\n",
      "Iteration 22, Batch: 34, Loss: 0.49447953701019287\n",
      "Iteration 22, Batch: 35, Loss: 0.5283133387565613\n",
      "Iteration 22, Batch: 36, Loss: 0.4885803759098053\n",
      "Iteration 22, Batch: 37, Loss: 0.47457417845726013\n",
      "Iteration 22, Batch: 38, Loss: 0.5534529685974121\n",
      "Iteration 22, Batch: 39, Loss: 0.508433997631073\n",
      "Iteration 22, Batch: 40, Loss: 0.5039119124412537\n",
      "Iteration 22, Batch: 41, Loss: 0.48929283022880554\n",
      "Iteration 22, Batch: 42, Loss: 0.528181791305542\n",
      "Iteration 22, Batch: 43, Loss: 0.5283140540122986\n",
      "Iteration 22, Batch: 44, Loss: 0.5283912420272827\n",
      "Iteration 22, Batch: 45, Loss: 0.5893524885177612\n",
      "Iteration 22, Batch: 46, Loss: 0.5288453698158264\n",
      "Iteration 22, Batch: 47, Loss: 0.5439485907554626\n",
      "Iteration 22, Batch: 48, Loss: 0.5592316389083862\n",
      "Iteration 22, Batch: 49, Loss: 0.4886836111545563\n",
      "Avg loss: 0.5032728952169419\n",
      "Iteration 23, Batch: 0, Loss: 0.4598049819469452\n",
      "Iteration 23, Batch: 1, Loss: 0.5085443258285522\n",
      "Iteration 23, Batch: 2, Loss: 0.46362003684043884\n",
      "Iteration 23, Batch: 3, Loss: 0.4738469123840332\n",
      "Iteration 23, Batch: 4, Loss: 0.509016215801239\n",
      "Iteration 23, Batch: 5, Loss: 0.47474774718284607\n",
      "Iteration 23, Batch: 6, Loss: 0.523997962474823\n",
      "Iteration 23, Batch: 7, Loss: 0.46439817547798157\n",
      "Iteration 23, Batch: 8, Loss: 0.518760085105896\n",
      "Iteration 23, Batch: 9, Loss: 0.4792153835296631\n",
      "Iteration 23, Batch: 10, Loss: 0.5086440443992615\n",
      "Iteration 23, Batch: 11, Loss: 0.5139950513839722\n",
      "Iteration 23, Batch: 12, Loss: 0.5431644320487976\n",
      "Iteration 23, Batch: 13, Loss: 0.4841611683368683\n",
      "Iteration 23, Batch: 14, Loss: 0.4894344210624695\n",
      "Iteration 23, Batch: 15, Loss: 0.5875852108001709\n",
      "Iteration 23, Batch: 16, Loss: 0.5137207508087158\n",
      "Iteration 23, Batch: 17, Loss: 0.46899500489234924\n",
      "Iteration 23, Batch: 18, Loss: 0.47337648272514343\n",
      "Iteration 23, Batch: 19, Loss: 0.543263852596283\n",
      "Iteration 23, Batch: 20, Loss: 0.5381961464881897\n",
      "Iteration 23, Batch: 21, Loss: 0.5682644248008728\n",
      "Iteration 23, Batch: 22, Loss: 0.5038517117500305\n",
      "Iteration 23, Batch: 23, Loss: 0.4843762218952179\n",
      "Iteration 23, Batch: 24, Loss: 0.4636462330818176\n",
      "Iteration 23, Batch: 25, Loss: 0.5739258527755737\n",
      "Iteration 23, Batch: 26, Loss: 0.43374869227409363\n",
      "Iteration 23, Batch: 27, Loss: 0.5227954983711243\n",
      "Iteration 23, Batch: 28, Loss: 0.5132225155830383\n",
      "Iteration 23, Batch: 29, Loss: 0.5083311200141907\n",
      "Iteration 23, Batch: 30, Loss: 0.48410528898239136\n",
      "Iteration 23, Batch: 31, Loss: 0.4892512261867523\n",
      "Iteration 23, Batch: 32, Loss: 0.4587761461734772\n",
      "Iteration 23, Batch: 33, Loss: 0.5286740660667419\n",
      "Iteration 23, Batch: 34, Loss: 0.528867244720459\n",
      "Iteration 23, Batch: 35, Loss: 0.5490829944610596\n",
      "Iteration 23, Batch: 36, Loss: 0.5045623183250427\n",
      "Iteration 23, Batch: 37, Loss: 0.514279305934906\n",
      "Iteration 23, Batch: 38, Loss: 0.5233930349349976\n",
      "Iteration 23, Batch: 39, Loss: 0.4788435995578766\n",
      "Iteration 23, Batch: 40, Loss: 0.473694384098053\n",
      "Iteration 23, Batch: 41, Loss: 0.46854767203330994\n",
      "Iteration 23, Batch: 42, Loss: 0.5397109985351562\n",
      "Iteration 23, Batch: 43, Loss: 0.4896298348903656\n",
      "Iteration 23, Batch: 44, Loss: 0.5188615322113037\n",
      "Iteration 23, Batch: 45, Loss: 0.47435152530670166\n",
      "Iteration 23, Batch: 46, Loss: 0.4395850598812103\n",
      "Iteration 23, Batch: 47, Loss: 0.4996079206466675\n",
      "Iteration 23, Batch: 48, Loss: 0.5390990376472473\n",
      "Iteration 23, Batch: 49, Loss: 0.5291372537612915\n",
      "Avg loss: 0.5028942221403122\n",
      "Iteration 24, Batch: 0, Loss: 0.4992607533931732\n",
      "Iteration 24, Batch: 1, Loss: 0.5194115042686462\n",
      "Iteration 24, Batch: 2, Loss: 0.4941943883895874\n",
      "Iteration 24, Batch: 3, Loss: 0.48891302943229675\n",
      "Iteration 24, Batch: 4, Loss: 0.5385423302650452\n",
      "Iteration 24, Batch: 5, Loss: 0.4628433585166931\n",
      "Iteration 24, Batch: 6, Loss: 0.49279075860977173\n",
      "Iteration 24, Batch: 7, Loss: 0.4686216711997986\n",
      "Iteration 24, Batch: 8, Loss: 0.4532924294471741\n",
      "Iteration 24, Batch: 9, Loss: 0.5435718297958374\n",
      "Iteration 24, Batch: 10, Loss: 0.5243006944656372\n",
      "Iteration 24, Batch: 11, Loss: 0.49449506402015686\n",
      "Iteration 24, Batch: 12, Loss: 0.5092719197273254\n",
      "Iteration 24, Batch: 13, Loss: 0.49937233328819275\n",
      "Iteration 24, Batch: 14, Loss: 0.5234012603759766\n",
      "Iteration 24, Batch: 15, Loss: 0.523531973361969\n",
      "Iteration 24, Batch: 16, Loss: 0.4731767177581787\n",
      "Iteration 24, Batch: 17, Loss: 0.49903711676597595\n",
      "Iteration 24, Batch: 18, Loss: 0.5068327188491821\n",
      "Iteration 24, Batch: 19, Loss: 0.479821115732193\n",
      "Iteration 24, Batch: 20, Loss: 0.4790153503417969\n",
      "Iteration 24, Batch: 21, Loss: 0.4640440344810486\n",
      "Iteration 24, Batch: 22, Loss: 0.4578331708908081\n",
      "Iteration 24, Batch: 23, Loss: 0.5289357900619507\n",
      "Iteration 24, Batch: 24, Loss: 0.5336241126060486\n",
      "Iteration 24, Batch: 25, Loss: 0.5092781186103821\n",
      "Iteration 24, Batch: 26, Loss: 0.4845927655696869\n",
      "Iteration 24, Batch: 27, Loss: 0.5288889408111572\n",
      "Iteration 24, Batch: 28, Loss: 0.5184463262557983\n",
      "Iteration 24, Batch: 29, Loss: 0.5229108333587646\n",
      "Iteration 24, Batch: 30, Loss: 0.5091999769210815\n",
      "Iteration 24, Batch: 31, Loss: 0.45378321409225464\n",
      "Iteration 24, Batch: 32, Loss: 0.4343128800392151\n",
      "Iteration 24, Batch: 33, Loss: 0.5397226810455322\n",
      "Iteration 24, Batch: 34, Loss: 0.5138201713562012\n",
      "Iteration 24, Batch: 35, Loss: 0.4939885139465332\n",
      "Iteration 24, Batch: 36, Loss: 0.4645566940307617\n",
      "Iteration 24, Batch: 37, Loss: 0.5390840172767639\n",
      "Iteration 24, Batch: 38, Loss: 0.5286285877227783\n",
      "Iteration 24, Batch: 39, Loss: 0.4591233730316162\n",
      "Iteration 24, Batch: 40, Loss: 0.4998658001422882\n",
      "Iteration 24, Batch: 41, Loss: 0.5076095461845398\n",
      "Iteration 24, Batch: 42, Loss: 0.5233179330825806\n",
      "Iteration 24, Batch: 43, Loss: 0.48410162329673767\n",
      "Iteration 24, Batch: 44, Loss: 0.4892958104610443\n",
      "Iteration 24, Batch: 45, Loss: 0.5985206365585327\n",
      "Iteration 24, Batch: 46, Loss: 0.5037409663200378\n",
      "Iteration 24, Batch: 47, Loss: 0.5268958806991577\n",
      "Iteration 24, Batch: 48, Loss: 0.5628653764724731\n",
      "Iteration 24, Batch: 49, Loss: 0.483153373003006\n",
      "Avg loss: 0.5027567893266678\n",
      "Iteration 25, Batch: 0, Loss: 0.45859572291374207\n",
      "Iteration 25, Batch: 1, Loss: 0.489021897315979\n",
      "Iteration 25, Batch: 2, Loss: 0.5578603744506836\n",
      "Iteration 25, Batch: 3, Loss: 0.4693693518638611\n",
      "Iteration 25, Batch: 4, Loss: 0.45398107171058655\n",
      "Iteration 25, Batch: 5, Loss: 0.47404998540878296\n",
      "Iteration 25, Batch: 6, Loss: 0.4385793209075928\n",
      "Iteration 25, Batch: 7, Loss: 0.5224859714508057\n",
      "Iteration 25, Batch: 8, Loss: 0.5333490371704102\n",
      "Iteration 25, Batch: 9, Loss: 0.49920928478240967\n",
      "Iteration 25, Batch: 10, Loss: 0.44500982761383057\n",
      "Iteration 25, Batch: 11, Loss: 0.5181125998497009\n",
      "Iteration 25, Batch: 12, Loss: 0.5680643916130066\n",
      "Iteration 25, Batch: 13, Loss: 0.4934280812740326\n",
      "Iteration 25, Batch: 14, Loss: 0.5591086745262146\n",
      "Iteration 25, Batch: 15, Loss: 0.5297661423683167\n",
      "Iteration 25, Batch: 16, Loss: 0.4981456696987152\n",
      "Iteration 25, Batch: 17, Loss: 0.5388185977935791\n",
      "Iteration 25, Batch: 18, Loss: 0.4832035303115845\n",
      "Iteration 25, Batch: 19, Loss: 0.5369104146957397\n",
      "Iteration 25, Batch: 20, Loss: 0.4889257848262787\n",
      "Iteration 25, Batch: 21, Loss: 0.5195920467376709\n",
      "Iteration 25, Batch: 22, Loss: 0.49407413601875305\n",
      "Iteration 25, Batch: 23, Loss: 0.48218727111816406\n",
      "Iteration 25, Batch: 24, Loss: 0.4123559594154358\n",
      "Iteration 25, Batch: 25, Loss: 0.49958091974258423\n",
      "Iteration 25, Batch: 26, Loss: 0.4539736211299896\n",
      "Iteration 25, Batch: 27, Loss: 0.562320351600647\n",
      "Iteration 25, Batch: 28, Loss: 0.44902461767196655\n",
      "Iteration 25, Batch: 29, Loss: 0.5582694411277771\n",
      "Iteration 25, Batch: 30, Loss: 0.5891066193580627\n",
      "Iteration 25, Batch: 31, Loss: 0.5376560688018799\n",
      "Iteration 25, Batch: 32, Loss: 0.4892600178718567\n",
      "Iteration 25, Batch: 33, Loss: 0.46911829710006714\n",
      "Iteration 25, Batch: 34, Loss: 0.49435317516326904\n",
      "Iteration 25, Batch: 35, Loss: 0.48335516452789307\n",
      "Iteration 25, Batch: 36, Loss: 0.5283732414245605\n",
      "Iteration 25, Batch: 37, Loss: 0.523580014705658\n",
      "Iteration 25, Batch: 38, Loss: 0.49284589290618896\n",
      "Iteration 25, Batch: 39, Loss: 0.5634281039237976\n",
      "Iteration 25, Batch: 40, Loss: 0.4895923435688019\n",
      "Iteration 25, Batch: 41, Loss: 0.45370015501976013\n",
      "Iteration 25, Batch: 42, Loss: 0.474433034658432\n",
      "Iteration 25, Batch: 43, Loss: 0.5081580877304077\n",
      "Iteration 25, Batch: 44, Loss: 0.5137068629264832\n",
      "Iteration 25, Batch: 45, Loss: 0.5132689476013184\n",
      "Iteration 25, Batch: 46, Loss: 0.4986761510372162\n",
      "Iteration 25, Batch: 47, Loss: 0.504213809967041\n",
      "Iteration 25, Batch: 48, Loss: 0.5178818702697754\n",
      "Iteration 25, Batch: 49, Loss: 0.4982195198535919\n",
      "Avg loss: 0.502606029510498\n",
      "Iteration 26, Batch: 0, Loss: 0.47317931056022644\n",
      "Iteration 26, Batch: 1, Loss: 0.4991942346096039\n",
      "Iteration 26, Batch: 2, Loss: 0.4878714382648468\n",
      "Iteration 26, Batch: 3, Loss: 0.43951326608657837\n",
      "Iteration 26, Batch: 4, Loss: 0.4984194338321686\n",
      "Iteration 26, Batch: 5, Loss: 0.5084437727928162\n",
      "Iteration 26, Batch: 6, Loss: 0.5092381834983826\n",
      "Iteration 26, Batch: 7, Loss: 0.4683464765548706\n",
      "Iteration 26, Batch: 8, Loss: 0.4837401509284973\n",
      "Iteration 26, Batch: 9, Loss: 0.5937775373458862\n",
      "Iteration 26, Batch: 10, Loss: 0.49820151925086975\n",
      "Iteration 26, Batch: 11, Loss: 0.42933374643325806\n",
      "Iteration 26, Batch: 12, Loss: 0.46013838052749634\n",
      "Iteration 26, Batch: 13, Loss: 0.5430513024330139\n",
      "Iteration 26, Batch: 14, Loss: 0.5528618097305298\n",
      "Iteration 26, Batch: 15, Loss: 0.5038869976997375\n",
      "Iteration 26, Batch: 16, Loss: 0.4689818322658539\n",
      "Iteration 26, Batch: 17, Loss: 0.4149116277694702\n",
      "Iteration 26, Batch: 18, Loss: 0.45434844493865967\n",
      "Iteration 26, Batch: 19, Loss: 0.4740675389766693\n",
      "Iteration 26, Batch: 20, Loss: 0.4746023714542389\n",
      "Iteration 26, Batch: 21, Loss: 0.5886410474777222\n",
      "Iteration 26, Batch: 22, Loss: 0.5634307265281677\n",
      "Iteration 26, Batch: 23, Loss: 0.5139772891998291\n",
      "Iteration 26, Batch: 24, Loss: 0.46886083483695984\n",
      "Iteration 26, Batch: 25, Loss: 0.46913421154022217\n",
      "Iteration 26, Batch: 26, Loss: 0.5140175819396973\n",
      "Iteration 26, Batch: 27, Loss: 0.5394780039787292\n",
      "Iteration 26, Batch: 28, Loss: 0.5028761029243469\n",
      "Iteration 26, Batch: 29, Loss: 0.5695928931236267\n",
      "Iteration 26, Batch: 30, Loss: 0.48379501700401306\n",
      "Iteration 26, Batch: 31, Loss: 0.5145667195320129\n",
      "Iteration 26, Batch: 32, Loss: 0.4138205945491791\n",
      "Iteration 26, Batch: 33, Loss: 0.4845409393310547\n",
      "Iteration 26, Batch: 34, Loss: 0.49298226833343506\n",
      "Iteration 26, Batch: 35, Loss: 0.538597047328949\n",
      "Iteration 26, Batch: 36, Loss: 0.5042166709899902\n",
      "Iteration 26, Batch: 37, Loss: 0.518339991569519\n",
      "Iteration 26, Batch: 38, Loss: 0.5395755171775818\n",
      "Iteration 26, Batch: 39, Loss: 0.5372362732887268\n",
      "Iteration 26, Batch: 40, Loss: 0.46273073554039\n",
      "Iteration 26, Batch: 41, Loss: 0.48226937651634216\n",
      "Iteration 26, Batch: 42, Loss: 0.5293153524398804\n",
      "Iteration 26, Batch: 43, Loss: 0.5481387376785278\n",
      "Iteration 26, Batch: 44, Loss: 0.5430387258529663\n",
      "Iteration 26, Batch: 45, Loss: 0.5437718629837036\n",
      "Iteration 26, Batch: 46, Loss: 0.47296375036239624\n",
      "Iteration 26, Batch: 47, Loss: 0.5136874318122864\n",
      "Iteration 26, Batch: 48, Loss: 0.5333238840103149\n",
      "Iteration 26, Batch: 49, Loss: 0.5146378874778748\n",
      "Avg loss: 0.5027533370256424\n",
      "Iteration 27, Batch: 0, Loss: 0.5020315647125244\n",
      "Iteration 27, Batch: 1, Loss: 0.45447301864624023\n",
      "Iteration 27, Batch: 2, Loss: 0.48821601271629333\n",
      "Iteration 27, Batch: 3, Loss: 0.5508677363395691\n",
      "Iteration 27, Batch: 4, Loss: 0.49890294671058655\n",
      "Iteration 27, Batch: 5, Loss: 0.5424586534500122\n",
      "Iteration 27, Batch: 6, Loss: 0.5175848007202148\n",
      "Iteration 27, Batch: 7, Loss: 0.5219408273696899\n",
      "Iteration 27, Batch: 8, Loss: 0.5341432094573975\n",
      "Iteration 27, Batch: 9, Loss: 0.4837033748626709\n",
      "Iteration 27, Batch: 10, Loss: 0.5368189215660095\n",
      "Iteration 27, Batch: 11, Loss: 0.5082634091377258\n",
      "Iteration 27, Batch: 12, Loss: 0.5168496370315552\n",
      "Iteration 27, Batch: 13, Loss: 0.5394373536109924\n",
      "Iteration 27, Batch: 14, Loss: 0.4790022075176239\n",
      "Iteration 27, Batch: 15, Loss: 0.5055049657821655\n",
      "Iteration 27, Batch: 16, Loss: 0.45196521282196045\n",
      "Iteration 27, Batch: 17, Loss: 0.5298032164573669\n",
      "Iteration 27, Batch: 18, Loss: 0.4222326874732971\n",
      "Iteration 27, Batch: 19, Loss: 0.5197833776473999\n",
      "Iteration 27, Batch: 20, Loss: 0.5226949453353882\n",
      "Iteration 27, Batch: 21, Loss: 0.526414155960083\n",
      "Iteration 27, Batch: 22, Loss: 0.5205492973327637\n",
      "Iteration 27, Batch: 23, Loss: 0.48011064529418945\n",
      "Iteration 27, Batch: 24, Loss: 0.46514520049095154\n",
      "Iteration 27, Batch: 25, Loss: 0.5463221669197083\n",
      "Iteration 27, Batch: 26, Loss: 0.47801774740219116\n",
      "Iteration 27, Batch: 27, Loss: 0.47434699535369873\n",
      "Iteration 27, Batch: 28, Loss: 0.38923975825309753\n",
      "Iteration 27, Batch: 29, Loss: 0.49911513924598694\n",
      "Iteration 27, Batch: 30, Loss: 0.48388776183128357\n",
      "Iteration 27, Batch: 31, Loss: 0.5075695514678955\n",
      "Iteration 27, Batch: 32, Loss: 0.5486579537391663\n",
      "Iteration 27, Batch: 33, Loss: 0.5138241648674011\n",
      "Iteration 27, Batch: 34, Loss: 0.4735301733016968\n",
      "Iteration 27, Batch: 35, Loss: 0.5271909236907959\n",
      "Iteration 27, Batch: 36, Loss: 0.49392563104629517\n",
      "Iteration 27, Batch: 37, Loss: 0.4731605052947998\n",
      "Iteration 27, Batch: 38, Loss: 0.5365679264068604\n",
      "Iteration 27, Batch: 39, Loss: 0.5016981959342957\n",
      "Iteration 27, Batch: 40, Loss: 0.5202724933624268\n",
      "Iteration 27, Batch: 41, Loss: 0.44552838802337646\n",
      "Iteration 27, Batch: 42, Loss: 0.5038723945617676\n",
      "Iteration 27, Batch: 43, Loss: 0.5129078030586243\n",
      "Iteration 27, Batch: 44, Loss: 0.4840029776096344\n",
      "Iteration 27, Batch: 45, Loss: 0.5186599493026733\n",
      "Iteration 27, Batch: 46, Loss: 0.4624643921852112\n",
      "Iteration 27, Batch: 47, Loss: 0.5079964399337769\n",
      "Iteration 27, Batch: 48, Loss: 0.5144738554954529\n",
      "Iteration 27, Batch: 49, Loss: 0.5386865735054016\n",
      "Avg loss: 0.5014963448047638\n",
      "Iteration 28, Batch: 0, Loss: 0.5039913654327393\n",
      "Iteration 28, Batch: 1, Loss: 0.5422582030296326\n",
      "Iteration 28, Batch: 2, Loss: 0.5383763909339905\n",
      "Iteration 28, Batch: 3, Loss: 0.47742345929145813\n",
      "Iteration 28, Batch: 4, Loss: 0.5064778923988342\n",
      "Iteration 28, Batch: 5, Loss: 0.5053606629371643\n",
      "Iteration 28, Batch: 6, Loss: 0.462715208530426\n",
      "Iteration 28, Batch: 7, Loss: 0.49871283769607544\n",
      "Iteration 28, Batch: 8, Loss: 0.5160370469093323\n",
      "Iteration 28, Batch: 9, Loss: 0.5440918803215027\n",
      "Iteration 28, Batch: 10, Loss: 0.4383193850517273\n",
      "Iteration 28, Batch: 11, Loss: 0.5159968733787537\n",
      "Iteration 28, Batch: 12, Loss: 0.515366792678833\n",
      "Iteration 28, Batch: 13, Loss: 0.581328809261322\n",
      "Iteration 28, Batch: 14, Loss: 0.4919951260089874\n",
      "Iteration 28, Batch: 15, Loss: 0.5229940414428711\n",
      "Iteration 28, Batch: 16, Loss: 0.5329360961914062\n",
      "Iteration 28, Batch: 17, Loss: 0.5378837585449219\n",
      "Iteration 28, Batch: 18, Loss: 0.5223941206932068\n",
      "Iteration 28, Batch: 19, Loss: 0.5071123242378235\n",
      "Iteration 28, Batch: 20, Loss: 0.47641485929489136\n",
      "Iteration 28, Batch: 21, Loss: 0.4931342303752899\n",
      "Iteration 28, Batch: 22, Loss: 0.5281302332878113\n",
      "Iteration 28, Batch: 23, Loss: 0.513249933719635\n",
      "Iteration 28, Batch: 24, Loss: 0.5383560061454773\n",
      "Iteration 28, Batch: 25, Loss: 0.40705302357673645\n",
      "Iteration 28, Batch: 26, Loss: 0.477129191160202\n",
      "Iteration 28, Batch: 27, Loss: 0.5121988654136658\n",
      "Iteration 28, Batch: 28, Loss: 0.5402412414550781\n",
      "Iteration 28, Batch: 29, Loss: 0.5069707632064819\n",
      "Iteration 28, Batch: 30, Loss: 0.4547466039657593\n",
      "Iteration 28, Batch: 31, Loss: 0.5160769820213318\n",
      "Iteration 28, Batch: 32, Loss: 0.45150497555732727\n",
      "Iteration 28, Batch: 33, Loss: 0.463245689868927\n",
      "Iteration 28, Batch: 34, Loss: 0.4872388243675232\n",
      "Iteration 28, Batch: 35, Loss: 0.49395322799682617\n",
      "Iteration 28, Batch: 36, Loss: 0.5477249026298523\n",
      "Iteration 28, Batch: 37, Loss: 0.4627128541469574\n",
      "Iteration 28, Batch: 38, Loss: 0.4980298578739166\n",
      "Iteration 28, Batch: 39, Loss: 0.5282666683197021\n",
      "Iteration 28, Batch: 40, Loss: 0.48446017503738403\n",
      "Iteration 28, Batch: 41, Loss: 0.5078436732292175\n",
      "Iteration 28, Batch: 42, Loss: 0.4788563549518585\n",
      "Iteration 28, Batch: 43, Loss: 0.41707560420036316\n",
      "Iteration 28, Batch: 44, Loss: 0.511962890625\n",
      "Iteration 28, Batch: 45, Loss: 0.49704596400260925\n",
      "Iteration 28, Batch: 46, Loss: 0.44020918011665344\n",
      "Iteration 28, Batch: 47, Loss: 0.5544678568840027\n",
      "Iteration 28, Batch: 48, Loss: 0.4817005693912506\n",
      "Iteration 28, Batch: 49, Loss: 0.4854123890399933\n",
      "Avg loss: 0.5003437173366546\n",
      "Iteration 29, Batch: 0, Loss: 0.5109103322029114\n",
      "Iteration 29, Batch: 1, Loss: 0.4568079710006714\n",
      "Iteration 29, Batch: 2, Loss: 0.47058209776878357\n",
      "Iteration 29, Batch: 3, Loss: 0.5166515707969666\n",
      "Iteration 29, Batch: 4, Loss: 0.49724674224853516\n",
      "Iteration 29, Batch: 5, Loss: 0.5006592273712158\n",
      "Iteration 29, Batch: 6, Loss: 0.47082212567329407\n",
      "Iteration 29, Batch: 7, Loss: 0.4729372560977936\n",
      "Iteration 29, Batch: 8, Loss: 0.5290065407752991\n",
      "Iteration 29, Batch: 9, Loss: 0.5365710854530334\n",
      "Iteration 29, Batch: 10, Loss: 0.5330826640129089\n",
      "Iteration 29, Batch: 11, Loss: 0.508486807346344\n",
      "Iteration 29, Batch: 12, Loss: 0.5148757696151733\n",
      "Iteration 29, Batch: 13, Loss: 0.41297340393066406\n",
      "Iteration 29, Batch: 14, Loss: 0.5271443724632263\n",
      "Iteration 29, Batch: 15, Loss: 0.4373122453689575\n",
      "Iteration 29, Batch: 16, Loss: 0.45949363708496094\n",
      "Iteration 29, Batch: 17, Loss: 0.48321396112442017\n",
      "Iteration 29, Batch: 18, Loss: 0.4613262414932251\n",
      "Iteration 29, Batch: 19, Loss: 0.5202345252037048\n",
      "Iteration 29, Batch: 20, Loss: 0.5609341263771057\n",
      "Iteration 29, Batch: 21, Loss: 0.49652794003486633\n",
      "Iteration 29, Batch: 22, Loss: 0.5171407461166382\n",
      "Iteration 29, Batch: 23, Loss: 0.44763973355293274\n",
      "Iteration 29, Batch: 24, Loss: 0.5015475749969482\n",
      "Iteration 29, Batch: 25, Loss: 0.5571829080581665\n",
      "Iteration 29, Batch: 26, Loss: 0.49326834082603455\n",
      "Iteration 29, Batch: 27, Loss: 0.5061177611351013\n",
      "Iteration 29, Batch: 28, Loss: 0.4966370761394501\n",
      "Iteration 29, Batch: 29, Loss: 0.5219124555587769\n",
      "Iteration 29, Batch: 30, Loss: 0.5120725035667419\n",
      "Iteration 29, Batch: 31, Loss: 0.4986627995967865\n",
      "Iteration 29, Batch: 32, Loss: 0.5055537223815918\n",
      "Iteration 29, Batch: 33, Loss: 0.6076247692108154\n",
      "Iteration 29, Batch: 34, Loss: 0.4678008556365967\n",
      "Iteration 29, Batch: 35, Loss: 0.4727667272090912\n",
      "Iteration 29, Batch: 36, Loss: 0.46362531185150146\n",
      "Iteration 29, Batch: 37, Loss: 0.4977510869503021\n",
      "Iteration 29, Batch: 38, Loss: 0.5184364914894104\n",
      "Iteration 29, Batch: 39, Loss: 0.45742565393447876\n",
      "Iteration 29, Batch: 40, Loss: 0.5576245784759521\n",
      "Iteration 29, Batch: 41, Loss: 0.49864280223846436\n",
      "Iteration 29, Batch: 42, Loss: 0.5313730239868164\n",
      "Iteration 29, Batch: 43, Loss: 0.5023122429847717\n",
      "Iteration 29, Batch: 44, Loss: 0.5001209378242493\n",
      "Iteration 29, Batch: 45, Loss: 0.5060324668884277\n",
      "Iteration 29, Batch: 46, Loss: 0.505933403968811\n",
      "Iteration 29, Batch: 47, Loss: 0.5255661606788635\n",
      "Iteration 29, Batch: 48, Loss: 0.4764796793460846\n",
      "Iteration 29, Batch: 49, Loss: 0.4417452812194824\n",
      "Avg loss: 0.499335954785347\n",
      "Iteration 30, Batch: 0, Loss: 0.5073776841163635\n",
      "Iteration 30, Batch: 1, Loss: 0.48130813241004944\n",
      "Iteration 30, Batch: 2, Loss: 0.5296319723129272\n",
      "Iteration 30, Batch: 3, Loss: 0.4454374313354492\n",
      "Iteration 30, Batch: 4, Loss: 0.5110307931900024\n",
      "Iteration 30, Batch: 5, Loss: 0.5526641011238098\n",
      "Iteration 30, Batch: 6, Loss: 0.5118237733840942\n",
      "Iteration 30, Batch: 7, Loss: 0.5323958396911621\n",
      "Iteration 30, Batch: 8, Loss: 0.4875916540622711\n",
      "Iteration 30, Batch: 9, Loss: 0.5820677280426025\n",
      "Iteration 30, Batch: 10, Loss: 0.5231421589851379\n",
      "Iteration 30, Batch: 11, Loss: 0.44209960103034973\n",
      "Iteration 30, Batch: 12, Loss: 0.5148907899856567\n",
      "Iteration 30, Batch: 13, Loss: 0.49608755111694336\n",
      "Iteration 30, Batch: 14, Loss: 0.440000057220459\n",
      "Iteration 30, Batch: 15, Loss: 0.4248200058937073\n",
      "Iteration 30, Batch: 16, Loss: 0.4368523359298706\n",
      "Iteration 30, Batch: 17, Loss: 0.5015671253204346\n",
      "Iteration 30, Batch: 18, Loss: 0.4825561046600342\n",
      "Iteration 30, Batch: 19, Loss: 0.5165042281150818\n",
      "Iteration 30, Batch: 20, Loss: 0.46498680114746094\n",
      "Iteration 30, Batch: 21, Loss: 0.5042920708656311\n",
      "Iteration 30, Batch: 22, Loss: 0.47934943437576294\n",
      "Iteration 30, Batch: 23, Loss: 0.4662665128707886\n",
      "Iteration 30, Batch: 24, Loss: 0.5603162050247192\n",
      "Iteration 30, Batch: 25, Loss: 0.5153825879096985\n",
      "Iteration 30, Batch: 26, Loss: 0.5368096232414246\n",
      "Iteration 30, Batch: 27, Loss: 0.5170149803161621\n",
      "Iteration 30, Batch: 28, Loss: 0.5009507536888123\n",
      "Iteration 30, Batch: 29, Loss: 0.45593175292015076\n",
      "Iteration 30, Batch: 30, Loss: 0.4743861258029938\n",
      "Iteration 30, Batch: 31, Loss: 0.5047680735588074\n",
      "Iteration 30, Batch: 32, Loss: 0.4569537341594696\n",
      "Iteration 30, Batch: 33, Loss: 0.5384871363639832\n",
      "Iteration 30, Batch: 34, Loss: 0.5063453316688538\n",
      "Iteration 30, Batch: 35, Loss: 0.5010516047477722\n",
      "Iteration 30, Batch: 36, Loss: 0.523841917514801\n",
      "Iteration 30, Batch: 37, Loss: 0.4187820255756378\n",
      "Iteration 30, Batch: 38, Loss: 0.47535181045532227\n",
      "Iteration 30, Batch: 39, Loss: 0.43757712841033936\n",
      "Iteration 30, Batch: 40, Loss: 0.4943941831588745\n",
      "Iteration 30, Batch: 41, Loss: 0.5217680335044861\n",
      "Iteration 30, Batch: 42, Loss: 0.5286325216293335\n",
      "Iteration 30, Batch: 43, Loss: 0.5167414546012878\n",
      "Iteration 30, Batch: 44, Loss: 0.554533064365387\n",
      "Iteration 30, Batch: 45, Loss: 0.5123863816261292\n",
      "Iteration 30, Batch: 46, Loss: 0.5212278366088867\n",
      "Iteration 30, Batch: 47, Loss: 0.4681050777435303\n",
      "Iteration 30, Batch: 48, Loss: 0.495705246925354\n",
      "Iteration 30, Batch: 49, Loss: 0.5020580887794495\n",
      "Avg loss: 0.49748493134975436\n",
      "Iteration 31, Batch: 0, Loss: 0.48648935556411743\n",
      "Iteration 31, Batch: 1, Loss: 0.502289891242981\n",
      "Iteration 31, Batch: 2, Loss: 0.5083305239677429\n",
      "Iteration 31, Batch: 3, Loss: 0.5710666179656982\n",
      "Iteration 31, Batch: 4, Loss: 0.48825040459632874\n",
      "Iteration 31, Batch: 5, Loss: 0.4515092372894287\n",
      "Iteration 31, Batch: 6, Loss: 0.5149142146110535\n",
      "Iteration 31, Batch: 7, Loss: 0.46309125423431396\n",
      "Iteration 31, Batch: 8, Loss: 0.5270051956176758\n",
      "Iteration 31, Batch: 9, Loss: 0.5423848032951355\n",
      "Iteration 31, Batch: 10, Loss: 0.5395722985267639\n",
      "Iteration 31, Batch: 11, Loss: 0.5025035738945007\n",
      "Iteration 31, Batch: 12, Loss: 0.5082411170005798\n",
      "Iteration 31, Batch: 13, Loss: 0.5290419459342957\n",
      "Iteration 31, Batch: 14, Loss: 0.41280096769332886\n",
      "Iteration 31, Batch: 15, Loss: 0.49826231598854065\n",
      "Iteration 31, Batch: 16, Loss: 0.49600493907928467\n",
      "Iteration 31, Batch: 17, Loss: 0.5212893486022949\n",
      "Iteration 31, Batch: 18, Loss: 0.44935378432273865\n",
      "Iteration 31, Batch: 19, Loss: 0.4657336175441742\n",
      "Iteration 31, Batch: 20, Loss: 0.495755672454834\n",
      "Iteration 31, Batch: 21, Loss: 0.4855130612850189\n",
      "Iteration 31, Batch: 22, Loss: 0.47510823607444763\n",
      "Iteration 31, Batch: 23, Loss: 0.5295868515968323\n",
      "Iteration 31, Batch: 24, Loss: 0.5010225772857666\n",
      "Iteration 31, Batch: 25, Loss: 0.5319555401802063\n",
      "Iteration 31, Batch: 26, Loss: 0.4684947431087494\n",
      "Iteration 31, Batch: 27, Loss: 0.4835180640220642\n",
      "Iteration 31, Batch: 28, Loss: 0.4884924292564392\n",
      "Iteration 31, Batch: 29, Loss: 0.4901413321495056\n",
      "Iteration 31, Batch: 30, Loss: 0.49329471588134766\n",
      "Iteration 31, Batch: 31, Loss: 0.4918743073940277\n",
      "Iteration 31, Batch: 32, Loss: 0.5283547639846802\n",
      "Iteration 31, Batch: 33, Loss: 0.5212101340293884\n",
      "Iteration 31, Batch: 34, Loss: 0.532908022403717\n",
      "Iteration 31, Batch: 35, Loss: 0.5077232718467712\n",
      "Iteration 31, Batch: 36, Loss: 0.45103248953819275\n",
      "Iteration 31, Batch: 37, Loss: 0.48238134384155273\n",
      "Iteration 31, Batch: 38, Loss: 0.4919564723968506\n",
      "Iteration 31, Batch: 39, Loss: 0.505992591381073\n",
      "Iteration 31, Batch: 40, Loss: 0.43318504095077515\n",
      "Iteration 31, Batch: 41, Loss: 0.5358971357345581\n",
      "Iteration 31, Batch: 42, Loss: 0.5368427038192749\n",
      "Iteration 31, Batch: 43, Loss: 0.5163397192955017\n",
      "Iteration 31, Batch: 44, Loss: 0.48950767517089844\n",
      "Iteration 31, Batch: 45, Loss: 0.49240824580192566\n",
      "Iteration 31, Batch: 46, Loss: 0.45142820477485657\n",
      "Iteration 31, Batch: 47, Loss: 0.5014874339103699\n",
      "Iteration 31, Batch: 48, Loss: 0.5421857237815857\n",
      "Iteration 31, Batch: 49, Loss: 0.4917632043361664\n",
      "Avg loss: 0.49850994229316714\n",
      "Iteration 32, Batch: 0, Loss: 0.4651942253112793\n",
      "Iteration 32, Batch: 1, Loss: 0.5468319058418274\n",
      "Iteration 32, Batch: 2, Loss: 0.47949057817459106\n",
      "Iteration 32, Batch: 3, Loss: 0.4185539186000824\n",
      "Iteration 32, Batch: 4, Loss: 0.49232229590415955\n",
      "Iteration 32, Batch: 5, Loss: 0.4588072896003723\n",
      "Iteration 32, Batch: 6, Loss: 0.4709681570529938\n",
      "Iteration 32, Batch: 7, Loss: 0.4255860149860382\n",
      "Iteration 32, Batch: 8, Loss: 0.5298528075218201\n",
      "Iteration 32, Batch: 9, Loss: 0.45813554525375366\n",
      "Iteration 32, Batch: 10, Loss: 0.45619437098503113\n",
      "Iteration 32, Batch: 11, Loss: 0.5320839285850525\n",
      "Iteration 32, Batch: 12, Loss: 0.5086601376533508\n",
      "Iteration 32, Batch: 13, Loss: 0.467140793800354\n",
      "Iteration 32, Batch: 14, Loss: 0.5709108710289001\n",
      "Iteration 32, Batch: 15, Loss: 0.5132340788841248\n",
      "Iteration 32, Batch: 16, Loss: 0.4470829367637634\n",
      "Iteration 32, Batch: 17, Loss: 0.48349013924598694\n",
      "Iteration 32, Batch: 18, Loss: 0.5479068756103516\n",
      "Iteration 32, Batch: 19, Loss: 0.6253470182418823\n",
      "Iteration 32, Batch: 20, Loss: 0.4891420602798462\n",
      "Iteration 32, Batch: 21, Loss: 0.525034487247467\n",
      "Iteration 32, Batch: 22, Loss: 0.5490099787712097\n",
      "Iteration 32, Batch: 23, Loss: 0.4925350844860077\n",
      "Iteration 32, Batch: 24, Loss: 0.49750587344169617\n",
      "Iteration 32, Batch: 25, Loss: 0.4856041371822357\n",
      "Iteration 32, Batch: 26, Loss: 0.5074232220649719\n",
      "Iteration 32, Batch: 27, Loss: 0.47098013758659363\n",
      "Iteration 32, Batch: 28, Loss: 0.47943899035453796\n",
      "Iteration 32, Batch: 29, Loss: 0.4877789318561554\n",
      "Iteration 32, Batch: 30, Loss: 0.48628321290016174\n",
      "Iteration 32, Batch: 31, Loss: 0.49442675709724426\n",
      "Iteration 32, Batch: 32, Loss: 0.4831298291683197\n",
      "Iteration 32, Batch: 33, Loss: 0.46123602986335754\n",
      "Iteration 32, Batch: 34, Loss: 0.4578998386859894\n",
      "Iteration 32, Batch: 35, Loss: 0.5219510793685913\n",
      "Iteration 32, Batch: 36, Loss: 0.5049616694450378\n",
      "Iteration 32, Batch: 37, Loss: 0.49225690960884094\n",
      "Iteration 32, Batch: 38, Loss: 0.523509681224823\n",
      "Iteration 32, Batch: 39, Loss: 0.5192832350730896\n",
      "Iteration 32, Batch: 40, Loss: 0.4987407922744751\n",
      "Iteration 32, Batch: 41, Loss: 0.5505878925323486\n",
      "Iteration 32, Batch: 42, Loss: 0.4865952134132385\n",
      "Iteration 32, Batch: 43, Loss: 0.5158639550209045\n",
      "Iteration 32, Batch: 44, Loss: 0.44257456064224243\n",
      "Iteration 32, Batch: 45, Loss: 0.5098804831504822\n",
      "Iteration 32, Batch: 46, Loss: 0.5136163234710693\n",
      "Iteration 32, Batch: 47, Loss: 0.49813804030418396\n",
      "Iteration 32, Batch: 48, Loss: 0.5005836486816406\n",
      "Iteration 32, Batch: 49, Loss: 0.5366066694259644\n",
      "Avg loss: 0.49760745227336883\n",
      "Iteration 33, Batch: 0, Loss: 0.5107187032699585\n",
      "Iteration 33, Batch: 1, Loss: 0.45014193654060364\n",
      "Iteration 33, Batch: 2, Loss: 0.4799894094467163\n",
      "Iteration 33, Batch: 3, Loss: 0.4445050060749054\n",
      "Iteration 33, Batch: 4, Loss: 0.4733273983001709\n",
      "Iteration 33, Batch: 5, Loss: 0.4868273138999939\n",
      "Iteration 33, Batch: 6, Loss: 0.46555206179618835\n",
      "Iteration 33, Batch: 7, Loss: 0.558891236782074\n",
      "Iteration 33, Batch: 8, Loss: 0.5095305442810059\n",
      "Iteration 33, Batch: 9, Loss: 0.5681517124176025\n",
      "Iteration 33, Batch: 10, Loss: 0.4817400872707367\n",
      "Iteration 33, Batch: 11, Loss: 0.5229799747467041\n",
      "Iteration 33, Batch: 12, Loss: 0.49648407101631165\n",
      "Iteration 33, Batch: 13, Loss: 0.4700654447078705\n",
      "Iteration 33, Batch: 14, Loss: 0.5053396821022034\n",
      "Iteration 33, Batch: 15, Loss: 0.5248484015464783\n",
      "Iteration 33, Batch: 16, Loss: 0.4162207245826721\n",
      "Iteration 33, Batch: 17, Loss: 0.44836145639419556\n",
      "Iteration 33, Batch: 18, Loss: 0.45761817693710327\n",
      "Iteration 33, Batch: 19, Loss: 0.4399643540382385\n",
      "Iteration 33, Batch: 20, Loss: 0.5080284476280212\n",
      "Iteration 33, Batch: 21, Loss: 0.5078178644180298\n",
      "Iteration 33, Batch: 22, Loss: 0.4876342713832855\n",
      "Iteration 33, Batch: 23, Loss: 0.500346839427948\n",
      "Iteration 33, Batch: 24, Loss: 0.5255253314971924\n",
      "Iteration 33, Batch: 25, Loss: 0.44831615686416626\n",
      "Iteration 33, Batch: 26, Loss: 0.53446364402771\n",
      "Iteration 33, Batch: 27, Loss: 0.5312492251396179\n",
      "Iteration 33, Batch: 28, Loss: 0.4685923755168915\n",
      "Iteration 33, Batch: 29, Loss: 0.4673319458961487\n",
      "Iteration 33, Batch: 30, Loss: 0.4978230893611908\n",
      "Iteration 33, Batch: 31, Loss: 0.488831102848053\n",
      "Iteration 33, Batch: 32, Loss: 0.5049378275871277\n",
      "Iteration 33, Batch: 33, Loss: 0.47635748982429504\n",
      "Iteration 33, Batch: 34, Loss: 0.5271701216697693\n",
      "Iteration 33, Batch: 35, Loss: 0.4747920036315918\n",
      "Iteration 33, Batch: 36, Loss: 0.4839107394218445\n",
      "Iteration 33, Batch: 37, Loss: 0.49488019943237305\n",
      "Iteration 33, Batch: 38, Loss: 0.5345027446746826\n",
      "Iteration 33, Batch: 39, Loss: 0.5384239554405212\n",
      "Iteration 33, Batch: 40, Loss: 0.532384991645813\n",
      "Iteration 33, Batch: 41, Loss: 0.5253629684448242\n",
      "Iteration 33, Batch: 42, Loss: 0.4927065968513489\n",
      "Iteration 33, Batch: 43, Loss: 0.477740615606308\n",
      "Iteration 33, Batch: 44, Loss: 0.5155044198036194\n",
      "Iteration 33, Batch: 45, Loss: 0.49852997064590454\n",
      "Iteration 33, Batch: 46, Loss: 0.4827613830566406\n",
      "Iteration 33, Batch: 47, Loss: 0.5531883239746094\n",
      "Iteration 33, Batch: 48, Loss: 0.492705762386322\n",
      "Iteration 33, Batch: 49, Loss: 0.5270460247993469\n",
      "Avg loss: 0.49620248258113864\n",
      "Iteration 34, Batch: 0, Loss: 0.5514331459999084\n",
      "Iteration 34, Batch: 1, Loss: 0.47470006346702576\n",
      "Iteration 34, Batch: 2, Loss: 0.45829513669013977\n",
      "Iteration 34, Batch: 3, Loss: 0.4885619878768921\n",
      "Iteration 34, Batch: 4, Loss: 0.5250257253646851\n",
      "Iteration 34, Batch: 5, Loss: 0.45122966170310974\n",
      "Iteration 34, Batch: 6, Loss: 0.4794570505619049\n",
      "Iteration 34, Batch: 7, Loss: 0.49470052123069763\n",
      "Iteration 34, Batch: 8, Loss: 0.4409816563129425\n",
      "Iteration 34, Batch: 9, Loss: 0.5162634253501892\n",
      "Iteration 34, Batch: 10, Loss: 0.46547985076904297\n",
      "Iteration 34, Batch: 11, Loss: 0.4956061542034149\n",
      "Iteration 34, Batch: 12, Loss: 0.5004590749740601\n",
      "Iteration 34, Batch: 13, Loss: 0.5393510460853577\n",
      "Iteration 34, Batch: 14, Loss: 0.5250682234764099\n",
      "Iteration 34, Batch: 15, Loss: 0.5109013915061951\n",
      "Iteration 34, Batch: 16, Loss: 0.44717609882354736\n",
      "Iteration 34, Batch: 17, Loss: 0.4336318373680115\n",
      "Iteration 34, Batch: 18, Loss: 0.49897444248199463\n",
      "Iteration 34, Batch: 19, Loss: 0.5436574220657349\n",
      "Iteration 34, Batch: 20, Loss: 0.5350046157836914\n",
      "Iteration 34, Batch: 21, Loss: 0.5160455703735352\n",
      "Iteration 34, Batch: 22, Loss: 0.5124778747558594\n",
      "Iteration 34, Batch: 23, Loss: 0.43807485699653625\n",
      "Iteration 34, Batch: 24, Loss: 0.5238522291183472\n",
      "Iteration 34, Batch: 25, Loss: 0.5522695779800415\n",
      "Iteration 34, Batch: 26, Loss: 0.5015842318534851\n",
      "Iteration 34, Batch: 27, Loss: 0.5287336111068726\n",
      "Iteration 34, Batch: 28, Loss: 0.497257798910141\n",
      "Iteration 34, Batch: 29, Loss: 0.5004276037216187\n",
      "Iteration 34, Batch: 30, Loss: 0.4859451949596405\n",
      "Iteration 34, Batch: 31, Loss: 0.46626242995262146\n",
      "Iteration 34, Batch: 32, Loss: 0.47991666197776794\n",
      "Iteration 34, Batch: 33, Loss: 0.5073683857917786\n",
      "Iteration 34, Batch: 34, Loss: 0.5045415163040161\n",
      "Iteration 34, Batch: 35, Loss: 0.4510323107242584\n",
      "Iteration 34, Batch: 36, Loss: 0.553833544254303\n",
      "Iteration 34, Batch: 37, Loss: 0.49245601892471313\n",
      "Iteration 34, Batch: 38, Loss: 0.477477490901947\n",
      "Iteration 34, Batch: 39, Loss: 0.43485721945762634\n",
      "Iteration 34, Batch: 40, Loss: 0.49087414145469666\n",
      "Iteration 34, Batch: 41, Loss: 0.4850550591945648\n",
      "Iteration 34, Batch: 42, Loss: 0.5326189398765564\n",
      "Iteration 34, Batch: 43, Loss: 0.48765647411346436\n",
      "Iteration 34, Batch: 44, Loss: 0.5082876682281494\n",
      "Iteration 34, Batch: 45, Loss: 0.5486239790916443\n",
      "Iteration 34, Batch: 46, Loss: 0.5042611360549927\n",
      "Iteration 34, Batch: 47, Loss: 0.46937376260757446\n",
      "Iteration 34, Batch: 48, Loss: 0.4780544638633728\n",
      "Iteration 34, Batch: 49, Loss: 0.5279831290245056\n",
      "Avg loss: 0.4966632282733917\n",
      "Iteration 35, Batch: 0, Loss: 0.4877515733242035\n",
      "Iteration 35, Batch: 1, Loss: 0.4693380296230316\n",
      "Iteration 35, Batch: 2, Loss: 0.5322996973991394\n",
      "Iteration 35, Batch: 3, Loss: 0.44516199827194214\n",
      "Iteration 35, Batch: 4, Loss: 0.5283564925193787\n",
      "Iteration 35, Batch: 5, Loss: 0.4279964864253998\n",
      "Iteration 35, Batch: 6, Loss: 0.4930152893066406\n",
      "Iteration 35, Batch: 7, Loss: 0.5291140675544739\n",
      "Iteration 35, Batch: 8, Loss: 0.4440646767616272\n",
      "Iteration 35, Batch: 9, Loss: 0.5981490015983582\n",
      "Iteration 35, Batch: 10, Loss: 0.5196656584739685\n",
      "Iteration 35, Batch: 11, Loss: 0.44887882471084595\n",
      "Iteration 35, Batch: 12, Loss: 0.4881856441497803\n",
      "Iteration 35, Batch: 13, Loss: 0.4491875469684601\n",
      "Iteration 35, Batch: 14, Loss: 0.5121944546699524\n",
      "Iteration 35, Batch: 15, Loss: 0.49810752272605896\n",
      "Iteration 35, Batch: 16, Loss: 0.5066002011299133\n",
      "Iteration 35, Batch: 17, Loss: 0.46849215030670166\n",
      "Iteration 35, Batch: 18, Loss: 0.4989297389984131\n",
      "Iteration 35, Batch: 19, Loss: 0.47316381335258484\n",
      "Iteration 35, Batch: 20, Loss: 0.5331442952156067\n",
      "Iteration 35, Batch: 21, Loss: 0.4708169102668762\n",
      "Iteration 35, Batch: 22, Loss: 0.4977368116378784\n",
      "Iteration 35, Batch: 23, Loss: 0.4680275321006775\n",
      "Iteration 35, Batch: 24, Loss: 0.48733171820640564\n",
      "Iteration 35, Batch: 25, Loss: 0.48309585452079773\n",
      "Iteration 35, Batch: 26, Loss: 0.5173291563987732\n",
      "Iteration 35, Batch: 27, Loss: 0.5177373290061951\n",
      "Iteration 35, Batch: 28, Loss: 0.5603339076042175\n",
      "Iteration 35, Batch: 29, Loss: 0.5148089528083801\n",
      "Iteration 35, Batch: 30, Loss: 0.48547452688217163\n",
      "Iteration 35, Batch: 31, Loss: 0.49264344573020935\n",
      "Iteration 35, Batch: 32, Loss: 0.5134685635566711\n",
      "Iteration 35, Batch: 33, Loss: 0.507095456123352\n",
      "Iteration 35, Batch: 34, Loss: 0.48833656311035156\n",
      "Iteration 35, Batch: 35, Loss: 0.5178683400154114\n",
      "Iteration 35, Batch: 36, Loss: 0.5030680894851685\n",
      "Iteration 35, Batch: 37, Loss: 0.5093815326690674\n",
      "Iteration 35, Batch: 38, Loss: 0.5534175038337708\n",
      "Iteration 35, Batch: 39, Loss: 0.4870431423187256\n",
      "Iteration 35, Batch: 40, Loss: 0.48834532499313354\n",
      "Iteration 35, Batch: 41, Loss: 0.4829159379005432\n",
      "Iteration 35, Batch: 42, Loss: 0.5468827486038208\n",
      "Iteration 35, Batch: 43, Loss: 0.49367135763168335\n",
      "Iteration 35, Batch: 44, Loss: 0.5406568050384521\n",
      "Iteration 35, Batch: 45, Loss: 0.5263971090316772\n",
      "Iteration 35, Batch: 46, Loss: 0.5823407769203186\n",
      "Iteration 35, Batch: 47, Loss: 0.4790794253349304\n",
      "Iteration 35, Batch: 48, Loss: 0.5185540914535522\n",
      "Iteration 35, Batch: 49, Loss: 0.5189260840415955\n",
      "Avg loss: 0.5020916432142257\n",
      "Iteration 36, Batch: 0, Loss: 0.45408350229263306\n",
      "Iteration 36, Batch: 1, Loss: 0.45976579189300537\n",
      "Iteration 36, Batch: 2, Loss: 0.5580328702926636\n",
      "Iteration 36, Batch: 3, Loss: 0.5192422270774841\n",
      "Iteration 36, Batch: 4, Loss: 0.47291502356529236\n",
      "Iteration 36, Batch: 5, Loss: 0.48802992701530457\n",
      "Iteration 36, Batch: 6, Loss: 0.47861045598983765\n",
      "Iteration 36, Batch: 7, Loss: 0.5077342987060547\n",
      "Iteration 36, Batch: 8, Loss: 0.48648542165756226\n",
      "Iteration 36, Batch: 9, Loss: 0.496670663356781\n",
      "Iteration 36, Batch: 10, Loss: 0.44209104776382446\n",
      "Iteration 36, Batch: 11, Loss: 0.522144615650177\n",
      "Iteration 36, Batch: 12, Loss: 0.5089590549468994\n",
      "Iteration 36, Batch: 13, Loss: 0.551942765712738\n",
      "Iteration 36, Batch: 14, Loss: 0.4687526524066925\n",
      "Iteration 36, Batch: 15, Loss: 0.533774197101593\n",
      "Iteration 36, Batch: 16, Loss: 0.5832058787345886\n",
      "Iteration 36, Batch: 17, Loss: 0.5553333163261414\n",
      "Iteration 36, Batch: 18, Loss: 0.49836134910583496\n",
      "Iteration 36, Batch: 19, Loss: 0.4730844497680664\n",
      "Iteration 36, Batch: 20, Loss: 0.47750985622406006\n",
      "Iteration 36, Batch: 21, Loss: 0.5023692846298218\n",
      "Iteration 36, Batch: 22, Loss: 0.5582502484321594\n",
      "Iteration 36, Batch: 23, Loss: 0.45909368991851807\n",
      "Iteration 36, Batch: 24, Loss: 0.4808749258518219\n",
      "Iteration 36, Batch: 25, Loss: 0.501261830329895\n",
      "Iteration 36, Batch: 26, Loss: 0.5466234087944031\n",
      "Iteration 36, Batch: 27, Loss: 0.5016005039215088\n",
      "Iteration 36, Batch: 28, Loss: 0.5074187517166138\n",
      "Iteration 36, Batch: 29, Loss: 0.4532351493835449\n",
      "Iteration 36, Batch: 30, Loss: 0.4336453080177307\n",
      "Iteration 36, Batch: 31, Loss: 0.48612692952156067\n",
      "Iteration 36, Batch: 32, Loss: 0.5577890872955322\n",
      "Iteration 36, Batch: 33, Loss: 0.48641839623451233\n",
      "Iteration 36, Batch: 34, Loss: 0.48980170488357544\n",
      "Iteration 36, Batch: 35, Loss: 0.5084213018417358\n",
      "Iteration 36, Batch: 36, Loss: 0.44358623027801514\n",
      "Iteration 36, Batch: 37, Loss: 0.5411763191223145\n",
      "Iteration 36, Batch: 38, Loss: 0.5517634749412537\n",
      "Iteration 36, Batch: 39, Loss: 0.44450297951698303\n",
      "Iteration 36, Batch: 40, Loss: 0.5425599813461304\n",
      "Iteration 36, Batch: 41, Loss: 0.5351151823997498\n",
      "Iteration 36, Batch: 42, Loss: 0.49400466680526733\n",
      "Iteration 36, Batch: 43, Loss: 0.5023011565208435\n",
      "Iteration 36, Batch: 44, Loss: 0.4914081394672394\n",
      "Iteration 36, Batch: 45, Loss: 0.5478548407554626\n",
      "Iteration 36, Batch: 46, Loss: 0.48536837100982666\n",
      "Iteration 36, Batch: 47, Loss: 0.4881044030189514\n",
      "Iteration 36, Batch: 48, Loss: 0.4885804355144501\n",
      "Iteration 36, Batch: 49, Loss: 0.5108945965766907\n",
      "Avg loss: 0.5015376132726669\n",
      "Iteration 37, Batch: 0, Loss: 0.4317491054534912\n",
      "Iteration 37, Batch: 1, Loss: 0.5562133193016052\n",
      "Iteration 37, Batch: 2, Loss: 0.5529131293296814\n",
      "Iteration 37, Batch: 3, Loss: 0.5025073885917664\n",
      "Iteration 37, Batch: 4, Loss: 0.5029495358467102\n",
      "Iteration 37, Batch: 5, Loss: 0.48796316981315613\n",
      "Iteration 37, Batch: 6, Loss: 0.4979656934738159\n",
      "Iteration 37, Batch: 7, Loss: 0.4580095708370209\n",
      "Iteration 37, Batch: 8, Loss: 0.5424249172210693\n",
      "Iteration 37, Batch: 9, Loss: 0.4881349205970764\n",
      "Iteration 37, Batch: 10, Loss: 0.4917413890361786\n",
      "Iteration 37, Batch: 11, Loss: 0.44672897458076477\n",
      "Iteration 37, Batch: 12, Loss: 0.5320664048194885\n",
      "Iteration 37, Batch: 13, Loss: 0.49671730399131775\n",
      "Iteration 37, Batch: 14, Loss: 0.5269792675971985\n",
      "Iteration 37, Batch: 15, Loss: 0.5009781718254089\n",
      "Iteration 37, Batch: 16, Loss: 0.4730031490325928\n",
      "Iteration 37, Batch: 17, Loss: 0.5021002292633057\n",
      "Iteration 37, Batch: 18, Loss: 0.5087043642997742\n",
      "Iteration 37, Batch: 19, Loss: 0.514739990234375\n",
      "Iteration 37, Batch: 20, Loss: 0.5569554567337036\n",
      "Iteration 37, Batch: 21, Loss: 0.45916301012039185\n",
      "Iteration 37, Batch: 22, Loss: 0.5293301939964294\n",
      "Iteration 37, Batch: 23, Loss: 0.5476519465446472\n",
      "Iteration 37, Batch: 24, Loss: 0.5501233339309692\n",
      "Iteration 37, Batch: 25, Loss: 0.5194481015205383\n",
      "Iteration 37, Batch: 26, Loss: 0.47275274991989136\n",
      "Iteration 37, Batch: 27, Loss: 0.4938841760158539\n",
      "Iteration 37, Batch: 28, Loss: 0.4872371256351471\n",
      "Iteration 37, Batch: 29, Loss: 0.45728567242622375\n",
      "Iteration 37, Batch: 30, Loss: 0.5635568499565125\n",
      "Iteration 37, Batch: 31, Loss: 0.4985854923725128\n",
      "Iteration 37, Batch: 32, Loss: 0.5131651163101196\n",
      "Iteration 37, Batch: 33, Loss: 0.41221940517425537\n",
      "Iteration 37, Batch: 34, Loss: 0.49262046813964844\n",
      "Iteration 37, Batch: 35, Loss: 0.4772079288959503\n",
      "Iteration 37, Batch: 36, Loss: 0.4590098559856415\n",
      "Iteration 37, Batch: 37, Loss: 0.4894055724143982\n",
      "Iteration 37, Batch: 38, Loss: 0.558275043964386\n",
      "Iteration 37, Batch: 39, Loss: 0.5091312527656555\n",
      "Iteration 37, Batch: 40, Loss: 0.5620021820068359\n",
      "Iteration 37, Batch: 41, Loss: 0.4740954339504242\n",
      "Iteration 37, Batch: 42, Loss: 0.46324172616004944\n",
      "Iteration 37, Batch: 43, Loss: 0.4922676682472229\n",
      "Iteration 37, Batch: 44, Loss: 0.5600920915603638\n",
      "Iteration 37, Batch: 45, Loss: 0.47145959734916687\n",
      "Iteration 37, Batch: 46, Loss: 0.4780564308166504\n",
      "Iteration 37, Batch: 47, Loss: 0.5380281209945679\n",
      "Iteration 37, Batch: 48, Loss: 0.4568539261817932\n",
      "Iteration 37, Batch: 49, Loss: 0.5017079710960388\n",
      "Avg loss: 0.5011880779266358\n",
      "Iteration 38, Batch: 0, Loss: 0.4626934230327606\n",
      "Iteration 38, Batch: 1, Loss: 0.46634960174560547\n",
      "Iteration 38, Batch: 2, Loss: 0.5071588158607483\n",
      "Iteration 38, Batch: 3, Loss: 0.4767693281173706\n",
      "Iteration 38, Batch: 4, Loss: 0.5483832955360413\n",
      "Iteration 38, Batch: 5, Loss: 0.49827468395233154\n",
      "Iteration 38, Batch: 6, Loss: 0.5330740809440613\n",
      "Iteration 38, Batch: 7, Loss: 0.5016893148422241\n",
      "Iteration 38, Batch: 8, Loss: 0.46751654148101807\n",
      "Iteration 38, Batch: 9, Loss: 0.46385428309440613\n",
      "Iteration 38, Batch: 10, Loss: 0.5924440622329712\n",
      "Iteration 38, Batch: 11, Loss: 0.4719793200492859\n",
      "Iteration 38, Batch: 12, Loss: 0.523459792137146\n",
      "Iteration 38, Batch: 13, Loss: 0.5519527792930603\n",
      "Iteration 38, Batch: 14, Loss: 0.46817946434020996\n",
      "Iteration 38, Batch: 15, Loss: 0.512919545173645\n",
      "Iteration 38, Batch: 16, Loss: 0.4526442587375641\n",
      "Iteration 38, Batch: 17, Loss: 0.5815199017524719\n",
      "Iteration 38, Batch: 18, Loss: 0.5108504891395569\n",
      "Iteration 38, Batch: 19, Loss: 0.4913334548473358\n",
      "Iteration 38, Batch: 20, Loss: 0.46711820363998413\n",
      "Iteration 38, Batch: 21, Loss: 0.5083322525024414\n",
      "Iteration 38, Batch: 22, Loss: 0.4669717252254486\n",
      "Iteration 38, Batch: 23, Loss: 0.5260051488876343\n",
      "Iteration 38, Batch: 24, Loss: 0.5624508261680603\n",
      "Iteration 38, Batch: 25, Loss: 0.4762171804904938\n",
      "Iteration 38, Batch: 26, Loss: 0.5799336433410645\n",
      "Iteration 38, Batch: 27, Loss: 0.49169614911079407\n",
      "Iteration 38, Batch: 28, Loss: 0.4314902126789093\n",
      "Iteration 38, Batch: 29, Loss: 0.4377017915248871\n",
      "Iteration 38, Batch: 30, Loss: 0.45545828342437744\n",
      "Iteration 38, Batch: 31, Loss: 0.5037652254104614\n",
      "Iteration 38, Batch: 32, Loss: 0.5552144646644592\n",
      "Iteration 38, Batch: 33, Loss: 0.5272184610366821\n",
      "Iteration 38, Batch: 34, Loss: 0.5711310505867004\n",
      "Iteration 38, Batch: 35, Loss: 0.498092383146286\n",
      "Iteration 38, Batch: 36, Loss: 0.4762614965438843\n",
      "Iteration 38, Batch: 37, Loss: 0.5307518839836121\n",
      "Iteration 38, Batch: 38, Loss: 0.5156236290931702\n",
      "Iteration 38, Batch: 39, Loss: 0.47659337520599365\n",
      "Iteration 38, Batch: 40, Loss: 0.47012704610824585\n",
      "Iteration 38, Batch: 41, Loss: 0.48079362511634827\n",
      "Iteration 38, Batch: 42, Loss: 0.46345052123069763\n",
      "Iteration 38, Batch: 43, Loss: 0.4470674395561218\n",
      "Iteration 38, Batch: 44, Loss: 0.4848014712333679\n",
      "Iteration 38, Batch: 45, Loss: 0.4956556558609009\n",
      "Iteration 38, Batch: 46, Loss: 0.4489569067955017\n",
      "Iteration 38, Batch: 47, Loss: 0.5249294638633728\n",
      "Iteration 38, Batch: 48, Loss: 0.4895625114440918\n",
      "Iteration 38, Batch: 49, Loss: 0.5560674667358398\n",
      "Avg loss: 0.500049718618393\n",
      "Iteration 39, Batch: 0, Loss: 0.5528395175933838\n",
      "Iteration 39, Batch: 1, Loss: 0.5278069972991943\n",
      "Iteration 39, Batch: 2, Loss: 0.503787636756897\n",
      "Iteration 39, Batch: 3, Loss: 0.5694003701210022\n",
      "Iteration 39, Batch: 4, Loss: 0.485880583524704\n",
      "Iteration 39, Batch: 5, Loss: 0.5007752776145935\n",
      "Iteration 39, Batch: 6, Loss: 0.40823301672935486\n",
      "Iteration 39, Batch: 7, Loss: 0.5460777282714844\n",
      "Iteration 39, Batch: 8, Loss: 0.4797399938106537\n",
      "Iteration 39, Batch: 9, Loss: 0.46288737654685974\n",
      "Iteration 39, Batch: 10, Loss: 0.5184778571128845\n",
      "Iteration 39, Batch: 11, Loss: 0.4682186543941498\n",
      "Iteration 39, Batch: 12, Loss: 0.46112990379333496\n",
      "Iteration 39, Batch: 13, Loss: 0.45908766984939575\n",
      "Iteration 39, Batch: 14, Loss: 0.5139325857162476\n",
      "Iteration 39, Batch: 15, Loss: 0.48693063855171204\n",
      "Iteration 39, Batch: 16, Loss: 0.4481857120990753\n",
      "Iteration 39, Batch: 17, Loss: 0.5306047201156616\n",
      "Iteration 39, Batch: 18, Loss: 0.4862506091594696\n",
      "Iteration 39, Batch: 19, Loss: 0.4621013402938843\n",
      "Iteration 39, Batch: 20, Loss: 0.5229694247245789\n",
      "Iteration 39, Batch: 21, Loss: 0.4568565785884857\n",
      "Iteration 39, Batch: 22, Loss: 0.4981931447982788\n",
      "Iteration 39, Batch: 23, Loss: 0.4939037263393402\n",
      "Iteration 39, Batch: 24, Loss: 0.4769406020641327\n",
      "Iteration 39, Batch: 25, Loss: 0.4623830020427704\n",
      "Iteration 39, Batch: 26, Loss: 0.532755970954895\n",
      "Iteration 39, Batch: 27, Loss: 0.5010432600975037\n",
      "Iteration 39, Batch: 28, Loss: 0.5483734607696533\n",
      "Iteration 39, Batch: 29, Loss: 0.49327346682548523\n",
      "Iteration 39, Batch: 30, Loss: 0.49556276202201843\n",
      "Iteration 39, Batch: 31, Loss: 0.4638003408908844\n",
      "Iteration 39, Batch: 32, Loss: 0.5239521265029907\n",
      "Iteration 39, Batch: 33, Loss: 0.41898828744888306\n",
      "Iteration 39, Batch: 34, Loss: 0.44565436244010925\n",
      "Iteration 39, Batch: 35, Loss: 0.4895576238632202\n",
      "Iteration 39, Batch: 36, Loss: 0.49235114455223083\n",
      "Iteration 39, Batch: 37, Loss: 0.49069803953170776\n",
      "Iteration 39, Batch: 38, Loss: 0.4608004689216614\n",
      "Iteration 39, Batch: 39, Loss: 0.5099992156028748\n",
      "Iteration 39, Batch: 40, Loss: 0.5306480526924133\n",
      "Iteration 39, Batch: 41, Loss: 0.4536043107509613\n",
      "Iteration 39, Batch: 42, Loss: 0.4856594502925873\n",
      "Iteration 39, Batch: 43, Loss: 0.5032329559326172\n",
      "Iteration 39, Batch: 44, Loss: 0.43166282773017883\n",
      "Iteration 39, Batch: 45, Loss: 0.5190275311470032\n",
      "Iteration 39, Batch: 46, Loss: 0.49948447942733765\n",
      "Iteration 39, Batch: 47, Loss: 0.4810475707054138\n",
      "Iteration 39, Batch: 48, Loss: 0.5046620965003967\n",
      "Iteration 39, Batch: 49, Loss: 0.5186057090759277\n",
      "Avg loss: 0.4915608036518097\n",
      "Iteration 40, Batch: 0, Loss: 0.5179668068885803\n",
      "Iteration 40, Batch: 1, Loss: 0.461399644613266\n",
      "Iteration 40, Batch: 2, Loss: 0.4453819990158081\n",
      "Iteration 40, Batch: 3, Loss: 0.433181494474411\n",
      "Iteration 40, Batch: 4, Loss: 0.47254958748817444\n",
      "Iteration 40, Batch: 5, Loss: 0.5660529732704163\n",
      "Iteration 40, Batch: 6, Loss: 0.533573567867279\n",
      "Iteration 40, Batch: 7, Loss: 0.458882600069046\n",
      "Iteration 40, Batch: 8, Loss: 0.4926120638847351\n",
      "Iteration 40, Batch: 9, Loss: 0.46589627861976624\n",
      "Iteration 40, Batch: 10, Loss: 0.42977073788642883\n",
      "Iteration 40, Batch: 11, Loss: 0.46824729442596436\n",
      "Iteration 40, Batch: 12, Loss: 0.5422160029411316\n",
      "Iteration 40, Batch: 13, Loss: 0.562390148639679\n",
      "Iteration 40, Batch: 14, Loss: 0.5058965682983398\n",
      "Iteration 40, Batch: 15, Loss: 0.4564271569252014\n",
      "Iteration 40, Batch: 16, Loss: 0.48305609822273254\n",
      "Iteration 40, Batch: 17, Loss: 0.5220645666122437\n",
      "Iteration 40, Batch: 18, Loss: 0.5188320279121399\n",
      "Iteration 40, Batch: 19, Loss: 0.47960206866264343\n",
      "Iteration 40, Batch: 20, Loss: 0.50164794921875\n",
      "Iteration 40, Batch: 21, Loss: 0.5438868403434753\n",
      "Iteration 40, Batch: 22, Loss: 0.4888871908187866\n",
      "Iteration 40, Batch: 23, Loss: 0.49020060896873474\n",
      "Iteration 40, Batch: 24, Loss: 0.4746530055999756\n",
      "Iteration 40, Batch: 25, Loss: 0.5184432864189148\n",
      "Iteration 40, Batch: 26, Loss: 0.4532168507575989\n",
      "Iteration 40, Batch: 27, Loss: 0.5355878472328186\n",
      "Iteration 40, Batch: 28, Loss: 0.4704117476940155\n",
      "Iteration 40, Batch: 29, Loss: 0.4851851463317871\n",
      "Iteration 40, Batch: 30, Loss: 0.44892874360084534\n",
      "Iteration 40, Batch: 31, Loss: 0.5374687314033508\n",
      "Iteration 40, Batch: 32, Loss: 0.5653281211853027\n",
      "Iteration 40, Batch: 33, Loss: 0.44176244735717773\n",
      "Iteration 40, Batch: 34, Loss: 0.43241989612579346\n",
      "Iteration 40, Batch: 35, Loss: 0.5146602392196655\n",
      "Iteration 40, Batch: 36, Loss: 0.4603740870952606\n",
      "Iteration 40, Batch: 37, Loss: 0.5408048629760742\n",
      "Iteration 40, Batch: 38, Loss: 0.4194906949996948\n",
      "Iteration 40, Batch: 39, Loss: 0.4735594391822815\n",
      "Iteration 40, Batch: 40, Loss: 0.5406953692436218\n",
      "Iteration 40, Batch: 41, Loss: 0.43120262026786804\n",
      "Iteration 40, Batch: 42, Loss: 0.545592725276947\n",
      "Iteration 40, Batch: 43, Loss: 0.48082518577575684\n",
      "Iteration 40, Batch: 44, Loss: 0.5122466087341309\n",
      "Iteration 40, Batch: 45, Loss: 0.5145604014396667\n",
      "Iteration 40, Batch: 46, Loss: 0.5052074790000916\n",
      "Iteration 40, Batch: 47, Loss: 0.4587383270263672\n",
      "Iteration 40, Batch: 48, Loss: 0.46795782446861267\n",
      "Iteration 40, Batch: 49, Loss: 0.47922980785369873\n",
      "Avg loss: 0.49098347544670107\n",
      "Iteration 41, Batch: 0, Loss: 0.5540928840637207\n",
      "Iteration 41, Batch: 1, Loss: 0.4399009346961975\n",
      "Iteration 41, Batch: 2, Loss: 0.45659056305885315\n",
      "Iteration 41, Batch: 3, Loss: 0.5475142598152161\n",
      "Iteration 41, Batch: 4, Loss: 0.4668038487434387\n",
      "Iteration 41, Batch: 5, Loss: 0.4906691014766693\n",
      "Iteration 41, Batch: 6, Loss: 0.5012499094009399\n",
      "Iteration 41, Batch: 7, Loss: 0.5019506216049194\n",
      "Iteration 41, Batch: 8, Loss: 0.5449705123901367\n",
      "Iteration 41, Batch: 9, Loss: 0.45501935482025146\n",
      "Iteration 41, Batch: 10, Loss: 0.5222498774528503\n",
      "Iteration 41, Batch: 11, Loss: 0.4739592671394348\n",
      "Iteration 41, Batch: 12, Loss: 0.5510106682777405\n",
      "Iteration 41, Batch: 13, Loss: 0.49463945627212524\n",
      "Iteration 41, Batch: 14, Loss: 0.4322076737880707\n",
      "Iteration 41, Batch: 15, Loss: 0.42664897441864014\n",
      "Iteration 41, Batch: 16, Loss: 0.46637222170829773\n",
      "Iteration 41, Batch: 17, Loss: 0.52511066198349\n",
      "Iteration 41, Batch: 18, Loss: 0.512468695640564\n",
      "Iteration 41, Batch: 19, Loss: 0.5158902406692505\n",
      "Iteration 41, Batch: 20, Loss: 0.4841592311859131\n",
      "Iteration 41, Batch: 21, Loss: 0.48729032278060913\n",
      "Iteration 41, Batch: 22, Loss: 0.4813156723976135\n",
      "Iteration 41, Batch: 23, Loss: 0.5178762674331665\n",
      "Iteration 41, Batch: 24, Loss: 0.4429166615009308\n",
      "Iteration 41, Batch: 25, Loss: 0.4325820207595825\n",
      "Iteration 41, Batch: 26, Loss: 0.4770924150943756\n",
      "Iteration 41, Batch: 27, Loss: 0.4903980791568756\n",
      "Iteration 41, Batch: 28, Loss: 0.4725539982318878\n",
      "Iteration 41, Batch: 29, Loss: 0.4107547700405121\n",
      "Iteration 41, Batch: 30, Loss: 0.48867067694664\n",
      "Iteration 41, Batch: 31, Loss: 0.4597128927707672\n",
      "Iteration 41, Batch: 32, Loss: 0.4482389986515045\n",
      "Iteration 41, Batch: 33, Loss: 0.5114239454269409\n",
      "Iteration 41, Batch: 34, Loss: 0.4747253954410553\n",
      "Iteration 41, Batch: 35, Loss: 0.4270457327365875\n",
      "Iteration 41, Batch: 36, Loss: 0.4465295672416687\n",
      "Iteration 41, Batch: 37, Loss: 0.5060857534408569\n",
      "Iteration 41, Batch: 38, Loss: 0.4582636058330536\n",
      "Iteration 41, Batch: 39, Loss: 0.43437302112579346\n",
      "Iteration 41, Batch: 40, Loss: 0.4319329857826233\n",
      "Iteration 41, Batch: 41, Loss: 0.43263164162635803\n",
      "Iteration 41, Batch: 42, Loss: 0.4899350106716156\n",
      "Iteration 41, Batch: 43, Loss: 0.4805651605129242\n",
      "Iteration 41, Batch: 44, Loss: 0.5472906231880188\n",
      "Iteration 41, Batch: 45, Loss: 0.46434035897254944\n",
      "Iteration 41, Batch: 46, Loss: 0.46401503682136536\n",
      "Iteration 41, Batch: 47, Loss: 0.45910724997520447\n",
      "Iteration 41, Batch: 48, Loss: 0.5316031575202942\n",
      "Iteration 41, Batch: 49, Loss: 0.4151459336280823\n",
      "Avg loss: 0.47895791828632356\n",
      "Iteration 42, Batch: 0, Loss: 0.5242676138877869\n",
      "Iteration 42, Batch: 1, Loss: 0.4619160294532776\n",
      "Iteration 42, Batch: 2, Loss: 0.5035067796707153\n",
      "Iteration 42, Batch: 3, Loss: 0.4859345257282257\n",
      "Iteration 42, Batch: 4, Loss: 0.4715433418750763\n",
      "Iteration 42, Batch: 5, Loss: 0.38565924763679504\n",
      "Iteration 42, Batch: 6, Loss: 0.47962379455566406\n",
      "Iteration 42, Batch: 7, Loss: 0.5263406038284302\n",
      "Iteration 42, Batch: 8, Loss: 0.45948928594589233\n",
      "Iteration 42, Batch: 9, Loss: 0.4881238043308258\n",
      "Iteration 42, Batch: 10, Loss: 0.449044793844223\n",
      "Iteration 42, Batch: 11, Loss: 0.5051468014717102\n",
      "Iteration 42, Batch: 12, Loss: 0.46051156520843506\n",
      "Iteration 42, Batch: 13, Loss: 0.4469227194786072\n",
      "Iteration 42, Batch: 14, Loss: 0.4441656470298767\n",
      "Iteration 42, Batch: 15, Loss: 0.4645821750164032\n",
      "Iteration 42, Batch: 16, Loss: 0.43258655071258545\n",
      "Iteration 42, Batch: 17, Loss: 0.516700029373169\n",
      "Iteration 42, Batch: 18, Loss: 0.5064972639083862\n",
      "Iteration 42, Batch: 19, Loss: 0.4046401083469391\n",
      "Iteration 42, Batch: 20, Loss: 0.3880687355995178\n",
      "Iteration 42, Batch: 21, Loss: 0.4141891300678253\n",
      "Iteration 42, Batch: 22, Loss: 0.531520664691925\n",
      "Iteration 42, Batch: 23, Loss: 0.4098466634750366\n",
      "Iteration 42, Batch: 24, Loss: 0.4345635175704956\n",
      "Iteration 42, Batch: 25, Loss: 0.4830440878868103\n",
      "Iteration 42, Batch: 26, Loss: 0.4772912859916687\n",
      "Iteration 42, Batch: 27, Loss: 0.4243754744529724\n",
      "Iteration 42, Batch: 28, Loss: 0.5225772857666016\n",
      "Iteration 42, Batch: 29, Loss: 0.45060020685195923\n",
      "Iteration 42, Batch: 30, Loss: 0.42685189843177795\n",
      "Iteration 42, Batch: 31, Loss: 0.4770887792110443\n",
      "Iteration 42, Batch: 32, Loss: 0.5085409283638\n",
      "Iteration 42, Batch: 33, Loss: 0.4388556480407715\n",
      "Iteration 42, Batch: 34, Loss: 0.45608171820640564\n",
      "Iteration 42, Batch: 35, Loss: 0.5023771524429321\n",
      "Iteration 42, Batch: 36, Loss: 0.426437646150589\n",
      "Iteration 42, Batch: 37, Loss: 0.439066618680954\n",
      "Iteration 42, Batch: 38, Loss: 0.4912935495376587\n",
      "Iteration 42, Batch: 39, Loss: 0.47106409072875977\n",
      "Iteration 42, Batch: 40, Loss: 0.4672132134437561\n",
      "Iteration 42, Batch: 41, Loss: 0.45379066467285156\n",
      "Iteration 42, Batch: 42, Loss: 0.46169957518577576\n",
      "Iteration 42, Batch: 43, Loss: 0.4385768175125122\n",
      "Iteration 42, Batch: 44, Loss: 0.5077741742134094\n",
      "Iteration 42, Batch: 45, Loss: 0.44401815533638\n",
      "Iteration 42, Batch: 46, Loss: 0.4805554747581482\n",
      "Iteration 42, Batch: 47, Loss: 0.5034483671188354\n",
      "Iteration 42, Batch: 48, Loss: 0.4523266553878784\n",
      "Iteration 42, Batch: 49, Loss: 0.41117140650749207\n",
      "Avg loss: 0.46423024535179136\n",
      "Iteration 43, Batch: 0, Loss: 0.46547508239746094\n",
      "Iteration 43, Batch: 1, Loss: 0.4334139823913574\n",
      "Iteration 43, Batch: 2, Loss: 0.4927080571651459\n",
      "Iteration 43, Batch: 3, Loss: 0.49311503767967224\n",
      "Iteration 43, Batch: 4, Loss: 0.4070223569869995\n",
      "Iteration 43, Batch: 5, Loss: 0.45403945446014404\n",
      "Iteration 43, Batch: 6, Loss: 0.4765077233314514\n",
      "Iteration 43, Batch: 7, Loss: 0.4427100718021393\n",
      "Iteration 43, Batch: 8, Loss: 0.530089259147644\n",
      "Iteration 43, Batch: 9, Loss: 0.4511418044567108\n",
      "Iteration 43, Batch: 10, Loss: 0.4527071416378021\n",
      "Iteration 43, Batch: 11, Loss: 0.4535597264766693\n",
      "Iteration 43, Batch: 12, Loss: 0.4820483326911926\n",
      "Iteration 43, Batch: 13, Loss: 0.5325496792793274\n",
      "Iteration 43, Batch: 14, Loss: 0.45931288599967957\n",
      "Iteration 43, Batch: 15, Loss: 0.41444075107574463\n",
      "Iteration 43, Batch: 16, Loss: 0.393795907497406\n",
      "Iteration 43, Batch: 17, Loss: 0.470184326171875\n",
      "Iteration 43, Batch: 18, Loss: 0.399312287569046\n",
      "Iteration 43, Batch: 19, Loss: 0.42878371477127075\n",
      "Iteration 43, Batch: 20, Loss: 0.5364795327186584\n",
      "Iteration 43, Batch: 21, Loss: 0.4522491991519928\n",
      "Iteration 43, Batch: 22, Loss: 0.4195235073566437\n",
      "Iteration 43, Batch: 23, Loss: 0.4077548086643219\n",
      "Iteration 43, Batch: 24, Loss: 0.38123640418052673\n",
      "Iteration 43, Batch: 25, Loss: 0.46637868881225586\n",
      "Iteration 43, Batch: 26, Loss: 0.4296658933162689\n",
      "Iteration 43, Batch: 27, Loss: 0.38933244347572327\n",
      "Iteration 43, Batch: 28, Loss: 0.4331144094467163\n",
      "Iteration 43, Batch: 29, Loss: 0.41905200481414795\n",
      "Iteration 43, Batch: 30, Loss: 0.44927847385406494\n",
      "Iteration 43, Batch: 31, Loss: 0.46702077984809875\n",
      "Iteration 43, Batch: 32, Loss: 0.39021947979927063\n",
      "Iteration 43, Batch: 33, Loss: 0.4075571596622467\n",
      "Iteration 43, Batch: 34, Loss: 0.49106764793395996\n",
      "Iteration 43, Batch: 35, Loss: 0.39576414227485657\n",
      "Iteration 43, Batch: 36, Loss: 0.42263540625572205\n",
      "Iteration 43, Batch: 37, Loss: 0.48502135276794434\n",
      "Iteration 43, Batch: 38, Loss: 0.44448500871658325\n",
      "Iteration 43, Batch: 39, Loss: 0.49966561794281006\n",
      "Iteration 43, Batch: 40, Loss: 0.4651312828063965\n",
      "Iteration 43, Batch: 41, Loss: 0.4259394109249115\n",
      "Iteration 43, Batch: 42, Loss: 0.4334574043750763\n",
      "Iteration 43, Batch: 43, Loss: 0.423929363489151\n",
      "Iteration 43, Batch: 44, Loss: 0.4852934181690216\n",
      "Iteration 43, Batch: 45, Loss: 0.3696998953819275\n",
      "Iteration 43, Batch: 46, Loss: 0.40739667415618896\n",
      "Iteration 43, Batch: 47, Loss: 0.4659977853298187\n",
      "Iteration 43, Batch: 48, Loss: 0.40571677684783936\n",
      "Iteration 43, Batch: 49, Loss: 0.37250205874443054\n",
      "Avg loss: 0.4435096722841263\n",
      "Iteration 44, Batch: 0, Loss: 0.42383238673210144\n",
      "Iteration 44, Batch: 1, Loss: 0.3644642233848572\n",
      "Iteration 44, Batch: 2, Loss: 0.3949190378189087\n",
      "Iteration 44, Batch: 3, Loss: 0.44101423025131226\n",
      "Iteration 44, Batch: 4, Loss: 0.44356828927993774\n",
      "Iteration 44, Batch: 5, Loss: 0.43512189388275146\n",
      "Iteration 44, Batch: 6, Loss: 0.4041001498699188\n",
      "Iteration 44, Batch: 7, Loss: 0.4406006634235382\n",
      "Iteration 44, Batch: 8, Loss: 0.45940107107162476\n",
      "Iteration 44, Batch: 9, Loss: 0.46599826216697693\n",
      "Iteration 44, Batch: 10, Loss: 0.40148481726646423\n",
      "Iteration 44, Batch: 11, Loss: 0.4499029815196991\n",
      "Iteration 44, Batch: 12, Loss: 0.35740143060684204\n",
      "Iteration 44, Batch: 13, Loss: 0.4260224401950836\n",
      "Iteration 44, Batch: 14, Loss: 0.37247008085250854\n",
      "Iteration 44, Batch: 15, Loss: 0.3524482548236847\n",
      "Iteration 44, Batch: 16, Loss: 0.378305584192276\n",
      "Iteration 44, Batch: 17, Loss: 0.46780088543891907\n",
      "Iteration 44, Batch: 18, Loss: 0.49345770478248596\n",
      "Iteration 44, Batch: 19, Loss: 0.39195716381073\n",
      "Iteration 44, Batch: 20, Loss: 0.4276711046695709\n",
      "Iteration 44, Batch: 21, Loss: 0.4215679168701172\n",
      "Iteration 44, Batch: 22, Loss: 0.45079749822616577\n",
      "Iteration 44, Batch: 23, Loss: 0.45534902811050415\n",
      "Iteration 44, Batch: 24, Loss: 0.4040695130825043\n",
      "Iteration 44, Batch: 25, Loss: 0.42153671383857727\n",
      "Iteration 44, Batch: 26, Loss: 0.3938734829425812\n",
      "Iteration 44, Batch: 27, Loss: 0.39443865418434143\n",
      "Iteration 44, Batch: 28, Loss: 0.40254664421081543\n",
      "Iteration 44, Batch: 29, Loss: 0.4024158716201782\n",
      "Iteration 44, Batch: 30, Loss: 0.4142378866672516\n",
      "Iteration 44, Batch: 31, Loss: 0.4442621171474457\n",
      "Iteration 44, Batch: 32, Loss: 0.4157157838344574\n",
      "Iteration 44, Batch: 33, Loss: 0.4206691384315491\n",
      "Iteration 44, Batch: 34, Loss: 0.39842283725738525\n",
      "Iteration 44, Batch: 35, Loss: 0.4421226680278778\n",
      "Iteration 44, Batch: 36, Loss: 0.5090789794921875\n",
      "Iteration 44, Batch: 37, Loss: 0.45605340600013733\n",
      "Iteration 44, Batch: 38, Loss: 0.41016456484794617\n",
      "Iteration 44, Batch: 39, Loss: 0.3688250184059143\n",
      "Iteration 44, Batch: 40, Loss: 0.4586387574672699\n",
      "Iteration 44, Batch: 41, Loss: 0.4095718264579773\n",
      "Iteration 44, Batch: 42, Loss: 0.39372748136520386\n",
      "Iteration 44, Batch: 43, Loss: 0.38327673077583313\n",
      "Iteration 44, Batch: 44, Loss: 0.44067656993865967\n",
      "Iteration 44, Batch: 45, Loss: 0.4420080780982971\n",
      "Iteration 44, Batch: 46, Loss: 0.3983265459537506\n",
      "Iteration 44, Batch: 47, Loss: 0.3271245062351227\n",
      "Iteration 44, Batch: 48, Loss: 0.3764117360115051\n",
      "Iteration 44, Batch: 49, Loss: 0.4040672481060028\n",
      "Avg loss: 0.417038397192955\n",
      "Iteration 45, Batch: 0, Loss: 0.42831987142562866\n",
      "Iteration 45, Batch: 1, Loss: 0.444704532623291\n",
      "Iteration 45, Batch: 2, Loss: 0.3722413182258606\n",
      "Iteration 45, Batch: 3, Loss: 0.42720702290534973\n",
      "Iteration 45, Batch: 4, Loss: 0.3839316964149475\n",
      "Iteration 45, Batch: 5, Loss: 0.34306907653808594\n",
      "Iteration 45, Batch: 6, Loss: 0.40169525146484375\n",
      "Iteration 45, Batch: 7, Loss: 0.3901485502719879\n",
      "Iteration 45, Batch: 8, Loss: 0.42409712076187134\n",
      "Iteration 45, Batch: 9, Loss: 0.39311593770980835\n",
      "Iteration 45, Batch: 10, Loss: 0.39056670665740967\n",
      "Iteration 45, Batch: 11, Loss: 0.39918288588523865\n",
      "Iteration 45, Batch: 12, Loss: 0.36689528822898865\n",
      "Iteration 45, Batch: 13, Loss: 0.3909403085708618\n",
      "Iteration 45, Batch: 14, Loss: 0.39324766397476196\n",
      "Iteration 45, Batch: 15, Loss: 0.3695807456970215\n",
      "Iteration 45, Batch: 16, Loss: 0.3943346440792084\n",
      "Iteration 45, Batch: 17, Loss: 0.4290604293346405\n",
      "Iteration 45, Batch: 18, Loss: 0.3842346966266632\n",
      "Iteration 45, Batch: 19, Loss: 0.42708495259284973\n",
      "Iteration 45, Batch: 20, Loss: 0.37784233689308167\n",
      "Iteration 45, Batch: 21, Loss: 0.4095385670661926\n",
      "Iteration 45, Batch: 22, Loss: 0.41866278648376465\n",
      "Iteration 45, Batch: 23, Loss: 0.37703269720077515\n",
      "Iteration 45, Batch: 24, Loss: 0.3752327263355255\n",
      "Iteration 45, Batch: 25, Loss: 0.3111530542373657\n",
      "Iteration 45, Batch: 26, Loss: 0.4208540916442871\n",
      "Iteration 45, Batch: 27, Loss: 0.3526947796344757\n",
      "Iteration 45, Batch: 28, Loss: 0.3574768006801605\n",
      "Iteration 45, Batch: 29, Loss: 0.3788347840309143\n",
      "Iteration 45, Batch: 30, Loss: 0.4048542380332947\n",
      "Iteration 45, Batch: 31, Loss: 0.36838585138320923\n",
      "Iteration 45, Batch: 32, Loss: 0.35870710015296936\n",
      "Iteration 45, Batch: 33, Loss: 0.38834571838378906\n",
      "Iteration 45, Batch: 34, Loss: 0.35810211300849915\n",
      "Iteration 45, Batch: 35, Loss: 0.39727768301963806\n",
      "Iteration 45, Batch: 36, Loss: 0.43866217136383057\n",
      "Iteration 45, Batch: 37, Loss: 0.42803525924682617\n",
      "Iteration 45, Batch: 38, Loss: 0.3391481041908264\n",
      "Iteration 45, Batch: 39, Loss: 0.43332168459892273\n",
      "Iteration 45, Batch: 40, Loss: 0.4122944474220276\n",
      "Iteration 45, Batch: 41, Loss: 0.3815053403377533\n",
      "Iteration 45, Batch: 42, Loss: 0.4298972487449646\n",
      "Iteration 45, Batch: 43, Loss: 0.4664390981197357\n",
      "Iteration 45, Batch: 44, Loss: 0.4106900691986084\n",
      "Iteration 45, Batch: 45, Loss: 0.3994119167327881\n",
      "Iteration 45, Batch: 46, Loss: 0.4167747497558594\n",
      "Iteration 45, Batch: 47, Loss: 0.3854767382144928\n",
      "Iteration 45, Batch: 48, Loss: 0.4327647387981415\n",
      "Iteration 45, Batch: 49, Loss: 0.4122706651687622\n",
      "Avg loss: 0.395906925201416\n",
      "Iteration 46, Batch: 0, Loss: 0.36076971888542175\n",
      "Iteration 46, Batch: 1, Loss: 0.36577850580215454\n",
      "Iteration 46, Batch: 2, Loss: 0.369205504655838\n",
      "Iteration 46, Batch: 3, Loss: 0.3706226348876953\n",
      "Iteration 46, Batch: 4, Loss: 0.3875429928302765\n",
      "Iteration 46, Batch: 5, Loss: 0.38481971621513367\n",
      "Iteration 46, Batch: 6, Loss: 0.4008026123046875\n",
      "Iteration 46, Batch: 7, Loss: 0.3996008336544037\n",
      "Iteration 46, Batch: 8, Loss: 0.3742271959781647\n",
      "Iteration 46, Batch: 9, Loss: 0.36819106340408325\n",
      "Iteration 46, Batch: 10, Loss: 0.33596745133399963\n",
      "Iteration 46, Batch: 11, Loss: 0.4688917398452759\n",
      "Iteration 46, Batch: 12, Loss: 0.31980472803115845\n",
      "Iteration 46, Batch: 13, Loss: 0.3377925157546997\n",
      "Iteration 46, Batch: 14, Loss: 0.3840698301792145\n",
      "Iteration 46, Batch: 15, Loss: 0.3487462103366852\n",
      "Iteration 46, Batch: 16, Loss: 0.36287349462509155\n",
      "Iteration 46, Batch: 17, Loss: 0.39755964279174805\n",
      "Iteration 46, Batch: 18, Loss: 0.39723262190818787\n",
      "Iteration 46, Batch: 19, Loss: 0.3666875660419464\n",
      "Iteration 46, Batch: 20, Loss: 0.3477456569671631\n",
      "Iteration 46, Batch: 21, Loss: 0.3996770679950714\n",
      "Iteration 46, Batch: 22, Loss: 0.38681575655937195\n",
      "Iteration 46, Batch: 23, Loss: 0.34805014729499817\n",
      "Iteration 46, Batch: 24, Loss: 0.35522955656051636\n",
      "Iteration 46, Batch: 25, Loss: 0.3814713656902313\n",
      "Iteration 46, Batch: 26, Loss: 0.3216255009174347\n",
      "Iteration 46, Batch: 27, Loss: 0.38317734003067017\n",
      "Iteration 46, Batch: 28, Loss: 0.4263013005256653\n",
      "Iteration 46, Batch: 29, Loss: 0.3738465905189514\n",
      "Iteration 46, Batch: 30, Loss: 0.43031439185142517\n",
      "Iteration 46, Batch: 31, Loss: 0.38456961512565613\n",
      "Iteration 46, Batch: 32, Loss: 0.3796633780002594\n",
      "Iteration 46, Batch: 33, Loss: 0.34991687536239624\n",
      "Iteration 46, Batch: 34, Loss: 0.37714752554893494\n",
      "Iteration 46, Batch: 35, Loss: 0.4142068326473236\n",
      "Iteration 46, Batch: 36, Loss: 0.3504859507083893\n",
      "Iteration 46, Batch: 37, Loss: 0.4011486768722534\n",
      "Iteration 46, Batch: 38, Loss: 0.42451316118240356\n",
      "Iteration 46, Batch: 39, Loss: 0.3821481168270111\n",
      "Iteration 46, Batch: 40, Loss: 0.3517790138721466\n",
      "Iteration 46, Batch: 41, Loss: 0.37405893206596375\n",
      "Iteration 46, Batch: 42, Loss: 0.3686677813529968\n",
      "Iteration 46, Batch: 43, Loss: 0.4003590941429138\n",
      "Iteration 46, Batch: 44, Loss: 0.375427782535553\n",
      "Iteration 46, Batch: 45, Loss: 0.3279412090778351\n",
      "Iteration 46, Batch: 46, Loss: 0.32952675223350525\n",
      "Iteration 46, Batch: 47, Loss: 0.2950272262096405\n",
      "Iteration 46, Batch: 48, Loss: 0.37387919425964355\n",
      "Iteration 46, Batch: 49, Loss: 0.34051448106765747\n",
      "Avg loss: 0.37312845706939696\n",
      "Iteration 47, Batch: 0, Loss: 0.34288477897644043\n",
      "Iteration 47, Batch: 1, Loss: 0.36950716376304626\n",
      "Iteration 47, Batch: 2, Loss: 0.37619534134864807\n",
      "Iteration 47, Batch: 3, Loss: 0.3472541868686676\n",
      "Iteration 47, Batch: 4, Loss: 0.356302946805954\n",
      "Iteration 47, Batch: 5, Loss: 0.3556952476501465\n",
      "Iteration 47, Batch: 6, Loss: 0.372609406709671\n",
      "Iteration 47, Batch: 7, Loss: 0.28367263078689575\n",
      "Iteration 47, Batch: 8, Loss: 0.34792354702949524\n",
      "Iteration 47, Batch: 9, Loss: 0.40043333172798157\n",
      "Iteration 47, Batch: 10, Loss: 0.3345162868499756\n",
      "Iteration 47, Batch: 11, Loss: 0.33855369687080383\n",
      "Iteration 47, Batch: 12, Loss: 0.3382117748260498\n",
      "Iteration 47, Batch: 13, Loss: 0.3242025673389435\n",
      "Iteration 47, Batch: 14, Loss: 0.30983152985572815\n",
      "Iteration 47, Batch: 15, Loss: 0.32078033685684204\n",
      "Iteration 47, Batch: 16, Loss: 0.3591231405735016\n",
      "Iteration 47, Batch: 17, Loss: 0.3644586205482483\n",
      "Iteration 47, Batch: 18, Loss: 0.37275099754333496\n",
      "Iteration 47, Batch: 19, Loss: 0.33411672711372375\n",
      "Iteration 47, Batch: 20, Loss: 0.32638755440711975\n",
      "Iteration 47, Batch: 21, Loss: 0.32571032643318176\n",
      "Iteration 47, Batch: 22, Loss: 0.36443114280700684\n",
      "Iteration 47, Batch: 23, Loss: 0.3859950304031372\n",
      "Iteration 47, Batch: 24, Loss: 0.35375717282295227\n",
      "Iteration 47, Batch: 25, Loss: 0.32892906665802\n",
      "Iteration 47, Batch: 26, Loss: 0.3390389084815979\n",
      "Iteration 47, Batch: 27, Loss: 0.3603113889694214\n",
      "Iteration 47, Batch: 28, Loss: 0.34837087988853455\n",
      "Iteration 47, Batch: 29, Loss: 0.31258755922317505\n",
      "Iteration 47, Batch: 30, Loss: 0.3446415662765503\n",
      "Iteration 47, Batch: 31, Loss: 0.332293301820755\n",
      "Iteration 47, Batch: 32, Loss: 0.38226306438446045\n",
      "Iteration 47, Batch: 33, Loss: 0.3152860701084137\n",
      "Iteration 47, Batch: 34, Loss: 0.35417595505714417\n",
      "Iteration 47, Batch: 35, Loss: 0.3179098665714264\n",
      "Iteration 47, Batch: 36, Loss: 0.40456146001815796\n",
      "Iteration 47, Batch: 37, Loss: 0.32868728041648865\n",
      "Iteration 47, Batch: 38, Loss: 0.34243640303611755\n",
      "Iteration 47, Batch: 39, Loss: 0.34588778018951416\n",
      "Iteration 47, Batch: 40, Loss: 0.34337377548217773\n",
      "Iteration 47, Batch: 41, Loss: 0.3386487364768982\n",
      "Iteration 47, Batch: 42, Loss: 0.31109270453453064\n",
      "Iteration 47, Batch: 43, Loss: 0.3493247628211975\n",
      "Iteration 47, Batch: 44, Loss: 0.3362709879875183\n",
      "Iteration 47, Batch: 45, Loss: 0.35946956276893616\n",
      "Iteration 47, Batch: 46, Loss: 0.3417353630065918\n",
      "Iteration 47, Batch: 47, Loss: 0.36503130197525024\n",
      "Iteration 47, Batch: 48, Loss: 0.35046684741973877\n",
      "Iteration 47, Batch: 49, Loss: 0.37778231501579285\n",
      "Avg loss: 0.3467176479101181\n",
      "Iteration 48, Batch: 0, Loss: 0.35976797342300415\n",
      "Iteration 48, Batch: 1, Loss: 0.27008089423179626\n",
      "Iteration 48, Batch: 2, Loss: 0.37931540608406067\n",
      "Iteration 48, Batch: 3, Loss: 0.2965293526649475\n",
      "Iteration 48, Batch: 4, Loss: 0.334737092256546\n",
      "Iteration 48, Batch: 5, Loss: 0.33617228269577026\n",
      "Iteration 48, Batch: 6, Loss: 0.3496430814266205\n",
      "Iteration 48, Batch: 7, Loss: 0.3409567177295685\n",
      "Iteration 48, Batch: 8, Loss: 0.31041422486305237\n",
      "Iteration 48, Batch: 9, Loss: 0.3811390697956085\n",
      "Iteration 48, Batch: 10, Loss: 0.29543623328208923\n",
      "Iteration 48, Batch: 11, Loss: 0.3684404194355011\n",
      "Iteration 48, Batch: 12, Loss: 0.32352301478385925\n",
      "Iteration 48, Batch: 13, Loss: 0.341922402381897\n",
      "Iteration 48, Batch: 14, Loss: 0.34273964166641235\n",
      "Iteration 48, Batch: 15, Loss: 0.29940181970596313\n",
      "Iteration 48, Batch: 16, Loss: 0.30375269055366516\n",
      "Iteration 48, Batch: 17, Loss: 0.2897832989692688\n",
      "Iteration 48, Batch: 18, Loss: 0.3363626003265381\n",
      "Iteration 48, Batch: 19, Loss: 0.3058567941188812\n",
      "Iteration 48, Batch: 20, Loss: 0.302927702665329\n",
      "Iteration 48, Batch: 21, Loss: 0.3216693699359894\n",
      "Iteration 48, Batch: 22, Loss: 0.301482617855072\n",
      "Iteration 48, Batch: 23, Loss: 0.3354329466819763\n",
      "Iteration 48, Batch: 24, Loss: 0.30504095554351807\n",
      "Iteration 48, Batch: 25, Loss: 0.28853103518486023\n",
      "Iteration 48, Batch: 26, Loss: 0.32552090287208557\n",
      "Iteration 48, Batch: 27, Loss: 0.3164205551147461\n",
      "Iteration 48, Batch: 28, Loss: 0.31564196944236755\n",
      "Iteration 48, Batch: 29, Loss: 0.31838271021842957\n",
      "Iteration 48, Batch: 30, Loss: 0.30094337463378906\n",
      "Iteration 48, Batch: 31, Loss: 0.2967641055583954\n",
      "Iteration 48, Batch: 32, Loss: 0.32478952407836914\n",
      "Iteration 48, Batch: 33, Loss: 0.3015773594379425\n",
      "Iteration 48, Batch: 34, Loss: 0.2914394438266754\n",
      "Iteration 48, Batch: 35, Loss: 0.3152685761451721\n",
      "Iteration 48, Batch: 36, Loss: 0.31279611587524414\n",
      "Iteration 48, Batch: 37, Loss: 0.2743789553642273\n",
      "Iteration 48, Batch: 38, Loss: 0.3349907696247101\n",
      "Iteration 48, Batch: 39, Loss: 0.3291785418987274\n",
      "Iteration 48, Batch: 40, Loss: 0.375364750623703\n",
      "Iteration 48, Batch: 41, Loss: 0.3244019150733948\n",
      "Iteration 48, Batch: 42, Loss: 0.3529714047908783\n",
      "Iteration 48, Batch: 43, Loss: 0.28212448954582214\n",
      "Iteration 48, Batch: 44, Loss: 0.27699393033981323\n",
      "Iteration 48, Batch: 45, Loss: 0.27013301849365234\n",
      "Iteration 48, Batch: 46, Loss: 0.33550989627838135\n",
      "Iteration 48, Batch: 47, Loss: 0.2987944185733795\n",
      "Iteration 48, Batch: 48, Loss: 0.35346952080726624\n",
      "Iteration 48, Batch: 49, Loss: 0.3532456159591675\n",
      "Avg loss: 0.3200432300567627\n",
      "Iteration 49, Batch: 0, Loss: 0.326587438583374\n",
      "Iteration 49, Batch: 1, Loss: 0.3053598403930664\n",
      "Iteration 49, Batch: 2, Loss: 0.3391368091106415\n",
      "Iteration 49, Batch: 3, Loss: 0.32635053992271423\n",
      "Iteration 49, Batch: 4, Loss: 0.24751265347003937\n",
      "Iteration 49, Batch: 5, Loss: 0.3172045350074768\n",
      "Iteration 49, Batch: 6, Loss: 0.2910141944885254\n",
      "Iteration 49, Batch: 7, Loss: 0.35907942056655884\n",
      "Iteration 49, Batch: 8, Loss: 0.3356185853481293\n",
      "Iteration 49, Batch: 9, Loss: 0.3629515469074249\n",
      "Iteration 49, Batch: 10, Loss: 0.30615249276161194\n",
      "Iteration 49, Batch: 11, Loss: 0.3539307415485382\n",
      "Iteration 49, Batch: 12, Loss: 0.26184797286987305\n",
      "Iteration 49, Batch: 13, Loss: 0.3508039116859436\n",
      "Iteration 49, Batch: 14, Loss: 0.33279159665107727\n",
      "Iteration 49, Batch: 15, Loss: 0.29617273807525635\n",
      "Iteration 49, Batch: 16, Loss: 0.27207449078559875\n",
      "Iteration 49, Batch: 17, Loss: 0.33533167839050293\n",
      "Iteration 49, Batch: 18, Loss: 0.33005091547966003\n",
      "Iteration 49, Batch: 19, Loss: 0.33919066190719604\n",
      "Iteration 49, Batch: 20, Loss: 0.26038822531700134\n",
      "Iteration 49, Batch: 21, Loss: 0.35404181480407715\n",
      "Iteration 49, Batch: 22, Loss: 0.29796817898750305\n",
      "Iteration 49, Batch: 23, Loss: 0.34444549679756165\n",
      "Iteration 49, Batch: 24, Loss: 0.3655165135860443\n",
      "Iteration 49, Batch: 25, Loss: 0.30618542432785034\n",
      "Iteration 49, Batch: 26, Loss: 0.3229601979255676\n",
      "Iteration 49, Batch: 27, Loss: 0.30535194277763367\n",
      "Iteration 49, Batch: 28, Loss: 0.2664307653903961\n",
      "Iteration 49, Batch: 29, Loss: 0.2911495268344879\n",
      "Iteration 49, Batch: 30, Loss: 0.3156857192516327\n",
      "Iteration 49, Batch: 31, Loss: 0.35559144616127014\n",
      "Iteration 49, Batch: 32, Loss: 0.28888165950775146\n",
      "Iteration 49, Batch: 33, Loss: 0.3256874680519104\n",
      "Iteration 49, Batch: 34, Loss: 0.36336633563041687\n",
      "Iteration 49, Batch: 35, Loss: 0.2880136966705322\n",
      "Iteration 49, Batch: 36, Loss: 0.2657272517681122\n",
      "Iteration 49, Batch: 37, Loss: 0.307722806930542\n",
      "Iteration 49, Batch: 38, Loss: 0.3051581084728241\n",
      "Iteration 49, Batch: 39, Loss: 0.3104420304298401\n",
      "Iteration 49, Batch: 40, Loss: 0.27980169653892517\n",
      "Iteration 49, Batch: 41, Loss: 0.34272223711013794\n",
      "Iteration 49, Batch: 42, Loss: 0.2741169035434723\n",
      "Iteration 49, Batch: 43, Loss: 0.3521333336830139\n",
      "Iteration 49, Batch: 44, Loss: 0.3028702735900879\n",
      "Iteration 49, Batch: 45, Loss: 0.3223866820335388\n",
      "Iteration 49, Batch: 46, Loss: 0.33861440420150757\n",
      "Iteration 49, Batch: 47, Loss: 0.3958861529827118\n",
      "Iteration 49, Batch: 48, Loss: 0.306382954120636\n",
      "Iteration 49, Batch: 49, Loss: 0.304931104183197\n",
      "Avg loss: 0.3169944623112679\n",
      "Iteration 50, Batch: 0, Loss: 0.28235602378845215\n",
      "Iteration 50, Batch: 1, Loss: 0.2885865867137909\n",
      "Iteration 50, Batch: 2, Loss: 0.30671226978302\n",
      "Iteration 50, Batch: 3, Loss: 0.2929370403289795\n",
      "Iteration 50, Batch: 4, Loss: 0.2980169951915741\n",
      "Iteration 50, Batch: 5, Loss: 0.34404146671295166\n",
      "Iteration 50, Batch: 6, Loss: 0.31207039952278137\n",
      "Iteration 50, Batch: 7, Loss: 0.3125561475753784\n",
      "Iteration 50, Batch: 8, Loss: 0.32499778270721436\n",
      "Iteration 50, Batch: 9, Loss: 0.25035879015922546\n",
      "Iteration 50, Batch: 10, Loss: 0.3279425799846649\n",
      "Iteration 50, Batch: 11, Loss: 0.33591896295547485\n",
      "Iteration 50, Batch: 12, Loss: 0.29625800251960754\n",
      "Iteration 50, Batch: 13, Loss: 0.35180899500846863\n",
      "Iteration 50, Batch: 14, Loss: 0.30339133739471436\n",
      "Iteration 50, Batch: 15, Loss: 0.27777665853500366\n",
      "Iteration 50, Batch: 16, Loss: 0.36593756079673767\n",
      "Iteration 50, Batch: 17, Loss: 0.33088234066963196\n",
      "Iteration 50, Batch: 18, Loss: 0.31280645728111267\n",
      "Iteration 50, Batch: 19, Loss: 0.29642078280448914\n",
      "Iteration 50, Batch: 20, Loss: 0.29354938864707947\n",
      "Iteration 50, Batch: 21, Loss: 0.32990431785583496\n",
      "Iteration 50, Batch: 22, Loss: 0.26969337463378906\n",
      "Iteration 50, Batch: 23, Loss: 0.2679945230484009\n",
      "Iteration 50, Batch: 24, Loss: 0.29905086755752563\n",
      "Iteration 50, Batch: 25, Loss: 0.2882530987262726\n",
      "Iteration 50, Batch: 26, Loss: 0.33772337436676025\n",
      "Iteration 50, Batch: 27, Loss: 0.29645341634750366\n",
      "Iteration 50, Batch: 28, Loss: 0.29848363995552063\n",
      "Iteration 50, Batch: 29, Loss: 0.3391864001750946\n",
      "Iteration 50, Batch: 30, Loss: 0.3117388188838959\n",
      "Iteration 50, Batch: 31, Loss: 0.30488333106040955\n",
      "Iteration 50, Batch: 32, Loss: 0.2927674353122711\n",
      "Iteration 50, Batch: 33, Loss: 0.2605167031288147\n",
      "Iteration 50, Batch: 34, Loss: 0.28167688846588135\n",
      "Iteration 50, Batch: 35, Loss: 0.27234524488449097\n",
      "Iteration 50, Batch: 36, Loss: 0.31967395544052124\n",
      "Iteration 50, Batch: 37, Loss: 0.2801864445209503\n",
      "Iteration 50, Batch: 38, Loss: 0.23403553664684296\n",
      "Iteration 50, Batch: 39, Loss: 0.2897035777568817\n",
      "Iteration 50, Batch: 40, Loss: 0.33403462171554565\n",
      "Iteration 50, Batch: 41, Loss: 0.298176109790802\n",
      "Iteration 50, Batch: 42, Loss: 0.24065783619880676\n",
      "Iteration 50, Batch: 43, Loss: 0.25192585587501526\n",
      "Iteration 50, Batch: 44, Loss: 0.2640381455421448\n",
      "Iteration 50, Batch: 45, Loss: 0.2687704265117645\n",
      "Iteration 50, Batch: 46, Loss: 0.272163987159729\n",
      "Iteration 50, Batch: 47, Loss: 0.24566993117332458\n",
      "Iteration 50, Batch: 48, Loss: 0.30327308177948\n",
      "Iteration 50, Batch: 49, Loss: 0.25307512283325195\n",
      "Avg loss: 0.2962276527285576\n",
      "Iteration 51, Batch: 0, Loss: 0.3161155581474304\n",
      "Iteration 51, Batch: 1, Loss: 0.24831247329711914\n",
      "Iteration 51, Batch: 2, Loss: 0.2949645221233368\n",
      "Iteration 51, Batch: 3, Loss: 0.28215351700782776\n",
      "Iteration 51, Batch: 4, Loss: 0.33397483825683594\n",
      "Iteration 51, Batch: 5, Loss: 0.31018996238708496\n",
      "Iteration 51, Batch: 6, Loss: 0.2817237675189972\n",
      "Iteration 51, Batch: 7, Loss: 0.257098525762558\n",
      "Iteration 51, Batch: 8, Loss: 0.3002352714538574\n",
      "Iteration 51, Batch: 9, Loss: 0.30603060126304626\n",
      "Iteration 51, Batch: 10, Loss: 0.27285197377204895\n",
      "Iteration 51, Batch: 11, Loss: 0.2660132944583893\n",
      "Iteration 51, Batch: 12, Loss: 0.28578704595565796\n",
      "Iteration 51, Batch: 13, Loss: 0.2632030248641968\n",
      "Iteration 51, Batch: 14, Loss: 0.2786601483821869\n",
      "Iteration 51, Batch: 15, Loss: 0.30806320905685425\n",
      "Iteration 51, Batch: 16, Loss: 0.2730500102043152\n",
      "Iteration 51, Batch: 17, Loss: 0.276065468788147\n",
      "Iteration 51, Batch: 18, Loss: 0.3011700212955475\n",
      "Iteration 51, Batch: 19, Loss: 0.29176604747772217\n",
      "Iteration 51, Batch: 20, Loss: 0.2618332803249359\n",
      "Iteration 51, Batch: 21, Loss: 0.27155017852783203\n",
      "Iteration 51, Batch: 22, Loss: 0.277116984128952\n",
      "Iteration 51, Batch: 23, Loss: 0.26672258973121643\n",
      "Iteration 51, Batch: 24, Loss: 0.2564021944999695\n",
      "Iteration 51, Batch: 25, Loss: 0.2857450544834137\n",
      "Iteration 51, Batch: 26, Loss: 0.30394119024276733\n",
      "Iteration 51, Batch: 27, Loss: 0.27443161606788635\n",
      "Iteration 51, Batch: 28, Loss: 0.26967552304267883\n",
      "Iteration 51, Batch: 29, Loss: 0.27058249711990356\n",
      "Iteration 51, Batch: 30, Loss: 0.2567848563194275\n",
      "Iteration 51, Batch: 31, Loss: 0.24777570366859436\n",
      "Iteration 51, Batch: 32, Loss: 0.25802528858184814\n",
      "Iteration 51, Batch: 33, Loss: 0.2461567223072052\n",
      "Iteration 51, Batch: 34, Loss: 0.23700152337551117\n",
      "Iteration 51, Batch: 35, Loss: 0.3410833775997162\n",
      "Iteration 51, Batch: 36, Loss: 0.2491888403892517\n",
      "Iteration 51, Batch: 37, Loss: 0.25137075781822205\n",
      "Iteration 51, Batch: 38, Loss: 0.2975516617298126\n",
      "Iteration 51, Batch: 39, Loss: 0.32789602875709534\n",
      "Iteration 51, Batch: 40, Loss: 0.2927670180797577\n",
      "Iteration 51, Batch: 41, Loss: 0.2734207808971405\n",
      "Iteration 51, Batch: 42, Loss: 0.24467585980892181\n",
      "Iteration 51, Batch: 43, Loss: 0.2611192762851715\n",
      "Iteration 51, Batch: 44, Loss: 0.2641616761684418\n",
      "Iteration 51, Batch: 45, Loss: 0.25956037640571594\n",
      "Iteration 51, Batch: 46, Loss: 0.3779500126838684\n",
      "Iteration 51, Batch: 47, Loss: 0.29444438219070435\n",
      "Iteration 51, Batch: 48, Loss: 0.27628326416015625\n",
      "Iteration 51, Batch: 49, Loss: 0.2631743550300598\n",
      "Avg loss: 0.2801164430379868\n",
      "Iteration 52, Batch: 0, Loss: 0.2768664062023163\n",
      "Iteration 52, Batch: 1, Loss: 0.2811893820762634\n",
      "Iteration 52, Batch: 2, Loss: 0.26868554949760437\n",
      "Iteration 52, Batch: 3, Loss: 0.31888091564178467\n",
      "Iteration 52, Batch: 4, Loss: 0.26019904017448425\n",
      "Iteration 52, Batch: 5, Loss: 0.29874658584594727\n",
      "Iteration 52, Batch: 6, Loss: 0.3154987692832947\n",
      "Iteration 52, Batch: 7, Loss: 0.2965070903301239\n",
      "Iteration 52, Batch: 8, Loss: 0.24086838960647583\n",
      "Iteration 52, Batch: 9, Loss: 0.31784892082214355\n",
      "Iteration 52, Batch: 10, Loss: 0.29666030406951904\n",
      "Iteration 52, Batch: 11, Loss: 0.2846345603466034\n",
      "Iteration 52, Batch: 12, Loss: 0.27144452929496765\n",
      "Iteration 52, Batch: 13, Loss: 0.3044630289077759\n",
      "Iteration 52, Batch: 14, Loss: 0.25998061895370483\n",
      "Iteration 52, Batch: 15, Loss: 0.31351953744888306\n",
      "Iteration 52, Batch: 16, Loss: 0.33995893597602844\n",
      "Iteration 52, Batch: 17, Loss: 0.29101118445396423\n",
      "Iteration 52, Batch: 18, Loss: 0.25894320011138916\n",
      "Iteration 52, Batch: 19, Loss: 0.31103697419166565\n",
      "Iteration 52, Batch: 20, Loss: 0.244978666305542\n",
      "Iteration 52, Batch: 21, Loss: 0.2642606794834137\n",
      "Iteration 52, Batch: 22, Loss: 0.33646610379219055\n",
      "Iteration 52, Batch: 23, Loss: 0.2743428349494934\n",
      "Iteration 52, Batch: 24, Loss: 0.2746196687221527\n",
      "Iteration 52, Batch: 25, Loss: 0.230869859457016\n",
      "Iteration 52, Batch: 26, Loss: 0.2583029866218567\n",
      "Iteration 52, Batch: 27, Loss: 0.30153268575668335\n",
      "Iteration 52, Batch: 28, Loss: 0.23893336951732635\n",
      "Iteration 52, Batch: 29, Loss: 0.3105763792991638\n",
      "Iteration 52, Batch: 30, Loss: 0.3073387145996094\n",
      "Iteration 52, Batch: 31, Loss: 0.30184057354927063\n",
      "Iteration 52, Batch: 32, Loss: 0.3101278841495514\n",
      "Iteration 52, Batch: 33, Loss: 0.22963516414165497\n",
      "Iteration 52, Batch: 34, Loss: 0.2923409640789032\n",
      "Iteration 52, Batch: 35, Loss: 0.3011702597141266\n",
      "Iteration 52, Batch: 36, Loss: 0.27482175827026367\n",
      "Iteration 52, Batch: 37, Loss: 0.2545372247695923\n",
      "Iteration 52, Batch: 38, Loss: 0.3045859932899475\n",
      "Iteration 52, Batch: 39, Loss: 0.28062665462493896\n",
      "Iteration 52, Batch: 40, Loss: 0.256978839635849\n",
      "Iteration 52, Batch: 41, Loss: 0.24489827454090118\n",
      "Iteration 52, Batch: 42, Loss: 0.2470530867576599\n",
      "Iteration 52, Batch: 43, Loss: 0.20254355669021606\n",
      "Iteration 52, Batch: 44, Loss: 0.24218444526195526\n",
      "Iteration 52, Batch: 45, Loss: 0.2638453543186188\n",
      "Iteration 52, Batch: 46, Loss: 0.26021453738212585\n",
      "Iteration 52, Batch: 47, Loss: 0.2840901017189026\n",
      "Iteration 52, Batch: 48, Loss: 0.2579660713672638\n",
      "Iteration 52, Batch: 49, Loss: 0.29755130410194397\n",
      "Avg loss: 0.2791235584020615\n",
      "Iteration 53, Batch: 0, Loss: 0.2803347706794739\n",
      "Iteration 53, Batch: 1, Loss: 0.26487478613853455\n",
      "Iteration 53, Batch: 2, Loss: 0.27177926898002625\n",
      "Iteration 53, Batch: 3, Loss: 0.2911149561405182\n",
      "Iteration 53, Batch: 4, Loss: 0.3083118498325348\n",
      "Iteration 53, Batch: 5, Loss: 0.2621132731437683\n",
      "Iteration 53, Batch: 6, Loss: 0.30399423837661743\n",
      "Iteration 53, Batch: 7, Loss: 0.25298163294792175\n",
      "Iteration 53, Batch: 8, Loss: 0.2759512960910797\n",
      "Iteration 53, Batch: 9, Loss: 0.2537723183631897\n",
      "Iteration 53, Batch: 10, Loss: 0.3116874098777771\n",
      "Iteration 53, Batch: 11, Loss: 0.2555263042449951\n",
      "Iteration 53, Batch: 12, Loss: 0.2417474240064621\n",
      "Iteration 53, Batch: 13, Loss: 0.2455574870109558\n",
      "Iteration 53, Batch: 14, Loss: 0.255187064409256\n",
      "Iteration 53, Batch: 15, Loss: 0.3410731852054596\n",
      "Iteration 53, Batch: 16, Loss: 0.2619098722934723\n",
      "Iteration 53, Batch: 17, Loss: 0.26095813512802124\n",
      "Iteration 53, Batch: 18, Loss: 0.2810162603855133\n",
      "Iteration 53, Batch: 19, Loss: 0.22857804596424103\n",
      "Iteration 53, Batch: 20, Loss: 0.2597143054008484\n",
      "Iteration 53, Batch: 21, Loss: 0.231904536485672\n",
      "Iteration 53, Batch: 22, Loss: 0.22730688750743866\n",
      "Iteration 53, Batch: 23, Loss: 0.283297061920166\n",
      "Iteration 53, Batch: 24, Loss: 0.2695029079914093\n",
      "Iteration 53, Batch: 25, Loss: 0.23905618488788605\n",
      "Iteration 53, Batch: 26, Loss: 0.26354971528053284\n",
      "Iteration 53, Batch: 27, Loss: 0.2693518400192261\n",
      "Iteration 53, Batch: 28, Loss: 0.23734907805919647\n",
      "Iteration 53, Batch: 29, Loss: 0.24481073021888733\n",
      "Iteration 53, Batch: 30, Loss: 0.2656402289867401\n",
      "Iteration 53, Batch: 31, Loss: 0.2380979061126709\n",
      "Iteration 53, Batch: 32, Loss: 0.26741912961006165\n",
      "Iteration 53, Batch: 33, Loss: 0.2800882160663605\n",
      "Iteration 53, Batch: 34, Loss: 0.23963825404644012\n",
      "Iteration 53, Batch: 35, Loss: 0.29972603917121887\n",
      "Iteration 53, Batch: 36, Loss: 0.2508092224597931\n",
      "Iteration 53, Batch: 37, Loss: 0.2796703279018402\n",
      "Iteration 53, Batch: 38, Loss: 0.26783186197280884\n",
      "Iteration 53, Batch: 39, Loss: 0.2827080488204956\n",
      "Iteration 53, Batch: 40, Loss: 0.2936171591281891\n",
      "Iteration 53, Batch: 41, Loss: 0.24597090482711792\n",
      "Iteration 53, Batch: 42, Loss: 0.2715626060962677\n",
      "Iteration 53, Batch: 43, Loss: 0.215875506401062\n",
      "Iteration 53, Batch: 44, Loss: 0.26986950635910034\n",
      "Iteration 53, Batch: 45, Loss: 0.24464736878871918\n",
      "Iteration 53, Batch: 46, Loss: 0.2842135429382324\n",
      "Iteration 53, Batch: 47, Loss: 0.264514297246933\n",
      "Iteration 53, Batch: 48, Loss: 0.22052104771137238\n",
      "Iteration 53, Batch: 49, Loss: 0.21669970452785492\n",
      "Avg loss: 0.2634686741232872\n",
      "Iteration 54, Batch: 0, Loss: 0.28279486298561096\n",
      "Iteration 54, Batch: 1, Loss: 0.2813187539577484\n",
      "Iteration 54, Batch: 2, Loss: 0.272966206073761\n",
      "Iteration 54, Batch: 3, Loss: 0.27018168568611145\n",
      "Iteration 54, Batch: 4, Loss: 0.23332644999027252\n",
      "Iteration 54, Batch: 5, Loss: 0.24860064685344696\n",
      "Iteration 54, Batch: 6, Loss: 0.27680662274360657\n",
      "Iteration 54, Batch: 7, Loss: 0.26120930910110474\n",
      "Iteration 54, Batch: 8, Loss: 0.22504445910453796\n",
      "Iteration 54, Batch: 9, Loss: 0.23679473996162415\n",
      "Iteration 54, Batch: 10, Loss: 0.2774163484573364\n",
      "Iteration 54, Batch: 11, Loss: 0.22862336039543152\n",
      "Iteration 54, Batch: 12, Loss: 0.22128482162952423\n",
      "Iteration 54, Batch: 13, Loss: 0.24569611251354218\n",
      "Iteration 54, Batch: 14, Loss: 0.2799268662929535\n",
      "Iteration 54, Batch: 15, Loss: 0.2386246621608734\n",
      "Iteration 54, Batch: 16, Loss: 0.21381987631320953\n",
      "Iteration 54, Batch: 17, Loss: 0.24824412167072296\n",
      "Iteration 54, Batch: 18, Loss: 0.2816353440284729\n",
      "Iteration 54, Batch: 19, Loss: 0.30669987201690674\n",
      "Iteration 54, Batch: 20, Loss: 0.2722497880458832\n",
      "Iteration 54, Batch: 21, Loss: 0.2510632574558258\n",
      "Iteration 54, Batch: 22, Loss: 0.29136455059051514\n",
      "Iteration 54, Batch: 23, Loss: 0.2706744968891144\n",
      "Iteration 54, Batch: 24, Loss: 0.25214332342147827\n",
      "Iteration 54, Batch: 25, Loss: 0.2568961977958679\n",
      "Iteration 54, Batch: 26, Loss: 0.22803957760334015\n",
      "Iteration 54, Batch: 27, Loss: 0.2676146924495697\n",
      "Iteration 54, Batch: 28, Loss: 0.2288634479045868\n",
      "Iteration 54, Batch: 29, Loss: 0.25074753165245056\n",
      "Iteration 54, Batch: 30, Loss: 0.2521158754825592\n",
      "Iteration 54, Batch: 31, Loss: 0.22541217505931854\n",
      "Iteration 54, Batch: 32, Loss: 0.22415713965892792\n",
      "Iteration 54, Batch: 33, Loss: 0.2750024199485779\n",
      "Iteration 54, Batch: 34, Loss: 0.24306681752204895\n",
      "Iteration 54, Batch: 35, Loss: 0.24219650030136108\n",
      "Iteration 54, Batch: 36, Loss: 0.2449415922164917\n",
      "Iteration 54, Batch: 37, Loss: 0.2644276022911072\n",
      "Iteration 54, Batch: 38, Loss: 0.27947962284088135\n",
      "Iteration 54, Batch: 39, Loss: 0.2613629102706909\n",
      "Iteration 54, Batch: 40, Loss: 0.24165087938308716\n",
      "Iteration 54, Batch: 41, Loss: 0.20266352593898773\n",
      "Iteration 54, Batch: 42, Loss: 0.19897611439228058\n",
      "Iteration 54, Batch: 43, Loss: 0.2611011266708374\n",
      "Iteration 54, Batch: 44, Loss: 0.21030645072460175\n",
      "Iteration 54, Batch: 45, Loss: 0.2363259494304657\n",
      "Iteration 54, Batch: 46, Loss: 0.25336945056915283\n",
      "Iteration 54, Batch: 47, Loss: 0.23769961297512054\n",
      "Iteration 54, Batch: 48, Loss: 0.2537595331668854\n",
      "Iteration 54, Batch: 49, Loss: 0.19583499431610107\n",
      "Avg loss: 0.2500904455780983\n",
      "Iteration 55, Batch: 0, Loss: 0.20745304226875305\n",
      "Iteration 55, Batch: 1, Loss: 0.2342291921377182\n",
      "Iteration 55, Batch: 2, Loss: 0.25024205446243286\n",
      "Iteration 55, Batch: 3, Loss: 0.2345172017812729\n",
      "Iteration 55, Batch: 4, Loss: 0.24937108159065247\n",
      "Iteration 55, Batch: 5, Loss: 0.2683808505535126\n",
      "Iteration 55, Batch: 6, Loss: 0.2001899927854538\n",
      "Iteration 55, Batch: 7, Loss: 0.2219880074262619\n",
      "Iteration 55, Batch: 8, Loss: 0.2599348723888397\n",
      "Iteration 55, Batch: 9, Loss: 0.25283366441726685\n",
      "Iteration 55, Batch: 10, Loss: 0.23060345649719238\n",
      "Iteration 55, Batch: 11, Loss: 0.2444181889295578\n",
      "Iteration 55, Batch: 12, Loss: 0.2952423393726349\n",
      "Iteration 55, Batch: 13, Loss: 0.21271811425685883\n",
      "Iteration 55, Batch: 14, Loss: 0.2038903832435608\n",
      "Iteration 55, Batch: 15, Loss: 0.24750736355781555\n",
      "Iteration 55, Batch: 16, Loss: 0.2773588001728058\n",
      "Iteration 55, Batch: 17, Loss: 0.21305018663406372\n",
      "Iteration 55, Batch: 18, Loss: 0.23621690273284912\n",
      "Iteration 55, Batch: 19, Loss: 0.20875361561775208\n",
      "Iteration 55, Batch: 20, Loss: 0.2675424814224243\n",
      "Iteration 55, Batch: 21, Loss: 0.22613553702831268\n",
      "Iteration 55, Batch: 22, Loss: 0.2554684281349182\n",
      "Iteration 55, Batch: 23, Loss: 0.19838789105415344\n",
      "Iteration 55, Batch: 24, Loss: 0.21803122758865356\n",
      "Iteration 55, Batch: 25, Loss: 0.23923945426940918\n",
      "Iteration 55, Batch: 26, Loss: 0.20874133706092834\n",
      "Iteration 55, Batch: 27, Loss: 0.2251059114933014\n",
      "Iteration 55, Batch: 28, Loss: 0.24124090373516083\n",
      "Iteration 55, Batch: 29, Loss: 0.22309352457523346\n",
      "Iteration 55, Batch: 30, Loss: 0.1967979371547699\n",
      "Iteration 55, Batch: 31, Loss: 0.22623199224472046\n",
      "Iteration 55, Batch: 32, Loss: 0.25304996967315674\n",
      "Iteration 55, Batch: 33, Loss: 0.2371637225151062\n",
      "Iteration 55, Batch: 34, Loss: 0.24306583404541016\n",
      "Iteration 55, Batch: 35, Loss: 0.24549643695354462\n",
      "Iteration 55, Batch: 36, Loss: 0.21735988557338715\n",
      "Iteration 55, Batch: 37, Loss: 0.22578276693820953\n",
      "Iteration 55, Batch: 38, Loss: 0.21524858474731445\n",
      "Iteration 55, Batch: 39, Loss: 0.1967964470386505\n",
      "Iteration 55, Batch: 40, Loss: 0.2253929227590561\n",
      "Iteration 55, Batch: 41, Loss: 0.25181570649147034\n",
      "Iteration 55, Batch: 42, Loss: 0.26285770535469055\n",
      "Iteration 55, Batch: 43, Loss: 0.23355579376220703\n",
      "Iteration 55, Batch: 44, Loss: 0.21243000030517578\n",
      "Iteration 55, Batch: 45, Loss: 0.24926120042800903\n",
      "Iteration 55, Batch: 46, Loss: 0.21792319416999817\n",
      "Iteration 55, Batch: 47, Loss: 0.22574390470981598\n",
      "Iteration 55, Batch: 48, Loss: 0.24478770792484283\n",
      "Iteration 55, Batch: 49, Loss: 0.21317481994628906\n",
      "Avg loss: 0.2329164507985115\n",
      "Iteration 56, Batch: 0, Loss: 0.2254687398672104\n",
      "Iteration 56, Batch: 1, Loss: 0.1667472869157791\n",
      "Iteration 56, Batch: 2, Loss: 0.21261100471019745\n",
      "Iteration 56, Batch: 3, Loss: 0.19712218642234802\n",
      "Iteration 56, Batch: 4, Loss: 0.2304128259420395\n",
      "Iteration 56, Batch: 5, Loss: 0.21772243082523346\n",
      "Iteration 56, Batch: 6, Loss: 0.2343258261680603\n",
      "Iteration 56, Batch: 7, Loss: 0.24735689163208008\n",
      "Iteration 56, Batch: 8, Loss: 0.25011149048805237\n",
      "Iteration 56, Batch: 9, Loss: 0.28050342202186584\n",
      "Iteration 56, Batch: 10, Loss: 0.2654182016849518\n",
      "Iteration 56, Batch: 11, Loss: 0.22443069517612457\n",
      "Iteration 56, Batch: 12, Loss: 0.25338229537010193\n",
      "Iteration 56, Batch: 13, Loss: 0.20607119798660278\n",
      "Iteration 56, Batch: 14, Loss: 0.23844380676746368\n",
      "Iteration 56, Batch: 15, Loss: 0.2470247745513916\n",
      "Iteration 56, Batch: 16, Loss: 0.26030462980270386\n",
      "Iteration 56, Batch: 17, Loss: 0.23016677796840668\n",
      "Iteration 56, Batch: 18, Loss: 0.25785624980926514\n",
      "Iteration 56, Batch: 19, Loss: 0.22886398434638977\n",
      "Iteration 56, Batch: 20, Loss: 0.25064387917518616\n",
      "Iteration 56, Batch: 21, Loss: 0.2091834545135498\n",
      "Iteration 56, Batch: 22, Loss: 0.2424994856119156\n",
      "Iteration 56, Batch: 23, Loss: 0.22359248995780945\n",
      "Iteration 56, Batch: 24, Loss: 0.1981116235256195\n",
      "Iteration 56, Batch: 25, Loss: 0.19396397471427917\n",
      "Iteration 56, Batch: 26, Loss: 0.2460547238588333\n",
      "Iteration 56, Batch: 27, Loss: 0.21215426921844482\n",
      "Iteration 56, Batch: 28, Loss: 0.2533239424228668\n",
      "Iteration 56, Batch: 29, Loss: 0.30947157740592957\n",
      "Iteration 56, Batch: 30, Loss: 0.2676665186882019\n",
      "Iteration 56, Batch: 31, Loss: 0.21046942472457886\n",
      "Iteration 56, Batch: 32, Loss: 0.20947615802288055\n",
      "Iteration 56, Batch: 33, Loss: 0.20817001163959503\n",
      "Iteration 56, Batch: 34, Loss: 0.2198915034532547\n",
      "Iteration 56, Batch: 35, Loss: 0.2569980323314667\n",
      "Iteration 56, Batch: 36, Loss: 0.18210135400295258\n",
      "Iteration 56, Batch: 37, Loss: 0.1884569674730301\n",
      "Iteration 56, Batch: 38, Loss: 0.2238204926252365\n",
      "Iteration 56, Batch: 39, Loss: 0.2056589275598526\n",
      "Iteration 56, Batch: 40, Loss: 0.29770204424858093\n",
      "Iteration 56, Batch: 41, Loss: 0.21103519201278687\n",
      "Iteration 56, Batch: 42, Loss: 0.24691596627235413\n",
      "Iteration 56, Batch: 43, Loss: 0.24727660417556763\n",
      "Iteration 56, Batch: 44, Loss: 0.21895480155944824\n",
      "Iteration 56, Batch: 45, Loss: 0.2455771267414093\n",
      "Iteration 56, Batch: 46, Loss: 0.20460912585258484\n",
      "Iteration 56, Batch: 47, Loss: 0.22132804989814758\n",
      "Iteration 56, Batch: 48, Loss: 0.195143461227417\n",
      "Iteration 56, Batch: 49, Loss: 0.2243584841489792\n",
      "Avg loss: 0.22997908771038056\n",
      "Iteration 57, Batch: 0, Loss: 0.19127045571804047\n",
      "Iteration 57, Batch: 1, Loss: 0.23427170515060425\n",
      "Iteration 57, Batch: 2, Loss: 0.23640155792236328\n",
      "Iteration 57, Batch: 3, Loss: 0.1975390911102295\n",
      "Iteration 57, Batch: 4, Loss: 0.23150214552879333\n",
      "Iteration 57, Batch: 5, Loss: 0.21788610517978668\n",
      "Iteration 57, Batch: 6, Loss: 0.22979535162448883\n",
      "Iteration 57, Batch: 7, Loss: 0.17231769859790802\n",
      "Iteration 57, Batch: 8, Loss: 0.21086758375167847\n",
      "Iteration 57, Batch: 9, Loss: 0.2199353128671646\n",
      "Iteration 57, Batch: 10, Loss: 0.23214353621006012\n",
      "Iteration 57, Batch: 11, Loss: 0.17364515364170074\n",
      "Iteration 57, Batch: 12, Loss: 0.2060108482837677\n",
      "Iteration 57, Batch: 13, Loss: 0.22390469908714294\n",
      "Iteration 57, Batch: 14, Loss: 0.18152354657649994\n",
      "Iteration 57, Batch: 15, Loss: 0.20822674036026\n",
      "Iteration 57, Batch: 16, Loss: 0.23505428433418274\n",
      "Iteration 57, Batch: 17, Loss: 0.22628726065158844\n",
      "Iteration 57, Batch: 18, Loss: 0.20225650072097778\n",
      "Iteration 57, Batch: 19, Loss: 0.2552216947078705\n",
      "Iteration 57, Batch: 20, Loss: 0.21271783113479614\n",
      "Iteration 57, Batch: 21, Loss: 0.19597502052783966\n",
      "Iteration 57, Batch: 22, Loss: 0.2074449509382248\n",
      "Iteration 57, Batch: 23, Loss: 0.20744578540325165\n",
      "Iteration 57, Batch: 24, Loss: 0.2348761111497879\n",
      "Iteration 57, Batch: 25, Loss: 0.19721241295337677\n",
      "Iteration 57, Batch: 26, Loss: 0.2557145655155182\n",
      "Iteration 57, Batch: 27, Loss: 0.2527438998222351\n",
      "Iteration 57, Batch: 28, Loss: 0.22498534619808197\n",
      "Iteration 57, Batch: 29, Loss: 0.19598989188671112\n",
      "Iteration 57, Batch: 30, Loss: 0.1810026913881302\n",
      "Iteration 57, Batch: 31, Loss: 0.14330944418907166\n",
      "Iteration 57, Batch: 32, Loss: 0.22216418385505676\n",
      "Iteration 57, Batch: 33, Loss: 0.14197938144207\n",
      "Iteration 57, Batch: 34, Loss: 0.22539670765399933\n",
      "Iteration 57, Batch: 35, Loss: 0.18035192787647247\n",
      "Iteration 57, Batch: 36, Loss: 0.20306900143623352\n",
      "Iteration 57, Batch: 37, Loss: 0.19857941567897797\n",
      "Iteration 57, Batch: 38, Loss: 0.24055740237236023\n",
      "Iteration 57, Batch: 39, Loss: 0.24066682159900665\n",
      "Iteration 57, Batch: 40, Loss: 0.26220032572746277\n",
      "Iteration 57, Batch: 41, Loss: 0.22102485597133636\n",
      "Iteration 57, Batch: 42, Loss: 0.17534033954143524\n",
      "Iteration 57, Batch: 43, Loss: 0.2272351086139679\n",
      "Iteration 57, Batch: 44, Loss: 0.19263871014118195\n",
      "Iteration 57, Batch: 45, Loss: 0.21079657971858978\n",
      "Iteration 57, Batch: 46, Loss: 0.20418210327625275\n",
      "Iteration 57, Batch: 47, Loss: 0.21774551272392273\n",
      "Iteration 57, Batch: 48, Loss: 0.2734929919242859\n",
      "Iteration 57, Batch: 49, Loss: 0.25724855065345764\n",
      "Avg loss: 0.21380298286676408\n",
      "Iteration 58, Batch: 0, Loss: 0.2261749804019928\n",
      "Iteration 58, Batch: 1, Loss: 0.26992523670196533\n",
      "Iteration 58, Batch: 2, Loss: 0.2345697283744812\n",
      "Iteration 58, Batch: 3, Loss: 0.2419864535331726\n",
      "Iteration 58, Batch: 4, Loss: 0.20805062353610992\n",
      "Iteration 58, Batch: 5, Loss: 0.2507283091545105\n",
      "Iteration 58, Batch: 6, Loss: 0.2211109846830368\n",
      "Iteration 58, Batch: 7, Loss: 0.2056834101676941\n",
      "Iteration 58, Batch: 8, Loss: 0.20793074369430542\n",
      "Iteration 58, Batch: 9, Loss: 0.2044130265712738\n",
      "Iteration 58, Batch: 10, Loss: 0.22102954983711243\n",
      "Iteration 58, Batch: 11, Loss: 0.22887985408306122\n",
      "Iteration 58, Batch: 12, Loss: 0.16516031324863434\n",
      "Iteration 58, Batch: 13, Loss: 0.18925483524799347\n",
      "Iteration 58, Batch: 14, Loss: 0.2250000536441803\n",
      "Iteration 58, Batch: 15, Loss: 0.2221480756998062\n",
      "Iteration 58, Batch: 16, Loss: 0.269703209400177\n",
      "Iteration 58, Batch: 17, Loss: 0.20413224399089813\n",
      "Iteration 58, Batch: 18, Loss: 0.24744445085525513\n",
      "Iteration 58, Batch: 19, Loss: 0.28935614228248596\n",
      "Iteration 58, Batch: 20, Loss: 0.227140873670578\n",
      "Iteration 58, Batch: 21, Loss: 0.24129065871238708\n",
      "Iteration 58, Batch: 22, Loss: 0.23927642405033112\n",
      "Iteration 58, Batch: 23, Loss: 0.16719698905944824\n",
      "Iteration 58, Batch: 24, Loss: 0.20276786386966705\n",
      "Iteration 58, Batch: 25, Loss: 0.2210160791873932\n",
      "Iteration 58, Batch: 26, Loss: 0.3026982843875885\n",
      "Iteration 58, Batch: 27, Loss: 0.25089752674102783\n",
      "Iteration 58, Batch: 28, Loss: 0.24391624331474304\n",
      "Iteration 58, Batch: 29, Loss: 0.22827699780464172\n",
      "Iteration 58, Batch: 30, Loss: 0.2304464876651764\n",
      "Iteration 58, Batch: 31, Loss: 0.22229509055614471\n",
      "Iteration 58, Batch: 32, Loss: 0.20967939496040344\n",
      "Iteration 58, Batch: 33, Loss: 0.21761944890022278\n",
      "Iteration 58, Batch: 34, Loss: 0.22796912491321564\n",
      "Iteration 58, Batch: 35, Loss: 0.25183406472206116\n",
      "Iteration 58, Batch: 36, Loss: 0.22842611372470856\n",
      "Iteration 58, Batch: 37, Loss: 0.2308708131313324\n",
      "Iteration 58, Batch: 38, Loss: 0.2300085425376892\n",
      "Iteration 58, Batch: 39, Loss: 0.17460320889949799\n",
      "Iteration 58, Batch: 40, Loss: 0.2017146497964859\n",
      "Iteration 58, Batch: 41, Loss: 0.23841409385204315\n",
      "Iteration 58, Batch: 42, Loss: 0.17921584844589233\n",
      "Iteration 58, Batch: 43, Loss: 0.22904586791992188\n",
      "Iteration 58, Batch: 44, Loss: 0.23438258469104767\n",
      "Iteration 58, Batch: 45, Loss: 0.2173803299665451\n",
      "Iteration 58, Batch: 46, Loss: 0.21096403896808624\n",
      "Iteration 58, Batch: 47, Loss: 0.19365066289901733\n",
      "Iteration 58, Batch: 48, Loss: 0.24522270262241364\n",
      "Iteration 58, Batch: 49, Loss: 0.22595283389091492\n",
      "Avg loss: 0.22513712137937547\n",
      "Iteration 59, Batch: 0, Loss: 0.2621554434299469\n",
      "Iteration 59, Batch: 1, Loss: 0.22623297572135925\n",
      "Iteration 59, Batch: 2, Loss: 0.21268023550510406\n",
      "Iteration 59, Batch: 3, Loss: 0.21235018968582153\n",
      "Iteration 59, Batch: 4, Loss: 0.21996013820171356\n",
      "Iteration 59, Batch: 5, Loss: 0.22019068896770477\n",
      "Iteration 59, Batch: 6, Loss: 0.1776932328939438\n",
      "Iteration 59, Batch: 7, Loss: 0.21477797627449036\n",
      "Iteration 59, Batch: 8, Loss: 0.21489498019218445\n",
      "Iteration 59, Batch: 9, Loss: 0.23191656172275543\n",
      "Iteration 59, Batch: 10, Loss: 0.20714126527309418\n",
      "Iteration 59, Batch: 11, Loss: 0.2482175976037979\n",
      "Iteration 59, Batch: 12, Loss: 0.21933400630950928\n",
      "Iteration 59, Batch: 13, Loss: 0.2595520615577698\n",
      "Iteration 59, Batch: 14, Loss: 0.18189814686775208\n",
      "Iteration 59, Batch: 15, Loss: 0.16842754185199738\n",
      "Iteration 59, Batch: 16, Loss: 0.22839659452438354\n",
      "Iteration 59, Batch: 17, Loss: 0.20852912962436676\n",
      "Iteration 59, Batch: 18, Loss: 0.24386946856975555\n",
      "Iteration 59, Batch: 19, Loss: 0.21661078929901123\n",
      "Iteration 59, Batch: 20, Loss: 0.18377873301506042\n",
      "Iteration 59, Batch: 21, Loss: 0.2138787806034088\n",
      "Iteration 59, Batch: 22, Loss: 0.24400557577610016\n",
      "Iteration 59, Batch: 23, Loss: 0.16176825761795044\n",
      "Iteration 59, Batch: 24, Loss: 0.21970388293266296\n",
      "Iteration 59, Batch: 25, Loss: 0.20317493379116058\n",
      "Iteration 59, Batch: 26, Loss: 0.20122797787189484\n",
      "Iteration 59, Batch: 27, Loss: 0.16594645380973816\n",
      "Iteration 59, Batch: 28, Loss: 0.24700920283794403\n",
      "Iteration 59, Batch: 29, Loss: 0.20133140683174133\n",
      "Iteration 59, Batch: 30, Loss: 0.2011594921350479\n",
      "Iteration 59, Batch: 31, Loss: 0.1870017945766449\n",
      "Iteration 59, Batch: 32, Loss: 0.1896648406982422\n",
      "Iteration 59, Batch: 33, Loss: 0.1677265167236328\n",
      "Iteration 59, Batch: 34, Loss: 0.20759883522987366\n",
      "Iteration 59, Batch: 35, Loss: 0.18910422921180725\n",
      "Iteration 59, Batch: 36, Loss: 0.1883232444524765\n",
      "Iteration 59, Batch: 37, Loss: 0.19095174968242645\n",
      "Iteration 59, Batch: 38, Loss: 0.21164405345916748\n",
      "Iteration 59, Batch: 39, Loss: 0.2143770307302475\n",
      "Iteration 59, Batch: 40, Loss: 0.18881376087665558\n",
      "Iteration 59, Batch: 41, Loss: 0.19463293254375458\n",
      "Iteration 59, Batch: 42, Loss: 0.23058867454528809\n",
      "Iteration 59, Batch: 43, Loss: 0.1760713756084442\n",
      "Iteration 59, Batch: 44, Loss: 0.17789790034294128\n",
      "Iteration 59, Batch: 45, Loss: 0.19484397768974304\n",
      "Iteration 59, Batch: 46, Loss: 0.19104152917861938\n",
      "Iteration 59, Batch: 47, Loss: 0.22787708044052124\n",
      "Iteration 59, Batch: 48, Loss: 0.1784380078315735\n",
      "Iteration 59, Batch: 49, Loss: 0.19734565913677216\n",
      "Avg loss: 0.20643513828516005\n",
      "Iteration 60, Batch: 0, Loss: 0.16667549312114716\n",
      "Iteration 60, Batch: 1, Loss: 0.20450980961322784\n",
      "Iteration 60, Batch: 2, Loss: 0.20379693806171417\n",
      "Iteration 60, Batch: 3, Loss: 0.2124439924955368\n",
      "Iteration 60, Batch: 4, Loss: 0.17556487023830414\n",
      "Iteration 60, Batch: 5, Loss: 0.22740663588047028\n",
      "Iteration 60, Batch: 6, Loss: 0.16312049329280853\n",
      "Iteration 60, Batch: 7, Loss: 0.1942092627286911\n",
      "Iteration 60, Batch: 8, Loss: 0.15747632086277008\n",
      "Iteration 60, Batch: 9, Loss: 0.24305713176727295\n",
      "Iteration 60, Batch: 10, Loss: 0.18781575560569763\n",
      "Iteration 60, Batch: 11, Loss: 0.1552480310201645\n",
      "Iteration 60, Batch: 12, Loss: 0.20681831240653992\n",
      "Iteration 60, Batch: 13, Loss: 0.21380853652954102\n",
      "Iteration 60, Batch: 14, Loss: 0.16910341382026672\n",
      "Iteration 60, Batch: 15, Loss: 0.16800618171691895\n",
      "Iteration 60, Batch: 16, Loss: 0.16029685735702515\n",
      "Iteration 60, Batch: 17, Loss: 0.20540721714496613\n",
      "Iteration 60, Batch: 18, Loss: 0.2235168069601059\n",
      "Iteration 60, Batch: 19, Loss: 0.19846609234809875\n",
      "Iteration 60, Batch: 20, Loss: 0.15645572543144226\n",
      "Iteration 60, Batch: 21, Loss: 0.18591560423374176\n",
      "Iteration 60, Batch: 22, Loss: 0.22740782797336578\n",
      "Iteration 60, Batch: 23, Loss: 0.19764429330825806\n",
      "Iteration 60, Batch: 24, Loss: 0.23231978714466095\n",
      "Iteration 60, Batch: 25, Loss: 0.21879717707633972\n",
      "Iteration 60, Batch: 26, Loss: 0.16921141743659973\n",
      "Iteration 60, Batch: 27, Loss: 0.19434215128421783\n",
      "Iteration 60, Batch: 28, Loss: 0.1821136772632599\n",
      "Iteration 60, Batch: 29, Loss: 0.20782405138015747\n",
      "Iteration 60, Batch: 30, Loss: 0.19045870006084442\n",
      "Iteration 60, Batch: 31, Loss: 0.227205291390419\n",
      "Iteration 60, Batch: 32, Loss: 0.1885734498500824\n",
      "Iteration 60, Batch: 33, Loss: 0.1837480515241623\n",
      "Iteration 60, Batch: 34, Loss: 0.2035541534423828\n",
      "Iteration 60, Batch: 35, Loss: 0.1984313577413559\n",
      "Iteration 60, Batch: 36, Loss: 0.213409423828125\n",
      "Iteration 60, Batch: 37, Loss: 0.18427570164203644\n",
      "Iteration 60, Batch: 38, Loss: 0.18308402597904205\n",
      "Iteration 60, Batch: 39, Loss: 0.17602427303791046\n",
      "Iteration 60, Batch: 40, Loss: 0.2248920351266861\n",
      "Iteration 60, Batch: 41, Loss: 0.16992740333080292\n",
      "Iteration 60, Batch: 42, Loss: 0.20293040573596954\n",
      "Iteration 60, Batch: 43, Loss: 0.1774682253599167\n",
      "Iteration 60, Batch: 44, Loss: 0.2283286303281784\n",
      "Iteration 60, Batch: 45, Loss: 0.1891612559556961\n",
      "Iteration 60, Batch: 46, Loss: 0.20130059123039246\n",
      "Iteration 60, Batch: 47, Loss: 0.20197312533855438\n",
      "Iteration 60, Batch: 48, Loss: 0.1857004463672638\n",
      "Iteration 60, Batch: 49, Loss: 0.1743740290403366\n",
      "Avg loss: 0.19427200883626938\n",
      "Iteration 61, Batch: 0, Loss: 0.1581544578075409\n",
      "Iteration 61, Batch: 1, Loss: 0.21345621347427368\n",
      "Iteration 61, Batch: 2, Loss: 0.15999463200569153\n",
      "Iteration 61, Batch: 3, Loss: 0.19294650852680206\n",
      "Iteration 61, Batch: 4, Loss: 0.1553441435098648\n",
      "Iteration 61, Batch: 5, Loss: 0.2050357609987259\n",
      "Iteration 61, Batch: 6, Loss: 0.17213128507137299\n",
      "Iteration 61, Batch: 7, Loss: 0.23784033954143524\n",
      "Iteration 61, Batch: 8, Loss: 0.18307246267795563\n",
      "Iteration 61, Batch: 9, Loss: 0.16829444468021393\n",
      "Iteration 61, Batch: 10, Loss: 0.1765742003917694\n",
      "Iteration 61, Batch: 11, Loss: 0.174229234457016\n",
      "Iteration 61, Batch: 12, Loss: 0.1912723183631897\n",
      "Iteration 61, Batch: 13, Loss: 0.16763772070407867\n",
      "Iteration 61, Batch: 14, Loss: 0.1394578367471695\n",
      "Iteration 61, Batch: 15, Loss: 0.20637187361717224\n",
      "Iteration 61, Batch: 16, Loss: 0.16691884398460388\n",
      "Iteration 61, Batch: 17, Loss: 0.18006150424480438\n",
      "Iteration 61, Batch: 18, Loss: 0.19362136721611023\n",
      "Iteration 61, Batch: 19, Loss: 0.17975950241088867\n",
      "Iteration 61, Batch: 20, Loss: 0.19477511942386627\n",
      "Iteration 61, Batch: 21, Loss: 0.2083410620689392\n",
      "Iteration 61, Batch: 22, Loss: 0.20621506869792938\n",
      "Iteration 61, Batch: 23, Loss: 0.16284160315990448\n",
      "Iteration 61, Batch: 24, Loss: 0.1484825313091278\n",
      "Iteration 61, Batch: 25, Loss: 0.19014950096607208\n",
      "Iteration 61, Batch: 26, Loss: 0.19545230269432068\n",
      "Iteration 61, Batch: 27, Loss: 0.1986047774553299\n",
      "Iteration 61, Batch: 28, Loss: 0.2374190241098404\n",
      "Iteration 61, Batch: 29, Loss: 0.21357297897338867\n",
      "Iteration 61, Batch: 30, Loss: 0.20330137014389038\n",
      "Iteration 61, Batch: 31, Loss: 0.22336703538894653\n",
      "Iteration 61, Batch: 32, Loss: 0.19433942437171936\n",
      "Iteration 61, Batch: 33, Loss: 0.21831028163433075\n",
      "Iteration 61, Batch: 34, Loss: 0.22168834507465363\n",
      "Iteration 61, Batch: 35, Loss: 0.18744170665740967\n",
      "Iteration 61, Batch: 36, Loss: 0.1690504401922226\n",
      "Iteration 61, Batch: 37, Loss: 0.234974667429924\n",
      "Iteration 61, Batch: 38, Loss: 0.21693971753120422\n",
      "Iteration 61, Batch: 39, Loss: 0.19446714222431183\n",
      "Iteration 61, Batch: 40, Loss: 0.21179233491420746\n",
      "Iteration 61, Batch: 41, Loss: 0.21342192590236664\n",
      "Iteration 61, Batch: 42, Loss: 0.22734832763671875\n",
      "Iteration 61, Batch: 43, Loss: 0.21815453469753265\n",
      "Iteration 61, Batch: 44, Loss: 0.203541100025177\n",
      "Iteration 61, Batch: 45, Loss: 0.1881134808063507\n",
      "Iteration 61, Batch: 46, Loss: 0.1897183507680893\n",
      "Iteration 61, Batch: 47, Loss: 0.16953431069850922\n",
      "Iteration 61, Batch: 48, Loss: 0.20023253560066223\n",
      "Iteration 61, Batch: 49, Loss: 0.1687544733285904\n",
      "Avg loss: 0.1926504024863243\n",
      "Iteration 62, Batch: 0, Loss: 0.18911369144916534\n",
      "Iteration 62, Batch: 1, Loss: 0.1880805790424347\n",
      "Iteration 62, Batch: 2, Loss: 0.20325377583503723\n",
      "Iteration 62, Batch: 3, Loss: 0.20206771790981293\n",
      "Iteration 62, Batch: 4, Loss: 0.20370621979236603\n",
      "Iteration 62, Batch: 5, Loss: 0.17873378098011017\n",
      "Iteration 62, Batch: 6, Loss: 0.24151690304279327\n",
      "Iteration 62, Batch: 7, Loss: 0.17624494433403015\n",
      "Iteration 62, Batch: 8, Loss: 0.1891777366399765\n",
      "Iteration 62, Batch: 9, Loss: 0.22868941724300385\n",
      "Iteration 62, Batch: 10, Loss: 0.1984097957611084\n",
      "Iteration 62, Batch: 11, Loss: 0.19238385558128357\n",
      "Iteration 62, Batch: 12, Loss: 0.19905029237270355\n",
      "Iteration 62, Batch: 13, Loss: 0.19562704861164093\n",
      "Iteration 62, Batch: 14, Loss: 0.15277603268623352\n",
      "Iteration 62, Batch: 15, Loss: 0.21777679026126862\n",
      "Iteration 62, Batch: 16, Loss: 0.18044234812259674\n",
      "Iteration 62, Batch: 17, Loss: 0.20219357311725616\n",
      "Iteration 62, Batch: 18, Loss: 0.1494223028421402\n",
      "Iteration 62, Batch: 19, Loss: 0.16290852427482605\n",
      "Iteration 62, Batch: 20, Loss: 0.17077460885047913\n",
      "Iteration 62, Batch: 21, Loss: 0.21824152767658234\n",
      "Iteration 62, Batch: 22, Loss: 0.1827363222837448\n",
      "Iteration 62, Batch: 23, Loss: 0.18767958879470825\n",
      "Iteration 62, Batch: 24, Loss: 0.18941260874271393\n",
      "Iteration 62, Batch: 25, Loss: 0.19070681929588318\n",
      "Iteration 62, Batch: 26, Loss: 0.19683948159217834\n",
      "Iteration 62, Batch: 27, Loss: 0.18485821783542633\n",
      "Iteration 62, Batch: 28, Loss: 0.17375220358371735\n",
      "Iteration 62, Batch: 29, Loss: 0.15241461992263794\n",
      "Iteration 62, Batch: 30, Loss: 0.16806277632713318\n",
      "Iteration 62, Batch: 31, Loss: 0.1808374971151352\n",
      "Iteration 62, Batch: 32, Loss: 0.1741638332605362\n",
      "Iteration 62, Batch: 33, Loss: 0.20876829326152802\n",
      "Iteration 62, Batch: 34, Loss: 0.20537284016609192\n",
      "Iteration 62, Batch: 35, Loss: 0.20378346741199493\n",
      "Iteration 62, Batch: 36, Loss: 0.17683131992816925\n",
      "Iteration 62, Batch: 37, Loss: 0.1956464946269989\n",
      "Iteration 62, Batch: 38, Loss: 0.21201469004154205\n",
      "Iteration 62, Batch: 39, Loss: 0.16917510330677032\n",
      "Iteration 62, Batch: 40, Loss: 0.16619473695755005\n",
      "Iteration 62, Batch: 41, Loss: 0.15485981106758118\n",
      "Iteration 62, Batch: 42, Loss: 0.17295266687870026\n",
      "Iteration 62, Batch: 43, Loss: 0.19112002849578857\n",
      "Iteration 62, Batch: 44, Loss: 0.2100207805633545\n",
      "Iteration 62, Batch: 45, Loss: 0.1757333129644394\n",
      "Iteration 62, Batch: 46, Loss: 0.19249019026756287\n",
      "Iteration 62, Batch: 47, Loss: 0.1818823516368866\n",
      "Iteration 62, Batch: 48, Loss: 0.18996261060237885\n",
      "Iteration 62, Batch: 49, Loss: 0.1764679253101349\n",
      "Avg loss: 0.18810664117336273\n",
      "Iteration 63, Batch: 0, Loss: 0.1882733553647995\n",
      "Iteration 63, Batch: 1, Loss: 0.17773985862731934\n",
      "Iteration 63, Batch: 2, Loss: 0.17517967522144318\n",
      "Iteration 63, Batch: 3, Loss: 0.17336256802082062\n",
      "Iteration 63, Batch: 4, Loss: 0.15891893208026886\n",
      "Iteration 63, Batch: 5, Loss: 0.19144085049629211\n",
      "Iteration 63, Batch: 6, Loss: 0.21307532489299774\n",
      "Iteration 63, Batch: 7, Loss: 0.1805221140384674\n",
      "Iteration 63, Batch: 8, Loss: 0.2043570876121521\n",
      "Iteration 63, Batch: 9, Loss: 0.19673176109790802\n",
      "Iteration 63, Batch: 10, Loss: 0.17844001948833466\n",
      "Iteration 63, Batch: 11, Loss: 0.18115290999412537\n",
      "Iteration 63, Batch: 12, Loss: 0.1467030942440033\n",
      "Iteration 63, Batch: 13, Loss: 0.18692295253276825\n",
      "Iteration 63, Batch: 14, Loss: 0.22175048291683197\n",
      "Iteration 63, Batch: 15, Loss: 0.19709178805351257\n",
      "Iteration 63, Batch: 16, Loss: 0.22058746218681335\n",
      "Iteration 63, Batch: 17, Loss: 0.1413530856370926\n",
      "Iteration 63, Batch: 18, Loss: 0.18117274343967438\n",
      "Iteration 63, Batch: 19, Loss: 0.16926860809326172\n",
      "Iteration 63, Batch: 20, Loss: 0.15121711790561676\n",
      "Iteration 63, Batch: 21, Loss: 0.1716606169939041\n",
      "Iteration 63, Batch: 22, Loss: 0.18788032233715057\n",
      "Iteration 63, Batch: 23, Loss: 0.1752551943063736\n",
      "Iteration 63, Batch: 24, Loss: 0.15852926671504974\n",
      "Iteration 63, Batch: 25, Loss: 0.14553603529930115\n",
      "Iteration 63, Batch: 26, Loss: 0.1807917356491089\n",
      "Iteration 63, Batch: 27, Loss: 0.1628798395395279\n",
      "Iteration 63, Batch: 28, Loss: 0.19831249117851257\n",
      "Iteration 63, Batch: 29, Loss: 0.18328757584095\n",
      "Iteration 63, Batch: 30, Loss: 0.1558024138212204\n",
      "Iteration 63, Batch: 31, Loss: 0.22437764704227448\n",
      "Iteration 63, Batch: 32, Loss: 0.1908777803182602\n",
      "Iteration 63, Batch: 33, Loss: 0.18360736966133118\n",
      "Iteration 63, Batch: 34, Loss: 0.2549320161342621\n",
      "Iteration 63, Batch: 35, Loss: 0.16253972053527832\n",
      "Iteration 63, Batch: 36, Loss: 0.15897151827812195\n",
      "Iteration 63, Batch: 37, Loss: 0.20026637613773346\n",
      "Iteration 63, Batch: 38, Loss: 0.21823538839817047\n",
      "Iteration 63, Batch: 39, Loss: 0.20400755107402802\n",
      "Iteration 63, Batch: 40, Loss: 0.21110457181930542\n",
      "Iteration 63, Batch: 41, Loss: 0.16735699772834778\n",
      "Iteration 63, Batch: 42, Loss: 0.16324028372764587\n",
      "Iteration 63, Batch: 43, Loss: 0.16757896542549133\n",
      "Iteration 63, Batch: 44, Loss: 0.16220736503601074\n",
      "Iteration 63, Batch: 45, Loss: 0.17993998527526855\n",
      "Iteration 63, Batch: 46, Loss: 0.14498357474803925\n",
      "Iteration 63, Batch: 47, Loss: 0.2048531472682953\n",
      "Iteration 63, Batch: 48, Loss: 0.20588906109333038\n",
      "Iteration 63, Batch: 49, Loss: 0.18731233477592468\n",
      "Avg loss: 0.18294957876205445\n",
      "Iteration 64, Batch: 0, Loss: 0.21633502840995789\n",
      "Iteration 64, Batch: 1, Loss: 0.1996604949235916\n",
      "Iteration 64, Batch: 2, Loss: 0.1629127860069275\n",
      "Iteration 64, Batch: 3, Loss: 0.17640574276447296\n",
      "Iteration 64, Batch: 4, Loss: 0.1857006549835205\n",
      "Iteration 64, Batch: 5, Loss: 0.18480247259140015\n",
      "Iteration 64, Batch: 6, Loss: 0.18159431219100952\n",
      "Iteration 64, Batch: 7, Loss: 0.15970714390277863\n",
      "Iteration 64, Batch: 8, Loss: 0.13268858194351196\n",
      "Iteration 64, Batch: 9, Loss: 0.20066222548484802\n",
      "Iteration 64, Batch: 10, Loss: 0.18094249069690704\n",
      "Iteration 64, Batch: 11, Loss: 0.17784930765628815\n",
      "Iteration 64, Batch: 12, Loss: 0.1779450625181198\n",
      "Iteration 64, Batch: 13, Loss: 0.1931234747171402\n",
      "Iteration 64, Batch: 14, Loss: 0.17812149226665497\n",
      "Iteration 64, Batch: 15, Loss: 0.20858997106552124\n",
      "Iteration 64, Batch: 16, Loss: 0.1942870318889618\n",
      "Iteration 64, Batch: 17, Loss: 0.15424616634845734\n",
      "Iteration 64, Batch: 18, Loss: 0.1904105693101883\n",
      "Iteration 64, Batch: 19, Loss: 0.2694515287876129\n",
      "Iteration 64, Batch: 20, Loss: 0.18342959880828857\n",
      "Iteration 64, Batch: 21, Loss: 0.20714476704597473\n",
      "Iteration 64, Batch: 22, Loss: 0.20056194067001343\n",
      "Iteration 64, Batch: 23, Loss: 0.1371770203113556\n",
      "Iteration 64, Batch: 24, Loss: 0.2060810625553131\n",
      "Iteration 64, Batch: 25, Loss: 0.16268767416477203\n",
      "Iteration 64, Batch: 26, Loss: 0.15763172507286072\n",
      "Iteration 64, Batch: 27, Loss: 0.14316320419311523\n",
      "Iteration 64, Batch: 28, Loss: 0.1716374158859253\n",
      "Iteration 64, Batch: 29, Loss: 0.16839198768138885\n",
      "Iteration 64, Batch: 30, Loss: 0.17999468743801117\n",
      "Iteration 64, Batch: 31, Loss: 0.1647326946258545\n",
      "Iteration 64, Batch: 32, Loss: 0.17396560311317444\n",
      "Iteration 64, Batch: 33, Loss: 0.21040739119052887\n",
      "Iteration 64, Batch: 34, Loss: 0.20479558408260345\n",
      "Iteration 64, Batch: 35, Loss: 0.14431068301200867\n",
      "Iteration 64, Batch: 36, Loss: 0.18309040367603302\n",
      "Iteration 64, Batch: 37, Loss: 0.2404627501964569\n",
      "Iteration 64, Batch: 38, Loss: 0.1855391263961792\n",
      "Iteration 64, Batch: 39, Loss: 0.17645709216594696\n",
      "Iteration 64, Batch: 40, Loss: 0.19865471124649048\n",
      "Iteration 64, Batch: 41, Loss: 0.18518497049808502\n",
      "Iteration 64, Batch: 42, Loss: 0.17025235295295715\n",
      "Iteration 64, Batch: 43, Loss: 0.1720794290304184\n",
      "Iteration 64, Batch: 44, Loss: 0.20542070269584656\n",
      "Iteration 64, Batch: 45, Loss: 0.16305160522460938\n",
      "Iteration 64, Batch: 46, Loss: 0.16599513590335846\n",
      "Iteration 64, Batch: 47, Loss: 0.17564912140369415\n",
      "Iteration 64, Batch: 48, Loss: 0.1375831514596939\n",
      "Iteration 64, Batch: 49, Loss: 0.18350602686405182\n",
      "Avg loss: 0.1816895231604576\n",
      "Iteration 65, Batch: 0, Loss: 0.1591574102640152\n",
      "Iteration 65, Batch: 1, Loss: 0.20209118723869324\n",
      "Iteration 65, Batch: 2, Loss: 0.12524433434009552\n",
      "Iteration 65, Batch: 3, Loss: 0.16195552051067352\n",
      "Iteration 65, Batch: 4, Loss: 0.2122543603181839\n",
      "Iteration 65, Batch: 5, Loss: 0.16462016105651855\n",
      "Iteration 65, Batch: 6, Loss: 0.1519201695919037\n",
      "Iteration 65, Batch: 7, Loss: 0.16487625241279602\n",
      "Iteration 65, Batch: 8, Loss: 0.16030433773994446\n",
      "Iteration 65, Batch: 9, Loss: 0.1903351992368698\n",
      "Iteration 65, Batch: 10, Loss: 0.157429039478302\n",
      "Iteration 65, Batch: 11, Loss: 0.18039637804031372\n",
      "Iteration 65, Batch: 12, Loss: 0.15098772943019867\n",
      "Iteration 65, Batch: 13, Loss: 0.20852886140346527\n",
      "Iteration 65, Batch: 14, Loss: 0.15290062129497528\n",
      "Iteration 65, Batch: 15, Loss: 0.18946030735969543\n",
      "Iteration 65, Batch: 16, Loss: 0.19016021490097046\n",
      "Iteration 65, Batch: 17, Loss: 0.14377184212207794\n",
      "Iteration 65, Batch: 18, Loss: 0.16054177284240723\n",
      "Iteration 65, Batch: 19, Loss: 0.17713676393032074\n",
      "Iteration 65, Batch: 20, Loss: 0.13313502073287964\n",
      "Iteration 65, Batch: 21, Loss: 0.1683492213487625\n",
      "Iteration 65, Batch: 22, Loss: 0.15541982650756836\n",
      "Iteration 65, Batch: 23, Loss: 0.1420079916715622\n",
      "Iteration 65, Batch: 24, Loss: 0.1748545616865158\n",
      "Iteration 65, Batch: 25, Loss: 0.1713045984506607\n",
      "Iteration 65, Batch: 26, Loss: 0.14819873869419098\n",
      "Iteration 65, Batch: 27, Loss: 0.1880614012479782\n",
      "Iteration 65, Batch: 28, Loss: 0.1280858814716339\n",
      "Iteration 65, Batch: 29, Loss: 0.1655866652727127\n",
      "Iteration 65, Batch: 30, Loss: 0.14965659379959106\n",
      "Iteration 65, Batch: 31, Loss: 0.18132515251636505\n",
      "Iteration 65, Batch: 32, Loss: 0.16859473288059235\n",
      "Iteration 65, Batch: 33, Loss: 0.19325122237205505\n",
      "Iteration 65, Batch: 34, Loss: 0.20040223002433777\n",
      "Iteration 65, Batch: 35, Loss: 0.1794697493314743\n",
      "Iteration 65, Batch: 36, Loss: 0.177589550614357\n",
      "Iteration 65, Batch: 37, Loss: 0.2264840453863144\n",
      "Iteration 65, Batch: 38, Loss: 0.1754244565963745\n",
      "Iteration 65, Batch: 39, Loss: 0.1552376002073288\n",
      "Iteration 65, Batch: 40, Loss: 0.19412608444690704\n",
      "Iteration 65, Batch: 41, Loss: 0.17845405638217926\n",
      "Iteration 65, Batch: 42, Loss: 0.1391427367925644\n",
      "Iteration 65, Batch: 43, Loss: 0.153400719165802\n",
      "Iteration 65, Batch: 44, Loss: 0.15817226469516754\n",
      "Iteration 65, Batch: 45, Loss: 0.18525132536888123\n",
      "Iteration 65, Batch: 46, Loss: 0.16098137199878693\n",
      "Iteration 65, Batch: 47, Loss: 0.14144596457481384\n",
      "Iteration 65, Batch: 48, Loss: 0.16596153378486633\n",
      "Iteration 65, Batch: 49, Loss: 0.1424976885318756\n",
      "Avg loss: 0.1681189090013504\n",
      "Iteration 66, Batch: 0, Loss: 0.19716094434261322\n",
      "Iteration 66, Batch: 1, Loss: 0.1671762615442276\n",
      "Iteration 66, Batch: 2, Loss: 0.16886843740940094\n",
      "Iteration 66, Batch: 3, Loss: 0.19377806782722473\n",
      "Iteration 66, Batch: 4, Loss: 0.13807517290115356\n",
      "Iteration 66, Batch: 5, Loss: 0.1714809536933899\n",
      "Iteration 66, Batch: 6, Loss: 0.15377797186374664\n",
      "Iteration 66, Batch: 7, Loss: 0.18001693487167358\n",
      "Iteration 66, Batch: 8, Loss: 0.16467897593975067\n",
      "Iteration 66, Batch: 9, Loss: 0.17780566215515137\n",
      "Iteration 66, Batch: 10, Loss: 0.18544840812683105\n",
      "Iteration 66, Batch: 11, Loss: 0.18392062187194824\n",
      "Iteration 66, Batch: 12, Loss: 0.14796149730682373\n",
      "Iteration 66, Batch: 13, Loss: 0.15380574762821198\n",
      "Iteration 66, Batch: 14, Loss: 0.13738973438739777\n",
      "Iteration 66, Batch: 15, Loss: 0.12968239188194275\n",
      "Iteration 66, Batch: 16, Loss: 0.16968199610710144\n",
      "Iteration 66, Batch: 17, Loss: 0.14109523594379425\n",
      "Iteration 66, Batch: 18, Loss: 0.16889198124408722\n",
      "Iteration 66, Batch: 19, Loss: 0.19392207264900208\n",
      "Iteration 66, Batch: 20, Loss: 0.13872425258159637\n",
      "Iteration 66, Batch: 21, Loss: 0.1524210274219513\n",
      "Iteration 66, Batch: 22, Loss: 0.15293942391872406\n",
      "Iteration 66, Batch: 23, Loss: 0.1398167908191681\n",
      "Iteration 66, Batch: 24, Loss: 0.196745827794075\n",
      "Iteration 66, Batch: 25, Loss: 0.160410538315773\n",
      "Iteration 66, Batch: 26, Loss: 0.1703440397977829\n",
      "Iteration 66, Batch: 27, Loss: 0.20291325449943542\n",
      "Iteration 66, Batch: 28, Loss: 0.17182780802249908\n",
      "Iteration 66, Batch: 29, Loss: 0.17479732632637024\n",
      "Iteration 66, Batch: 30, Loss: 0.14812153577804565\n",
      "Iteration 66, Batch: 31, Loss: 0.18036727607250214\n",
      "Iteration 66, Batch: 32, Loss: 0.1447192281484604\n",
      "Iteration 66, Batch: 33, Loss: 0.1690184324979782\n",
      "Iteration 66, Batch: 34, Loss: 0.19588589668273926\n",
      "Iteration 66, Batch: 35, Loss: 0.16436637938022614\n",
      "Iteration 66, Batch: 36, Loss: 0.1511315554380417\n",
      "Iteration 66, Batch: 37, Loss: 0.2018800675868988\n",
      "Iteration 66, Batch: 38, Loss: 0.19731684029102325\n",
      "Iteration 66, Batch: 39, Loss: 0.17166846990585327\n",
      "Iteration 66, Batch: 40, Loss: 0.16478335857391357\n",
      "Iteration 66, Batch: 41, Loss: 0.153784841299057\n",
      "Iteration 66, Batch: 42, Loss: 0.18039001524448395\n",
      "Iteration 66, Batch: 43, Loss: 0.14037510752677917\n",
      "Iteration 66, Batch: 44, Loss: 0.14347942173480988\n",
      "Iteration 66, Batch: 45, Loss: 0.17265388369560242\n",
      "Iteration 66, Batch: 46, Loss: 0.17163188755512238\n",
      "Iteration 66, Batch: 47, Loss: 0.16546331346035004\n",
      "Iteration 66, Batch: 48, Loss: 0.1839737445116043\n",
      "Iteration 66, Batch: 49, Loss: 0.17105723917484283\n",
      "Avg loss: 0.16715255707502366\n",
      "Iteration 67, Batch: 0, Loss: 0.181487575173378\n",
      "Iteration 67, Batch: 1, Loss: 0.1685498207807541\n",
      "Iteration 67, Batch: 2, Loss: 0.14338427782058716\n",
      "Iteration 67, Batch: 3, Loss: 0.16694507002830505\n",
      "Iteration 67, Batch: 4, Loss: 0.15606427192687988\n",
      "Iteration 67, Batch: 5, Loss: 0.17901979386806488\n",
      "Iteration 67, Batch: 6, Loss: 0.1733756810426712\n",
      "Iteration 67, Batch: 7, Loss: 0.18447871506214142\n",
      "Iteration 67, Batch: 8, Loss: 0.18494343757629395\n",
      "Iteration 67, Batch: 9, Loss: 0.20648005604743958\n",
      "Iteration 67, Batch: 10, Loss: 0.18485338985919952\n",
      "Iteration 67, Batch: 11, Loss: 0.15543952584266663\n",
      "Iteration 67, Batch: 12, Loss: 0.17858950793743134\n",
      "Iteration 67, Batch: 13, Loss: 0.16933688521385193\n",
      "Iteration 67, Batch: 14, Loss: 0.17196770012378693\n",
      "Iteration 67, Batch: 15, Loss: 0.1808149367570877\n",
      "Iteration 67, Batch: 16, Loss: 0.19906863570213318\n",
      "Iteration 67, Batch: 17, Loss: 0.14920850098133087\n",
      "Iteration 67, Batch: 18, Loss: 0.17011664807796478\n",
      "Iteration 67, Batch: 19, Loss: 0.17914921045303345\n",
      "Iteration 67, Batch: 20, Loss: 0.16641487181186676\n",
      "Iteration 67, Batch: 21, Loss: 0.18519653379917145\n",
      "Iteration 67, Batch: 22, Loss: 0.16817374527454376\n",
      "Iteration 67, Batch: 23, Loss: 0.1301305592060089\n",
      "Iteration 67, Batch: 24, Loss: 0.13164663314819336\n",
      "Iteration 67, Batch: 25, Loss: 0.1695171743631363\n",
      "Iteration 67, Batch: 26, Loss: 0.16896916925907135\n",
      "Iteration 67, Batch: 27, Loss: 0.1764988899230957\n",
      "Iteration 67, Batch: 28, Loss: 0.17935128509998322\n",
      "Iteration 67, Batch: 29, Loss: 0.1897169053554535\n",
      "Iteration 67, Batch: 30, Loss: 0.17159897089004517\n",
      "Iteration 67, Batch: 31, Loss: 0.1432792991399765\n",
      "Iteration 67, Batch: 32, Loss: 0.12684623897075653\n",
      "Iteration 67, Batch: 33, Loss: 0.19080302119255066\n",
      "Iteration 67, Batch: 34, Loss: 0.15133771300315857\n",
      "Iteration 67, Batch: 35, Loss: 0.20463895797729492\n",
      "Iteration 67, Batch: 36, Loss: 0.18016135692596436\n",
      "Iteration 67, Batch: 37, Loss: 0.1411304622888565\n",
      "Iteration 67, Batch: 38, Loss: 0.1343713253736496\n",
      "Iteration 67, Batch: 39, Loss: 0.1661601960659027\n",
      "Iteration 67, Batch: 40, Loss: 0.14213941991329193\n",
      "Iteration 67, Batch: 41, Loss: 0.17280593514442444\n",
      "Iteration 67, Batch: 42, Loss: 0.17362205684185028\n",
      "Iteration 67, Batch: 43, Loss: 0.13749590516090393\n",
      "Iteration 67, Batch: 44, Loss: 0.1517801433801651\n",
      "Iteration 67, Batch: 45, Loss: 0.15359778702259064\n",
      "Iteration 67, Batch: 46, Loss: 0.11594809591770172\n",
      "Iteration 67, Batch: 47, Loss: 0.19440890848636627\n",
      "Iteration 67, Batch: 48, Loss: 0.17505455017089844\n",
      "Iteration 67, Batch: 49, Loss: 0.15999476611614227\n",
      "Avg loss: 0.16672129034996033\n",
      "Iteration 68, Batch: 0, Loss: 0.17330417037010193\n",
      "Iteration 68, Batch: 1, Loss: 0.21568484604358673\n",
      "Iteration 68, Batch: 2, Loss: 0.17590321600437164\n",
      "Iteration 68, Batch: 3, Loss: 0.15717332065105438\n",
      "Iteration 68, Batch: 4, Loss: 0.15764956176280975\n",
      "Iteration 68, Batch: 5, Loss: 0.1885889321565628\n",
      "Iteration 68, Batch: 6, Loss: 0.15441688895225525\n",
      "Iteration 68, Batch: 7, Loss: 0.16405510902404785\n",
      "Iteration 68, Batch: 8, Loss: 0.15026269853115082\n",
      "Iteration 68, Batch: 9, Loss: 0.14943987131118774\n",
      "Iteration 68, Batch: 10, Loss: 0.15174178779125214\n",
      "Iteration 68, Batch: 11, Loss: 0.21917404234409332\n",
      "Iteration 68, Batch: 12, Loss: 0.18294303119182587\n",
      "Iteration 68, Batch: 13, Loss: 0.1598730981349945\n",
      "Iteration 68, Batch: 14, Loss: 0.19655948877334595\n",
      "Iteration 68, Batch: 15, Loss: 0.16650478541851044\n",
      "Iteration 68, Batch: 16, Loss: 0.16502375900745392\n",
      "Iteration 68, Batch: 17, Loss: 0.13884314894676208\n",
      "Iteration 68, Batch: 18, Loss: 0.21031543612480164\n",
      "Iteration 68, Batch: 19, Loss: 0.1569468230009079\n",
      "Iteration 68, Batch: 20, Loss: 0.16535229980945587\n",
      "Iteration 68, Batch: 21, Loss: 0.11970654875040054\n",
      "Iteration 68, Batch: 22, Loss: 0.15388210117816925\n",
      "Iteration 68, Batch: 23, Loss: 0.21823178231716156\n",
      "Iteration 68, Batch: 24, Loss: 0.1792275607585907\n",
      "Iteration 68, Batch: 25, Loss: 0.149087056517601\n",
      "Iteration 68, Batch: 26, Loss: 0.22587226331233978\n",
      "Iteration 68, Batch: 27, Loss: 0.1607656478881836\n",
      "Iteration 68, Batch: 28, Loss: 0.17338381707668304\n",
      "Iteration 68, Batch: 29, Loss: 0.17456607520580292\n",
      "Iteration 68, Batch: 30, Loss: 0.15899695456027985\n",
      "Iteration 68, Batch: 31, Loss: 0.16813312470912933\n",
      "Iteration 68, Batch: 32, Loss: 0.1322413831949234\n",
      "Iteration 68, Batch: 33, Loss: 0.18547217547893524\n",
      "Iteration 68, Batch: 34, Loss: 0.1728142946958542\n",
      "Iteration 68, Batch: 35, Loss: 0.17942537367343903\n",
      "Iteration 68, Batch: 36, Loss: 0.17803886532783508\n",
      "Iteration 68, Batch: 37, Loss: 0.13969388604164124\n",
      "Iteration 68, Batch: 38, Loss: 0.13743004202842712\n",
      "Iteration 68, Batch: 39, Loss: 0.17146849632263184\n",
      "Iteration 68, Batch: 40, Loss: 0.1142929270863533\n",
      "Iteration 68, Batch: 41, Loss: 0.20553652942180634\n",
      "Iteration 68, Batch: 42, Loss: 0.15380240976810455\n",
      "Iteration 68, Batch: 43, Loss: 0.14509493112564087\n",
      "Iteration 68, Batch: 44, Loss: 0.19707472622394562\n",
      "Iteration 68, Batch: 45, Loss: 0.1675366759300232\n",
      "Iteration 68, Batch: 46, Loss: 0.14509990811347961\n",
      "Iteration 68, Batch: 47, Loss: 0.1581752598285675\n",
      "Iteration 68, Batch: 48, Loss: 0.14371047914028168\n",
      "Iteration 68, Batch: 49, Loss: 0.15651771426200867\n",
      "Avg loss: 0.16730070650577544\n",
      "Iteration 69, Batch: 0, Loss: 0.170572429895401\n",
      "Iteration 69, Batch: 1, Loss: 0.16518789529800415\n",
      "Iteration 69, Batch: 2, Loss: 0.19517511129379272\n",
      "Iteration 69, Batch: 3, Loss: 0.1993858814239502\n",
      "Iteration 69, Batch: 4, Loss: 0.1403302103281021\n",
      "Iteration 69, Batch: 5, Loss: 0.17272602021694183\n",
      "Iteration 69, Batch: 6, Loss: 0.13369120657444\n",
      "Iteration 69, Batch: 7, Loss: 0.1348855048418045\n",
      "Iteration 69, Batch: 8, Loss: 0.13219501078128815\n",
      "Iteration 69, Batch: 9, Loss: 0.16608485579490662\n",
      "Iteration 69, Batch: 10, Loss: 0.19102588295936584\n",
      "Iteration 69, Batch: 11, Loss: 0.15391883254051208\n",
      "Iteration 69, Batch: 12, Loss: 0.16147586703300476\n",
      "Iteration 69, Batch: 13, Loss: 0.16515228152275085\n",
      "Iteration 69, Batch: 14, Loss: 0.1506991684436798\n",
      "Iteration 69, Batch: 15, Loss: 0.1252620816230774\n",
      "Iteration 69, Batch: 16, Loss: 0.15490791201591492\n",
      "Iteration 69, Batch: 17, Loss: 0.16953130066394806\n",
      "Iteration 69, Batch: 18, Loss: 0.12654389441013336\n",
      "Iteration 69, Batch: 19, Loss: 0.15808339416980743\n",
      "Iteration 69, Batch: 20, Loss: 0.1560242772102356\n",
      "Iteration 69, Batch: 21, Loss: 0.1195785328745842\n",
      "Iteration 69, Batch: 22, Loss: 0.19199343025684357\n",
      "Iteration 69, Batch: 23, Loss: 0.1730589121580124\n",
      "Iteration 69, Batch: 24, Loss: 0.17820462584495544\n",
      "Iteration 69, Batch: 25, Loss: 0.18501533567905426\n",
      "Iteration 69, Batch: 26, Loss: 0.15491795539855957\n",
      "Iteration 69, Batch: 27, Loss: 0.15580564737319946\n",
      "Iteration 69, Batch: 28, Loss: 0.1656333953142166\n",
      "Iteration 69, Batch: 29, Loss: 0.15852561593055725\n",
      "Iteration 69, Batch: 30, Loss: 0.14639507234096527\n",
      "Iteration 69, Batch: 31, Loss: 0.1736506223678589\n",
      "Iteration 69, Batch: 32, Loss: 0.1532534956932068\n",
      "Iteration 69, Batch: 33, Loss: 0.19044631719589233\n",
      "Iteration 69, Batch: 34, Loss: 0.14303408563137054\n",
      "Iteration 69, Batch: 35, Loss: 0.15381139516830444\n",
      "Iteration 69, Batch: 36, Loss: 0.15344101190567017\n",
      "Iteration 69, Batch: 37, Loss: 0.16226062178611755\n",
      "Iteration 69, Batch: 38, Loss: 0.18685632944107056\n",
      "Iteration 69, Batch: 39, Loss: 0.1458495855331421\n",
      "Iteration 69, Batch: 40, Loss: 0.13011668622493744\n",
      "Iteration 69, Batch: 41, Loss: 0.16461573541164398\n",
      "Iteration 69, Batch: 42, Loss: 0.14900922775268555\n",
      "Iteration 69, Batch: 43, Loss: 0.14356182515621185\n",
      "Iteration 69, Batch: 44, Loss: 0.14117944240570068\n",
      "Iteration 69, Batch: 45, Loss: 0.13052593171596527\n",
      "Iteration 69, Batch: 46, Loss: 0.10248870402574539\n",
      "Iteration 69, Batch: 47, Loss: 0.1525401622056961\n",
      "Iteration 69, Batch: 48, Loss: 0.16482608020305634\n",
      "Iteration 69, Batch: 49, Loss: 0.1562599390745163\n",
      "Avg loss: 0.15699429482221602\n",
      "Iteration 70, Batch: 0, Loss: 0.14443328976631165\n",
      "Iteration 70, Batch: 1, Loss: 0.1475723683834076\n",
      "Iteration 70, Batch: 2, Loss: 0.1827370971441269\n",
      "Iteration 70, Batch: 3, Loss: 0.17798744142055511\n",
      "Iteration 70, Batch: 4, Loss: 0.1304955631494522\n",
      "Iteration 70, Batch: 5, Loss: 0.16078311204910278\n",
      "Iteration 70, Batch: 6, Loss: 0.1332712024450302\n",
      "Iteration 70, Batch: 7, Loss: 0.177122563123703\n",
      "Iteration 70, Batch: 8, Loss: 0.15538378059864044\n",
      "Iteration 70, Batch: 9, Loss: 0.16777095198631287\n",
      "Iteration 70, Batch: 10, Loss: 0.17538274824619293\n",
      "Iteration 70, Batch: 11, Loss: 0.14044994115829468\n",
      "Iteration 70, Batch: 12, Loss: 0.19796720147132874\n",
      "Iteration 70, Batch: 13, Loss: 0.11826026439666748\n",
      "Iteration 70, Batch: 14, Loss: 0.1712711751461029\n",
      "Iteration 70, Batch: 15, Loss: 0.13162212073802948\n",
      "Iteration 70, Batch: 16, Loss: 0.1825437694787979\n",
      "Iteration 70, Batch: 17, Loss: 0.14865921437740326\n",
      "Iteration 70, Batch: 18, Loss: 0.16398771107196808\n",
      "Iteration 70, Batch: 19, Loss: 0.16089734435081482\n",
      "Iteration 70, Batch: 20, Loss: 0.1627008616924286\n",
      "Iteration 70, Batch: 21, Loss: 0.14495675265789032\n",
      "Iteration 70, Batch: 22, Loss: 0.1645016372203827\n",
      "Iteration 70, Batch: 23, Loss: 0.13749267160892487\n",
      "Iteration 70, Batch: 24, Loss: 0.1299920529127121\n",
      "Iteration 70, Batch: 25, Loss: 0.1823825091123581\n",
      "Iteration 70, Batch: 26, Loss: 0.16042636334896088\n",
      "Iteration 70, Batch: 27, Loss: 0.11631953716278076\n",
      "Iteration 70, Batch: 28, Loss: 0.15769045054912567\n",
      "Iteration 70, Batch: 29, Loss: 0.13382305204868317\n",
      "Iteration 70, Batch: 30, Loss: 0.12298276275396347\n",
      "Iteration 70, Batch: 31, Loss: 0.16561496257781982\n",
      "Iteration 70, Batch: 32, Loss: 0.15188080072402954\n",
      "Iteration 70, Batch: 33, Loss: 0.13656362891197205\n",
      "Iteration 70, Batch: 34, Loss: 0.20304706692695618\n",
      "Iteration 70, Batch: 35, Loss: 0.14394791424274445\n",
      "Iteration 70, Batch: 36, Loss: 0.1611219197511673\n",
      "Iteration 70, Batch: 37, Loss: 0.1569141000509262\n",
      "Iteration 70, Batch: 38, Loss: 0.18164218962192535\n",
      "Iteration 70, Batch: 39, Loss: 0.11572878807783127\n",
      "Iteration 70, Batch: 40, Loss: 0.13518856465816498\n",
      "Iteration 70, Batch: 41, Loss: 0.12208133190870285\n",
      "Iteration 70, Batch: 42, Loss: 0.12550415098667145\n",
      "Iteration 70, Batch: 43, Loss: 0.16784942150115967\n",
      "Iteration 70, Batch: 44, Loss: 0.13896171748638153\n",
      "Iteration 70, Batch: 45, Loss: 0.1657487004995346\n",
      "Iteration 70, Batch: 46, Loss: 0.1270623356103897\n",
      "Iteration 70, Batch: 47, Loss: 0.13905878365039825\n",
      "Iteration 70, Batch: 48, Loss: 0.14467711746692657\n",
      "Iteration 70, Batch: 49, Loss: 0.15486323833465576\n",
      "Avg loss: 0.15238648489117623\n",
      "Iteration 71, Batch: 0, Loss: 0.12521138787269592\n",
      "Iteration 71, Batch: 1, Loss: 0.14354278147220612\n",
      "Iteration 71, Batch: 2, Loss: 0.1844533532857895\n",
      "Iteration 71, Batch: 3, Loss: 0.15714311599731445\n",
      "Iteration 71, Batch: 4, Loss: 0.1721765249967575\n",
      "Iteration 71, Batch: 5, Loss: 0.13943946361541748\n",
      "Iteration 71, Batch: 6, Loss: 0.11427167803049088\n",
      "Iteration 71, Batch: 7, Loss: 0.13525287806987762\n",
      "Iteration 71, Batch: 8, Loss: 0.1330927610397339\n",
      "Iteration 71, Batch: 9, Loss: 0.18829593062400818\n",
      "Iteration 71, Batch: 10, Loss: 0.18785998225212097\n",
      "Iteration 71, Batch: 11, Loss: 0.21998415887355804\n",
      "Iteration 71, Batch: 12, Loss: 0.26003921031951904\n",
      "Iteration 71, Batch: 13, Loss: 0.18933112919330597\n",
      "Iteration 71, Batch: 14, Loss: 0.15209908783435822\n",
      "Iteration 71, Batch: 15, Loss: 0.16000190377235413\n",
      "Iteration 71, Batch: 16, Loss: 0.17222148180007935\n",
      "Iteration 71, Batch: 17, Loss: 0.1796339899301529\n",
      "Iteration 71, Batch: 18, Loss: 0.24527041614055634\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Backpropagate\u001b[39;00m\n\u001b[1;32m     51\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Batch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(it, counter, loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Update counter\u001b[39;00m\n\u001b[1;32m     56\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_trials = 20\n",
    "num_iters_ls = []\n",
    "losses = []\n",
    "for _ in range(num_trials):\n",
    "    # Specify parameters\n",
    "    N = 10000\n",
    "    n = 350\n",
    "    k = 20\n",
    "    \n",
    "    # Create dataset for training\n",
    "    train_data = sample_data(N, n, k, device=device)\n",
    "    train_dataset = SubmatrixDataset(train_data)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "    \n",
    "    # Specify parameters\n",
    "    num_epochs = 2000\n",
    "    \n",
    "    # Make an array of time points\n",
    "    time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "    \n",
    "    # Training routine\n",
    "    model = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=8, num_layers=10)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Train with 7 layers to see when it hits good local solution\n",
    "    model.num_layers = 7\n",
    "    print(\"Number of layers: {}\".format(model.num_layers))\n",
    "    \n",
    "    # Specify the optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "    \n",
    "    # Flag that signals early termination\n",
    "    flag = False\n",
    "    \n",
    "    for it in range(num_epochs):\n",
    "        counter = 0\n",
    "        loss_total = 0\n",
    "        \n",
    "        # Iterate through batches\n",
    "        for batch in train_dataloader:\n",
    "            # Empty gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Get loss from model\n",
    "            loss = mc_loss_batch_simul(model, batch, time_array, n, k, time_threshold=400, p_bad=0.05, device=device)\n",
    "            loss.backward()\n",
    "    \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "    \n",
    "            # Backpropagate\n",
    "            optimizer.step()\n",
    "            \n",
    "            print(\"Iteration {}, Batch: {}, Loss: {}\".format(it, counter, loss.item()))\n",
    "    \n",
    "            # Update counter\n",
    "            counter += 1\n",
    "            loss_total += loss.item() * batch.shape[0]\n",
    "    \n",
    "        # Update best model\n",
    "        avg = (loss_total / len(train_dataloader.dataset))\n",
    "        print(\"Avg loss: {}\".format(avg))\n",
    "        if avg <= 0.12:\n",
    "            flag = True\n",
    "            break\n",
    "    \n",
    "    print(\"Number of iterations needed: {}\".format(it))\n",
    "    num_iters_ls.append(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1dfb38b2-8499-40d0-96db-188021d9ce48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([1.3223], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model.readout_coefs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7322a47-47d9-434a-818d-0bd8db636de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(43.8304688544396)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(num_iters_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce43bd4-3487-4d36-915a-779a18f90ccf",
   "metadata": {},
   "source": [
    "**3.1.2. Replicating the pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b6d9831-8d32-4dfb-b648-7bef21421ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part_1(model, n, k, train_dataloader, num_epochs):\n",
    "    # Make an array of time points\n",
    "    time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "    \n",
    "    # Initialize optimal model\n",
    "    best_weights = None\n",
    "    min_loss = float(\"inf\")\n",
    "    \n",
    "    # Specify the optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "    \n",
    "    for it in range(num_epochs):\n",
    "        counter = 0\n",
    "        loss_total = 0\n",
    "        \n",
    "        # If iteration is small, train with 9 layers\n",
    "        if it < 150:\n",
    "            model.num_layers = 7\n",
    "        elif it < 250:\n",
    "            model.num_layers = 9\n",
    "        else:\n",
    "            model.num_layers = 10\n",
    "    \n",
    "        # Iterate through batches\n",
    "        for batch in train_dataloader:\n",
    "            # Get loss from model\n",
    "            loss = mc_loss_batch_simul(model, batch, time_array, n, k, time_threshold=400, p_bad=0.05, device=device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "        \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "            # Backpropagate\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Update counter\n",
    "            counter += 1\n",
    "            loss_total += loss.item() * batch.shape[0]\n",
    "    \n",
    "        # Update best model\n",
    "        with torch.no_grad():\n",
    "            avg = loss_total / len(train_dataloader.dataset)\n",
    "            print(\"Average loss per iteration {}: {}\".format(it, avg))\n",
    "\n",
    "            # Only start updating when model is fully training\n",
    "            if avg < min_loss and model.num_layers == 10:\n",
    "                min_loss = avg\n",
    "                best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    # Return optimal model\n",
    "    return best_weights, min_loss\n",
    "\n",
    "\n",
    "def train_part_2(model, n, k, train_dataloader, num_epochs):\n",
    "    # Make an array of time points\n",
    "    time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "    \n",
    "    # Specify the optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # When fine-tune, have to reset min_loss to take into account the new points\n",
    "    best_weights = None\n",
    "    min_loss = float(\"inf\")\n",
    "    \n",
    "    for it in range(num_epochs):\n",
    "        counter = 0\n",
    "        loss_total = 0\n",
    "        for batch in train_dataloader:\n",
    "            # Get loss from model\n",
    "            loss = mc_loss_batch_simul(model, batch, time_array, n, k, time_threshold=325, p_bad=0.05, device=device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "        \n",
    "            # Clip gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "            # Backpropagate\n",
    "            optimizer.step()\n",
    "    \n",
    "            # Update counter\n",
    "            counter += 1\n",
    "            loss_total += loss.item() * batch.shape[0]\n",
    "    \n",
    "        # Update best model\n",
    "        with torch.no_grad():\n",
    "            avg = loss_total / len(train_dataloader.dataset)\n",
    "            print(\"Average loss per iteration {}: {}\".format(it, avg))\n",
    "            if avg < min_loss:\n",
    "                min_loss = avg\n",
    "                best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "    \n",
    "    # Save optimal model to save time for generation\n",
    "    return best_weights, min_loss\n",
    "\n",
    "\n",
    "def train_part_3(model, n, k, train_dataloader, num_epochs):    \n",
    "    # Make an array of time points\n",
    "    time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "    \n",
    "    # Specify the optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=7e-4)\n",
    "    \n",
    "    # When fine-tune, have to reset min_loss to take into account the new points\n",
    "    best_weights = None\n",
    "    min_loss = float(\"inf\")\n",
    "\n",
    "    # Fishing pole at large time to make sure that the model does not overfit to the time region\n",
    "    t_large = 10000\n",
    "    \n",
    "    for it in range(num_epochs):\n",
    "        counter = 0\n",
    "        loss_total = 0\n",
    "        if it % 6 < 5:\n",
    "            for batch in train_dataloader:\n",
    "                # Get loss from model\n",
    "                loss = mc_loss_batch_simul(model, batch, time_array, n, k, time_threshold=205, p_bad=0.05, device=device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "            \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.9)\n",
    "            \n",
    "                # Backpropagate\n",
    "                optimizer.step()\n",
    "        \n",
    "                # Update counter\n",
    "                counter += 1\n",
    "                loss_total += loss.item() * batch.shape[0]\n",
    "        \n",
    "            # Update best model\n",
    "            with torch.no_grad():\n",
    "                avg = loss_total / len(train_dataloader.dataset)\n",
    "                print(\"Average loss per iteration {}: {}\".format(it, avg))\n",
    "                if avg < min_loss and it >= 12:\n",
    "                    min_loss = avg\n",
    "                    best_weights = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "\n",
    "        else:\n",
    "            for batch in train_dataloader:\n",
    "                # Get loss from model\n",
    "                loss = mc_loss_batch_fixed(model, batch, t_large, n, k, predict=False, device=device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "            \n",
    "                # Clip gradients\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.9)\n",
    "            \n",
    "                # Backpropagate\n",
    "                optimizer.step()\n",
    "        \n",
    "                # Update counter\n",
    "                counter += 1\n",
    "                loss_total += loss.item() * batch.shape[0]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                avg = loss_total / len(train_dataloader.dataset)\n",
    "                print(\"Average loss per fishing iteration {}: {}\".format(it, avg))\n",
    "            \n",
    "    \n",
    "    return best_weights, min_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fc986d7-e99e-4821-afcb-f93f5a6d6db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfee73f4-e0dc-4d00-af86-aca06bfdf89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n, k, num_epochs_ls = [600, 300, 300], seed=None):\n",
    "    if seed is not None:\n",
    "        set_global_seed(seed)\n",
    "\n",
    "    # Part 1\n",
    "    # Sample data\n",
    "    N = 10000\n",
    "    \n",
    "    # Create dataset for training\n",
    "    train_data = sample_data(N, n, k, device=device)\n",
    "    train_dataset = SubmatrixDataset(train_data)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "    model.to(device)\n",
    "\n",
    "    # Get back optimal model from part 1\n",
    "    weights_ckpt1, min_loss_1 = train_part_1(model, n, k, train_dataloader, num_epochs_ls[0])\n",
    "    print(\"Rep loss: {}\".format(min_loss_1))\n",
    "\n",
    "    # Reload model\n",
    "    model.load_state_dict(weights_ckpt1)\n",
    "\n",
    "    # Train part 2\n",
    "    # Re-sample data\n",
    "    train_data = sample_data(N, n, k, device=device)\n",
    "    train_dataset = SubmatrixDataset(train_data)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "    # Get best weights\n",
    "    weights_ckpt2, min_loss_2 = train_part_2(model, n, k, train_dataloader, num_epochs_ls[1])\n",
    "\n",
    "    # Reload model\n",
    "    model.load_state_dict(weights_ckpt2)\n",
    "\n",
    "    # Train part 3\n",
    "    # Re-sample data\n",
    "    train_data = sample_data(N, n, k, device=device)\n",
    "    train_dataset = SubmatrixDataset(train_data)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "    # Get best weights\n",
    "    weights_ckpt3, min_loss_3 = train_part_3(model, n, k, train_dataloader, num_epochs_ls[2])\n",
    "\n",
    "    # Reload model\n",
    "    model.load_state_dict(weights_ckpt3)\n",
    "\n",
    "    # Return model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cce6df38-462c-489e-8b07-7e9dbf4ef093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4173183967\n",
      "Average loss per iteration 0: 0.49980385959148405\n",
      "Average loss per iteration 1: 0.4979304575920105\n",
      "Average loss per iteration 2: 0.49793975949287417\n",
      "Average loss per iteration 3: 0.4979251688718796\n",
      "Average loss per iteration 4: 0.4979348790645599\n",
      "Average loss per iteration 5: 0.49791278183460236\n",
      "Average loss per iteration 6: 0.49792447209358215\n",
      "Average loss per iteration 7: 0.4979270988702774\n",
      "Average loss per iteration 8: 0.497920498251915\n",
      "Average loss per iteration 9: 0.49793502032756803\n",
      "Average loss per iteration 10: 0.4979510414600372\n",
      "Average loss per iteration 11: 0.4979063069820404\n",
      "Average loss per iteration 12: 0.4978839433193207\n",
      "Average loss per iteration 13: 0.4976448255777359\n",
      "Average loss per iteration 14: 0.49758054375648497\n",
      "Average loss per iteration 15: 0.497478266954422\n",
      "Average loss per iteration 16: 0.49754117846488954\n",
      "Average loss per iteration 17: 0.49763017654418945\n",
      "Average loss per iteration 18: 0.4975340622663498\n",
      "Average loss per iteration 19: 0.49734738588333127\n",
      "Average loss per iteration 20: 0.4974315673112869\n",
      "Average loss per iteration 21: 0.49736507594585416\n",
      "Average loss per iteration 22: 0.4971171551942825\n",
      "Average loss per iteration 23: 0.49705818653106687\n",
      "Average loss per iteration 24: 0.49706242442131043\n",
      "Average loss per iteration 25: 0.49673424243927\n",
      "Average loss per iteration 26: 0.4964827901124954\n",
      "Average loss per iteration 27: 0.49625026285648344\n",
      "Average loss per iteration 28: 0.49608194053173066\n",
      "Average loss per iteration 29: 0.4930791288614273\n",
      "Average loss per iteration 30: 0.4890078455209732\n",
      "Average loss per iteration 31: 0.47917986631393433\n",
      "Average loss per iteration 32: 0.4639179641008377\n",
      "Average loss per iteration 33: 0.4576891773939133\n",
      "Average loss per iteration 34: 0.4467575913667679\n",
      "Average loss per iteration 35: 0.4262490063905716\n",
      "Average loss per iteration 36: 0.40730961203575133\n",
      "Average loss per iteration 37: 0.3821700268983841\n",
      "Average loss per iteration 38: 0.3871592599153519\n",
      "Average loss per iteration 39: 0.3560090780258179\n",
      "Average loss per iteration 40: 0.33788525462150576\n",
      "Average loss per iteration 41: 0.320686981678009\n",
      "Average loss per iteration 42: 0.2948470491170883\n",
      "Average loss per iteration 43: 0.276825375854969\n",
      "Average loss per iteration 44: 0.2747952589392662\n",
      "Average loss per iteration 45: 0.2693027600646019\n",
      "Average loss per iteration 46: 0.24840981036424636\n",
      "Average loss per iteration 47: 0.2393745219707489\n",
      "Average loss per iteration 48: 0.2333043572306633\n",
      "Average loss per iteration 49: 0.22541452705860138\n",
      "Average loss per iteration 50: 0.22386079728603364\n",
      "Average loss per iteration 51: 0.21978028923273085\n",
      "Average loss per iteration 52: 0.22158994078636168\n",
      "Average loss per iteration 53: 0.2050076299905777\n",
      "Average loss per iteration 54: 0.20622431308031083\n",
      "Average loss per iteration 55: 0.2029054981470108\n",
      "Average loss per iteration 56: 0.20131944954395295\n",
      "Average loss per iteration 57: 0.19398851096630096\n",
      "Average loss per iteration 58: 0.19009662181138992\n",
      "Average loss per iteration 59: 0.1857897475361824\n",
      "Average loss per iteration 60: 0.17787318974733352\n",
      "Average loss per iteration 61: 0.18189924985170364\n",
      "Average loss per iteration 62: 0.17760802656412125\n",
      "Average loss per iteration 63: 0.17262417942285538\n",
      "Average loss per iteration 64: 0.16540116250514983\n",
      "Average loss per iteration 65: 0.15891927257180213\n",
      "Average loss per iteration 66: 0.15276751101016997\n",
      "Average loss per iteration 67: 0.14615028396248816\n",
      "Average loss per iteration 68: 0.14653808489441872\n",
      "Average loss per iteration 69: 0.1418912610411644\n",
      "Average loss per iteration 70: 0.1428109163045883\n",
      "Average loss per iteration 71: 0.1383048662543297\n",
      "Average loss per iteration 72: 0.13437668606638908\n",
      "Average loss per iteration 73: 0.13821212217211723\n",
      "Average loss per iteration 74: 0.1367037895321846\n",
      "Average loss per iteration 75: 0.13569270625710486\n",
      "Average loss per iteration 76: 0.13486805200576782\n",
      "Average loss per iteration 77: 0.12022303104400635\n",
      "Average loss per iteration 78: 0.1288285708427429\n",
      "Average loss per iteration 79: 0.13083948835730552\n",
      "Average loss per iteration 80: 0.11771854355931283\n",
      "Average loss per iteration 81: 0.12070888057351112\n",
      "Average loss per iteration 82: 0.11092164553701878\n",
      "Average loss per iteration 83: 0.11905963778495789\n",
      "Average loss per iteration 84: 0.11148240074515342\n",
      "Average loss per iteration 85: 0.11164098620414734\n",
      "Average loss per iteration 86: 0.1028413650393486\n",
      "Average loss per iteration 87: 0.11224735721945762\n",
      "Average loss per iteration 88: 0.11985161691904068\n",
      "Average loss per iteration 89: 0.10866735771298408\n",
      "Average loss per iteration 90: 0.11837284073233605\n",
      "Average loss per iteration 91: 0.10794540017843246\n",
      "Average loss per iteration 92: 0.11150098286569118\n",
      "Average loss per iteration 93: 0.1054516888409853\n",
      "Average loss per iteration 94: 0.1133667729794979\n",
      "Average loss per iteration 95: 0.10727037280797959\n",
      "Average loss per iteration 96: 0.11285383433103562\n",
      "Average loss per iteration 97: 0.11092917948961258\n",
      "Average loss per iteration 98: 0.1117110151797533\n",
      "Average loss per iteration 99: 0.09846393644809723\n",
      "Average loss per iteration 100: 0.11413351826369762\n",
      "Average loss per iteration 101: 0.12406039744615555\n",
      "Average loss per iteration 102: 0.1115215539932251\n",
      "Average loss per iteration 103: 0.11069649770855904\n",
      "Average loss per iteration 104: 0.10521486155688763\n",
      "Average loss per iteration 105: 0.1052819201350212\n",
      "Average loss per iteration 106: 0.1089100742340088\n",
      "Average loss per iteration 107: 0.10740301840007305\n",
      "Average loss per iteration 108: 0.10082553312182427\n",
      "Average loss per iteration 109: 0.11191661834716797\n",
      "Average loss per iteration 110: 0.11580192565917968\n",
      "Average loss per iteration 111: 0.1083025123178959\n",
      "Average loss per iteration 112: 0.10509680420160293\n",
      "Average loss per iteration 113: 0.10479100026190281\n",
      "Average loss per iteration 114: 0.10039329826831818\n",
      "Average loss per iteration 115: 0.10815700620412827\n",
      "Average loss per iteration 116: 0.10736413113772869\n",
      "Average loss per iteration 117: 0.11119478106498719\n",
      "Average loss per iteration 118: 0.09764028742909431\n",
      "Average loss per iteration 119: 0.09696253024041653\n",
      "Average loss per iteration 120: 0.10362855970859527\n",
      "Average loss per iteration 121: 0.12129284396767616\n",
      "Average loss per iteration 122: 0.11713896378874779\n",
      "Average loss per iteration 123: 0.10864629872143268\n",
      "Average loss per iteration 124: 0.09711791001260281\n",
      "Average loss per iteration 125: 0.09925297178328037\n",
      "Average loss per iteration 126: 0.09055025845766068\n",
      "Average loss per iteration 127: 0.11761678323149681\n",
      "Average loss per iteration 128: 0.08935102611780167\n",
      "Average loss per iteration 129: 0.09158311657607555\n",
      "Average loss per iteration 130: 0.09362267464399338\n",
      "Average loss per iteration 131: 0.09135247357189655\n",
      "Average loss per iteration 132: 0.08437937147915363\n",
      "Average loss per iteration 133: 0.08893872857093811\n",
      "Average loss per iteration 134: 0.0895962419360876\n",
      "Average loss per iteration 135: 0.09141910411417484\n",
      "Average loss per iteration 136: 0.08437457397580146\n",
      "Average loss per iteration 137: 0.09808720350265503\n",
      "Average loss per iteration 138: 0.08635826982557773\n",
      "Average loss per iteration 139: 0.08499843627214432\n",
      "Average loss per iteration 140: 0.07713612549006939\n",
      "Average loss per iteration 141: 0.0919904239475727\n",
      "Average loss per iteration 142: 0.09565579645335674\n",
      "Average loss per iteration 143: 0.08970740310847759\n",
      "Average loss per iteration 144: 0.08435114778578282\n",
      "Average loss per iteration 145: 0.08283952459692955\n",
      "Average loss per iteration 146: 0.08450501829385758\n",
      "Average loss per iteration 147: 0.0862938628345728\n",
      "Average loss per iteration 148: 0.08653140664100648\n",
      "Average loss per iteration 149: 0.08589571230113506\n",
      "Average loss per iteration 150: 0.2810488629341126\n",
      "Average loss per iteration 151: 0.13045520305633546\n",
      "Average loss per iteration 152: 0.10244647085666657\n",
      "Average loss per iteration 153: 0.103151985257864\n",
      "Average loss per iteration 154: 0.08999738804996013\n",
      "Average loss per iteration 155: 0.0866012755781412\n",
      "Average loss per iteration 156: 0.08972602918744087\n",
      "Average loss per iteration 157: 0.08122337646782399\n",
      "Average loss per iteration 158: 0.0807869315892458\n",
      "Average loss per iteration 159: 0.07919065818190575\n",
      "Average loss per iteration 160: 0.08644293576478958\n",
      "Average loss per iteration 161: 0.07637842401862144\n",
      "Average loss per iteration 162: 0.07172747775912285\n",
      "Average loss per iteration 163: 0.07543229810893536\n",
      "Average loss per iteration 164: 0.07678671963512898\n",
      "Average loss per iteration 165: 0.07860700488090515\n",
      "Average loss per iteration 166: 0.0743164499104023\n",
      "Average loss per iteration 167: 0.07257312133908272\n",
      "Average loss per iteration 168: 0.07093626275658607\n",
      "Average loss per iteration 169: 0.07926267333328724\n",
      "Average loss per iteration 170: 0.0797680937498808\n",
      "Average loss per iteration 171: 0.0776057880371809\n",
      "Average loss per iteration 172: 0.07470090344548225\n",
      "Average loss per iteration 173: 0.08256488427519798\n",
      "Average loss per iteration 174: 0.0789643406867981\n",
      "Average loss per iteration 175: 0.07651559181511403\n",
      "Average loss per iteration 176: 0.075685750618577\n",
      "Average loss per iteration 177: 0.07255344487726688\n",
      "Average loss per iteration 178: 0.070205197930336\n",
      "Average loss per iteration 179: 0.07365142785012722\n",
      "Average loss per iteration 180: 0.07610438942909241\n",
      "Average loss per iteration 181: 0.07779065929353238\n",
      "Average loss per iteration 182: 0.076484454870224\n",
      "Average loss per iteration 183: 0.08209597587585449\n",
      "Average loss per iteration 184: 0.07571961969137192\n",
      "Average loss per iteration 185: 0.0764281479269266\n",
      "Average loss per iteration 186: 0.07740741275250912\n",
      "Average loss per iteration 187: 0.07127615734934807\n",
      "Average loss per iteration 188: 0.07553150236606598\n",
      "Average loss per iteration 189: 0.06782724119722844\n",
      "Average loss per iteration 190: 0.0765702050179243\n",
      "Average loss per iteration 191: 0.0773177070915699\n",
      "Average loss per iteration 192: 0.0745106291770935\n",
      "Average loss per iteration 193: 0.07591489344835281\n",
      "Average loss per iteration 194: 0.07447132647037506\n",
      "Average loss per iteration 195: 0.07598848454654217\n",
      "Average loss per iteration 196: 0.07624790802598\n",
      "Average loss per iteration 197: 0.07713289715349675\n",
      "Average loss per iteration 198: 0.07179871786385775\n",
      "Average loss per iteration 199: 0.07338984951376915\n",
      "Average loss per iteration 200: 0.0683202163875103\n",
      "Average loss per iteration 201: 3.03963318888098\n",
      "Average loss per iteration 202: 0.0750134439021349\n",
      "Average loss per iteration 203: 0.07171434096992016\n",
      "Average loss per iteration 204: 0.07903839483857154\n",
      "Average loss per iteration 205: 0.07984923593699932\n",
      "Average loss per iteration 206: 0.06525723494589329\n",
      "Average loss per iteration 207: 0.0672272240370512\n",
      "Average loss per iteration 208: 0.0672228891402483\n",
      "Average loss per iteration 209: 0.07044050507247449\n",
      "Average loss per iteration 210: 0.06673328049480914\n",
      "Average loss per iteration 211: 0.07011839002370834\n",
      "Average loss per iteration 212: 0.06808505218476057\n",
      "Average loss per iteration 213: 0.06783403970301151\n",
      "Average loss per iteration 214: 0.06951777055859566\n",
      "Average loss per iteration 215: 0.07011559769511223\n",
      "Average loss per iteration 216: 0.07084880597889423\n",
      "Average loss per iteration 217: 0.06390958573669195\n",
      "Average loss per iteration 218: 0.07070161812007428\n",
      "Average loss per iteration 219: 0.06860831946134567\n",
      "Average loss per iteration 220: 0.07292703121900558\n",
      "Average loss per iteration 221: 0.0706377687305212\n",
      "Average loss per iteration 222: 0.0776423405110836\n",
      "Average loss per iteration 223: 0.06976423911750317\n",
      "Average loss per iteration 224: 0.06960138112306595\n",
      "Average loss per iteration 225: 0.06346036225557328\n",
      "Average loss per iteration 226: 0.06918561473488807\n",
      "Average loss per iteration 227: 0.0631320471316576\n",
      "Average loss per iteration 228: 0.06259984776377678\n",
      "Average loss per iteration 229: 0.06786680787801742\n",
      "Average loss per iteration 230: 0.06732843935489655\n",
      "Average loss per iteration 231: 0.06621048189699649\n",
      "Average loss per iteration 232: 0.06703198254108429\n",
      "Average loss per iteration 233: 0.06776191495358944\n",
      "Average loss per iteration 234: 0.06872550942003727\n",
      "Average loss per iteration 235: 0.07526925288140773\n",
      "Average loss per iteration 236: 0.069926318526268\n",
      "Average loss per iteration 237: 0.0590366030484438\n",
      "Average loss per iteration 238: 0.0670391259342432\n",
      "Average loss per iteration 239: 0.06317634552717209\n",
      "Average loss per iteration 240: 0.07077711950987578\n",
      "Average loss per iteration 241: 0.0675635340809822\n",
      "Average loss per iteration 242: 0.06850952371954917\n",
      "Average loss per iteration 243: 0.07013391830027103\n",
      "Average loss per iteration 244: 0.08067728333175182\n",
      "Average loss per iteration 245: 0.06655205857008696\n",
      "Average loss per iteration 246: 0.07098513282835484\n",
      "Average loss per iteration 247: 0.06925174858421088\n",
      "Average loss per iteration 248: 0.06778013154864311\n",
      "Average loss per iteration 249: 0.06948981937021018\n",
      "Average loss per iteration 250: 0.1583572330325842\n",
      "Average loss per iteration 251: 0.0668996350467205\n",
      "Average loss per iteration 252: 0.06542063504457474\n",
      "Average loss per iteration 253: 0.06378611408174038\n",
      "Average loss per iteration 254: 0.06501293905079365\n",
      "Average loss per iteration 255: 0.06527547970414162\n",
      "Average loss per iteration 256: 0.07050877511501312\n",
      "Average loss per iteration 257: 0.06686012238264084\n",
      "Average loss per iteration 258: 0.06467436034232378\n",
      "Average loss per iteration 259: 0.0674610000476241\n",
      "Average loss per iteration 260: 0.06711319841444492\n",
      "Average loss per iteration 261: 0.06714100785553455\n",
      "Average loss per iteration 262: 0.0685809026658535\n",
      "Average loss per iteration 263: 0.06696043588221073\n",
      "Average loss per iteration 264: 0.07296203505247831\n",
      "Average loss per iteration 265: 0.06400684423744679\n",
      "Average loss per iteration 266: 0.07082568116486072\n",
      "Average loss per iteration 267: 0.06918269164860248\n",
      "Average loss per iteration 268: 0.06631326984614133\n",
      "Average loss per iteration 269: 0.0699457734823227\n",
      "Average loss per iteration 270: 0.0670670561864972\n",
      "Average loss per iteration 271: 0.06562989585101604\n",
      "Average loss per iteration 272: 0.0660628692060709\n",
      "Average loss per iteration 273: 0.06691179975867272\n",
      "Average loss per iteration 274: 0.06157337501645088\n",
      "Average loss per iteration 275: 0.06877414979040623\n",
      "Average loss per iteration 276: 0.06650146216154099\n",
      "Average loss per iteration 277: 0.08339191056787967\n",
      "Average loss per iteration 278: 0.07799342900514603\n",
      "Average loss per iteration 279: 0.07358037069439888\n",
      "Average loss per iteration 280: 0.06409459128975868\n",
      "Average loss per iteration 281: 0.06269008554518223\n",
      "Average loss per iteration 282: 0.0734183556586504\n",
      "Average loss per iteration 283: 0.08427499614655971\n",
      "Average loss per iteration 284: 0.07042025163769722\n",
      "Average loss per iteration 285: 0.060308241173624995\n",
      "Average loss per iteration 286: 0.06475939750671386\n",
      "Average loss per iteration 287: 0.0724005200713873\n",
      "Average loss per iteration 288: 0.06474400117993355\n",
      "Average loss per iteration 289: 0.06792166072875261\n",
      "Average loss per iteration 290: 0.063247559517622\n",
      "Average loss per iteration 291: 0.05783457607030869\n",
      "Average loss per iteration 292: 0.06250010423362255\n",
      "Average loss per iteration 293: 0.059767816364765164\n",
      "Average loss per iteration 294: 0.06262971311807633\n",
      "Average loss per iteration 295: 0.06510587960481644\n",
      "Average loss per iteration 296: 0.06768414869904518\n",
      "Average loss per iteration 297: 0.07115597240626811\n",
      "Average loss per iteration 298: 0.06800452642142772\n",
      "Average loss per iteration 299: 0.06367116585373879\n",
      "Average loss per iteration 300: 0.05930814035236835\n",
      "Average loss per iteration 301: 0.05803929846733809\n",
      "Average loss per iteration 302: 0.05927060272544622\n",
      "Average loss per iteration 303: 0.06183215819299221\n",
      "Average loss per iteration 304: 0.05745130278170109\n",
      "Average loss per iteration 305: 0.05846202913671732\n",
      "Average loss per iteration 306: 0.05571569085121155\n",
      "Average loss per iteration 307: 0.062279614210128786\n",
      "Average loss per iteration 308: 0.054518905468285084\n",
      "Average loss per iteration 309: 0.056942476220428945\n",
      "Average loss per iteration 310: 0.06065113462507725\n",
      "Average loss per iteration 311: 0.06322402007877827\n",
      "Average loss per iteration 312: 0.05486819118261337\n",
      "Average loss per iteration 313: 0.05852577783167362\n",
      "Average loss per iteration 314: 0.05110449213534594\n",
      "Average loss per iteration 315: 0.05488128818571567\n",
      "Average loss per iteration 316: 0.05932908739894629\n",
      "Average loss per iteration 317: 0.05802896272391081\n",
      "Average loss per iteration 318: 0.06044190265238285\n",
      "Average loss per iteration 319: 0.053756826743483546\n",
      "Average loss per iteration 320: 0.06224376052618027\n",
      "Average loss per iteration 321: 1182.8552629568428\n",
      "Average loss per iteration 322: 0.055182469710707664\n",
      "Average loss per iteration 323: 0.05405694574117661\n",
      "Average loss per iteration 324: 0.06043530553579331\n",
      "Average loss per iteration 325: 0.06436313301324845\n",
      "Average loss per iteration 326: 0.0617878645285964\n",
      "Average loss per iteration 327: 0.05058642584830522\n",
      "Average loss per iteration 328: 0.059073755480349065\n",
      "Average loss per iteration 329: 0.0590901966765523\n",
      "Average loss per iteration 330: 0.05961194146424532\n",
      "Average loss per iteration 331: 0.05884987886995077\n",
      "Average loss per iteration 332: 0.05696796711534262\n",
      "Average loss per iteration 333: 0.057409617714583874\n",
      "Average loss per iteration 334: 0.052458118125796316\n",
      "Average loss per iteration 335: 0.058571256529539824\n",
      "Average loss per iteration 336: 0.054761944189667705\n",
      "Average loss per iteration 337: 0.06117514483630657\n",
      "Average loss per iteration 338: 0.05375112142413854\n",
      "Average loss per iteration 339: 0.05221473468467593\n",
      "Average loss per iteration 340: 0.05595439970493317\n",
      "Average loss per iteration 341: 0.05911057434976101\n",
      "Average loss per iteration 342: 0.06926461204886436\n",
      "Average loss per iteration 343: 0.06696623347699643\n",
      "Average loss per iteration 344: 0.06433881036937236\n",
      "Average loss per iteration 345: 0.06160616680979729\n",
      "Average loss per iteration 346: 0.04865379709750414\n",
      "Average loss per iteration 347: 0.049248315244913105\n",
      "Average loss per iteration 348: 0.05595748145133257\n",
      "Average loss per iteration 349: 0.04852564040571451\n",
      "Average loss per iteration 350: 0.05055622324347496\n",
      "Average loss per iteration 351: 0.050858704671263695\n",
      "Average loss per iteration 352: 0.046568976417183876\n",
      "Average loss per iteration 353: 0.04653431661427021\n",
      "Average loss per iteration 354: 0.055334395058453084\n",
      "Average loss per iteration 355: 0.06895845584571361\n",
      "Average loss per iteration 356: 0.07653064332902432\n",
      "Average loss per iteration 357: 0.06006142430007458\n",
      "Average loss per iteration 358: 0.059681922942399976\n",
      "Average loss per iteration 359: 0.060334191173315045\n",
      "Average loss per iteration 360: 0.05746799919754267\n",
      "Average loss per iteration 361: 0.09205188609659672\n",
      "Average loss per iteration 362: 0.06916343212127686\n",
      "Average loss per iteration 363: 0.07050778578966856\n",
      "Average loss per iteration 364: 0.06327312983572483\n",
      "Average loss per iteration 365: 0.05469057589769363\n",
      "Average loss per iteration 366: 0.05052278764545917\n",
      "Average loss per iteration 367: 0.05277620106935501\n",
      "Average loss per iteration 368: 0.05515962984412909\n",
      "Average loss per iteration 369: 0.05955059103667736\n",
      "Average loss per iteration 370: 0.06042166978120804\n",
      "Average loss per iteration 371: 0.05071517661213875\n",
      "Average loss per iteration 372: 0.058959783166646955\n",
      "Average loss per iteration 373: 0.061276569738984106\n",
      "Average loss per iteration 374: 0.07402263723313808\n",
      "Average loss per iteration 375: 0.05397288039326668\n",
      "Average loss per iteration 376: 0.05700563408434391\n",
      "Average loss per iteration 377: 0.05656632501631975\n",
      "Average loss per iteration 378: 0.05631036382168531\n",
      "Average loss per iteration 379: 0.05512749619781971\n",
      "Average loss per iteration 380: 0.06092385582625866\n",
      "Average loss per iteration 381: 0.05354437530040741\n",
      "Average loss per iteration 382: 0.0476749550178647\n",
      "Average loss per iteration 383: 0.055641678534448145\n",
      "Average loss per iteration 384: 0.05700001336634159\n",
      "Average loss per iteration 385: 0.053698734790086744\n",
      "Average loss per iteration 386: 0.051191906072199346\n",
      "Average loss per iteration 387: 0.06886933770030737\n",
      "Average loss per iteration 388: 0.07104530476033688\n",
      "Average loss per iteration 389: 0.08611478798091411\n",
      "Average loss per iteration 390: 0.058159872591495514\n",
      "Average loss per iteration 391: 0.06351528268307448\n",
      "Average loss per iteration 392: 0.07145066112279892\n",
      "Average loss per iteration 393: 0.059309122376143934\n",
      "Average loss per iteration 394: 0.060363149046897886\n",
      "Average loss per iteration 395: 0.057673976868391034\n",
      "Average loss per iteration 396: 0.06202908892184496\n",
      "Average loss per iteration 397: 0.05322472725063562\n",
      "Average loss per iteration 398: 0.056672641262412074\n",
      "Average loss per iteration 399: 0.05079566296190023\n",
      "Average loss per iteration 400: 0.06174259252846241\n",
      "Average loss per iteration 401: 0.05476237326860428\n",
      "Average loss per iteration 402: 0.05199303291738033\n",
      "Average loss per iteration 403: 0.05714679893106222\n",
      "Average loss per iteration 404: 0.05290980588644743\n",
      "Average loss per iteration 405: 0.05980743803083897\n",
      "Average loss per iteration 406: 0.06740895375609397\n",
      "Average loss per iteration 407: 0.0661628457158804\n",
      "Average loss per iteration 408: 0.05945933911949396\n",
      "Average loss per iteration 409: 0.051916965059936045\n",
      "Average loss per iteration 410: 0.05683833759278059\n",
      "Average loss per iteration 411: 0.06234080225229263\n",
      "Average loss per iteration 412: 0.04933707300573587\n",
      "Average loss per iteration 413: 0.05464089561253786\n",
      "Average loss per iteration 414: 0.05737967375665903\n",
      "Average loss per iteration 415: 0.08048404190689325\n",
      "Average loss per iteration 416: 0.06367902014404535\n",
      "Average loss per iteration 417: 0.046396811418235304\n",
      "Average loss per iteration 418: 0.051735594905912875\n",
      "Average loss per iteration 419: 0.04678649894893169\n",
      "Average loss per iteration 420: 0.049217952340841295\n",
      "Average loss per iteration 421: 0.04959062457084656\n",
      "Average loss per iteration 422: 0.0526764215901494\n",
      "Average loss per iteration 423: 0.06357642378658056\n",
      "Average loss per iteration 424: 0.04025068461894989\n",
      "Average loss per iteration 425: 0.04995394550263882\n",
      "Average loss per iteration 426: 0.05069372694939375\n",
      "Average loss per iteration 427: 0.0456075506657362\n",
      "Average loss per iteration 428: 0.0481603080779314\n",
      "Average loss per iteration 429: 0.04861638009548187\n",
      "Average loss per iteration 430: 0.04682336315512657\n",
      "Average loss per iteration 431: 0.04962199181318283\n",
      "Average loss per iteration 432: 0.04133988227695227\n",
      "Average loss per iteration 433: 0.04303413163870573\n",
      "Average loss per iteration 434: 0.04632565960288048\n",
      "Average loss per iteration 435: 0.04428244423121214\n",
      "Average loss per iteration 436: 0.0577956860139966\n",
      "Average loss per iteration 437: 0.04011784717440605\n",
      "Average loss per iteration 438: 0.055697010383009914\n",
      "Average loss per iteration 439: 0.08938696652650834\n",
      "Average loss per iteration 440: 0.0659080471098423\n",
      "Average loss per iteration 441: 0.06673578191548586\n",
      "Average loss per iteration 442: 0.07560808025300503\n",
      "Average loss per iteration 443: 0.06564201168715954\n",
      "Average loss per iteration 444: 0.06787419881671668\n",
      "Average loss per iteration 445: 0.052247009389102456\n",
      "Average loss per iteration 446: 0.06755954228341579\n",
      "Average loss per iteration 447: 0.05974207013845444\n",
      "Average loss per iteration 448: 0.06578095480799676\n",
      "Average loss per iteration 449: 0.05800398390740156\n",
      "Average loss per iteration 450: 0.05800109203904867\n",
      "Average loss per iteration 451: 0.05610433779656887\n",
      "Average loss per iteration 452: 0.06273683045059443\n",
      "Average loss per iteration 453: 0.05549379520118237\n",
      "Average loss per iteration 454: 0.056021043732762336\n",
      "Average loss per iteration 455: 0.06804485373198986\n",
      "Average loss per iteration 456: 0.0653228559345007\n",
      "Average loss per iteration 457: 0.05536663953214884\n",
      "Average loss per iteration 458: 0.05379029430449009\n",
      "Average loss per iteration 459: 0.06928124312311411\n",
      "Average loss per iteration 460: 0.06185229655355215\n",
      "Average loss per iteration 461: 0.05075538992881775\n",
      "Average loss per iteration 462: 0.05622782588005066\n",
      "Average loss per iteration 463: 0.04801604922860861\n",
      "Average loss per iteration 464: 0.05450272724032402\n",
      "Average loss per iteration 465: 0.051326007451862096\n",
      "Average loss per iteration 466: 0.06302318584173917\n",
      "Average loss per iteration 467: 0.05625928863883019\n",
      "Average loss per iteration 468: 0.0535721979662776\n",
      "Average loss per iteration 469: 0.05746702775359154\n",
      "Average loss per iteration 470: 0.060025101266801356\n",
      "Average loss per iteration 471: 0.05406604096293449\n",
      "Average loss per iteration 472: 0.05253196403384209\n",
      "Average loss per iteration 473: 0.060126033537089826\n",
      "Average loss per iteration 474: 0.06429376743733883\n",
      "Average loss per iteration 475: 0.07039379216730594\n",
      "Average loss per iteration 476: 0.06725017786026001\n",
      "Average loss per iteration 477: 0.06337133057415485\n",
      "Average loss per iteration 478: 0.04718250617384911\n",
      "Average loss per iteration 479: 0.049527451880276206\n",
      "Average loss per iteration 480: 0.05473070546984673\n",
      "Average loss per iteration 481: 0.04990268498659134\n",
      "Average loss per iteration 482: 0.058749171048402785\n",
      "Average loss per iteration 483: 0.0547016679495573\n",
      "Average loss per iteration 484: 0.3929838165268302\n",
      "Average loss per iteration 485: 0.04760726448148489\n",
      "Average loss per iteration 486: 0.052031514756381514\n",
      "Average loss per iteration 487: 0.06420367546379566\n",
      "Average loss per iteration 488: 0.056866669282317164\n",
      "Average loss per iteration 489: 0.054790806770324704\n",
      "Average loss per iteration 490: 0.04623693451285362\n",
      "Average loss per iteration 491: 0.04578976884484291\n",
      "Average loss per iteration 492: 0.04447911966592073\n",
      "Average loss per iteration 493: 0.04864625733345747\n",
      "Average loss per iteration 494: 0.054428161233663556\n",
      "Average loss per iteration 495: 0.05222115807235241\n",
      "Average loss per iteration 496: 0.05359805095940828\n",
      "Average loss per iteration 497: 0.05203077841550112\n",
      "Average loss per iteration 498: 0.05887576699256897\n",
      "Average loss per iteration 499: 0.04896266423165798\n",
      "Average loss per iteration 500: 0.048036699146032334\n",
      "Average loss per iteration 501: 0.06583411116153001\n",
      "Average loss per iteration 502: 0.05321595031768084\n",
      "Average loss per iteration 503: 0.050297459680587055\n",
      "Average loss per iteration 504: 0.04708867859095335\n",
      "Average loss per iteration 505: 0.046753764972090724\n",
      "Average loss per iteration 506: 0.04921537440270186\n",
      "Average loss per iteration 507: 0.054114350751042366\n",
      "Average loss per iteration 508: 0.05611345883458853\n",
      "Average loss per iteration 509: 0.04940459180623293\n",
      "Average loss per iteration 510: 0.04541382007300854\n",
      "Average loss per iteration 511: 0.049775411821901795\n",
      "Average loss per iteration 512: 0.05424739010632038\n",
      "Average loss per iteration 513: 0.05793967317789793\n",
      "Average loss per iteration 514: 0.0605126217007637\n",
      "Average loss per iteration 515: 0.04948376506567001\n",
      "Average loss per iteration 516: 0.059395182617008685\n",
      "Average loss per iteration 517: 0.05771255802363157\n",
      "Average loss per iteration 518: 0.04785003323107958\n",
      "Average loss per iteration 519: 0.05779718019068241\n",
      "Average loss per iteration 520: 0.05935069341212511\n",
      "Average loss per iteration 521: 0.05934951078146696\n",
      "Average loss per iteration 522: 0.062269516736268994\n",
      "Average loss per iteration 523: 0.05735262367874384\n",
      "Average loss per iteration 524: 0.054626166000962255\n",
      "Average loss per iteration 525: 0.055805414468050006\n",
      "Average loss per iteration 526: 0.05903866738080978\n",
      "Average loss per iteration 527: 0.07634204167872667\n",
      "Average loss per iteration 528: 0.059879286028444764\n",
      "Average loss per iteration 529: 0.050103383548557756\n",
      "Average loss per iteration 530: 0.053905451968312264\n",
      "Average loss per iteration 531: 0.05390929628163576\n",
      "Average loss per iteration 532: 0.056043716259300706\n",
      "Average loss per iteration 533: 0.047604024261236194\n",
      "Average loss per iteration 534: 0.06208673667162657\n",
      "Average loss per iteration 535: 0.056824417524039744\n",
      "Average loss per iteration 536: 0.05812689796090126\n",
      "Average loss per iteration 537: 0.40948812775313853\n",
      "Average loss per iteration 538: 0.052263037376105784\n",
      "Average loss per iteration 539: 0.046131628341972826\n",
      "Average loss per iteration 540: 0.055871925167739395\n",
      "Average loss per iteration 541: 0.05371551360934973\n",
      "Average loss per iteration 542: 0.050920834951102736\n",
      "Average loss per iteration 543: 0.047523731999099254\n",
      "Average loss per iteration 544: 0.053137683495879176\n",
      "Average loss per iteration 545: 0.05427845928817988\n",
      "Average loss per iteration 546: 0.05228935182094574\n",
      "Average loss per iteration 547: 0.05526818878948689\n",
      "Average loss per iteration 548: 0.05177492123097181\n",
      "Average loss per iteration 549: 0.0445064365118742\n",
      "Average loss per iteration 550: 0.04924214627593756\n",
      "Average loss per iteration 551: 0.050559978764504195\n",
      "Average loss per iteration 552: 0.05198005665093661\n",
      "Average loss per iteration 553: 0.05450314953923226\n",
      "Average loss per iteration 554: 0.04367981877177954\n",
      "Average loss per iteration 555: 0.052412249483168125\n",
      "Average loss per iteration 556: 0.0671365236863494\n",
      "Average loss per iteration 557: 0.05025007624179125\n",
      "Average loss per iteration 558: 0.05980767346918583\n",
      "Average loss per iteration 559: 0.04664051692932844\n",
      "Average loss per iteration 560: 0.052675531581044197\n",
      "Average loss per iteration 561: 0.06000762335956097\n",
      "Average loss per iteration 562: 0.0521967513859272\n",
      "Average loss per iteration 563: 0.052078049890697004\n",
      "Average loss per iteration 564: 0.05009003717452288\n",
      "Average loss per iteration 565: 0.05246470499783754\n",
      "Average loss per iteration 566: 0.05139640387147665\n",
      "Average loss per iteration 567: 0.06073042910546064\n",
      "Average loss per iteration 568: 0.050811122059822085\n",
      "Average loss per iteration 569: 0.05434603866189718\n",
      "Average loss per iteration 570: 0.04887382682412863\n",
      "Average loss per iteration 571: 0.04981144044548273\n",
      "Average loss per iteration 572: 0.04675631068646908\n",
      "Average loss per iteration 573: 0.048077996708452704\n",
      "Average loss per iteration 574: 0.048767279051244256\n",
      "Average loss per iteration 575: 0.04811669755727053\n",
      "Average loss per iteration 576: 0.043486055955290794\n",
      "Average loss per iteration 577: 0.04885607730597258\n",
      "Average loss per iteration 578: 0.051137177906930446\n",
      "Average loss per iteration 579: 0.05183686286211014\n",
      "Average loss per iteration 580: 0.0486381059512496\n",
      "Average loss per iteration 581: 0.04698270447552204\n",
      "Average loss per iteration 582: 0.05424622818827629\n",
      "Average loss per iteration 583: 0.07513439573347569\n",
      "Average loss per iteration 584: 0.061714569255709645\n",
      "Average loss per iteration 585: 0.05483863517642021\n",
      "Average loss per iteration 586: 0.053897878378629686\n",
      "Average loss per iteration 587: 0.05179412249475718\n",
      "Average loss per iteration 588: 0.0571824462339282\n",
      "Average loss per iteration 589: 0.07278889633715152\n",
      "Average loss per iteration 590: 0.06149277232587338\n",
      "Average loss per iteration 591: 0.054781005680561067\n",
      "Average loss per iteration 592: 0.04987468335777521\n",
      "Average loss per iteration 593: 0.04991071332246065\n",
      "Average loss per iteration 594: 0.04896759022027254\n",
      "Average loss per iteration 595: 0.04548307459801435\n",
      "Average loss per iteration 596: 0.0466228499263525\n",
      "Average loss per iteration 597: 0.05142849460244179\n",
      "Average loss per iteration 598: 0.050671553313732146\n",
      "Average loss per iteration 599: 0.04458281181752682\n",
      "Rep loss: 0.04011784717440605\n",
      "Average loss per iteration 0: 0.060427363216876986\n",
      "Average loss per iteration 1: 0.06078574560582638\n",
      "Average loss per iteration 2: 0.06093001138418913\n",
      "Average loss per iteration 3: 0.057653467804193496\n",
      "Average loss per iteration 4: 0.06425552442669868\n",
      "Average loss per iteration 5: 0.06244290933012962\n",
      "Average loss per iteration 6: 0.05709235705435276\n",
      "Average loss per iteration 7: 0.05639720492064953\n",
      "Average loss per iteration 8: 0.05457288607954979\n",
      "Average loss per iteration 9: 0.0540221706777811\n",
      "Average loss per iteration 10: 0.053598950020968913\n",
      "Average loss per iteration 11: 0.057260420247912405\n",
      "Average loss per iteration 12: 0.05789955884218216\n",
      "Average loss per iteration 13: 0.0556256601959467\n",
      "Average loss per iteration 14: 0.05128537412732839\n",
      "Average loss per iteration 15: 0.0607797060161829\n",
      "Average loss per iteration 16: 0.05935467522591353\n",
      "Average loss per iteration 17: 0.052827095091342924\n",
      "Average loss per iteration 18: 0.056351173222064975\n",
      "Average loss per iteration 19: 0.05976603176444769\n",
      "Average loss per iteration 20: 0.05262302227318287\n",
      "Average loss per iteration 21: 0.05747172087430954\n",
      "Average loss per iteration 22: 0.05285447791218758\n",
      "Average loss per iteration 23: 0.053648566082119944\n",
      "Average loss per iteration 24: 0.053993067368865014\n",
      "Average loss per iteration 25: 0.05134464494884014\n",
      "Average loss per iteration 26: 0.051779754124581814\n",
      "Average loss per iteration 27: 0.05548224329948425\n",
      "Average loss per iteration 28: 0.05994346871972084\n",
      "Average loss per iteration 29: 0.06070368073880673\n",
      "Average loss per iteration 30: 0.05899620212614536\n",
      "Average loss per iteration 31: 0.056704883724451066\n",
      "Average loss per iteration 32: 0.06232997342944145\n",
      "Average loss per iteration 33: 0.06256210409104825\n",
      "Average loss per iteration 34: 0.05712458442896604\n",
      "Average loss per iteration 35: 0.055455894470214845\n",
      "Average loss per iteration 36: 0.05031922586262226\n",
      "Average loss per iteration 37: 0.0499788804911077\n",
      "Average loss per iteration 38: 0.05603123657405376\n",
      "Average loss per iteration 39: 0.05596475966274738\n",
      "Average loss per iteration 40: 0.06294313423335553\n",
      "Average loss per iteration 41: 0.06284290090203286\n",
      "Average loss per iteration 42: 0.05920468870550394\n",
      "Average loss per iteration 43: 0.06258624352514744\n",
      "Average loss per iteration 44: 0.06047270692884922\n",
      "Average loss per iteration 45: 0.05643374428153038\n",
      "Average loss per iteration 46: 0.056160235032439235\n",
      "Average loss per iteration 47: 0.055001532286405565\n",
      "Average loss per iteration 48: 0.06292485296726227\n",
      "Average loss per iteration 49: 0.05472147602587938\n",
      "Average loss per iteration 50: 0.055424079112708566\n",
      "Average loss per iteration 51: 0.05846208795905113\n",
      "Average loss per iteration 52: 0.05450900912284851\n",
      "Average loss per iteration 53: 0.055262396894395355\n",
      "Average loss per iteration 54: 0.054204545617103576\n",
      "Average loss per iteration 55: 0.05863469868898392\n",
      "Average loss per iteration 56: 0.05641779407858848\n",
      "Average loss per iteration 57: 0.057390246540308\n",
      "Average loss per iteration 58: 0.053973080776631835\n",
      "Average loss per iteration 59: 0.05604940552264452\n",
      "Average loss per iteration 60: 0.0586653745546937\n",
      "Average loss per iteration 61: 0.05193245880305767\n",
      "Average loss per iteration 62: 0.05567384991794824\n",
      "Average loss per iteration 63: 0.05369194991886616\n",
      "Average loss per iteration 64: 0.05718525927513838\n",
      "Average loss per iteration 65: 0.05763252582401037\n",
      "Average loss per iteration 66: 0.05889179594814777\n",
      "Average loss per iteration 67: 0.05347930863499641\n",
      "Average loss per iteration 68: 0.05423097662627697\n",
      "Average loss per iteration 69: 0.0523736521974206\n",
      "Average loss per iteration 70: 0.05826448410749435\n",
      "Average loss per iteration 71: 0.052618055045604704\n",
      "Average loss per iteration 72: 0.05396602779626846\n",
      "Average loss per iteration 73: 0.0509409711509943\n",
      "Average loss per iteration 74: 0.056288089007139205\n",
      "Average loss per iteration 75: 0.0561271420866251\n",
      "Average loss per iteration 76: 0.05292085167020559\n",
      "Average loss per iteration 77: 0.052327100671827796\n",
      "Average loss per iteration 78: 0.047746411859989166\n",
      "Average loss per iteration 79: 0.053911587372422215\n",
      "Average loss per iteration 80: 0.06145948987454176\n",
      "Average loss per iteration 81: 0.06652576230466366\n",
      "Average loss per iteration 82: 0.06194948561489582\n",
      "Average loss per iteration 83: 0.06009178556501865\n",
      "Average loss per iteration 84: 0.06676988825201988\n",
      "Average loss per iteration 85: 0.06501973353326321\n",
      "Average loss per iteration 86: 0.05893480695784092\n",
      "Average loss per iteration 87: 0.056831950470805165\n",
      "Average loss per iteration 88: 0.07092889577150345\n",
      "Average loss per iteration 89: 0.0618777222186327\n",
      "Average loss per iteration 90: 0.060708336681127545\n",
      "Average loss per iteration 91: 0.057198846247047186\n",
      "Average loss per iteration 92: 0.05695375062525272\n",
      "Average loss per iteration 93: 0.060242745131254195\n",
      "Average loss per iteration 94: 0.0639412272721529\n",
      "Average loss per iteration 95: 0.06899303171783686\n",
      "Average loss per iteration 96: 0.07028304189443588\n",
      "Average loss per iteration 97: 0.05935314457863569\n",
      "Average loss per iteration 98: 0.05905999645590782\n",
      "Average loss per iteration 99: 0.05651073556393385\n",
      "Average loss per iteration 100: 0.05576263424009085\n",
      "Average loss per iteration 101: 0.0545398660749197\n",
      "Average loss per iteration 102: 0.058801224380731584\n",
      "Average loss per iteration 103: 0.05792007975280285\n",
      "Average loss per iteration 104: 0.05904821321368217\n",
      "Average loss per iteration 105: 0.05655805990099907\n",
      "Average loss per iteration 106: 0.06160721801221371\n",
      "Average loss per iteration 107: 0.056759233623743056\n",
      "Average loss per iteration 108: 0.29485636431723833\n",
      "Average loss per iteration 109: 0.07353765577077866\n",
      "Average loss per iteration 110: 0.06722318559885025\n",
      "Average loss per iteration 111: 0.06773572459816933\n",
      "Average loss per iteration 112: 0.06524422854185104\n",
      "Average loss per iteration 113: 0.06280527852475642\n",
      "Average loss per iteration 114: 0.05805432520806789\n",
      "Average loss per iteration 115: 0.05855666719377041\n",
      "Average loss per iteration 116: 0.05646746583282947\n",
      "Average loss per iteration 117: 0.058053657710552216\n",
      "Average loss per iteration 118: 0.0556720607727766\n",
      "Average loss per iteration 119: 0.04991468191146851\n",
      "Average loss per iteration 120: 0.053218266442418095\n",
      "Average loss per iteration 121: 0.054706283062696454\n",
      "Average loss per iteration 122: 0.06117426089942455\n",
      "Average loss per iteration 123: 0.05237213369458914\n",
      "Average loss per iteration 124: 0.05549939397722483\n",
      "Average loss per iteration 125: 0.05698630303144455\n",
      "Average loss per iteration 126: 0.06397108346223832\n",
      "Average loss per iteration 127: 0.05458877459168434\n",
      "Average loss per iteration 128: 0.05652971021831035\n",
      "Average loss per iteration 129: 0.050253603644669055\n",
      "Average loss per iteration 130: 0.055846013315021995\n",
      "Average loss per iteration 131: 0.05331004150211811\n",
      "Average loss per iteration 132: 0.0471896979957819\n",
      "Average loss per iteration 133: 0.05243446230888367\n",
      "Average loss per iteration 134: 0.05779987145215273\n",
      "Average loss per iteration 135: 0.058647887483239174\n",
      "Average loss per iteration 136: 0.06426723562180996\n",
      "Average loss per iteration 137: 0.0534616419300437\n",
      "Average loss per iteration 138: 0.05501407001167536\n",
      "Average loss per iteration 139: 0.054077687971293927\n",
      "Average loss per iteration 140: 0.05632666230201721\n",
      "Average loss per iteration 141: 0.057605517506599424\n",
      "Average loss per iteration 142: 0.060382579788565635\n",
      "Average loss per iteration 143: 0.05156555362045765\n",
      "Average loss per iteration 144: 0.05284593958407641\n",
      "Average loss per iteration 145: 0.058928813859820364\n",
      "Average loss per iteration 146: 0.059577924236655234\n",
      "Average loss per iteration 147: 0.05241850081831217\n",
      "Average loss per iteration 148: 0.05504050206393003\n",
      "Average loss per iteration 149: 0.053952806480228904\n",
      "Average loss per iteration 150: 0.06336043797433376\n",
      "Average loss per iteration 151: 0.1253183214366436\n",
      "Average loss per iteration 152: 0.11094511307775974\n",
      "Average loss per iteration 153: 0.09979907982051373\n",
      "Average loss per iteration 154: 0.0912841423600912\n",
      "Average loss per iteration 155: 0.10042109981179237\n",
      "Average loss per iteration 156: 0.09448545336723328\n",
      "Average loss per iteration 157: 0.08671302661299705\n",
      "Average loss per iteration 158: 0.08156626120209694\n",
      "Average loss per iteration 159: 0.08315887384116649\n",
      "Average loss per iteration 160: 0.09041742160916329\n",
      "Average loss per iteration 161: 0.07806040532886982\n",
      "Average loss per iteration 162: 0.0818193694204092\n",
      "Average loss per iteration 163: 0.0836003839969635\n",
      "Average loss per iteration 164: 0.07651589825749397\n",
      "Average loss per iteration 165: 0.08966336585581303\n",
      "Average loss per iteration 166: 0.09031565383076667\n",
      "Average loss per iteration 167: 0.08788171738386154\n",
      "Average loss per iteration 168: 0.07953889816999435\n",
      "Average loss per iteration 169: 0.06939013026654721\n",
      "Average loss per iteration 170: 0.0807595493644476\n",
      "Average loss per iteration 171: 0.08247364372015\n",
      "Average loss per iteration 172: 0.0834400587528944\n",
      "Average loss per iteration 173: 0.07511692479252816\n",
      "Average loss per iteration 174: 0.07219732709228993\n",
      "Average loss per iteration 175: 0.07524684906005859\n",
      "Average loss per iteration 176: 0.07704488426446915\n",
      "Average loss per iteration 177: 0.07073329247534275\n",
      "Average loss per iteration 178: 0.07422499321401119\n",
      "Average loss per iteration 179: 0.07986322693526744\n",
      "Average loss per iteration 180: 0.07464373610913753\n",
      "Average loss per iteration 181: 0.07288321934640407\n",
      "Average loss per iteration 182: 0.08187007762491703\n",
      "Average loss per iteration 183: 0.0719627557694912\n",
      "Average loss per iteration 184: 0.07885791465640068\n",
      "Average loss per iteration 185: 0.07499521784484386\n",
      "Average loss per iteration 186: 0.07381040595471859\n",
      "Average loss per iteration 187: 0.0729545682668686\n",
      "Average loss per iteration 188: 0.07541525855660439\n",
      "Average loss per iteration 189: 0.06780906111001968\n",
      "Average loss per iteration 190: 0.07409227639436722\n",
      "Average loss per iteration 191: 0.0679929105937481\n",
      "Average loss per iteration 192: 0.0778656854480505\n",
      "Average loss per iteration 193: 0.06910467058420182\n",
      "Average loss per iteration 194: 0.07185995310544968\n",
      "Average loss per iteration 195: 0.07185388907790184\n",
      "Average loss per iteration 196: 0.06953388258814812\n",
      "Average loss per iteration 197: 0.0686402104794979\n",
      "Average loss per iteration 198: 0.0736154055595398\n",
      "Average loss per iteration 199: 0.07143529638648033\n",
      "Average loss per iteration 200: 0.07261852979660034\n",
      "Average loss per iteration 201: 0.07431419782340526\n",
      "Average loss per iteration 202: 0.07129430137574673\n",
      "Average loss per iteration 203: 0.06701757296919823\n",
      "Average loss per iteration 204: 0.06746539082378149\n",
      "Average loss per iteration 205: 0.06773209109902383\n",
      "Average loss per iteration 206: 0.06914918847382069\n",
      "Average loss per iteration 207: 0.07173829831182957\n",
      "Average loss per iteration 208: 0.07949272252619266\n",
      "Average loss per iteration 209: 0.07524277918040752\n",
      "Average loss per iteration 210: 0.07055587358772755\n",
      "Average loss per iteration 211: 0.06909276485443115\n",
      "Average loss per iteration 212: 0.07036898605525493\n",
      "Average loss per iteration 213: 0.07072963185608387\n",
      "Average loss per iteration 214: 0.06998237304389476\n",
      "Average loss per iteration 215: 0.06998277865350247\n",
      "Average loss per iteration 216: 0.06878901317715645\n",
      "Average loss per iteration 217: 0.07905025333166123\n",
      "Average loss per iteration 218: 0.06829876638948917\n",
      "Average loss per iteration 219: 0.07060786321759224\n",
      "Average loss per iteration 220: 0.07234338961541653\n",
      "Average loss per iteration 221: 0.07300439037382603\n",
      "Average loss per iteration 222: 0.0667485947906971\n",
      "Average loss per iteration 223: 0.06746265985071659\n",
      "Average loss per iteration 224: 0.06589055635035038\n",
      "Average loss per iteration 225: 0.06354122303426266\n",
      "Average loss per iteration 226: 0.06384303487837314\n",
      "Average loss per iteration 227: 0.06712162740528584\n",
      "Average loss per iteration 228: 0.06520387340337037\n",
      "Average loss per iteration 229: 0.06742599293589592\n",
      "Average loss per iteration 230: 0.06829051427543163\n",
      "Average loss per iteration 231: 0.0643159420788288\n",
      "Average loss per iteration 232: 0.06497518718242645\n",
      "Average loss per iteration 233: 0.06720535330474377\n",
      "Average loss per iteration 234: 0.0725149841606617\n",
      "Average loss per iteration 235: 0.06923567526042461\n",
      "Average loss per iteration 236: 0.06197522018104792\n",
      "Average loss per iteration 237: 0.06483961716294288\n",
      "Average loss per iteration 238: 0.06400581695139408\n",
      "Average loss per iteration 239: 0.06264444403350353\n",
      "Average loss per iteration 240: 0.06206151340156794\n",
      "Average loss per iteration 241: 0.06470143139362335\n",
      "Average loss per iteration 242: 0.06616078115999699\n",
      "Average loss per iteration 243: 0.06588013358414173\n",
      "Average loss per iteration 244: 0.06613502368330955\n",
      "Average loss per iteration 245: 0.061275565475225446\n",
      "Average loss per iteration 246: 0.0692585102841258\n",
      "Average loss per iteration 247: 0.0628767029196024\n",
      "Average loss per iteration 248: 0.06488078698515892\n",
      "Average loss per iteration 249: 0.06336941502988339\n",
      "Average loss per iteration 250: 0.06649388603866101\n",
      "Average loss per iteration 251: 0.06222519520670176\n",
      "Average loss per iteration 252: 0.06383265063166618\n",
      "Average loss per iteration 253: 0.06270503528416156\n",
      "Average loss per iteration 254: 0.06418968133628368\n",
      "Average loss per iteration 255: 0.06277545392513276\n",
      "Average loss per iteration 256: 0.06657529950141906\n",
      "Average loss per iteration 257: 0.0659891352802515\n",
      "Average loss per iteration 258: 0.06654069982469082\n",
      "Average loss per iteration 259: 0.06782245896756649\n",
      "Average loss per iteration 260: 0.06883417464792728\n",
      "Average loss per iteration 261: 0.06523126099258661\n",
      "Average loss per iteration 262: 0.07077226541936397\n",
      "Average loss per iteration 263: 0.06757251307368278\n",
      "Average loss per iteration 264: 0.06705060191452503\n",
      "Average loss per iteration 265: 0.06112099416553974\n",
      "Average loss per iteration 266: 0.06521092347800732\n",
      "Average loss per iteration 267: 0.06703861743211746\n",
      "Average loss per iteration 268: 0.06588457353413105\n",
      "Average loss per iteration 269: 0.06508400656282902\n",
      "Average loss per iteration 270: 0.06576899632811546\n",
      "Average loss per iteration 271: 0.06302777010947466\n",
      "Average loss per iteration 272: 0.05828556697815657\n",
      "Average loss per iteration 273: 0.06813786700367927\n",
      "Average loss per iteration 274: 0.06475841246545315\n",
      "Average loss per iteration 275: 0.06086283009499312\n",
      "Average loss per iteration 276: 0.06941777214407921\n",
      "Average loss per iteration 277: 0.06251058261841536\n",
      "Average loss per iteration 278: 0.06803041703999042\n",
      "Average loss per iteration 279: 0.06657185494899749\n",
      "Average loss per iteration 280: 0.06607857473194599\n",
      "Average loss per iteration 281: 0.06817071363329888\n",
      "Average loss per iteration 282: 0.06798250615596771\n",
      "Average loss per iteration 283: 0.06171295814216137\n",
      "Average loss per iteration 284: 0.06042092863470316\n",
      "Average loss per iteration 285: 0.06364050958305598\n",
      "Average loss per iteration 286: 0.06762361962348223\n",
      "Average loss per iteration 287: 0.06375724323093891\n",
      "Average loss per iteration 288: 0.059500034265220166\n",
      "Average loss per iteration 289: 0.060969776809215545\n",
      "Average loss per iteration 290: 0.06343107432126999\n",
      "Average loss per iteration 291: 0.06260404780507088\n",
      "Average loss per iteration 292: 0.06381222449243068\n",
      "Average loss per iteration 293: 0.06131706602871418\n",
      "Average loss per iteration 294: 0.06314836695790291\n",
      "Average loss per iteration 295: 0.0648240127786994\n",
      "Average loss per iteration 296: 0.06326952092349529\n",
      "Average loss per iteration 297: 0.062450370267033574\n",
      "Average loss per iteration 298: 0.06236595191061497\n",
      "Average loss per iteration 299: 0.059883918948471546\n",
      "Average loss per iteration 0: 0.1216846051812172\n",
      "Average loss per iteration 1: 0.12861758664250375\n",
      "Average loss per iteration 2: 0.12542368724942207\n",
      "Average loss per iteration 3: 0.12416371613740922\n",
      "Average loss per iteration 4: 0.13146612003445626\n",
      "Average loss per fishing iteration 5: 0.0005898484431600082\n",
      "Average loss per iteration 6: 0.13326809495687486\n",
      "Average loss per iteration 7: 0.12444382071495057\n",
      "Average loss per iteration 8: 0.11894981600344182\n",
      "Average loss per iteration 9: 0.11468589693307876\n",
      "Average loss per iteration 10: 0.12223385900259018\n",
      "Average loss per fishing iteration 11: 0.001471753026089573\n",
      "Average loss per iteration 12: 0.1272422741353512\n",
      "Average loss per iteration 13: 0.12038932114839554\n",
      "Average loss per iteration 14: 0.12474917687475681\n",
      "Average loss per iteration 15: 0.1278044278919697\n",
      "Average loss per iteration 16: 0.12361360281705856\n",
      "Average loss per fishing iteration 17: 0.0014528312375114182\n",
      "Average loss per iteration 18: 0.1266665802896023\n",
      "Average loss per iteration 19: 0.12125793650746346\n",
      "Average loss per iteration 20: 0.11935207650065421\n",
      "Average loss per iteration 21: 0.11906731486320496\n",
      "Average loss per iteration 22: 0.11768135979771614\n",
      "Average loss per fishing iteration 23: 0.0008562826234810927\n",
      "Average loss per iteration 24: 0.12714008882641792\n",
      "Average loss per iteration 25: 0.12179188296198845\n",
      "Average loss per iteration 26: 0.12610729575157165\n",
      "Average loss per iteration 27: 0.12316712588071824\n",
      "Average loss per iteration 28: 0.1250964704155922\n",
      "Average loss per fishing iteration 29: 0.0006080391501018311\n",
      "Average loss per iteration 30: 0.11811981528997421\n",
      "Average loss per iteration 31: 0.11655656099319459\n",
      "Average loss per iteration 32: 0.12287584006786346\n",
      "Average loss per iteration 33: 0.11967814534902572\n",
      "Average loss per iteration 34: 0.11682638600468635\n",
      "Average loss per fishing iteration 35: 0.00025599994996355235\n",
      "Average loss per iteration 36: 0.12393049851059913\n",
      "Average loss per iteration 37: 0.1214861238002777\n",
      "Average loss per iteration 38: 0.12725150674581528\n",
      "Average loss per iteration 39: 0.11643478989601136\n",
      "Average loss per iteration 40: 0.11970561683177948\n",
      "Average loss per fishing iteration 41: 0.0006799779261564254\n",
      "Average loss per iteration 42: 0.11699503064155578\n",
      "Average loss per iteration 43: 0.11859717831015587\n",
      "Average loss per iteration 44: 0.12314507454633712\n",
      "Average loss per iteration 45: 0.12011490941047669\n",
      "Average loss per iteration 46: 0.1212451472878456\n",
      "Average loss per fishing iteration 47: 0.0004660433005665254\n",
      "Average loss per iteration 48: 0.1192079384624958\n",
      "Average loss per iteration 49: 0.1256425327062607\n",
      "Average loss per iteration 50: 0.11936999022960663\n",
      "Average loss per iteration 51: 0.11461330458521843\n",
      "Average loss per iteration 52: 0.1206477503478527\n",
      "Average loss per fishing iteration 53: 0.0002574960153924621\n",
      "Average loss per iteration 54: 0.12308216288685798\n",
      "Average loss per iteration 55: 0.12139191836118698\n",
      "Average loss per iteration 56: 0.12295080989599227\n",
      "Average loss per iteration 57: 0.11405678629875184\n",
      "Average loss per iteration 58: 0.11982727214694024\n",
      "Average loss per fishing iteration 59: 0.0003720489158422424\n",
      "Average loss per iteration 60: 0.12547167524695396\n",
      "Average loss per iteration 61: 0.12006843417882919\n",
      "Average loss per iteration 62: 0.11423261061310769\n",
      "Average loss per iteration 63: 0.1247741462290287\n",
      "Average loss per iteration 64: 0.13041016697883606\n",
      "Average loss per fishing iteration 65: 0.00014411732093094542\n",
      "Average loss per iteration 66: 0.12113333344459534\n",
      "Average loss per iteration 67: 0.11960164219141006\n",
      "Average loss per iteration 68: 0.12032439589500427\n",
      "Average loss per iteration 69: 0.11778731405735016\n",
      "Average loss per iteration 70: 0.11493032842874527\n",
      "Average loss per fishing iteration 71: 0.0001495459369789387\n",
      "Average loss per iteration 72: 0.1360574559867382\n",
      "Average loss per iteration 73: 0.11991523042321205\n",
      "Average loss per iteration 74: 0.11634706914424896\n",
      "Average loss per iteration 75: 0.11988965436816215\n",
      "Average loss per iteration 76: 0.11532403960824013\n",
      "Average loss per fishing iteration 77: 0.0005267759359730917\n",
      "Average loss per iteration 78: 0.12060234159231185\n",
      "Average loss per iteration 79: 0.11621221631765366\n",
      "Average loss per iteration 80: 0.1172913971543312\n",
      "Average loss per iteration 81: 0.12117028504610061\n",
      "Average loss per iteration 82: 0.11742628887295722\n",
      "Average loss per fishing iteration 83: 0.0002954357390763107\n",
      "Average loss per iteration 84: 0.12121951714158058\n",
      "Average loss per iteration 85: 0.11733696594834328\n",
      "Average loss per iteration 86: 0.12116734579205513\n",
      "Average loss per iteration 87: 0.1136750553548336\n",
      "Average loss per iteration 88: 0.11927627548575401\n",
      "Average loss per fishing iteration 89: 0.0005363342021428252\n",
      "Average loss per iteration 90: 0.11758192643523216\n",
      "Average loss per iteration 91: 0.12128893598914146\n",
      "Average loss per iteration 92: 0.11866970151662827\n",
      "Average loss per iteration 93: 0.12104062885046005\n",
      "Average loss per iteration 94: 0.11598667904734611\n",
      "Average loss per fishing iteration 95: 0.0009892143425804534\n",
      "Average loss per iteration 96: 0.12138665080070496\n",
      "Average loss per iteration 97: 0.11428482323884964\n",
      "Average loss per iteration 98: 0.11237636715173721\n",
      "Average loss per iteration 99: 0.11630166679620743\n",
      "Average loss per iteration 100: 0.11692170679569244\n",
      "Average loss per fishing iteration 101: 0.0010133818387885185\n",
      "Average loss per iteration 102: 0.11943235069513321\n",
      "Average loss per iteration 103: 0.11993736669421196\n",
      "Average loss per iteration 104: 0.12474365681409835\n",
      "Average loss per iteration 105: 0.12280753418803216\n",
      "Average loss per iteration 106: 0.12475779056549072\n",
      "Average loss per fishing iteration 107: 0.00043423302432074704\n",
      "Average loss per iteration 108: 0.1329498229920864\n",
      "Average loss per iteration 109: 0.12876839190721512\n",
      "Average loss per iteration 110: 0.12646742060780525\n",
      "Average loss per iteration 111: 0.12664320349693298\n",
      "Average loss per iteration 112: 0.12099048092961312\n",
      "Average loss per fishing iteration 113: 0.00013310595846633078\n",
      "Average loss per iteration 114: 0.12070311084389687\n",
      "Average loss per iteration 115: 0.11725399136543274\n",
      "Average loss per iteration 116: 0.11390304729342461\n",
      "Average loss per iteration 117: 0.1170656906068325\n",
      "Average loss per iteration 118: 0.11609194971621037\n",
      "Average loss per fishing iteration 119: 0.000210125672756476\n",
      "Average loss per iteration 120: 0.12297782465815545\n",
      "Average loss per iteration 121: 0.11551943019032479\n",
      "Average loss per iteration 122: 0.11241580620408058\n",
      "Average loss per iteration 123: 0.11568299390375614\n",
      "Average loss per iteration 124: 0.11896521478891373\n",
      "Average loss per fishing iteration 125: 0.0014717883720732061\n",
      "Average loss per iteration 126: 0.1228952331840992\n",
      "Average loss per iteration 127: 0.11451504409313201\n",
      "Average loss per iteration 128: 0.1246817660331726\n",
      "Average loss per iteration 129: 0.11338272467255592\n",
      "Average loss per iteration 130: 0.11478220403194428\n",
      "Average loss per fishing iteration 131: 0.00042284491853934015\n",
      "Average loss per iteration 132: 0.12516680911183356\n",
      "Average loss per iteration 133: 0.12328073889017105\n",
      "Average loss per iteration 134: 0.11639586314558983\n",
      "Average loss per iteration 135: 0.11949047416448594\n",
      "Average loss per iteration 136: 0.11609682947397232\n",
      "Average loss per fishing iteration 137: 0.0004406026971128085\n",
      "Average loss per iteration 138: 0.12128434792160987\n",
      "Average loss per iteration 139: 0.11803111225366593\n",
      "Average loss per iteration 140: 0.11664045467972756\n",
      "Average loss per iteration 141: 0.11686438649892807\n",
      "Average loss per iteration 142: 0.1258779776096344\n",
      "Average loss per fishing iteration 143: 3.626238787092007e-05\n",
      "Average loss per iteration 144: 0.1170471629500389\n",
      "Average loss per iteration 145: 0.12270519658923149\n",
      "Average loss per iteration 146: 0.12420879423618317\n",
      "Average loss per iteration 147: 0.11881062746047974\n",
      "Average loss per iteration 148: 0.12313856169581414\n",
      "Average loss per fishing iteration 149: 8.163457404180008e-05\n",
      "Average loss per iteration 150: 0.12233511805534363\n",
      "Average loss per iteration 151: 0.13587870985269546\n",
      "Average loss per iteration 152: 0.13390131875872613\n",
      "Average loss per iteration 153: 0.12412393763661385\n",
      "Average loss per iteration 154: 0.12166444137692452\n",
      "Average loss per fishing iteration 155: 0.00102317994592795\n",
      "Average loss per iteration 156: 0.13298507064580917\n",
      "Average loss per iteration 157: 0.12774555623531342\n",
      "Average loss per iteration 158: 0.1227934242784977\n",
      "Average loss per iteration 159: 0.12064726561307908\n",
      "Average loss per iteration 160: 0.12597506254911423\n",
      "Average loss per fishing iteration 161: 0.018105252028180984\n",
      "Average loss per iteration 162: 0.14442656427621842\n",
      "Average loss per iteration 163: 0.1278304500877857\n",
      "Average loss per iteration 164: 0.12414645344018936\n",
      "Average loss per iteration 165: 0.12345378801226616\n",
      "Average loss per iteration 166: 0.12186997175216675\n",
      "Average loss per fishing iteration 167: 0.0036968015454294802\n",
      "Average loss per iteration 168: 1.59292926043272\n",
      "Average loss per iteration 169: 0.13169000104069709\n",
      "Average loss per iteration 170: 0.12135022342205047\n",
      "Average loss per iteration 171: 0.11647904857993126\n",
      "Average loss per iteration 172: 0.12275510147213936\n",
      "Average loss per fishing iteration 173: 0.0011261845871558761\n",
      "Average loss per iteration 174: 0.12601522609591484\n",
      "Average loss per iteration 175: 0.11655876025557518\n",
      "Average loss per iteration 176: 0.11643648922443389\n",
      "Average loss per iteration 177: 0.12478261828422546\n",
      "Average loss per iteration 178: 0.11596257299184799\n",
      "Average loss per fishing iteration 179: 0.0004264869020880724\n",
      "Average loss per iteration 180: 0.12427165403962136\n",
      "Average loss per iteration 181: 0.12396953389048576\n",
      "Average loss per iteration 182: 0.11939145594835282\n",
      "Average loss per iteration 183: 0.11857849642634392\n",
      "Average loss per iteration 184: 0.11945313513278961\n",
      "Average loss per fishing iteration 185: 0.0002440699179578587\n",
      "Average loss per iteration 186: 0.12008713394403457\n",
      "Average loss per iteration 187: 0.11729456678032875\n",
      "Average loss per iteration 188: 0.1171228015422821\n",
      "Average loss per iteration 189: 0.13490319848060608\n",
      "Average loss per iteration 190: 0.1161824931204319\n",
      "Average loss per fishing iteration 191: 0.0013076293201993395\n",
      "Average loss per iteration 192: 0.131051008105278\n",
      "Average loss per iteration 193: 0.12738835021853448\n",
      "Average loss per iteration 194: 0.12605731680989266\n",
      "Average loss per iteration 195: 0.12148756667971611\n",
      "Average loss per iteration 196: 0.12402852281928062\n",
      "Average loss per fishing iteration 197: 0.0005998459445800109\n",
      "Average loss per iteration 198: 0.1275336740911007\n",
      "Average loss per iteration 199: 0.11961356624960899\n",
      "Average loss per iteration 200: 0.12003191128373146\n",
      "Average loss per iteration 201: 0.11375166997313499\n",
      "Average loss per iteration 202: 0.11353146120905876\n",
      "Average loss per fishing iteration 203: 0.0028736627989746923\n",
      "Average loss per iteration 204: 0.11935891136527062\n",
      "Average loss per iteration 205: 0.11752023831009865\n",
      "Average loss per iteration 206: 0.11853087961673736\n",
      "Average loss per iteration 207: 0.12548889875411987\n",
      "Average loss per iteration 208: 0.13019649550318718\n",
      "Average loss per fishing iteration 209: 4.496753726016323e-05\n",
      "Average loss per iteration 210: 0.12164448574185371\n",
      "Average loss per iteration 211: 0.12338871970772743\n",
      "Average loss per iteration 212: 0.1159372740983963\n",
      "Average loss per iteration 213: 0.12039651334285736\n",
      "Average loss per iteration 214: 0.11412712186574936\n",
      "Average loss per fishing iteration 215: 0.0002450515280725085\n",
      "Average loss per iteration 216: 0.11562888592481613\n",
      "Average loss per iteration 217: 0.11947138592600823\n",
      "Average loss per iteration 218: 0.11947090327739715\n",
      "Average loss per iteration 219: 0.11407517611980439\n",
      "Average loss per iteration 220: 0.11781439334154128\n",
      "Average loss per fishing iteration 221: 0.0020988872245106902\n",
      "Average loss per iteration 222: 0.11413270846009255\n",
      "Average loss per iteration 223: 0.11686473473906517\n",
      "Average loss per iteration 224: 0.11612286180257797\n",
      "Average loss per iteration 225: 0.11012870475649833\n",
      "Average loss per iteration 226: 0.1168454921245575\n",
      "Average loss per fishing iteration 227: 0.0005326863675509231\n",
      "Average loss per iteration 228: 0.11860253170132637\n",
      "Average loss per iteration 229: 0.114361512362957\n",
      "Average loss per iteration 230: 0.11397081315517425\n",
      "Average loss per iteration 231: 0.12267738088965416\n",
      "Average loss per iteration 232: 0.11599144831299782\n",
      "Average loss per fishing iteration 233: 0.0004962361446087016\n",
      "Average loss per iteration 234: 0.11291054546833039\n",
      "Average loss per iteration 235: 0.11660436429083347\n",
      "Average loss per iteration 236: 0.11551813304424285\n",
      "Average loss per iteration 237: 0.11290024355053901\n",
      "Average loss per iteration 238: 0.11197402596473693\n",
      "Average loss per fishing iteration 239: 0.00020750805786519776\n",
      "Average loss per iteration 240: 0.11593045368790626\n",
      "Average loss per iteration 241: 0.11638113617897033\n",
      "Average loss per iteration 242: 0.11735422849655151\n",
      "Average loss per iteration 243: 0.11388396576046944\n",
      "Average loss per iteration 244: 0.11398749455809593\n",
      "Average loss per fishing iteration 245: 0.0005581785147978735\n",
      "Average loss per iteration 246: 0.12459895610809327\n",
      "Average loss per iteration 247: 0.11868840828537941\n",
      "Average loss per iteration 248: 0.11669079273939133\n",
      "Average loss per iteration 249: 0.11652271464467048\n",
      "Average loss per iteration 250: 0.11282750740647315\n",
      "Average loss per fishing iteration 251: 0.0004892427090635465\n",
      "Average loss per iteration 252: 0.11851277604699134\n",
      "Average loss per iteration 253: 0.12088464125990868\n",
      "Average loss per iteration 254: 0.11614018619060516\n",
      "Average loss per iteration 255: 0.11892272502183915\n",
      "Average loss per iteration 256: 0.12158314898610115\n",
      "Average loss per fishing iteration 257: 0.0003417300455566874\n",
      "Average loss per iteration 258: 0.12275700524449348\n",
      "Average loss per iteration 259: 0.10853676572442054\n",
      "Average loss per iteration 260: 0.11205066964030266\n",
      "Average loss per iteration 261: 0.11060628771781922\n",
      "Average loss per iteration 262: 0.10870067104697227\n",
      "Average loss per fishing iteration 263: 0.00010365413009139956\n",
      "Average loss per iteration 264: 0.11718901082873344\n",
      "Average loss per iteration 265: 0.11084866046905517\n",
      "Average loss per iteration 266: 0.1124879465252161\n",
      "Average loss per iteration 267: 0.12044045940041542\n",
      "Average loss per iteration 268: 0.12169257372617721\n",
      "Average loss per fishing iteration 269: 0.0002555518568760817\n",
      "Average loss per iteration 270: 0.11845241874456405\n",
      "Average loss per iteration 271: 0.11437641382217408\n",
      "Average loss per iteration 272: 0.11508896574378014\n",
      "Average loss per iteration 273: 0.1148335725069046\n",
      "Average loss per iteration 274: 0.11633530169725419\n",
      "Average loss per fishing iteration 275: 0.0005593584438975085\n",
      "Average loss per iteration 276: 0.1185174785554409\n",
      "Average loss per iteration 277: 0.11527466416358947\n",
      "Average loss per iteration 278: 0.11175920814275742\n",
      "Average loss per iteration 279: 0.10807111591100693\n",
      "Average loss per iteration 280: 0.11258985280990601\n",
      "Average loss per fishing iteration 281: 0.00030707699789672916\n",
      "Average loss per iteration 282: 0.11342265531420707\n",
      "Average loss per iteration 283: 0.1074661248922348\n",
      "Average loss per iteration 284: 0.11205996632575989\n",
      "Average loss per iteration 285: 0.11291862338781357\n",
      "Average loss per iteration 286: 0.11245243892073631\n",
      "Average loss per fishing iteration 287: 0.0004743189436567263\n",
      "Average loss per iteration 288: 0.1176602004468441\n",
      "Average loss per iteration 289: 0.10874210432171821\n",
      "Average loss per iteration 290: 0.11286038294434547\n",
      "Average loss per iteration 291: 0.1129043608903885\n",
      "Average loss per iteration 292: 0.11154268100857735\n",
      "Average loss per fishing iteration 293: 0.0005842555944855121\n",
      "Average loss per iteration 294: 0.11625297993421554\n",
      "Average loss per iteration 295: 0.11125022500753402\n",
      "Average loss per iteration 296: 0.1128099186718464\n",
      "Average loss per iteration 297: 0.11404203593730927\n",
      "Average loss per iteration 298: 0.11271220207214355\n",
      "Average loss per fishing iteration 299: 0.00023721063189441338\n",
      "Iteration 0 finished!\n",
      "Iteration 1 finished!\n",
      "Iteration 2 finished!\n",
      "Iteration 3 finished!\n",
      "Iteration 4 finished!\n",
      "Iteration 5 finished!\n",
      "Iteration 6 finished!\n",
      "Iteration 7 finished!\n",
      "Iteration 8 finished!\n",
      "Iteration 9 finished!\n",
      "Iteration 10 finished!\n",
      "Iteration 11 finished!\n",
      "Iteration 12 finished!\n",
      "Iteration 13 finished!\n",
      "Iteration 14 finished!\n",
      "Iteration 15 finished!\n",
      "Iteration 16 finished!\n",
      "Iteration 17 finished!\n",
      "Iteration 18 finished!\n",
      "Iteration 19 finished!\n",
      "Iteration 20 finished!\n",
      "Iteration 21 finished!\n",
      "Iteration 22 finished!\n",
      "Iteration 23 finished!\n",
      "Iteration 24 finished!\n",
      "Iteration 25 finished!\n",
      "Iteration 26 finished!\n",
      "Iteration 27 finished!\n",
      "Iteration 28 finished!\n",
      "Iteration 29 finished!\n",
      "Iteration 30 finished!\n",
      "Iteration 31 finished!\n",
      "Iteration 32 finished!\n",
      "Iteration 33 finished!\n",
      "Iteration 34 finished!\n",
      "Iteration 35 finished!\n",
      "Iteration 36 finished!\n",
      "Iteration 37 finished!\n",
      "Iteration 38 finished!\n",
      "Iteration 39 finished!\n",
      "Iteration 40 finished!\n",
      "Iteration 41 finished!\n",
      "Iteration 42 finished!\n",
      "Iteration 43 finished!\n",
      "Iteration 44 finished!\n",
      "Iteration 45 finished!\n",
      "Iteration 46 finished!\n",
      "Iteration 47 finished!\n",
      "Iteration 48 finished!\n",
      "Iteration 49 finished!\n",
      "Iteration 50 finished!\n",
      "Iteration 51 finished!\n",
      "Iteration 52 finished!\n",
      "Iteration 53 finished!\n",
      "Iteration 54 finished!\n",
      "Iteration 55 finished!\n",
      "Iteration 56 finished!\n",
      "Iteration 57 finished!\n",
      "Iteration 58 finished!\n",
      "Iteration 59 finished!\n",
      "Iteration 60 finished!\n",
      "Iteration 61 finished!\n",
      "Iteration 62 finished!\n",
      "Iteration 63 finished!\n",
      "Iteration 64 finished!\n",
      "Iteration 65 finished!\n",
      "Iteration 66 finished!\n",
      "Iteration 67 finished!\n",
      "Iteration 68 finished!\n",
      "Iteration 69 finished!\n"
     ]
    }
   ],
   "source": [
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Generate random seeds\n",
    "#seeds = random.sample(range(2**32), 2)\n",
    "\n",
    "for seed in seeds[1 : ]:\n",
    "    print(seed)\n",
    "    # Train model\n",
    "    model = train(n, k, num_epochs_ls = [600, 300, 300], seed=seed)\n",
    "    loss_pts = evaluate_model(model)\n",
    "    \n",
    "    with open(\"numbers_{}.txt\".format(seed), \"w\") as f:\n",
    "        for num in loss_pts:\n",
    "            f.write(f\"{num}\\n\")\n",
    "\n",
    "    # Save the model weights for generation\n",
    "    torch.save(model.state_dict(), \"model_{}.pth\".format(seed))\n",
    "    \n",
    "    # Delete model\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a86b9b3-987b-4e94-b8c5-589451da3670",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m350\u001b[39m\n\u001b[1;32m      3\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m train(n, k, device\u001b[38;5;241m=\u001b[39mdevice, num_epochs_ls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m600\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m300\u001b[39m], seed\u001b[38;5;241m=\u001b[39mseeds[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      5\u001b[0m loss_pts \u001b[38;5;241m=\u001b[39m evaluate_model(model)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumbers_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(seed), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/home/groups/montanar/vietvu01/SparseDiffusion/train.py:197\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n, k, device, num_epochs_ls, seed)\u001b[0m\n\u001b[1;32m    195\u001b[0m train_data \u001b[38;5;241m=\u001b[39m sample_data(N, n, k, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    196\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SubmatrixDataset(train_data)\n\u001b[0;32m--> 197\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[1;32m    200\u001b[0m model \u001b[38;5;241m=\u001b[39m TestMPNN_3(k, hidden_dim_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, hidden_dim_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, hidden_dim_3\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "n = 350\n",
    "k = 20\n",
    "model = train(n, k, device=device, num_epochs_ls = [600, 300, 300], seed=seeds[1])\n",
    "loss_pts = evaluate_model(model)\n",
    "\n",
    "with open(\"numbers_{}.txt\".format(seed), \"w\") as f:\n",
    "    for num in loss_pts:\n",
    "        f.write(f\"{num}\\n\")\n",
    "\n",
    "# Save the model weights for generation\n",
    "torch.save(model.state_dict(), \"model_{}.pth\".format(seed))\n",
    "\n",
    "# Delete model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "597ad203-e36a-42d7-b2e8-0f9de842fa4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_part_1() missing 1 required positional argument: 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(n, k, device\u001b[38;5;241m=\u001b[39mdevice, num_epochs_ls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m600\u001b[39m, \u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m300\u001b[39m], seed\u001b[38;5;241m=\u001b[39mseeds[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/home/groups/montanar/vietvu01/SparseDiffusion/train.py:206\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(n, k, device, num_epochs_ls, seed)\u001b[0m\n\u001b[1;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "\u001b[0;31mTypeError\u001b[0m: train_part_1() missing 1 required positional argument: 'device'"
     ]
    }
   ],
   "source": [
    "train(n, k, device=device, num_epochs_ls = [600, 300, 300], seed=seeds[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b641ff78-7e71-46fd-bea6-22e9d8ade160",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = [564395852, 4173183967]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f66353e5-70ed-4359-b99a-c964b2262f4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 finished!\n",
      "Iteration 1 finished!\n",
      "Iteration 2 finished!\n",
      "Iteration 3 finished!\n",
      "Iteration 4 finished!\n",
      "Iteration 5 finished!\n",
      "Iteration 6 finished!\n",
      "Iteration 7 finished!\n",
      "Iteration 8 finished!\n",
      "Iteration 9 finished!\n",
      "Iteration 10 finished!\n",
      "Iteration 11 finished!\n",
      "Iteration 12 finished!\n",
      "Iteration 13 finished!\n",
      "Iteration 14 finished!\n",
      "Iteration 15 finished!\n",
      "Iteration 16 finished!\n",
      "Iteration 17 finished!\n",
      "Iteration 18 finished!\n",
      "Iteration 19 finished!\n",
      "Iteration 20 finished!\n",
      "Iteration 21 finished!\n",
      "Iteration 22 finished!\n",
      "Iteration 23 finished!\n",
      "Iteration 24 finished!\n",
      "Iteration 25 finished!\n",
      "Iteration 26 finished!\n",
      "Iteration 27 finished!\n",
      "Iteration 28 finished!\n",
      "Iteration 29 finished!\n",
      "Iteration 30 finished!\n",
      "Iteration 31 finished!\n",
      "Iteration 32 finished!\n",
      "Iteration 33 finished!\n",
      "Iteration 34 finished!\n",
      "Iteration 35 finished!\n",
      "Iteration 36 finished!\n",
      "Iteration 37 finished!\n",
      "Iteration 38 finished!\n",
      "Iteration 39 finished!\n",
      "Iteration 40 finished!\n",
      "Iteration 41 finished!\n",
      "Iteration 42 finished!\n",
      "Iteration 43 finished!\n",
      "Iteration 44 finished!\n",
      "Iteration 45 finished!\n",
      "Iteration 46 finished!\n",
      "Iteration 47 finished!\n",
      "Iteration 48 finished!\n",
      "Iteration 49 finished!\n",
      "Iteration 50 finished!\n",
      "Iteration 51 finished!\n",
      "Iteration 52 finished!\n",
      "Iteration 53 finished!\n",
      "Iteration 54 finished!\n",
      "Iteration 55 finished!\n",
      "Iteration 56 finished!\n",
      "Iteration 57 finished!\n",
      "Iteration 58 finished!\n",
      "Iteration 59 finished!\n",
      "Iteration 60 finished!\n",
      "Iteration 61 finished!\n",
      "Iteration 62 finished!\n",
      "Iteration 63 finished!\n",
      "Iteration 64 finished!\n",
      "Iteration 65 finished!\n",
      "Iteration 66 finished!\n",
      "Iteration 67 finished!\n",
      "Iteration 68 finished!\n",
      "Iteration 69 finished!\n"
     ]
    }
   ],
   "source": [
    "loss_pts = evaluate_model(model)\n",
    "    \n",
    "with open(\"numbers_{}.txt\".format(seed), \"w\") as f:\n",
    "    for num in loss_pts:\n",
    "        f.write(f\"{num}\\n\")\n",
    "\n",
    "# Save the model weights for generation\n",
    "torch.save(model.state_dict(), \"model_{}.pth\".format(seed))\n",
    "\n",
    "# Delete model\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79111e43-8b21-42ed-8256-ff788b98fe7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[564395852, 4173183967]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5fbc3773-d9d6-4e18-a5bf-1bbbeec5aef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"numbers_351198240.txt\", \"r\") as f:\n",
    "    loss_pts_1 = [float(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "with open(\"numbers_1388646581.txt\", \"r\") as f:\n",
    "    loss_pts_2 = [float(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "with open(\"numbers_1133021819.txt\", \"r\") as f:\n",
    "    loss_pts_3 = [float(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "with open(\"numbers_1234.txt\", \"r\") as f:\n",
    "    loss_pts_4 = [float(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "with open(\"numbers_4173183967.txt\", \"r\") as f:\n",
    "    loss_pts_5 = [float(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "with open(\"numbers_564395852.txt\", \"r\") as f:\n",
    "    loss_pts_6 = [float(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, np.array(loss_pts_1), label=\"Run 1\")\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, np.array(loss_pts_2), label=\"Run 2\")\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, np.array(loss_pts_3), label=\"Run 3\")\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, np.array(loss_pts_4), label=\"Run 4\")\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, np.array(loss_pts_5), label=\"Run 5\")\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, np.array(loss_pts_6), label=\"Run 6\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"time / n\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.savefig(\"run_plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b8ba80ae-abc9-43f0-8fcc-1324613f1a00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 20.0 finished!\n",
      "tensor(1.4955, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 40.0 finished!\n",
      "tensor(1.4925, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 60.0 finished!\n",
      "tensor(1.4515, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 80.0 finished!\n",
      "tensor(1.2365, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 100.0 finished!\n",
      "tensor(0.8791, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 120.0 finished!\n",
      "tensor(0.6282, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 140.0 finished!\n",
      "tensor(0.5378, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 160.0 finished!\n",
      "tensor(0.5157, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 180.0 finished!\n",
      "tensor(0.5075, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 200.0 finished!\n",
      "tensor(0.4963, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 220.0 finished!\n",
      "tensor(0.4810, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 240.0 finished!\n",
      "tensor(0.4541, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 260.0 finished!\n",
      "tensor(0.4183, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 280.0 finished!\n",
      "tensor(0.3666, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 300.0 finished!\n",
      "tensor(0.3171, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 320.0 finished!\n",
      "tensor(0.2570, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 340.0 finished!\n",
      "tensor(0.2044, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 360.0 finished!\n",
      "tensor(0.1647, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 380.0 finished!\n",
      "tensor(0.1227, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 400.0 finished!\n",
      "tensor(0.0905, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 420.0 finished!\n",
      "tensor(0.0676, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 440.0 finished!\n",
      "tensor(0.0496, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 460.0 finished!\n",
      "tensor(0.0345, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 480.0 finished!\n",
      "tensor(0.0262, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 500.0 finished!\n",
      "tensor(0.0182, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 520.0 finished!\n",
      "tensor(0.0116, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 540.0 finished!\n",
      "tensor(0.0086, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 560.0 finished!\n",
      "tensor(0.0054, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 580.0 finished!\n",
      "tensor(0.0036, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 600.0 finished!\n",
      "tensor(0.0025, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 620.0 finished!\n",
      "tensor(0.0017, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 640.0 finished!\n",
      "tensor(0.0009, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 660.0 finished!\n",
      "tensor(0.0008, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 680.0 finished!\n",
      "tensor(0.0003, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 700.0 finished!\n",
      "tensor(0.0002, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 720.0 finished!\n",
      "tensor(0.0001, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 740.0 finished!\n",
      "tensor(7.8000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 760.0 finished!\n",
      "tensor(5.2000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 780.0 finished!\n",
      "tensor(7.8000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 800.0 finished!\n",
      "tensor(1.3000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 820.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 840.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 860.0 finished!\n",
      "tensor(1.3000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 880.0 finished!\n",
      "tensor(1.3000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 900.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 920.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 940.0 finished!\n",
      "tensor(1.3000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 960.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 980.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1000.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1020.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1040.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1060.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1080.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1100.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1120.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1140.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1160.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1180.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1200.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1220.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1240.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1260.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1280.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1300.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1320.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1340.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1360.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1380.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1400.0 finished!\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Make test dataset\n",
    "N = 15000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "test_data = sample_data(N, n, k, device=device)\n",
    "test_dataset = SubmatrixDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=250, shuffle=True)\n",
    "\n",
    "# Draw a loss curve\n",
    "loss_opt = []\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "\n",
    "        # Evaluate the optimal polynomial time estimator\n",
    "        loss_1 = evaluate_estimator(optimal_polynomial_estimator, test_dataloader, t, n, k, device=device)\n",
    "        loss_opt.append(min(loss_1.item(), 1/2 - 1 / (4 * n)))\n",
    "\n",
    "        print(\"Time {} finished!\".format(t))\n",
    "        print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "358eaebd-eed2-4b55-be4e-862146ad2a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = [loss_pts_1, loss_pts_2, loss_pts_3, loss_pts_4, loss_pts_5, loss_pts_6]  # each runX is a list of length T\n",
    "\n",
    "# 1) Stack into an array: shape (6, T)\n",
    "loss_array = np.vstack(losses)\n",
    "\n",
    "# 2) Compute mean and std over the first axis (the runs):\n",
    "mean_loss = loss_array.mean(axis=0)\n",
    "std_loss  = loss_array.std(axis=0)\n",
    "\n",
    "# 3) Make an x‐axis (e.g. iterations or epochs)\n",
    "x = torch.linspace(20, 1400, 70)\n",
    "\n",
    "# 4a) Using errorbar:\n",
    "plt.fill_between(x / n, mean_loss - 2 * std_loss, mean_loss + 2 * std_loss, alpha=0.3, label='±2 std. bands')\n",
    "plt.plot(x / n, mean_loss, label=\"Average loss of GNN denoisers\")\n",
    "plt.plot(x / n, loss_opt, label=\"Loss of Spectral Algorithm (Alg. 1)\")\n",
    "\n",
    "plt.xlabel('time / n')\n",
    "plt.ylabel('loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"robust_plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "90f40826-a8e3-4b68-a424-71b6057577ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.5 updated\n",
      "Time 1.0 updated\n",
      "Time 1.5 updated\n",
      "Time 2.0 updated\n",
      "Time 2.5 updated\n",
      "Time 3.0 updated\n",
      "Time 3.5 updated\n",
      "Time 4.0 updated\n",
      "Time 4.5 updated\n",
      "Time 5.0 updated\n",
      "Time 5.5 updated\n",
      "Time 6.0 updated\n",
      "Time 6.5 updated\n",
      "Time 7.0 updated\n",
      "Time 7.5 updated\n",
      "Time 8.0 updated\n",
      "Time 8.5 updated\n",
      "Time 9.0 updated\n",
      "Time 9.5 updated\n",
      "Time 10.0 updated\n",
      "Time 10.5 updated\n",
      "Time 11.0 updated\n",
      "Time 11.5 updated\n",
      "Time 12.0 updated\n",
      "Time 12.5 updated\n",
      "Time 13.0 updated\n",
      "Time 13.5 updated\n",
      "Time 14.0 updated\n",
      "Time 14.5 updated\n",
      "Time 15.0 updated\n",
      "Time 15.5 updated\n",
      "Time 16.0 updated\n",
      "Time 16.5 updated\n",
      "Time 17.0 updated\n",
      "Time 17.5 updated\n",
      "Time 18.0 updated\n",
      "Time 18.5 updated\n",
      "Time 19.0 updated\n",
      "Time 19.5 updated\n",
      "Time 20.0 updated\n",
      "Time 20.5 updated\n",
      "Time 21.0 updated\n",
      "Time 21.5 updated\n",
      "Time 22.0 updated\n",
      "Time 22.5 updated\n",
      "Time 23.0 updated\n",
      "Time 23.5 updated\n",
      "Time 24.0 updated\n",
      "Time 24.5 updated\n",
      "Time 25.0 updated\n",
      "Time 25.5 updated\n",
      "Time 26.0 updated\n",
      "Time 26.5 updated\n",
      "Time 27.0 updated\n",
      "Time 27.5 updated\n",
      "Time 28.0 updated\n",
      "Time 28.5 updated\n",
      "Time 29.0 updated\n",
      "Time 29.5 updated\n",
      "Time 30.0 updated\n",
      "Time 30.5 updated\n",
      "Time 31.0 updated\n",
      "Time 31.5 updated\n",
      "Time 32.0 updated\n",
      "Time 32.5 updated\n",
      "Time 33.0 updated\n",
      "Time 33.5 updated\n",
      "Time 34.0 updated\n",
      "Time 34.5 updated\n",
      "Time 35.0 updated\n",
      "Time 35.5 updated\n",
      "Time 36.0 updated\n",
      "Time 36.5 updated\n",
      "Time 37.0 updated\n",
      "Time 37.5 updated\n",
      "Time 38.0 updated\n",
      "Time 38.5 updated\n",
      "Time 39.0 updated\n",
      "Time 39.5 updated\n",
      "Time 40.0 updated\n",
      "Time 40.5 updated\n",
      "Time 41.0 updated\n",
      "Time 41.5 updated\n",
      "Time 42.0 updated\n",
      "Time 42.5 updated\n",
      "Time 43.0 updated\n",
      "Time 43.5 updated\n",
      "Time 44.0 updated\n",
      "Time 44.5 updated\n",
      "Time 45.0 updated\n",
      "Time 45.5 updated\n",
      "Time 46.0 updated\n",
      "Time 46.5 updated\n",
      "Time 47.0 updated\n",
      "Time 47.5 updated\n",
      "Time 48.0 updated\n",
      "Time 48.5 updated\n",
      "Time 49.0 updated\n",
      "Time 49.5 updated\n",
      "Time 50.0 updated\n",
      "Time 50.5 updated\n",
      "Time 51.0 updated\n",
      "Time 51.5 updated\n",
      "Time 52.0 updated\n",
      "Time 52.5 updated\n",
      "Time 53.0 updated\n",
      "Time 53.5 updated\n",
      "Time 54.0 updated\n",
      "Time 54.5 updated\n",
      "Time 55.0 updated\n",
      "Time 55.5 updated\n",
      "Time 56.0 updated\n",
      "Time 56.5 updated\n",
      "Time 57.0 updated\n",
      "Time 57.5 updated\n",
      "Time 58.0 updated\n",
      "Time 58.5 updated\n",
      "Time 59.0 updated\n",
      "Time 59.5 updated\n",
      "Time 60.0 updated\n",
      "Time 60.5 updated\n",
      "Time 61.0 updated\n",
      "Time 61.5 updated\n",
      "Time 62.0 updated\n",
      "Time 62.5 updated\n",
      "Time 63.0 updated\n",
      "Time 63.5 updated\n",
      "Time 64.0 updated\n",
      "Time 64.5 updated\n",
      "Time 65.0 updated\n",
      "Time 65.5 updated\n",
      "Time 66.0 updated\n",
      "Time 66.5 updated\n",
      "Time 67.0 updated\n",
      "Time 67.5 updated\n",
      "Time 68.0 updated\n",
      "Time 68.5 updated\n",
      "Time 69.0 updated\n",
      "Time 69.5 updated\n",
      "Time 70.0 updated\n",
      "Time 70.5 updated\n",
      "Time 71.0 updated\n",
      "Time 71.5 updated\n",
      "Time 72.0 updated\n",
      "Time 72.5 updated\n",
      "Time 73.0 updated\n",
      "Time 73.5 updated\n",
      "Time 74.0 updated\n",
      "Time 74.5 updated\n",
      "Time 75.0 updated\n",
      "Time 75.5 updated\n",
      "Time 76.0 updated\n",
      "Time 76.5 updated\n",
      "Time 77.0 updated\n",
      "Time 77.5 updated\n",
      "Time 78.0 updated\n",
      "Time 78.5 updated\n",
      "Time 79.0 updated\n",
      "Time 79.5 updated\n",
      "Time 80.0 updated\n",
      "Time 80.5 updated\n",
      "Time 81.0 updated\n",
      "Time 81.5 updated\n",
      "Time 82.0 updated\n",
      "Time 82.5 updated\n",
      "Time 83.0 updated\n",
      "Time 83.5 updated\n",
      "Time 84.0 updated\n",
      "Time 84.5 updated\n",
      "Time 85.0 updated\n",
      "Time 85.5 updated\n",
      "Time 86.0 updated\n",
      "Time 86.5 updated\n",
      "Time 87.0 updated\n",
      "Time 87.5 updated\n",
      "Time 88.0 updated\n",
      "Time 88.5 updated\n",
      "Time 89.0 updated\n",
      "Time 89.5 updated\n",
      "Time 90.0 updated\n",
      "Time 90.5 updated\n",
      "Time 91.0 updated\n",
      "Time 91.5 updated\n",
      "Time 92.0 updated\n",
      "Time 92.5 updated\n",
      "Time 93.0 updated\n",
      "Time 93.5 updated\n",
      "Time 94.0 updated\n",
      "Time 94.5 updated\n",
      "Time 95.0 updated\n",
      "Time 95.5 updated\n",
      "Time 96.0 updated\n",
      "Time 96.5 updated\n",
      "Time 97.0 updated\n",
      "Time 97.5 updated\n",
      "Time 98.0 updated\n",
      "Time 98.5 updated\n",
      "Time 99.0 updated\n",
      "Time 99.5 updated\n",
      "Time 100.0 updated\n",
      "Time 100.5 updated\n",
      "Time 101.0 updated\n",
      "Time 101.5 updated\n",
      "Time 102.0 updated\n",
      "Time 102.5 updated\n",
      "Time 103.0 updated\n",
      "Time 103.5 updated\n",
      "Time 104.0 updated\n",
      "Time 104.5 updated\n",
      "Time 105.0 updated\n",
      "Time 105.5 updated\n",
      "Time 106.0 updated\n",
      "Time 106.5 updated\n",
      "Time 107.0 updated\n",
      "Time 107.5 updated\n",
      "Time 108.0 updated\n",
      "Time 108.5 updated\n",
      "Time 109.0 updated\n",
      "Time 109.5 updated\n",
      "Time 110.0 updated\n",
      "Time 110.5 updated\n",
      "Time 111.0 updated\n",
      "Time 111.5 updated\n",
      "Time 112.0 updated\n",
      "Time 112.5 updated\n",
      "Time 113.0 updated\n",
      "Time 113.5 updated\n",
      "Time 114.0 updated\n",
      "Time 114.5 updated\n",
      "Time 115.0 updated\n",
      "Time 115.5 updated\n",
      "Time 116.0 updated\n",
      "Time 116.5 updated\n",
      "Time 117.0 updated\n",
      "Time 117.5 updated\n",
      "Time 118.0 updated\n",
      "Time 118.5 updated\n",
      "Time 119.0 updated\n",
      "Time 119.5 updated\n",
      "Time 120.0 updated\n",
      "Time 120.5 updated\n",
      "Time 121.0 updated\n",
      "Time 121.5 updated\n",
      "Time 122.0 updated\n",
      "Time 122.5 updated\n",
      "Time 123.0 updated\n",
      "Time 123.5 updated\n",
      "Time 124.0 updated\n",
      "Time 124.5 updated\n",
      "Time 125.0 updated\n",
      "Time 125.5 updated\n",
      "Time 126.0 updated\n",
      "Time 126.5 updated\n",
      "Time 127.0 updated\n",
      "Time 127.5 updated\n",
      "Time 128.0 updated\n",
      "Time 128.5 updated\n",
      "Time 129.0 updated\n",
      "Time 129.5 updated\n",
      "Time 130.0 updated\n",
      "Time 130.5 updated\n",
      "Time 131.0 updated\n",
      "Time 131.5 updated\n",
      "Time 132.0 updated\n",
      "Time 132.5 updated\n",
      "Time 133.0 updated\n",
      "Time 133.5 updated\n",
      "Time 134.0 updated\n",
      "Time 134.5 updated\n",
      "Time 135.0 updated\n",
      "Time 135.5 updated\n",
      "Time 136.0 updated\n",
      "Time 136.5 updated\n",
      "Time 137.0 updated\n",
      "Time 137.5 updated\n",
      "Time 138.0 updated\n",
      "Time 138.5 updated\n",
      "Time 139.0 updated\n",
      "Time 139.5 updated\n",
      "Time 140.0 updated\n",
      "Time 140.5 updated\n",
      "Time 141.0 updated\n",
      "Time 141.5 updated\n",
      "Time 142.0 updated\n",
      "Time 142.5 updated\n",
      "Time 143.0 updated\n",
      "Time 143.5 updated\n",
      "Time 144.0 updated\n",
      "Time 144.5 updated\n",
      "Time 145.0 updated\n",
      "Time 145.5 updated\n",
      "Time 146.0 updated\n",
      "Time 146.5 updated\n",
      "Time 147.0 updated\n",
      "Time 147.5 updated\n",
      "Time 148.0 updated\n",
      "Time 148.5 updated\n",
      "Time 149.0 updated\n",
      "Time 149.5 updated\n",
      "Time 150.0 updated\n",
      "Time 150.5 updated\n",
      "Time 151.0 updated\n",
      "Time 151.5 updated\n",
      "Time 152.0 updated\n",
      "Time 152.5 updated\n",
      "Time 153.0 updated\n",
      "Time 153.5 updated\n",
      "Time 154.0 updated\n",
      "Time 154.5 updated\n",
      "Time 155.0 updated\n",
      "Time 155.5 updated\n",
      "Time 156.0 updated\n",
      "Time 156.5 updated\n",
      "Time 157.0 updated\n",
      "Time 157.5 updated\n",
      "Time 158.0 updated\n",
      "Time 158.5 updated\n",
      "Time 159.0 updated\n",
      "Time 159.5 updated\n",
      "Time 160.0 updated\n",
      "Time 160.5 updated\n",
      "Time 161.0 updated\n",
      "Time 161.5 updated\n",
      "Time 162.0 updated\n",
      "Time 162.5 updated\n",
      "Time 163.0 updated\n",
      "Time 163.5 updated\n",
      "Time 164.0 updated\n",
      "Time 164.5 updated\n",
      "Time 165.0 updated\n",
      "Time 165.5 updated\n",
      "Time 166.0 updated\n",
      "Time 166.5 updated\n",
      "Time 167.0 updated\n",
      "Time 167.5 updated\n",
      "Time 168.0 updated\n",
      "Time 168.5 updated\n",
      "Time 169.0 updated\n",
      "Time 169.5 updated\n",
      "Time 170.0 updated\n",
      "Time 170.5 updated\n",
      "Time 171.0 updated\n",
      "Time 171.5 updated\n",
      "Time 172.0 updated\n",
      "Time 172.5 updated\n",
      "Time 173.0 updated\n",
      "Time 173.5 updated\n",
      "Time 174.0 updated\n",
      "Time 174.5 updated\n",
      "Time 175.0 updated\n",
      "Time 175.5 updated\n",
      "Time 176.0 updated\n",
      "Time 176.5 updated\n",
      "Time 177.0 updated\n",
      "Time 177.5 updated\n",
      "Time 178.0 updated\n",
      "Time 178.5 updated\n",
      "Time 179.0 updated\n",
      "Time 179.5 updated\n",
      "Time 180.0 updated\n",
      "Time 180.5 updated\n",
      "Time 181.0 updated\n",
      "Time 181.5 updated\n",
      "Time 182.0 updated\n",
      "Time 182.5 updated\n",
      "Time 183.0 updated\n",
      "Time 183.5 updated\n",
      "Time 184.0 updated\n",
      "Time 184.5 updated\n",
      "Time 185.0 updated\n",
      "Time 185.5 updated\n",
      "Time 186.0 updated\n",
      "Time 186.5 updated\n",
      "Time 187.0 updated\n",
      "Time 187.5 updated\n",
      "Time 188.0 updated\n",
      "Time 188.5 updated\n",
      "Time 189.0 updated\n",
      "Time 189.5 updated\n",
      "Time 190.0 updated\n",
      "Time 190.5 updated\n",
      "Time 191.0 updated\n",
      "Time 191.5 updated\n",
      "Time 192.0 updated\n",
      "Time 192.5 updated\n",
      "Time 193.0 updated\n",
      "Time 193.5 updated\n",
      "Time 194.0 updated\n",
      "Time 194.5 updated\n",
      "Time 195.0 updated\n",
      "Time 195.5 updated\n",
      "Time 196.0 updated\n",
      "Time 196.5 updated\n",
      "Time 197.0 updated\n",
      "Time 197.5 updated\n",
      "Time 198.0 updated\n",
      "Time 198.5 updated\n",
      "Time 199.0 updated\n",
      "Time 199.5 updated\n",
      "Time 200.0 updated\n",
      "Time 200.5 updated\n",
      "Time 201.0 updated\n",
      "Time 201.5 updated\n",
      "Time 202.0 updated\n",
      "Time 202.5 updated\n",
      "Time 203.0 updated\n",
      "Time 203.5 updated\n",
      "Time 204.0 updated\n",
      "Time 204.5 updated\n",
      "Time 205.0 updated\n",
      "Time 205.5 updated\n",
      "Time 206.0 updated\n",
      "Time 206.5 updated\n",
      "Time 207.0 updated\n",
      "Time 207.5 updated\n",
      "Time 208.0 updated\n",
      "Time 208.5 updated\n",
      "Time 209.0 updated\n",
      "Time 209.5 updated\n",
      "Time 210.0 updated\n",
      "Time 210.5 updated\n",
      "Time 211.0 updated\n",
      "Time 211.5 updated\n",
      "Time 212.0 updated\n",
      "Time 212.5 updated\n",
      "Time 213.0 updated\n",
      "Time 213.5 updated\n",
      "Time 214.0 updated\n",
      "Time 214.5 updated\n",
      "Time 215.0 updated\n",
      "Time 215.5 updated\n",
      "Time 216.0 updated\n",
      "Time 216.5 updated\n",
      "Time 217.0 updated\n",
      "Time 217.5 updated\n",
      "Time 218.0 updated\n",
      "Time 218.5 updated\n",
      "Time 219.0 updated\n",
      "Time 219.5 updated\n",
      "Time 220.0 updated\n",
      "Time 220.5 updated\n",
      "Time 221.0 updated\n",
      "Time 221.5 updated\n",
      "Time 222.0 updated\n",
      "Time 222.5 updated\n",
      "Time 223.0 updated\n",
      "Time 223.5 updated\n",
      "Time 224.0 updated\n",
      "Time 224.5 updated\n",
      "Time 225.0 updated\n",
      "Time 225.5 updated\n",
      "Time 226.0 updated\n",
      "Time 226.5 updated\n",
      "Time 227.0 updated\n",
      "Time 227.5 updated\n",
      "Time 228.0 updated\n",
      "Time 228.5 updated\n",
      "Time 229.0 updated\n",
      "Time 229.5 updated\n",
      "Time 230.0 updated\n",
      "Time 230.5 updated\n",
      "Time 231.0 updated\n",
      "Time 231.5 updated\n",
      "Time 232.0 updated\n",
      "Time 232.5 updated\n",
      "Time 233.0 updated\n",
      "Time 233.5 updated\n",
      "Time 234.0 updated\n",
      "Time 234.5 updated\n",
      "Time 235.0 updated\n",
      "Time 235.5 updated\n",
      "Time 236.0 updated\n",
      "Time 236.5 updated\n",
      "Time 237.0 updated\n",
      "Time 237.5 updated\n",
      "Time 238.0 updated\n",
      "Time 238.5 updated\n",
      "Time 239.0 updated\n",
      "Time 239.5 updated\n",
      "Time 240.0 updated\n",
      "Time 240.5 updated\n",
      "Time 241.0 updated\n",
      "Time 241.5 updated\n",
      "Time 242.0 updated\n",
      "Time 242.5 updated\n",
      "Time 243.0 updated\n",
      "Time 243.5 updated\n",
      "Time 244.0 updated\n",
      "Time 244.5 updated\n",
      "Time 245.0 updated\n",
      "Time 245.5 updated\n",
      "Time 246.0 updated\n",
      "Time 246.5 updated\n",
      "Time 247.0 updated\n",
      "Time 247.5 updated\n",
      "Time 248.0 updated\n",
      "Time 248.5 updated\n",
      "Time 249.0 updated\n",
      "Time 249.5 updated\n",
      "Time 250.0 updated\n",
      "Time 250.5 updated\n",
      "Time 251.0 updated\n",
      "Time 251.5 updated\n",
      "Time 252.0 updated\n",
      "Time 252.5 updated\n",
      "Time 253.0 updated\n",
      "Time 253.5 updated\n",
      "Time 254.0 updated\n",
      "Time 254.5 updated\n",
      "Time 255.0 updated\n",
      "Time 255.5 updated\n",
      "Time 256.0 updated\n",
      "Time 256.5 updated\n",
      "Time 257.0 updated\n",
      "Time 257.5 updated\n",
      "Time 258.0 updated\n",
      "Time 258.5 updated\n",
      "Time 259.0 updated\n",
      "Time 259.5 updated\n",
      "Time 260.0 updated\n",
      "Time 260.5 updated\n",
      "Time 261.0 updated\n",
      "Time 261.5 updated\n",
      "Time 262.0 updated\n",
      "Time 262.5 updated\n",
      "Time 263.0 updated\n",
      "Time 263.5 updated\n",
      "Time 264.0 updated\n",
      "Time 264.5 updated\n",
      "Time 265.0 updated\n",
      "Time 265.5 updated\n",
      "Time 266.0 updated\n",
      "Time 266.5 updated\n",
      "Time 267.0 updated\n",
      "Time 267.5 updated\n",
      "Time 268.0 updated\n",
      "Time 268.5 updated\n",
      "Time 269.0 updated\n",
      "Time 269.5 updated\n",
      "Time 270.0 updated\n",
      "Time 270.5 updated\n",
      "Time 271.0 updated\n",
      "Time 271.5 updated\n",
      "Time 272.0 updated\n",
      "Time 272.5 updated\n",
      "Time 273.0 updated\n",
      "Time 273.5 updated\n",
      "Time 274.0 updated\n",
      "Time 274.5 updated\n",
      "Time 275.0 updated\n",
      "Time 275.5 updated\n",
      "Time 276.0 updated\n",
      "Time 276.5 updated\n",
      "Time 277.0 updated\n",
      "Time 277.5 updated\n",
      "Time 278.0 updated\n",
      "Time 278.5 updated\n",
      "Time 279.0 updated\n",
      "Time 279.5 updated\n",
      "Time 280.0 updated\n",
      "Time 280.5 updated\n",
      "Time 281.0 updated\n",
      "Time 281.5 updated\n",
      "Time 282.0 updated\n",
      "Time 282.5 updated\n",
      "Time 283.0 updated\n",
      "Time 283.5 updated\n",
      "Time 284.0 updated\n",
      "Time 284.5 updated\n",
      "Time 285.0 updated\n",
      "Time 285.5 updated\n",
      "Time 286.0 updated\n",
      "Time 286.5 updated\n",
      "Time 287.0 updated\n",
      "Time 287.5 updated\n",
      "Time 288.0 updated\n",
      "Time 288.5 updated\n",
      "Time 289.0 updated\n",
      "Time 289.5 updated\n",
      "Time 290.0 updated\n",
      "Time 290.5 updated\n",
      "Time 291.0 updated\n",
      "Time 291.5 updated\n",
      "Time 292.0 updated\n",
      "Time 292.5 updated\n",
      "Time 293.0 updated\n",
      "Time 293.5 updated\n",
      "Time 294.0 updated\n",
      "Time 294.5 updated\n",
      "Time 295.0 updated\n",
      "Time 295.5 updated\n",
      "Time 296.0 updated\n",
      "Time 296.5 updated\n",
      "Time 297.0 updated\n",
      "Time 297.5 updated\n",
      "Time 298.0 updated\n",
      "Time 298.5 updated\n",
      "Time 299.0 updated\n",
      "Time 299.5 updated\n",
      "Time 300.0 updated\n",
      "Time 300.5 updated\n",
      "Time 301.0 updated\n",
      "Time 301.5 updated\n",
      "Time 302.0 updated\n",
      "Time 302.5 updated\n",
      "Time 303.0 updated\n",
      "Time 303.5 updated\n",
      "Time 304.0 updated\n",
      "Time 304.5 updated\n",
      "Time 305.0 updated\n",
      "Time 305.5 updated\n",
      "Time 306.0 updated\n",
      "Time 306.5 updated\n",
      "Time 307.0 updated\n",
      "Time 307.5 updated\n",
      "Time 308.0 updated\n",
      "Time 308.5 updated\n",
      "Time 309.0 updated\n",
      "Time 309.5 updated\n",
      "Time 310.0 updated\n",
      "Time 310.5 updated\n",
      "Time 311.0 updated\n",
      "Time 311.5 updated\n",
      "Time 312.0 updated\n",
      "Time 312.5 updated\n",
      "Time 313.0 updated\n",
      "Time 313.5 updated\n",
      "Time 314.0 updated\n",
      "Time 314.5 updated\n",
      "Time 315.0 updated\n",
      "Time 315.5 updated\n",
      "Time 316.0 updated\n",
      "Time 316.5 updated\n",
      "Time 317.0 updated\n",
      "Time 317.5 updated\n",
      "Time 318.0 updated\n",
      "Time 318.5 updated\n",
      "Time 319.0 updated\n",
      "Time 319.5 updated\n",
      "Time 320.0 updated\n",
      "Time 320.5 updated\n",
      "Time 321.0 updated\n",
      "Time 321.5 updated\n",
      "Time 322.0 updated\n",
      "Time 322.5 updated\n",
      "Time 323.0 updated\n",
      "Time 323.5 updated\n",
      "Time 324.0 updated\n",
      "Time 324.5 updated\n",
      "Time 325.0 updated\n",
      "Time 325.5 updated\n",
      "Time 326.0 updated\n",
      "Time 326.5 updated\n",
      "Time 327.0 updated\n",
      "Time 327.5 updated\n",
      "Time 328.0 updated\n",
      "Time 328.5 updated\n",
      "Time 329.0 updated\n",
      "Time 329.5 updated\n",
      "Time 330.0 updated\n",
      "Time 330.5 updated\n",
      "Time 331.0 updated\n",
      "Time 331.5 updated\n",
      "Time 332.0 updated\n",
      "Time 332.5 updated\n",
      "Time 333.0 updated\n",
      "Time 333.5 updated\n",
      "Time 334.0 updated\n",
      "Time 334.5 updated\n",
      "Time 335.0 updated\n",
      "Time 335.5 updated\n",
      "Time 336.0 updated\n",
      "Time 336.5 updated\n",
      "Time 337.0 updated\n",
      "Time 337.5 updated\n",
      "Time 338.0 updated\n",
      "Time 338.5 updated\n",
      "Time 339.0 updated\n",
      "Time 339.5 updated\n",
      "Time 340.0 updated\n",
      "Time 340.5 updated\n",
      "Time 341.0 updated\n",
      "Time 341.5 updated\n",
      "Time 342.0 updated\n",
      "Time 342.5 updated\n",
      "Time 343.0 updated\n",
      "Time 343.5 updated\n",
      "Time 344.0 updated\n",
      "Time 344.5 updated\n",
      "Time 345.0 updated\n",
      "Time 345.5 updated\n",
      "Time 346.0 updated\n",
      "Time 346.5 updated\n",
      "Time 347.0 updated\n",
      "Time 347.5 updated\n",
      "Time 348.0 updated\n",
      "Time 348.5 updated\n",
      "Time 349.0 updated\n",
      "Time 349.5 updated\n",
      "Time 350.0 updated\n",
      "Time 350.5 updated\n",
      "Time 351.0 updated\n",
      "Time 351.5 updated\n",
      "Time 352.0 updated\n",
      "Time 352.5 updated\n",
      "Time 353.0 updated\n",
      "Time 353.5 updated\n",
      "Time 354.0 updated\n",
      "Time 354.5 updated\n",
      "Time 355.0 updated\n",
      "Time 355.5 updated\n",
      "Time 356.0 updated\n",
      "Time 356.5 updated\n",
      "Time 357.0 updated\n",
      "Time 357.5 updated\n",
      "Time 358.0 updated\n",
      "Time 358.5 updated\n",
      "Time 359.0 updated\n",
      "Time 359.5 updated\n",
      "Time 360.0 updated\n",
      "Time 360.5 updated\n",
      "Time 361.0 updated\n",
      "Time 361.5 updated\n",
      "Time 362.0 updated\n",
      "Time 362.5 updated\n",
      "Time 363.0 updated\n",
      "Time 363.5 updated\n",
      "Time 364.0 updated\n",
      "Time 364.5 updated\n",
      "Time 365.0 updated\n",
      "Time 365.5 updated\n",
      "Time 366.0 updated\n",
      "Time 366.5 updated\n",
      "Time 367.0 updated\n",
      "Time 367.5 updated\n",
      "Time 368.0 updated\n",
      "Time 368.5 updated\n",
      "Time 369.0 updated\n",
      "Time 369.5 updated\n",
      "Time 370.0 updated\n",
      "Time 370.5 updated\n",
      "Time 371.0 updated\n",
      "Time 371.5 updated\n",
      "Time 372.0 updated\n",
      "Time 372.5 updated\n",
      "Time 373.0 updated\n",
      "Time 373.5 updated\n",
      "Time 374.0 updated\n",
      "Time 374.5 updated\n",
      "Time 375.0 updated\n",
      "Time 375.5 updated\n",
      "Time 376.0 updated\n",
      "Time 376.5 updated\n",
      "Time 377.0 updated\n",
      "Time 377.5 updated\n",
      "Time 378.0 updated\n",
      "Time 378.5 updated\n",
      "Time 379.0 updated\n",
      "Time 379.5 updated\n",
      "Time 380.0 updated\n",
      "Time 380.5 updated\n",
      "Time 381.0 updated\n",
      "Time 381.5 updated\n",
      "Time 382.0 updated\n",
      "Time 382.5 updated\n",
      "Time 383.0 updated\n",
      "Time 383.5 updated\n",
      "Time 384.0 updated\n",
      "Time 384.5 updated\n",
      "Time 385.0 updated\n",
      "Time 385.5 updated\n",
      "Time 386.0 updated\n",
      "Time 386.5 updated\n",
      "Time 387.0 updated\n",
      "Time 387.5 updated\n",
      "Time 388.0 updated\n",
      "Time 388.5 updated\n",
      "Time 389.0 updated\n",
      "Time 389.5 updated\n",
      "Time 390.0 updated\n",
      "Time 390.5 updated\n",
      "Time 391.0 updated\n",
      "Time 391.5 updated\n",
      "Time 392.0 updated\n",
      "Time 392.5 updated\n",
      "Time 393.0 updated\n",
      "Time 393.5 updated\n",
      "Time 394.0 updated\n",
      "Time 394.5 updated\n",
      "Time 395.0 updated\n",
      "Time 395.5 updated\n",
      "Time 396.0 updated\n",
      "Time 396.5 updated\n",
      "Time 397.0 updated\n",
      "Time 397.5 updated\n",
      "Time 398.0 updated\n",
      "Time 398.5 updated\n",
      "Time 399.0 updated\n",
      "Time 399.5 updated\n",
      "Time 400.0 updated\n",
      "Time 400.5 updated\n",
      "Time 401.0 updated\n",
      "Time 401.5 updated\n",
      "Time 402.0 updated\n",
      "Time 402.5 updated\n",
      "Time 403.0 updated\n",
      "Time 403.5 updated\n",
      "Time 404.0 updated\n",
      "Time 404.5 updated\n",
      "Time 405.0 updated\n",
      "Time 405.5 updated\n",
      "Time 406.0 updated\n",
      "Time 406.5 updated\n",
      "Time 407.0 updated\n",
      "Time 407.5 updated\n",
      "Time 408.0 updated\n",
      "Time 408.5 updated\n",
      "Time 409.0 updated\n",
      "Time 409.5 updated\n",
      "Time 410.0 updated\n",
      "Time 410.5 updated\n",
      "Time 411.0 updated\n",
      "Time 411.5 updated\n",
      "Time 412.0 updated\n",
      "Time 412.5 updated\n",
      "Time 413.0 updated\n",
      "Time 413.5 updated\n",
      "Time 414.0 updated\n",
      "Time 414.5 updated\n",
      "Time 415.0 updated\n",
      "Time 415.5 updated\n",
      "Time 416.0 updated\n",
      "Time 416.5 updated\n",
      "Time 417.0 updated\n",
      "Time 417.5 updated\n",
      "Time 418.0 updated\n",
      "Time 418.5 updated\n",
      "Time 419.0 updated\n",
      "Time 419.5 updated\n",
      "Time 420.0 updated\n",
      "Time 420.5 updated\n",
      "Time 421.0 updated\n",
      "Time 421.5 updated\n",
      "Time 422.0 updated\n",
      "Time 422.5 updated\n",
      "Time 423.0 updated\n",
      "Time 423.5 updated\n",
      "Time 424.0 updated\n",
      "Time 424.5 updated\n",
      "Time 425.0 updated\n",
      "Time 425.5 updated\n",
      "Time 426.0 updated\n",
      "Time 426.5 updated\n",
      "Time 427.0 updated\n",
      "Time 427.5 updated\n",
      "Time 428.0 updated\n",
      "Time 428.5 updated\n",
      "Time 429.0 updated\n",
      "Time 429.5 updated\n",
      "Time 430.0 updated\n",
      "Time 430.5 updated\n",
      "Time 431.0 updated\n",
      "Time 431.5 updated\n",
      "Time 432.0 updated\n",
      "Time 432.5 updated\n",
      "Time 433.0 updated\n",
      "Time 433.5 updated\n",
      "Time 434.0 updated\n",
      "Time 434.5 updated\n",
      "Time 435.0 updated\n",
      "Time 435.5 updated\n",
      "Time 436.0 updated\n",
      "Time 436.5 updated\n",
      "Time 437.0 updated\n",
      "Time 437.5 updated\n",
      "Time 438.0 updated\n",
      "Time 438.5 updated\n",
      "Time 439.0 updated\n",
      "Time 439.5 updated\n",
      "Time 440.0 updated\n",
      "Time 440.5 updated\n",
      "Time 441.0 updated\n",
      "Time 441.5 updated\n",
      "Time 442.0 updated\n",
      "Time 442.5 updated\n",
      "Time 443.0 updated\n",
      "Time 443.5 updated\n",
      "Time 444.0 updated\n",
      "Time 444.5 updated\n",
      "Time 445.0 updated\n",
      "Time 445.5 updated\n",
      "Time 446.0 updated\n",
      "Time 446.5 updated\n",
      "Time 447.0 updated\n",
      "Time 447.5 updated\n",
      "Time 448.0 updated\n",
      "Time 448.5 updated\n",
      "Time 449.0 updated\n",
      "Time 449.5 updated\n",
      "Time 450.0 updated\n",
      "Time 450.5 updated\n",
      "Time 451.0 updated\n",
      "Time 451.5 updated\n",
      "Time 452.0 updated\n",
      "Time 452.5 updated\n",
      "Time 453.0 updated\n",
      "Time 453.5 updated\n",
      "Time 454.0 updated\n",
      "Time 454.5 updated\n",
      "Time 455.0 updated\n",
      "Time 455.5 updated\n",
      "Time 456.0 updated\n",
      "Time 456.5 updated\n",
      "Time 457.0 updated\n",
      "Time 457.5 updated\n",
      "Time 458.0 updated\n",
      "Time 458.5 updated\n",
      "Time 459.0 updated\n",
      "Time 459.5 updated\n",
      "Time 460.0 updated\n",
      "Time 460.5 updated\n",
      "Time 461.0 updated\n",
      "Time 461.5 updated\n",
      "Time 462.0 updated\n",
      "Time 462.5 updated\n",
      "Time 463.0 updated\n",
      "Time 463.5 updated\n",
      "Time 464.0 updated\n",
      "Time 464.5 updated\n",
      "Time 465.0 updated\n",
      "Time 465.5 updated\n",
      "Time 466.0 updated\n",
      "Time 466.5 updated\n",
      "Time 467.0 updated\n",
      "Time 467.5 updated\n",
      "Time 468.0 updated\n",
      "Time 468.5 updated\n",
      "Time 469.0 updated\n",
      "Time 469.5 updated\n",
      "Time 470.0 updated\n",
      "Time 470.5 updated\n",
      "Time 471.0 updated\n",
      "Time 471.5 updated\n",
      "Time 472.0 updated\n",
      "Time 472.5 updated\n",
      "Time 473.0 updated\n",
      "Time 473.5 updated\n",
      "Time 474.0 updated\n",
      "Time 474.5 updated\n",
      "Time 475.0 updated\n",
      "Time 475.5 updated\n",
      "Time 476.0 updated\n",
      "Time 476.5 updated\n",
      "Time 477.0 updated\n",
      "Time 477.5 updated\n",
      "Time 478.0 updated\n",
      "Time 478.5 updated\n",
      "Time 479.0 updated\n",
      "Time 479.5 updated\n",
      "Time 480.0 updated\n",
      "Time 480.5 updated\n",
      "Time 481.0 updated\n",
      "Time 481.5 updated\n",
      "Time 482.0 updated\n",
      "Time 482.5 updated\n",
      "Time 483.0 updated\n",
      "Time 483.5 updated\n",
      "Time 484.0 updated\n",
      "Time 484.5 updated\n",
      "Time 485.0 updated\n",
      "Time 485.5 updated\n",
      "Time 486.0 updated\n",
      "Time 486.5 updated\n",
      "Time 487.0 updated\n",
      "Time 487.5 updated\n",
      "Time 488.0 updated\n",
      "Time 488.5 updated\n",
      "Time 489.0 updated\n",
      "Time 489.5 updated\n",
      "Time 490.0 updated\n",
      "Time 490.5 updated\n",
      "Time 491.0 updated\n",
      "Time 491.5 updated\n",
      "Time 492.0 updated\n",
      "Time 492.5 updated\n",
      "Time 493.0 updated\n",
      "Time 493.5 updated\n",
      "Time 494.0 updated\n",
      "Time 494.5 updated\n",
      "Time 495.0 updated\n",
      "Time 495.5 updated\n",
      "Time 496.0 updated\n",
      "Time 496.5 updated\n",
      "Time 497.0 updated\n",
      "Time 497.5 updated\n",
      "Time 498.0 updated\n",
      "Time 498.5 updated\n",
      "Time 499.0 updated\n",
      "Time 499.5 updated\n",
      "Time 500.0 updated\n",
      "Time 500.5 updated\n",
      "Time 501.0 updated\n",
      "Time 501.5 updated\n",
      "Time 502.0 updated\n",
      "Time 502.5 updated\n",
      "Time 503.0 updated\n",
      "Time 503.5 updated\n",
      "Time 504.0 updated\n",
      "Time 504.5 updated\n",
      "Time 505.0 updated\n",
      "Time 505.5 updated\n",
      "Time 506.0 updated\n",
      "Time 506.5 updated\n",
      "Time 507.0 updated\n",
      "Time 507.5 updated\n",
      "Time 508.0 updated\n",
      "Time 508.5 updated\n",
      "Time 509.0 updated\n",
      "Time 509.5 updated\n",
      "Time 510.0 updated\n",
      "Time 510.5 updated\n",
      "Time 511.0 updated\n",
      "Time 511.5 updated\n",
      "Time 512.0 updated\n",
      "Time 512.5 updated\n",
      "Time 513.0 updated\n",
      "Time 513.5 updated\n",
      "Time 514.0 updated\n",
      "Time 514.5 updated\n",
      "Time 515.0 updated\n",
      "Time 515.5 updated\n",
      "Time 516.0 updated\n",
      "Time 516.5 updated\n",
      "Time 517.0 updated\n",
      "Time 517.5 updated\n",
      "Time 518.0 updated\n",
      "Time 518.5 updated\n",
      "Time 519.0 updated\n",
      "Time 519.5 updated\n",
      "Time 520.0 updated\n",
      "Time 520.5 updated\n",
      "Time 521.0 updated\n",
      "Time 521.5 updated\n",
      "Time 522.0 updated\n",
      "Time 522.5 updated\n",
      "Time 523.0 updated\n",
      "Time 523.5 updated\n",
      "Time 524.0 updated\n",
      "Time 524.5 updated\n",
      "Time 525.0 updated\n",
      "Time 525.5 updated\n",
      "Time 526.0 updated\n",
      "Time 526.5 updated\n",
      "Time 527.0 updated\n",
      "Time 527.5 updated\n",
      "Time 528.0 updated\n",
      "Time 528.5 updated\n",
      "Time 529.0 updated\n",
      "Time 529.5 updated\n",
      "Time 530.0 updated\n",
      "Time 530.5 updated\n",
      "Time 531.0 updated\n",
      "Time 531.5 updated\n",
      "Time 532.0 updated\n",
      "Time 532.5 updated\n",
      "Time 533.0 updated\n",
      "Time 533.5 updated\n",
      "Time 534.0 updated\n",
      "Time 534.5 updated\n",
      "Time 535.0 updated\n",
      "Time 535.5 updated\n",
      "Time 536.0 updated\n",
      "Time 536.5 updated\n",
      "Time 537.0 updated\n",
      "Time 537.5 updated\n",
      "Time 538.0 updated\n",
      "Time 538.5 updated\n",
      "Time 539.0 updated\n",
      "Time 539.5 updated\n",
      "Time 540.0 updated\n",
      "Time 540.5 updated\n",
      "Time 541.0 updated\n",
      "Time 541.5 updated\n",
      "Time 542.0 updated\n",
      "Time 542.5 updated\n",
      "Time 543.0 updated\n",
      "Time 543.5 updated\n",
      "Time 544.0 updated\n",
      "Time 544.5 updated\n",
      "Time 545.0 updated\n",
      "Time 545.5 updated\n",
      "Time 546.0 updated\n",
      "Time 546.5 updated\n",
      "Time 547.0 updated\n",
      "Time 547.5 updated\n",
      "Time 548.0 updated\n",
      "Time 548.5 updated\n",
      "Time 549.0 updated\n",
      "Time 549.5 updated\n",
      "Time 550.0 updated\n",
      "Time 550.5 updated\n",
      "Time 551.0 updated\n",
      "Time 551.5 updated\n",
      "Time 552.0 updated\n",
      "Time 552.5 updated\n",
      "Time 553.0 updated\n",
      "Time 553.5 updated\n",
      "Time 554.0 updated\n",
      "Time 554.5 updated\n",
      "Time 555.0 updated\n",
      "Time 555.5 updated\n",
      "Time 556.0 updated\n",
      "Time 556.5 updated\n",
      "Time 557.0 updated\n",
      "Time 557.5 updated\n",
      "Time 558.0 updated\n",
      "Time 558.5 updated\n",
      "Time 559.0 updated\n",
      "Time 559.5 updated\n",
      "Time 560.0 updated\n",
      "Time 560.5 updated\n",
      "Time 561.0 updated\n",
      "Time 561.5 updated\n",
      "Time 562.0 updated\n",
      "Time 562.5 updated\n",
      "Time 563.0 updated\n",
      "Time 563.5 updated\n",
      "Time 564.0 updated\n",
      "Time 564.5 updated\n",
      "Time 565.0 updated\n",
      "Time 565.5 updated\n",
      "Time 566.0 updated\n",
      "Time 566.5 updated\n",
      "Time 567.0 updated\n",
      "Time 567.5 updated\n",
      "Time 568.0 updated\n",
      "Time 568.5 updated\n",
      "Time 569.0 updated\n",
      "Time 569.5 updated\n",
      "Time 570.0 updated\n",
      "Time 570.5 updated\n",
      "Time 571.0 updated\n",
      "Time 571.5 updated\n",
      "Time 572.0 updated\n",
      "Time 572.5 updated\n",
      "Time 573.0 updated\n",
      "Time 573.5 updated\n",
      "Time 574.0 updated\n",
      "Time 574.5 updated\n",
      "Time 575.0 updated\n",
      "Time 575.5 updated\n",
      "Time 576.0 updated\n",
      "Time 576.5 updated\n",
      "Time 577.0 updated\n",
      "Time 577.5 updated\n",
      "Time 578.0 updated\n",
      "Time 578.5 updated\n",
      "Time 579.0 updated\n",
      "Time 579.5 updated\n",
      "Time 580.0 updated\n",
      "Time 580.5 updated\n",
      "Time 581.0 updated\n",
      "Time 581.5 updated\n",
      "Time 582.0 updated\n",
      "Time 582.5 updated\n",
      "Time 583.0 updated\n",
      "Time 583.5 updated\n",
      "Time 584.0 updated\n",
      "Time 584.5 updated\n",
      "Time 585.0 updated\n",
      "Time 585.5 updated\n",
      "Time 586.0 updated\n",
      "Time 586.5 updated\n",
      "Time 587.0 updated\n",
      "Time 587.5 updated\n",
      "Time 588.0 updated\n",
      "Time 588.5 updated\n",
      "Time 589.0 updated\n",
      "Time 589.5 updated\n",
      "Time 590.0 updated\n",
      "Time 590.5 updated\n",
      "Time 591.0 updated\n",
      "Time 591.5 updated\n",
      "Time 592.0 updated\n",
      "Time 592.5 updated\n",
      "Time 593.0 updated\n",
      "Time 593.5 updated\n",
      "Time 594.0 updated\n",
      "Time 594.5 updated\n",
      "Time 595.0 updated\n",
      "Time 595.5 updated\n",
      "Time 596.0 updated\n",
      "Time 596.5 updated\n",
      "Time 597.0 updated\n",
      "Time 597.5 updated\n",
      "Time 598.0 updated\n",
      "Time 598.5 updated\n",
      "Time 599.0 updated\n",
      "Time 599.5 updated\n",
      "Time 600.0 updated\n",
      "Time 600.5 updated\n",
      "Time 601.0 updated\n",
      "Time 601.5 updated\n",
      "Time 602.0 updated\n",
      "Time 602.5 updated\n",
      "Time 603.0 updated\n",
      "Time 603.5 updated\n",
      "Time 604.0 updated\n",
      "Time 604.5 updated\n",
      "Time 605.0 updated\n",
      "Time 605.5 updated\n",
      "Time 606.0 updated\n",
      "Time 606.5 updated\n",
      "Time 607.0 updated\n",
      "Time 607.5 updated\n",
      "Time 608.0 updated\n",
      "Time 608.5 updated\n",
      "Time 609.0 updated\n",
      "Time 609.5 updated\n",
      "Time 610.0 updated\n",
      "Time 610.5 updated\n",
      "Time 611.0 updated\n",
      "Time 611.5 updated\n",
      "Time 612.0 updated\n",
      "Time 612.5 updated\n",
      "Time 613.0 updated\n",
      "Time 613.5 updated\n",
      "Time 614.0 updated\n",
      "Time 614.5 updated\n",
      "Time 615.0 updated\n",
      "Time 615.5 updated\n",
      "Time 616.0 updated\n",
      "Time 616.5 updated\n",
      "Time 617.0 updated\n",
      "Time 617.5 updated\n",
      "Time 618.0 updated\n",
      "Time 618.5 updated\n",
      "Time 619.0 updated\n",
      "Time 619.5 updated\n",
      "Time 620.0 updated\n",
      "Time 620.5 updated\n",
      "Time 621.0 updated\n",
      "Time 621.5 updated\n",
      "Time 622.0 updated\n",
      "Time 622.5 updated\n",
      "Time 623.0 updated\n",
      "Time 623.5 updated\n",
      "Time 624.0 updated\n",
      "Time 624.5 updated\n",
      "Time 625.0 updated\n",
      "Time 625.5 updated\n",
      "Time 626.0 updated\n",
      "Time 626.5 updated\n",
      "Time 627.0 updated\n",
      "Time 627.5 updated\n",
      "Time 628.0 updated\n",
      "Time 628.5 updated\n",
      "Time 629.0 updated\n",
      "Time 629.5 updated\n",
      "Time 630.0 updated\n",
      "Time 630.5 updated\n",
      "Time 631.0 updated\n",
      "Time 631.5 updated\n",
      "Time 632.0 updated\n",
      "Time 632.5 updated\n",
      "Time 633.0 updated\n",
      "Time 633.5 updated\n",
      "Time 634.0 updated\n",
      "Time 634.5 updated\n",
      "Time 635.0 updated\n",
      "Time 635.5 updated\n",
      "Time 636.0 updated\n",
      "Time 636.5 updated\n",
      "Time 637.0 updated\n",
      "Time 637.5 updated\n",
      "Time 638.0 updated\n",
      "Time 638.5 updated\n",
      "Time 639.0 updated\n",
      "Time 639.5 updated\n",
      "Time 640.0 updated\n",
      "Time 640.5 updated\n",
      "Time 641.0 updated\n",
      "Time 641.5 updated\n",
      "Time 642.0 updated\n",
      "Time 642.5 updated\n",
      "Time 643.0 updated\n",
      "Time 643.5 updated\n",
      "Time 644.0 updated\n",
      "Time 644.5 updated\n",
      "Time 645.0 updated\n",
      "Time 645.5 updated\n",
      "Time 646.0 updated\n",
      "Time 646.5 updated\n",
      "Time 647.0 updated\n",
      "Time 647.5 updated\n",
      "Time 648.0 updated\n",
      "Time 648.5 updated\n",
      "Time 649.0 updated\n",
      "Time 649.5 updated\n",
      "Time 650.0 updated\n",
      "Time 650.5 updated\n",
      "Time 651.0 updated\n",
      "Time 651.5 updated\n",
      "Time 652.0 updated\n",
      "Time 652.5 updated\n",
      "Time 653.0 updated\n",
      "Time 653.5 updated\n",
      "Time 654.0 updated\n",
      "Time 654.5 updated\n",
      "Time 655.0 updated\n",
      "Time 655.5 updated\n",
      "Time 656.0 updated\n",
      "Time 656.5 updated\n",
      "Time 657.0 updated\n",
      "Time 657.5 updated\n",
      "Time 658.0 updated\n",
      "Time 658.5 updated\n",
      "Time 659.0 updated\n",
      "Time 659.5 updated\n",
      "Time 660.0 updated\n",
      "Time 660.5 updated\n",
      "Time 661.0 updated\n",
      "Time 661.5 updated\n",
      "Time 662.0 updated\n",
      "Time 662.5 updated\n",
      "Time 663.0 updated\n",
      "Time 663.5 updated\n",
      "Time 664.0 updated\n",
      "Time 664.5 updated\n",
      "Time 665.0 updated\n",
      "Time 665.5 updated\n",
      "Time 666.0 updated\n",
      "Time 666.5 updated\n",
      "Time 667.0 updated\n",
      "Time 667.5 updated\n",
      "Time 668.0 updated\n",
      "Time 668.5 updated\n",
      "Time 669.0 updated\n",
      "Time 669.5 updated\n",
      "Time 670.0 updated\n",
      "Time 670.5 updated\n",
      "Time 671.0 updated\n",
      "Time 671.5 updated\n",
      "Time 672.0 updated\n",
      "Time 672.5 updated\n",
      "Time 673.0 updated\n",
      "Time 673.5 updated\n",
      "Time 674.0 updated\n",
      "Time 674.5 updated\n",
      "Time 675.0 updated\n",
      "Time 675.5 updated\n",
      "Time 676.0 updated\n",
      "Time 676.5 updated\n",
      "Time 677.0 updated\n",
      "Time 677.5 updated\n",
      "Time 678.0 updated\n",
      "Time 678.5 updated\n",
      "Time 679.0 updated\n",
      "Time 679.5 updated\n",
      "Time 680.0 updated\n",
      "Time 680.5 updated\n",
      "Time 681.0 updated\n",
      "Time 681.5 updated\n",
      "Time 682.0 updated\n",
      "Time 682.5 updated\n",
      "Time 683.0 updated\n",
      "Time 683.5 updated\n",
      "Time 684.0 updated\n",
      "Time 684.5 updated\n",
      "Time 685.0 updated\n",
      "Time 685.5 updated\n",
      "Time 686.0 updated\n",
      "Time 686.5 updated\n",
      "Time 687.0 updated\n",
      "Time 687.5 updated\n",
      "Time 688.0 updated\n",
      "Time 688.5 updated\n",
      "Time 689.0 updated\n",
      "Time 689.5 updated\n",
      "Time 690.0 updated\n",
      "Time 690.5 updated\n",
      "Time 691.0 updated\n",
      "Time 691.5 updated\n",
      "Time 692.0 updated\n",
      "Time 692.5 updated\n",
      "Time 693.0 updated\n",
      "Time 693.5 updated\n",
      "Time 694.0 updated\n",
      "Time 694.5 updated\n",
      "Time 695.0 updated\n",
      "Time 695.5 updated\n",
      "Time 696.0 updated\n",
      "Time 696.5 updated\n",
      "Time 697.0 updated\n",
      "Time 697.5 updated\n",
      "Time 698.0 updated\n",
      "Time 698.5 updated\n",
      "Time 699.0 updated\n",
      "Time 699.5 updated\n",
      "Time 700.0 updated\n",
      "Time 700.5 updated\n",
      "Time 701.0 updated\n",
      "Time 701.5 updated\n",
      "Time 702.0 updated\n",
      "Time 702.5 updated\n",
      "Time 703.0 updated\n",
      "Time 703.5 updated\n",
      "Time 704.0 updated\n",
      "Time 704.5 updated\n",
      "Time 705.0 updated\n",
      "Time 705.5 updated\n",
      "Time 706.0 updated\n",
      "Time 706.5 updated\n",
      "Time 707.0 updated\n",
      "Time 707.5 updated\n",
      "Time 708.0 updated\n",
      "Time 708.5 updated\n",
      "Time 709.0 updated\n",
      "Time 709.5 updated\n",
      "Time 710.0 updated\n",
      "Time 710.5 updated\n",
      "Time 711.0 updated\n",
      "Time 711.5 updated\n",
      "Time 712.0 updated\n",
      "Time 712.5 updated\n",
      "Time 713.0 updated\n",
      "Time 713.5 updated\n",
      "Time 714.0 updated\n",
      "Time 714.5 updated\n",
      "Time 715.0 updated\n",
      "Time 715.5 updated\n",
      "Time 716.0 updated\n",
      "Time 716.5 updated\n",
      "Time 717.0 updated\n",
      "Time 717.5 updated\n",
      "Time 718.0 updated\n",
      "Time 718.5 updated\n",
      "Time 719.0 updated\n",
      "Time 719.5 updated\n",
      "Time 720.0 updated\n",
      "Time 720.5 updated\n",
      "Time 721.0 updated\n",
      "Time 721.5 updated\n",
      "Time 722.0 updated\n",
      "Time 722.5 updated\n",
      "Time 723.0 updated\n",
      "Time 723.5 updated\n",
      "Time 724.0 updated\n",
      "Time 724.5 updated\n",
      "Time 725.0 updated\n",
      "Time 725.5 updated\n",
      "Time 726.0 updated\n",
      "Time 726.5 updated\n",
      "Time 727.0 updated\n",
      "Time 727.5 updated\n",
      "Time 728.0 updated\n",
      "Time 728.5 updated\n",
      "Time 729.0 updated\n",
      "Time 729.5 updated\n",
      "Time 730.0 updated\n",
      "Time 730.5 updated\n",
      "Time 731.0 updated\n",
      "Time 731.5 updated\n",
      "Time 732.0 updated\n",
      "Time 732.5 updated\n",
      "Time 733.0 updated\n",
      "Time 733.5 updated\n",
      "Time 734.0 updated\n",
      "Time 734.5 updated\n",
      "Time 735.0 updated\n",
      "Time 735.5 updated\n",
      "Time 736.0 updated\n",
      "Time 736.5 updated\n",
      "Time 737.0 updated\n",
      "Time 737.5 updated\n",
      "Time 738.0 updated\n",
      "Time 738.5 updated\n",
      "Time 739.0 updated\n",
      "Time 739.5 updated\n",
      "Time 740.0 updated\n",
      "Time 740.5 updated\n",
      "Time 741.0 updated\n",
      "Time 741.5 updated\n",
      "Time 742.0 updated\n",
      "Time 742.5 updated\n",
      "Time 743.0 updated\n",
      "Time 743.5 updated\n",
      "Time 744.0 updated\n",
      "Time 744.5 updated\n",
      "Time 745.0 updated\n",
      "Time 745.5 updated\n",
      "Time 746.0 updated\n",
      "Time 746.5 updated\n",
      "Time 747.0 updated\n",
      "Time 747.5 updated\n",
      "Time 748.0 updated\n",
      "Time 748.5 updated\n",
      "Time 749.0 updated\n",
      "Time 749.5 updated\n",
      "Time 750.0 updated\n",
      "Time 750.5 updated\n",
      "Time 751.0 updated\n",
      "Time 751.5 updated\n",
      "Time 752.0 updated\n",
      "Time 752.5 updated\n",
      "Time 753.0 updated\n",
      "Time 753.5 updated\n",
      "Time 754.0 updated\n",
      "Time 754.5 updated\n",
      "Time 755.0 updated\n",
      "Time 755.5 updated\n",
      "Time 756.0 updated\n",
      "Time 756.5 updated\n",
      "Time 757.0 updated\n",
      "Time 757.5 updated\n",
      "Time 758.0 updated\n",
      "Time 758.5 updated\n",
      "Time 759.0 updated\n",
      "Time 759.5 updated\n",
      "Time 760.0 updated\n",
      "Time 760.5 updated\n",
      "Time 761.0 updated\n",
      "Time 761.5 updated\n",
      "Time 762.0 updated\n",
      "Time 762.5 updated\n",
      "Time 763.0 updated\n",
      "Time 763.5 updated\n",
      "Time 764.0 updated\n",
      "Time 764.5 updated\n",
      "Time 765.0 updated\n",
      "Time 765.5 updated\n",
      "Time 766.0 updated\n",
      "Time 766.5 updated\n",
      "Time 767.0 updated\n",
      "Time 767.5 updated\n",
      "Time 768.0 updated\n",
      "Time 768.5 updated\n",
      "Time 769.0 updated\n",
      "Time 769.5 updated\n",
      "Time 770.0 updated\n",
      "Time 770.5 updated\n",
      "Time 771.0 updated\n",
      "Time 771.5 updated\n",
      "Time 772.0 updated\n",
      "Time 772.5 updated\n",
      "Time 773.0 updated\n",
      "Time 773.5 updated\n",
      "Time 774.0 updated\n",
      "Time 774.5 updated\n",
      "Time 775.0 updated\n",
      "Time 775.5 updated\n",
      "Time 776.0 updated\n",
      "Time 776.5 updated\n",
      "Time 777.0 updated\n",
      "Time 777.5 updated\n",
      "Time 778.0 updated\n",
      "Time 778.5 updated\n",
      "Time 779.0 updated\n",
      "Time 779.5 updated\n",
      "Time 780.0 updated\n",
      "Time 780.5 updated\n",
      "Time 781.0 updated\n",
      "Time 781.5 updated\n",
      "Time 782.0 updated\n",
      "Time 782.5 updated\n",
      "Time 783.0 updated\n",
      "Time 783.5 updated\n",
      "Time 784.0 updated\n",
      "Time 784.5 updated\n",
      "Time 785.0 updated\n",
      "Time 785.5 updated\n",
      "Time 786.0 updated\n",
      "Time 786.5 updated\n",
      "Time 787.0 updated\n",
      "Time 787.5 updated\n",
      "Time 788.0 updated\n",
      "Time 788.5 updated\n",
      "Time 789.0 updated\n",
      "Time 789.5 updated\n",
      "Time 790.0 updated\n",
      "Time 790.5 updated\n",
      "Time 791.0 updated\n",
      "Time 791.5 updated\n",
      "Time 792.0 updated\n",
      "Time 792.5 updated\n",
      "Time 793.0 updated\n",
      "Time 793.5 updated\n",
      "Time 794.0 updated\n",
      "Time 794.5 updated\n",
      "Time 795.0 updated\n",
      "Time 795.5 updated\n",
      "Time 796.0 updated\n",
      "Time 796.5 updated\n",
      "Time 797.0 updated\n",
      "Time 797.5 updated\n",
      "Time 798.0 updated\n",
      "Time 798.5 updated\n",
      "Time 799.0 updated\n",
      "Time 799.5 updated\n",
      "Time 800.0 updated\n",
      "Time 800.5 updated\n",
      "Time 801.0 updated\n",
      "Time 801.5 updated\n",
      "Time 802.0 updated\n",
      "Time 802.5 updated\n",
      "Time 803.0 updated\n",
      "Time 803.5 updated\n",
      "Time 804.0 updated\n",
      "Time 804.5 updated\n",
      "Time 805.0 updated\n",
      "Time 805.5 updated\n",
      "Time 806.0 updated\n",
      "Time 806.5 updated\n",
      "Time 807.0 updated\n",
      "Time 807.5 updated\n",
      "Time 808.0 updated\n",
      "Time 808.5 updated\n",
      "Time 809.0 updated\n",
      "Time 809.5 updated\n",
      "Time 810.0 updated\n",
      "Time 810.5 updated\n",
      "Time 811.0 updated\n",
      "Time 811.5 updated\n",
      "Time 812.0 updated\n",
      "Time 812.5 updated\n",
      "Time 813.0 updated\n",
      "Time 813.5 updated\n",
      "Time 814.0 updated\n",
      "Time 814.5 updated\n",
      "Time 815.0 updated\n",
      "Time 815.5 updated\n",
      "Time 816.0 updated\n",
      "Time 816.5 updated\n",
      "Time 817.0 updated\n",
      "Time 817.5 updated\n",
      "Time 818.0 updated\n",
      "Time 818.5 updated\n",
      "Time 819.0 updated\n",
      "Time 819.5 updated\n",
      "Time 820.0 updated\n",
      "Time 820.5 updated\n",
      "Time 821.0 updated\n",
      "Time 821.5 updated\n",
      "Time 822.0 updated\n",
      "Time 822.5 updated\n",
      "Time 823.0 updated\n",
      "Time 823.5 updated\n",
      "Time 824.0 updated\n",
      "Time 824.5 updated\n",
      "Time 825.0 updated\n",
      "Time 825.5 updated\n",
      "Time 826.0 updated\n",
      "Time 826.5 updated\n",
      "Time 827.0 updated\n",
      "Time 827.5 updated\n",
      "Time 828.0 updated\n",
      "Time 828.5 updated\n",
      "Time 829.0 updated\n",
      "Time 829.5 updated\n",
      "Time 830.0 updated\n",
      "Time 830.5 updated\n",
      "Time 831.0 updated\n",
      "Time 831.5 updated\n",
      "Time 832.0 updated\n",
      "Time 832.5 updated\n",
      "Time 833.0 updated\n",
      "Time 833.5 updated\n",
      "Time 834.0 updated\n",
      "Time 834.5 updated\n",
      "Time 835.0 updated\n",
      "Time 835.5 updated\n",
      "Time 836.0 updated\n",
      "Time 836.5 updated\n",
      "Time 837.0 updated\n",
      "Time 837.5 updated\n",
      "Time 838.0 updated\n",
      "Time 838.5 updated\n",
      "Time 839.0 updated\n",
      "Time 839.5 updated\n",
      "Time 840.0 updated\n",
      "Time 840.5 updated\n",
      "Time 841.0 updated\n",
      "Time 841.5 updated\n",
      "Time 842.0 updated\n",
      "Time 842.5 updated\n",
      "Time 843.0 updated\n",
      "Time 843.5 updated\n",
      "Time 844.0 updated\n",
      "Time 844.5 updated\n",
      "Time 845.0 updated\n",
      "Time 845.5 updated\n",
      "Time 846.0 updated\n",
      "Time 846.5 updated\n",
      "Time 847.0 updated\n",
      "Time 847.5 updated\n",
      "Time 848.0 updated\n",
      "Time 848.5 updated\n",
      "Time 849.0 updated\n",
      "Time 849.5 updated\n",
      "Time 850.0 updated\n",
      "Time 850.5 updated\n",
      "Time 851.0 updated\n",
      "Time 851.5 updated\n",
      "Time 852.0 updated\n",
      "Time 852.5 updated\n",
      "Time 853.0 updated\n",
      "Time 853.5 updated\n",
      "Time 854.0 updated\n",
      "Time 854.5 updated\n",
      "Time 855.0 updated\n",
      "Time 855.5 updated\n",
      "Time 856.0 updated\n",
      "Time 856.5 updated\n",
      "Time 857.0 updated\n",
      "Time 857.5 updated\n",
      "Time 858.0 updated\n",
      "Time 858.5 updated\n",
      "Time 859.0 updated\n",
      "Time 859.5 updated\n",
      "Time 860.0 updated\n",
      "Time 860.5 updated\n",
      "Time 861.0 updated\n",
      "Time 861.5 updated\n",
      "Time 862.0 updated\n",
      "Time 862.5 updated\n",
      "Time 863.0 updated\n",
      "Time 863.5 updated\n",
      "Time 864.0 updated\n",
      "Time 864.5 updated\n",
      "Time 865.0 updated\n",
      "Time 865.5 updated\n",
      "Time 866.0 updated\n",
      "Time 866.5 updated\n",
      "Time 867.0 updated\n",
      "Time 867.5 updated\n",
      "Time 868.0 updated\n",
      "Time 868.5 updated\n",
      "Time 869.0 updated\n",
      "Time 869.5 updated\n",
      "Time 870.0 updated\n",
      "Time 870.5 updated\n",
      "Time 871.0 updated\n",
      "Time 871.5 updated\n",
      "Time 872.0 updated\n",
      "Time 872.5 updated\n",
      "Time 873.0 updated\n",
      "Time 873.5 updated\n",
      "Time 874.0 updated\n",
      "Time 874.5 updated\n",
      "Time 875.0 updated\n",
      "Time 875.5 updated\n",
      "Time 876.0 updated\n",
      "Time 876.5 updated\n",
      "Time 877.0 updated\n",
      "Time 877.5 updated\n",
      "Time 878.0 updated\n",
      "Time 878.5 updated\n",
      "Time 879.0 updated\n",
      "Time 879.5 updated\n",
      "Time 880.0 updated\n",
      "Time 880.5 updated\n",
      "Time 881.0 updated\n",
      "Time 881.5 updated\n",
      "Time 882.0 updated\n",
      "Time 882.5 updated\n",
      "Time 883.0 updated\n",
      "Time 883.5 updated\n",
      "Time 884.0 updated\n",
      "Time 884.5 updated\n",
      "Time 885.0 updated\n",
      "Time 885.5 updated\n",
      "Time 886.0 updated\n",
      "Time 886.5 updated\n",
      "Time 887.0 updated\n",
      "Time 887.5 updated\n",
      "Time 888.0 updated\n",
      "Time 888.5 updated\n",
      "Time 889.0 updated\n",
      "Time 889.5 updated\n",
      "Time 890.0 updated\n",
      "Time 890.5 updated\n",
      "Time 891.0 updated\n",
      "Time 891.5 updated\n",
      "Time 892.0 updated\n",
      "Time 892.5 updated\n",
      "Time 893.0 updated\n",
      "Time 893.5 updated\n",
      "Time 894.0 updated\n",
      "Time 894.5 updated\n",
      "Time 895.0 updated\n",
      "Time 895.5 updated\n",
      "Time 896.0 updated\n",
      "Time 896.5 updated\n",
      "Time 897.0 updated\n",
      "Time 897.5 updated\n",
      "Time 898.0 updated\n",
      "Time 898.5 updated\n",
      "Time 899.0 updated\n",
      "Time 899.5 updated\n",
      "Time 900.0 updated\n",
      "Time 900.5 updated\n",
      "Time 901.0 updated\n",
      "Time 901.5 updated\n",
      "Time 902.0 updated\n",
      "Time 902.5 updated\n",
      "Time 903.0 updated\n",
      "Time 903.5 updated\n",
      "Time 904.0 updated\n",
      "Time 904.5 updated\n",
      "Time 905.0 updated\n",
      "Time 905.5 updated\n",
      "Time 906.0 updated\n",
      "Time 906.5 updated\n",
      "Time 907.0 updated\n",
      "Time 907.5 updated\n",
      "Time 908.0 updated\n",
      "Time 908.5 updated\n",
      "Time 909.0 updated\n",
      "Time 909.5 updated\n",
      "Time 910.0 updated\n",
      "Time 910.5 updated\n",
      "Time 911.0 updated\n",
      "Time 911.5 updated\n",
      "Time 912.0 updated\n",
      "Time 912.5 updated\n",
      "Time 913.0 updated\n",
      "Time 913.5 updated\n",
      "Time 914.0 updated\n",
      "Time 914.5 updated\n",
      "Time 915.0 updated\n",
      "Time 915.5 updated\n",
      "Time 916.0 updated\n",
      "Time 916.5 updated\n",
      "Time 917.0 updated\n",
      "Time 917.5 updated\n",
      "Time 918.0 updated\n",
      "Time 918.5 updated\n",
      "Time 919.0 updated\n",
      "Time 919.5 updated\n",
      "Time 920.0 updated\n",
      "Time 920.5 updated\n",
      "Time 921.0 updated\n",
      "Time 921.5 updated\n",
      "Time 922.0 updated\n",
      "Time 922.5 updated\n",
      "Time 923.0 updated\n",
      "Time 923.5 updated\n",
      "Time 924.0 updated\n",
      "Time 924.5 updated\n",
      "Time 925.0 updated\n",
      "Time 925.5 updated\n",
      "Time 926.0 updated\n",
      "Time 926.5 updated\n",
      "Time 927.0 updated\n",
      "Time 927.5 updated\n",
      "Time 928.0 updated\n",
      "Time 928.5 updated\n",
      "Time 929.0 updated\n",
      "Time 929.5 updated\n",
      "Time 930.0 updated\n",
      "Time 930.5 updated\n",
      "Time 931.0 updated\n",
      "Time 931.5 updated\n",
      "Time 932.0 updated\n",
      "Time 932.5 updated\n",
      "Time 933.0 updated\n",
      "Time 933.5 updated\n",
      "Time 934.0 updated\n",
      "Time 934.5 updated\n",
      "Time 935.0 updated\n",
      "Time 935.5 updated\n",
      "Time 936.0 updated\n",
      "Time 936.5 updated\n",
      "Time 937.0 updated\n",
      "Time 937.5 updated\n",
      "Time 938.0 updated\n",
      "Time 938.5 updated\n",
      "Time 939.0 updated\n",
      "Time 939.5 updated\n",
      "Time 940.0 updated\n",
      "Time 940.5 updated\n",
      "Time 941.0 updated\n",
      "Time 941.5 updated\n",
      "Time 942.0 updated\n",
      "Time 942.5 updated\n",
      "Time 943.0 updated\n",
      "Time 943.5 updated\n",
      "Time 944.0 updated\n",
      "Time 944.5 updated\n",
      "Time 945.0 updated\n",
      "Time 945.5 updated\n",
      "Time 946.0 updated\n",
      "Time 946.5 updated\n",
      "Time 947.0 updated\n",
      "Time 947.5 updated\n",
      "Time 948.0 updated\n",
      "Time 948.5 updated\n",
      "Time 949.0 updated\n",
      "Time 949.5 updated\n",
      "Time 950.0 updated\n",
      "Time 950.5 updated\n",
      "Time 951.0 updated\n",
      "Time 951.5 updated\n",
      "Time 952.0 updated\n",
      "Time 952.5 updated\n",
      "Time 953.0 updated\n",
      "Time 953.5 updated\n",
      "Time 954.0 updated\n",
      "Time 954.5 updated\n",
      "Time 955.0 updated\n",
      "Time 955.5 updated\n",
      "Time 956.0 updated\n",
      "Time 956.5 updated\n",
      "Time 957.0 updated\n",
      "Time 957.5 updated\n",
      "Time 958.0 updated\n",
      "Time 958.5 updated\n",
      "Time 959.0 updated\n",
      "Time 959.5 updated\n",
      "Time 960.0 updated\n",
      "Time 960.5 updated\n",
      "Time 961.0 updated\n",
      "Time 961.5 updated\n",
      "Time 962.0 updated\n",
      "Time 962.5 updated\n",
      "Time 963.0 updated\n",
      "Time 963.5 updated\n",
      "Time 964.0 updated\n",
      "Time 964.5 updated\n",
      "Time 965.0 updated\n",
      "Time 965.5 updated\n",
      "Time 966.0 updated\n",
      "Time 966.5 updated\n",
      "Time 967.0 updated\n",
      "Time 967.5 updated\n",
      "Time 968.0 updated\n",
      "Time 968.5 updated\n",
      "Time 969.0 updated\n",
      "Time 969.5 updated\n",
      "Time 970.0 updated\n",
      "Time 970.5 updated\n",
      "Time 971.0 updated\n",
      "Time 971.5 updated\n",
      "Time 972.0 updated\n",
      "Time 972.5 updated\n",
      "Time 973.0 updated\n",
      "Time 973.5 updated\n",
      "Time 974.0 updated\n",
      "Time 974.5 updated\n",
      "Time 975.0 updated\n",
      "Time 975.5 updated\n",
      "Time 976.0 updated\n",
      "Time 976.5 updated\n",
      "Time 977.0 updated\n",
      "Time 977.5 updated\n",
      "Time 978.0 updated\n",
      "Time 978.5 updated\n",
      "Time 979.0 updated\n",
      "Time 979.5 updated\n",
      "Time 980.0 updated\n",
      "Time 980.5 updated\n",
      "Time 981.0 updated\n",
      "Time 981.5 updated\n",
      "Time 982.0 updated\n",
      "Time 982.5 updated\n",
      "Time 983.0 updated\n",
      "Time 983.5 updated\n",
      "Time 984.0 updated\n",
      "Time 984.5 updated\n",
      "Time 985.0 updated\n",
      "Time 985.5 updated\n",
      "Time 986.0 updated\n",
      "Time 986.5 updated\n",
      "Time 987.0 updated\n",
      "Time 987.5 updated\n",
      "Time 988.0 updated\n",
      "Time 988.5 updated\n",
      "Time 989.0 updated\n",
      "Time 989.5 updated\n",
      "Time 990.0 updated\n",
      "Time 990.5 updated\n",
      "Time 991.0 updated\n",
      "Time 991.5 updated\n",
      "Time 992.0 updated\n",
      "Time 992.5 updated\n",
      "Time 993.0 updated\n",
      "Time 993.5 updated\n",
      "Time 994.0 updated\n",
      "Time 994.5 updated\n",
      "Time 995.0 updated\n",
      "Time 995.5 updated\n",
      "Time 996.0 updated\n",
      "Time 996.5 updated\n",
      "Time 997.0 updated\n",
      "Time 997.5 updated\n",
      "Time 998.0 updated\n",
      "Time 998.5 updated\n",
      "Time 999.0 updated\n",
      "Time 999.5 updated\n",
      "Time 1000.0 updated\n",
      "Time 1000.5 updated\n",
      "Time 1001.0 updated\n",
      "Time 1001.5 updated\n",
      "Time 1002.0 updated\n",
      "Time 1002.5 updated\n",
      "Time 1003.0 updated\n",
      "Time 1003.5 updated\n",
      "Time 1004.0 updated\n",
      "Time 1004.5 updated\n",
      "Time 1005.0 updated\n",
      "Time 1005.5 updated\n",
      "Time 1006.0 updated\n",
      "Time 1006.5 updated\n",
      "Time 1007.0 updated\n",
      "Time 1007.5 updated\n",
      "Time 1008.0 updated\n",
      "Time 1008.5 updated\n",
      "Time 1009.0 updated\n",
      "Time 1009.5 updated\n",
      "Time 1010.0 updated\n",
      "Time 1010.5 updated\n",
      "Time 1011.0 updated\n",
      "Time 1011.5 updated\n",
      "Time 1012.0 updated\n",
      "Time 1012.5 updated\n",
      "Time 1013.0 updated\n",
      "Time 1013.5 updated\n",
      "Time 1014.0 updated\n",
      "Time 1014.5 updated\n",
      "Time 1015.0 updated\n",
      "Time 1015.5 updated\n",
      "Time 1016.0 updated\n",
      "Time 1016.5 updated\n",
      "Time 1017.0 updated\n",
      "Time 1017.5 updated\n",
      "Time 1018.0 updated\n",
      "Time 1018.5 updated\n",
      "Time 1019.0 updated\n",
      "Time 1019.5 updated\n",
      "Time 1020.0 updated\n",
      "Time 1020.5 updated\n",
      "Time 1021.0 updated\n",
      "Time 1021.5 updated\n",
      "Time 1022.0 updated\n",
      "Time 1022.5 updated\n",
      "Time 1023.0 updated\n",
      "Time 1023.5 updated\n",
      "Time 1024.0 updated\n",
      "Time 1024.5 updated\n",
      "Time 1025.0 updated\n",
      "Time 1025.5 updated\n",
      "Time 1026.0 updated\n",
      "Time 1026.5 updated\n",
      "Time 1027.0 updated\n",
      "Time 1027.5 updated\n",
      "Time 1028.0 updated\n",
      "Time 1028.5 updated\n",
      "Time 1029.0 updated\n",
      "Time 1029.5 updated\n",
      "Time 1030.0 updated\n",
      "Time 1030.5 updated\n",
      "Time 1031.0 updated\n",
      "Time 1031.5 updated\n",
      "Time 1032.0 updated\n",
      "Time 1032.5 updated\n",
      "Time 1033.0 updated\n",
      "Time 1033.5 updated\n",
      "Time 1034.0 updated\n",
      "Time 1034.5 updated\n",
      "Time 1035.0 updated\n",
      "Time 1035.5 updated\n",
      "Time 1036.0 updated\n",
      "Time 1036.5 updated\n",
      "Time 1037.0 updated\n",
      "Time 1037.5 updated\n",
      "Time 1038.0 updated\n",
      "Time 1038.5 updated\n",
      "Time 1039.0 updated\n",
      "Time 1039.5 updated\n",
      "Time 1040.0 updated\n",
      "Time 1040.5 updated\n",
      "Time 1041.0 updated\n",
      "Time 1041.5 updated\n",
      "Time 1042.0 updated\n",
      "Time 1042.5 updated\n",
      "Time 1043.0 updated\n",
      "Time 1043.5 updated\n",
      "Time 1044.0 updated\n",
      "Time 1044.5 updated\n",
      "Time 1045.0 updated\n",
      "Time 1045.5 updated\n",
      "Time 1046.0 updated\n",
      "Time 1046.5 updated\n",
      "Time 1047.0 updated\n",
      "Time 1047.5 updated\n",
      "Time 1048.0 updated\n",
      "Time 1048.5 updated\n",
      "Time 1049.0 updated\n",
      "Time 1049.5 updated\n",
      "Time 1050.0 updated\n",
      "Time 1050.5 updated\n",
      "Time 1051.0 updated\n",
      "Time 1051.5 updated\n",
      "Time 1052.0 updated\n",
      "Time 1052.5 updated\n",
      "Time 1053.0 updated\n",
      "Time 1053.5 updated\n",
      "Time 1054.0 updated\n",
      "Time 1054.5 updated\n",
      "Time 1055.0 updated\n",
      "Time 1055.5 updated\n",
      "Time 1056.0 updated\n",
      "Time 1056.5 updated\n",
      "Time 1057.0 updated\n",
      "Time 1057.5 updated\n",
      "Time 1058.0 updated\n",
      "Time 1058.5 updated\n",
      "Time 1059.0 updated\n",
      "Time 1059.5 updated\n",
      "Time 1060.0 updated\n",
      "Time 1060.5 updated\n",
      "Time 1061.0 updated\n",
      "Time 1061.5 updated\n",
      "Time 1062.0 updated\n",
      "Time 1062.5 updated\n",
      "Time 1063.0 updated\n",
      "Time 1063.5 updated\n",
      "Time 1064.0 updated\n",
      "Time 1064.5 updated\n",
      "Time 1065.0 updated\n",
      "Time 1065.5 updated\n",
      "Time 1066.0 updated\n",
      "Time 1066.5 updated\n",
      "Time 1067.0 updated\n",
      "Time 1067.5 updated\n",
      "Time 1068.0 updated\n",
      "Time 1068.5 updated\n",
      "Time 1069.0 updated\n",
      "Time 1069.5 updated\n",
      "Time 1070.0 updated\n",
      "Time 1070.5 updated\n",
      "Time 1071.0 updated\n",
      "Time 1071.5 updated\n",
      "Time 1072.0 updated\n",
      "Time 1072.5 updated\n",
      "Time 1073.0 updated\n",
      "Time 1073.5 updated\n",
      "Time 1074.0 updated\n",
      "Time 1074.5 updated\n",
      "Time 1075.0 updated\n",
      "Time 1075.5 updated\n",
      "Time 1076.0 updated\n",
      "Time 1076.5 updated\n",
      "Time 1077.0 updated\n",
      "Time 1077.5 updated\n",
      "Time 1078.0 updated\n",
      "Time 1078.5 updated\n",
      "Time 1079.0 updated\n",
      "Time 1079.5 updated\n",
      "Time 1080.0 updated\n",
      "Time 1080.5 updated\n",
      "Time 1081.0 updated\n",
      "Time 1081.5 updated\n",
      "Time 1082.0 updated\n",
      "Time 1082.5 updated\n",
      "Time 1083.0 updated\n",
      "Time 1083.5 updated\n",
      "Time 1084.0 updated\n",
      "Time 1084.5 updated\n",
      "Time 1085.0 updated\n",
      "Time 1085.5 updated\n",
      "Time 1086.0 updated\n",
      "Time 1086.5 updated\n",
      "Time 1087.0 updated\n",
      "Time 1087.5 updated\n",
      "Time 1088.0 updated\n",
      "Time 1088.5 updated\n",
      "Time 1089.0 updated\n",
      "Time 1089.5 updated\n",
      "Time 1090.0 updated\n",
      "Time 1090.5 updated\n",
      "Time 1091.0 updated\n",
      "Time 1091.5 updated\n",
      "Time 1092.0 updated\n",
      "Time 1092.5 updated\n",
      "Time 1093.0 updated\n",
      "Time 1093.5 updated\n",
      "Time 1094.0 updated\n",
      "Time 1094.5 updated\n",
      "Time 1095.0 updated\n",
      "Time 1095.5 updated\n",
      "Time 1096.0 updated\n",
      "Time 1096.5 updated\n",
      "Time 1097.0 updated\n",
      "Time 1097.5 updated\n",
      "Time 1098.0 updated\n",
      "Time 1098.5 updated\n",
      "Time 1099.0 updated\n",
      "Time 1099.5 updated\n",
      "Time 1100.0 updated\n",
      "Time 1100.5 updated\n",
      "Time 1101.0 updated\n",
      "Time 1101.5 updated\n",
      "Time 1102.0 updated\n",
      "Time 1102.5 updated\n",
      "Time 1103.0 updated\n",
      "Time 1103.5 updated\n",
      "Time 1104.0 updated\n",
      "Time 1104.5 updated\n",
      "Time 1105.0 updated\n",
      "Time 1105.5 updated\n",
      "Time 1106.0 updated\n",
      "Time 1106.5 updated\n",
      "Time 1107.0 updated\n",
      "Time 1107.5 updated\n",
      "Time 1108.0 updated\n",
      "Time 1108.5 updated\n",
      "Time 1109.0 updated\n",
      "Time 1109.5 updated\n",
      "Time 1110.0 updated\n",
      "Time 1110.5 updated\n",
      "Time 1111.0 updated\n",
      "Time 1111.5 updated\n",
      "Time 1112.0 updated\n",
      "Time 1112.5 updated\n",
      "Time 1113.0 updated\n",
      "Time 1113.5 updated\n",
      "Time 1114.0 updated\n",
      "Time 1114.5 updated\n",
      "Time 1115.0 updated\n",
      "Time 1115.5 updated\n",
      "Time 1116.0 updated\n",
      "Time 1116.5 updated\n",
      "Time 1117.0 updated\n",
      "Time 1117.5 updated\n",
      "Time 1118.0 updated\n",
      "Time 1118.5 updated\n",
      "Time 1119.0 updated\n",
      "Time 1119.5 updated\n",
      "Time 1120.0 updated\n",
      "Time 1120.5 updated\n",
      "Time 1121.0 updated\n",
      "Time 1121.5 updated\n",
      "Time 1122.0 updated\n",
      "Time 1122.5 updated\n",
      "Time 1123.0 updated\n",
      "Time 1123.5 updated\n",
      "Time 1124.0 updated\n",
      "Time 1124.5 updated\n",
      "Time 1125.0 updated\n",
      "Time 1125.5 updated\n",
      "Time 1126.0 updated\n",
      "Time 1126.5 updated\n",
      "Time 1127.0 updated\n",
      "Time 1127.5 updated\n",
      "Time 1128.0 updated\n",
      "Time 1128.5 updated\n",
      "Time 1129.0 updated\n",
      "Time 1129.5 updated\n",
      "Time 1130.0 updated\n",
      "Time 1130.5 updated\n",
      "Time 1131.0 updated\n",
      "Time 1131.5 updated\n",
      "Time 1132.0 updated\n",
      "Time 1132.5 updated\n",
      "Time 1133.0 updated\n",
      "Time 1133.5 updated\n",
      "Time 1134.0 updated\n",
      "Time 1134.5 updated\n",
      "Time 1135.0 updated\n",
      "Time 1135.5 updated\n",
      "Time 1136.0 updated\n",
      "Time 1136.5 updated\n",
      "Time 1137.0 updated\n",
      "Time 1137.5 updated\n",
      "Time 1138.0 updated\n",
      "Time 1138.5 updated\n",
      "Time 1139.0 updated\n",
      "Time 1139.5 updated\n",
      "Time 1140.0 updated\n",
      "Time 1140.5 updated\n",
      "Time 1141.0 updated\n",
      "Time 1141.5 updated\n",
      "Time 1142.0 updated\n",
      "Time 1142.5 updated\n",
      "Time 1143.0 updated\n",
      "Time 1143.5 updated\n",
      "Time 1144.0 updated\n",
      "Time 1144.5 updated\n",
      "Time 1145.0 updated\n",
      "Time 1145.5 updated\n",
      "Time 1146.0 updated\n",
      "Time 1146.5 updated\n",
      "Time 1147.0 updated\n",
      "Time 1147.5 updated\n",
      "Time 1148.0 updated\n",
      "Time 1148.5 updated\n",
      "Time 1149.0 updated\n",
      "Time 1149.5 updated\n",
      "Time 1150.0 updated\n",
      "Time 1150.5 updated\n",
      "Time 1151.0 updated\n",
      "Time 1151.5 updated\n",
      "Time 1152.0 updated\n",
      "Time 1152.5 updated\n",
      "Time 1153.0 updated\n",
      "Time 1153.5 updated\n",
      "Time 1154.0 updated\n",
      "Time 1154.5 updated\n",
      "Time 1155.0 updated\n",
      "Time 1155.5 updated\n",
      "Time 1156.0 updated\n",
      "Time 1156.5 updated\n",
      "Time 1157.0 updated\n",
      "Time 1157.5 updated\n",
      "Time 1158.0 updated\n",
      "Time 1158.5 updated\n",
      "Time 1159.0 updated\n",
      "Time 1159.5 updated\n",
      "Time 1160.0 updated\n",
      "Time 1160.5 updated\n",
      "Time 1161.0 updated\n",
      "Time 1161.5 updated\n",
      "Time 1162.0 updated\n",
      "Time 1162.5 updated\n",
      "Time 1163.0 updated\n",
      "Time 1163.5 updated\n",
      "Time 1164.0 updated\n",
      "Time 1164.5 updated\n",
      "Time 1165.0 updated\n",
      "Time 1165.5 updated\n",
      "Time 1166.0 updated\n",
      "Time 1166.5 updated\n",
      "Time 1167.0 updated\n",
      "Time 1167.5 updated\n",
      "Time 1168.0 updated\n",
      "Time 1168.5 updated\n",
      "Time 1169.0 updated\n",
      "Time 1169.5 updated\n",
      "Time 1170.0 updated\n",
      "Time 1170.5 updated\n",
      "Time 1171.0 updated\n",
      "Time 1171.5 updated\n",
      "Time 1172.0 updated\n",
      "Time 1172.5 updated\n",
      "Time 1173.0 updated\n",
      "Time 1173.5 updated\n",
      "Time 1174.0 updated\n",
      "Time 1174.5 updated\n",
      "Time 1175.0 updated\n",
      "Time 1175.5 updated\n",
      "Time 1176.0 updated\n",
      "Time 1176.5 updated\n",
      "Time 1177.0 updated\n",
      "Time 1177.5 updated\n",
      "Time 1178.0 updated\n",
      "Time 1178.5 updated\n",
      "Time 1179.0 updated\n",
      "Time 1179.5 updated\n",
      "Time 1180.0 updated\n",
      "Time 1180.5 updated\n",
      "Time 1181.0 updated\n",
      "Time 1181.5 updated\n",
      "Time 1182.0 updated\n",
      "Time 1182.5 updated\n",
      "Time 1183.0 updated\n",
      "Time 1183.5 updated\n",
      "Time 1184.0 updated\n",
      "Time 1184.5 updated\n",
      "Time 1185.0 updated\n",
      "Time 1185.5 updated\n",
      "Time 1186.0 updated\n",
      "Time 1186.5 updated\n",
      "Time 1187.0 updated\n",
      "Time 1187.5 updated\n",
      "Time 1188.0 updated\n",
      "Time 1188.5 updated\n",
      "Time 1189.0 updated\n",
      "Time 1189.5 updated\n",
      "Time 1190.0 updated\n",
      "Time 1190.5 updated\n",
      "Time 1191.0 updated\n",
      "Time 1191.5 updated\n",
      "Time 1192.0 updated\n",
      "Time 1192.5 updated\n",
      "Time 1193.0 updated\n",
      "Time 1193.5 updated\n",
      "Time 1194.0 updated\n",
      "Time 1194.5 updated\n",
      "Time 1195.0 updated\n",
      "Time 1195.5 updated\n",
      "Time 1196.0 updated\n",
      "Time 1196.5 updated\n",
      "Time 1197.0 updated\n",
      "Time 1197.5 updated\n",
      "Time 1198.0 updated\n",
      "Time 1198.5 updated\n",
      "Time 1199.0 updated\n",
      "Time 1199.5 updated\n",
      "Time 1200.0 updated\n",
      "Time 1200.5 updated\n",
      "Time 1201.0 updated\n",
      "Time 1201.5 updated\n",
      "Time 1202.0 updated\n",
      "Time 1202.5 updated\n",
      "Time 1203.0 updated\n",
      "Time 1203.5 updated\n",
      "Time 1204.0 updated\n",
      "Time 1204.5 updated\n",
      "Time 1205.0 updated\n",
      "Time 1205.5 updated\n",
      "Time 1206.0 updated\n",
      "Time 1206.5 updated\n",
      "Time 1207.0 updated\n",
      "Time 1207.5 updated\n",
      "Time 1208.0 updated\n",
      "Time 1208.5 updated\n",
      "Time 1209.0 updated\n",
      "Time 1209.5 updated\n",
      "Time 1210.0 updated\n",
      "Time 1210.5 updated\n",
      "Time 1211.0 updated\n",
      "Time 1211.5 updated\n",
      "Time 1212.0 updated\n",
      "Time 1212.5 updated\n",
      "Time 1213.0 updated\n",
      "Time 1213.5 updated\n",
      "Time 1214.0 updated\n",
      "Time 1214.5 updated\n",
      "Time 1215.0 updated\n",
      "Time 1215.5 updated\n",
      "Time 1216.0 updated\n",
      "Time 1216.5 updated\n",
      "Time 1217.0 updated\n",
      "Time 1217.5 updated\n",
      "Time 1218.0 updated\n",
      "Time 1218.5 updated\n",
      "Time 1219.0 updated\n",
      "Time 1219.5 updated\n",
      "Time 1220.0 updated\n",
      "Time 1220.5 updated\n",
      "Time 1221.0 updated\n",
      "Time 1221.5 updated\n",
      "Time 1222.0 updated\n",
      "Time 1222.5 updated\n",
      "Time 1223.0 updated\n",
      "Time 1223.5 updated\n",
      "Time 1224.0 updated\n",
      "Time 1224.5 updated\n",
      "Time 1225.0 updated\n",
      "Time 1225.5 updated\n",
      "Time 1226.0 updated\n",
      "Time 1226.5 updated\n",
      "Time 1227.0 updated\n",
      "Time 1227.5 updated\n",
      "Time 1228.0 updated\n",
      "Time 1228.5 updated\n",
      "Time 1229.0 updated\n",
      "Time 1229.5 updated\n",
      "Time 1230.0 updated\n",
      "Time 1230.5 updated\n",
      "Time 1231.0 updated\n",
      "Time 1231.5 updated\n",
      "Time 1232.0 updated\n",
      "Time 1232.5 updated\n",
      "Time 1233.0 updated\n",
      "Time 1233.5 updated\n",
      "Time 1234.0 updated\n",
      "Time 1234.5 updated\n",
      "Time 1235.0 updated\n",
      "Time 1235.5 updated\n",
      "Time 1236.0 updated\n",
      "Time 1236.5 updated\n",
      "Time 1237.0 updated\n",
      "Time 1237.5 updated\n",
      "Time 1238.0 updated\n",
      "Time 1238.5 updated\n",
      "Time 1239.0 updated\n",
      "Time 1239.5 updated\n",
      "Time 1240.0 updated\n",
      "Time 1240.5 updated\n",
      "Time 1241.0 updated\n",
      "Time 1241.5 updated\n",
      "Time 1242.0 updated\n",
      "Time 1242.5 updated\n",
      "Time 1243.0 updated\n",
      "Time 1243.5 updated\n",
      "Time 1244.0 updated\n",
      "Time 1244.5 updated\n",
      "Time 1245.0 updated\n",
      "Time 1245.5 updated\n",
      "Time 1246.0 updated\n",
      "Time 1246.5 updated\n",
      "Time 1247.0 updated\n",
      "Time 1247.5 updated\n",
      "Time 1248.0 updated\n",
      "Time 1248.5 updated\n",
      "Time 1249.0 updated\n",
      "Time 1249.5 updated\n",
      "Time 1250.0 updated\n",
      "Time 1250.5 updated\n",
      "Time 1251.0 updated\n",
      "Time 1251.5 updated\n",
      "Time 1252.0 updated\n",
      "Time 1252.5 updated\n",
      "Time 1253.0 updated\n",
      "Time 1253.5 updated\n",
      "Time 1254.0 updated\n",
      "Time 1254.5 updated\n",
      "Time 1255.0 updated\n",
      "Time 1255.5 updated\n",
      "Time 1256.0 updated\n",
      "Time 1256.5 updated\n",
      "Time 1257.0 updated\n",
      "Time 1257.5 updated\n",
      "Time 1258.0 updated\n",
      "Time 1258.5 updated\n",
      "Time 1259.0 updated\n",
      "Time 1259.5 updated\n",
      "Time 1260.0 updated\n",
      "Time 1260.5 updated\n",
      "Time 1261.0 updated\n",
      "Time 1261.5 updated\n",
      "Time 1262.0 updated\n",
      "Time 1262.5 updated\n",
      "Time 1263.0 updated\n",
      "Time 1263.5 updated\n",
      "Time 1264.0 updated\n",
      "Time 1264.5 updated\n",
      "Time 1265.0 updated\n",
      "Time 1265.5 updated\n",
      "Time 1266.0 updated\n",
      "Time 1266.5 updated\n",
      "Time 1267.0 updated\n",
      "Time 1267.5 updated\n",
      "Time 1268.0 updated\n",
      "Time 1268.5 updated\n",
      "Time 1269.0 updated\n",
      "Time 1269.5 updated\n",
      "Time 1270.0 updated\n",
      "Time 1270.5 updated\n",
      "Time 1271.0 updated\n",
      "Time 1271.5 updated\n",
      "Time 1272.0 updated\n",
      "Time 1272.5 updated\n",
      "Time 1273.0 updated\n",
      "Time 1273.5 updated\n",
      "Time 1274.0 updated\n",
      "Time 1274.5 updated\n",
      "Time 1275.0 updated\n",
      "Time 1275.5 updated\n",
      "Time 1276.0 updated\n",
      "Time 1276.5 updated\n",
      "Time 1277.0 updated\n",
      "Time 1277.5 updated\n",
      "Time 1278.0 updated\n",
      "Time 1278.5 updated\n",
      "Time 1279.0 updated\n",
      "Time 1279.5 updated\n",
      "Time 1280.0 updated\n",
      "Time 1280.5 updated\n",
      "Time 1281.0 updated\n",
      "Time 1281.5 updated\n",
      "Time 1282.0 updated\n",
      "Time 1282.5 updated\n",
      "Time 1283.0 updated\n",
      "Time 1283.5 updated\n",
      "Time 1284.0 updated\n",
      "Time 1284.5 updated\n",
      "Time 1285.0 updated\n",
      "Time 1285.5 updated\n",
      "Time 1286.0 updated\n",
      "Time 1286.5 updated\n",
      "Time 1287.0 updated\n",
      "Time 1287.5 updated\n",
      "Time 1288.0 updated\n",
      "Time 1288.5 updated\n",
      "Time 1289.0 updated\n",
      "Time 1289.5 updated\n",
      "Time 1290.0 updated\n",
      "Time 1290.5 updated\n",
      "Time 1291.0 updated\n",
      "Time 1291.5 updated\n",
      "Time 1292.0 updated\n",
      "Time 1292.5 updated\n",
      "Time 1293.0 updated\n",
      "Time 1293.5 updated\n",
      "Time 1294.0 updated\n",
      "Time 1294.5 updated\n",
      "Time 1295.0 updated\n",
      "Time 1295.5 updated\n",
      "Time 1296.0 updated\n",
      "Time 1296.5 updated\n",
      "Time 1297.0 updated\n",
      "Time 1297.5 updated\n",
      "Time 1298.0 updated\n",
      "Time 1298.5 updated\n",
      "Time 1299.0 updated\n",
      "Time 1299.5 updated\n",
      "Time 1300.0 updated\n",
      "Time 1300.5 updated\n",
      "Time 1301.0 updated\n",
      "Time 1301.5 updated\n",
      "Time 1302.0 updated\n",
      "Time 1302.5 updated\n",
      "Time 1303.0 updated\n",
      "Time 1303.5 updated\n",
      "Time 1304.0 updated\n",
      "Time 1304.5 updated\n",
      "Time 1305.0 updated\n",
      "Time 1305.5 updated\n",
      "Time 1306.0 updated\n",
      "Time 1306.5 updated\n",
      "Time 1307.0 updated\n",
      "Time 1307.5 updated\n",
      "Time 1308.0 updated\n",
      "Time 1308.5 updated\n",
      "Time 1309.0 updated\n",
      "Time 1309.5 updated\n",
      "Time 1310.0 updated\n",
      "Time 1310.5 updated\n",
      "Time 1311.0 updated\n",
      "Time 1311.5 updated\n",
      "Time 1312.0 updated\n",
      "Time 1312.5 updated\n",
      "Time 1313.0 updated\n",
      "Time 1313.5 updated\n",
      "Time 1314.0 updated\n",
      "Time 1314.5 updated\n",
      "Time 1315.0 updated\n",
      "Time 1315.5 updated\n",
      "Time 1316.0 updated\n",
      "Time 1316.5 updated\n",
      "Time 1317.0 updated\n",
      "Time 1317.5 updated\n",
      "Time 1318.0 updated\n",
      "Time 1318.5 updated\n",
      "Time 1319.0 updated\n",
      "Time 1319.5 updated\n",
      "Time 1320.0 updated\n",
      "Time 1320.5 updated\n",
      "Time 1321.0 updated\n",
      "Time 1321.5 updated\n",
      "Time 1322.0 updated\n",
      "Time 1322.5 updated\n",
      "Time 1323.0 updated\n",
      "Time 1323.5 updated\n",
      "Time 1324.0 updated\n",
      "Time 1324.5 updated\n",
      "Time 1325.0 updated\n",
      "Time 1325.5 updated\n",
      "Time 1326.0 updated\n",
      "Time 1326.5 updated\n",
      "Time 1327.0 updated\n",
      "Time 1327.5 updated\n",
      "Time 1328.0 updated\n",
      "Time 1328.5 updated\n",
      "Time 1329.0 updated\n",
      "Time 1329.5 updated\n",
      "Time 1330.0 updated\n",
      "Time 1330.5 updated\n",
      "Time 1331.0 updated\n",
      "Time 1331.5 updated\n",
      "Time 1332.0 updated\n",
      "Time 1332.5 updated\n",
      "Time 1333.0 updated\n",
      "Time 1333.5 updated\n",
      "Time 1334.0 updated\n",
      "Time 1334.5 updated\n",
      "Time 1335.0 updated\n",
      "Time 1335.5 updated\n",
      "Time 1336.0 updated\n",
      "Time 1336.5 updated\n",
      "Time 1337.0 updated\n",
      "Time 1337.5 updated\n",
      "Time 1338.0 updated\n",
      "Time 1338.5 updated\n",
      "Time 1339.0 updated\n",
      "Time 1339.5 updated\n",
      "Time 1340.0 updated\n",
      "Time 1340.5 updated\n",
      "Time 1341.0 updated\n",
      "Time 1341.5 updated\n",
      "Time 1342.0 updated\n",
      "Time 1342.5 updated\n",
      "Time 1343.0 updated\n",
      "Time 1343.5 updated\n",
      "Time 1344.0 updated\n",
      "Time 1344.5 updated\n",
      "Time 1345.0 updated\n",
      "Time 1345.5 updated\n",
      "Time 1346.0 updated\n",
      "Time 1346.5 updated\n",
      "Time 1347.0 updated\n",
      "Time 1347.5 updated\n",
      "Time 1348.0 updated\n",
      "Time 1348.5 updated\n",
      "Time 1349.0 updated\n",
      "Time 1349.5 updated\n",
      "Time 1350.0 updated\n",
      "Time 1350.5 updated\n",
      "Time 1351.0 updated\n",
      "Time 1351.5 updated\n",
      "Time 1352.0 updated\n",
      "Time 1352.5 updated\n",
      "Time 1353.0 updated\n",
      "Time 1353.5 updated\n",
      "Time 1354.0 updated\n",
      "Time 1354.5 updated\n",
      "Time 1355.0 updated\n",
      "Time 1355.5 updated\n",
      "Time 1356.0 updated\n",
      "Time 1356.5 updated\n",
      "Time 1357.0 updated\n",
      "Time 1357.5 updated\n",
      "Time 1358.0 updated\n",
      "Time 1358.5 updated\n",
      "Time 1359.0 updated\n",
      "Time 1359.5 updated\n",
      "Time 1360.0 updated\n",
      "Time 1360.5 updated\n",
      "Time 1361.0 updated\n",
      "Time 1361.5 updated\n",
      "Time 1362.0 updated\n",
      "Time 1362.5 updated\n",
      "Time 1363.0 updated\n",
      "Time 1363.5 updated\n",
      "Time 1364.0 updated\n",
      "Time 1364.5 updated\n",
      "Time 1365.0 updated\n",
      "Time 1365.5 updated\n",
      "Time 1366.0 updated\n",
      "Time 1366.5 updated\n",
      "Time 1367.0 updated\n",
      "Time 1367.5 updated\n",
      "Time 1368.0 updated\n",
      "Time 1368.5 updated\n",
      "Time 1369.0 updated\n",
      "Time 1369.5 updated\n",
      "Time 1370.0 updated\n",
      "Time 1370.5 updated\n",
      "Time 1371.0 updated\n",
      "Time 1371.5 updated\n",
      "Time 1372.0 updated\n",
      "Time 1372.5 updated\n",
      "Time 1373.0 updated\n",
      "Time 1373.5 updated\n",
      "Time 1374.0 updated\n",
      "Time 1374.5 updated\n",
      "Time 1375.0 updated\n",
      "Time 1375.5 updated\n",
      "Time 1376.0 updated\n",
      "Time 1376.5 updated\n",
      "Time 1377.0 updated\n",
      "Time 1377.5 updated\n",
      "Time 1378.0 updated\n",
      "Time 1378.5 updated\n",
      "Time 1379.0 updated\n",
      "Time 1379.5 updated\n",
      "Time 1380.0 updated\n",
      "Time 1380.5 updated\n",
      "Time 1381.0 updated\n",
      "Time 1381.5 updated\n",
      "Time 1382.0 updated\n",
      "Time 1382.5 updated\n",
      "Time 1383.0 updated\n",
      "Time 1383.5 updated\n",
      "Time 1384.0 updated\n",
      "Time 1384.5 updated\n",
      "Time 1385.0 updated\n",
      "Time 1385.5 updated\n",
      "Time 1386.0 updated\n",
      "Time 1386.5 updated\n",
      "Time 1387.0 updated\n",
      "Time 1387.5 updated\n",
      "Time 1388.0 updated\n",
      "Time 1388.5 updated\n",
      "Time 1389.0 updated\n",
      "Time 1389.5 updated\n",
      "Time 1390.0 updated\n",
      "Time 1390.5 updated\n",
      "Time 1391.0 updated\n",
      "Time 1391.5 updated\n",
      "Time 1392.0 updated\n",
      "Time 1392.5 updated\n",
      "Time 1393.0 updated\n",
      "Time 1393.5 updated\n",
      "Time 1394.0 updated\n",
      "Time 1394.5 updated\n",
      "Time 1395.0 updated\n",
      "Time 1395.5 updated\n",
      "Time 1396.0 updated\n",
      "Time 1396.5 updated\n",
      "Time 1397.0 updated\n",
      "Time 1397.5 updated\n",
      "Time 1398.0 updated\n",
      "Time 1398.5 updated\n",
      "Time 1399.0 updated\n",
      "Time 1399.5 updated\n",
      "Time 1400.0 updated\n",
      "Time 1400.5 updated\n"
     ]
    }
   ],
   "source": [
    "model = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"model_4173183967.pth\", weights_only=False))\n",
    "\n",
    "# Generate\n",
    "diff_sample_1 = generation(model, 250, n, 0.5, 4 * n, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c5198f3c-f26a-414d-868a-0d4d590b37a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.5 updated\n",
      "Time 1.0 updated\n",
      "Time 1.5 updated\n",
      "Time 2.0 updated\n",
      "Time 2.5 updated\n",
      "Time 3.0 updated\n",
      "Time 3.5 updated\n",
      "Time 4.0 updated\n",
      "Time 4.5 updated\n",
      "Time 5.0 updated\n",
      "Time 5.5 updated\n",
      "Time 6.0 updated\n",
      "Time 6.5 updated\n",
      "Time 7.0 updated\n",
      "Time 7.5 updated\n",
      "Time 8.0 updated\n",
      "Time 8.5 updated\n",
      "Time 9.0 updated\n",
      "Time 9.5 updated\n",
      "Time 10.0 updated\n",
      "Time 10.5 updated\n",
      "Time 11.0 updated\n",
      "Time 11.5 updated\n",
      "Time 12.0 updated\n",
      "Time 12.5 updated\n",
      "Time 13.0 updated\n",
      "Time 13.5 updated\n",
      "Time 14.0 updated\n",
      "Time 14.5 updated\n",
      "Time 15.0 updated\n",
      "Time 15.5 updated\n",
      "Time 16.0 updated\n",
      "Time 16.5 updated\n",
      "Time 17.0 updated\n",
      "Time 17.5 updated\n",
      "Time 18.0 updated\n",
      "Time 18.5 updated\n",
      "Time 19.0 updated\n",
      "Time 19.5 updated\n",
      "Time 20.0 updated\n",
      "Time 20.5 updated\n",
      "Time 21.0 updated\n",
      "Time 21.5 updated\n",
      "Time 22.0 updated\n",
      "Time 22.5 updated\n",
      "Time 23.0 updated\n",
      "Time 23.5 updated\n",
      "Time 24.0 updated\n",
      "Time 24.5 updated\n",
      "Time 25.0 updated\n",
      "Time 25.5 updated\n",
      "Time 26.0 updated\n",
      "Time 26.5 updated\n",
      "Time 27.0 updated\n",
      "Time 27.5 updated\n",
      "Time 28.0 updated\n",
      "Time 28.5 updated\n",
      "Time 29.0 updated\n",
      "Time 29.5 updated\n",
      "Time 30.0 updated\n",
      "Time 30.5 updated\n",
      "Time 31.0 updated\n",
      "Time 31.5 updated\n",
      "Time 32.0 updated\n",
      "Time 32.5 updated\n",
      "Time 33.0 updated\n",
      "Time 33.5 updated\n",
      "Time 34.0 updated\n",
      "Time 34.5 updated\n",
      "Time 35.0 updated\n",
      "Time 35.5 updated\n",
      "Time 36.0 updated\n",
      "Time 36.5 updated\n",
      "Time 37.0 updated\n",
      "Time 37.5 updated\n",
      "Time 38.0 updated\n",
      "Time 38.5 updated\n",
      "Time 39.0 updated\n",
      "Time 39.5 updated\n",
      "Time 40.0 updated\n",
      "Time 40.5 updated\n",
      "Time 41.0 updated\n",
      "Time 41.5 updated\n",
      "Time 42.0 updated\n",
      "Time 42.5 updated\n",
      "Time 43.0 updated\n",
      "Time 43.5 updated\n",
      "Time 44.0 updated\n",
      "Time 44.5 updated\n",
      "Time 45.0 updated\n",
      "Time 45.5 updated\n",
      "Time 46.0 updated\n",
      "Time 46.5 updated\n",
      "Time 47.0 updated\n",
      "Time 47.5 updated\n",
      "Time 48.0 updated\n",
      "Time 48.5 updated\n",
      "Time 49.0 updated\n",
      "Time 49.5 updated\n",
      "Time 50.0 updated\n",
      "Time 50.5 updated\n",
      "Time 51.0 updated\n",
      "Time 51.5 updated\n",
      "Time 52.0 updated\n",
      "Time 52.5 updated\n",
      "Time 53.0 updated\n",
      "Time 53.5 updated\n",
      "Time 54.0 updated\n",
      "Time 54.5 updated\n",
      "Time 55.0 updated\n",
      "Time 55.5 updated\n",
      "Time 56.0 updated\n",
      "Time 56.5 updated\n",
      "Time 57.0 updated\n",
      "Time 57.5 updated\n",
      "Time 58.0 updated\n",
      "Time 58.5 updated\n",
      "Time 59.0 updated\n",
      "Time 59.5 updated\n",
      "Time 60.0 updated\n",
      "Time 60.5 updated\n",
      "Time 61.0 updated\n",
      "Time 61.5 updated\n",
      "Time 62.0 updated\n",
      "Time 62.5 updated\n",
      "Time 63.0 updated\n",
      "Time 63.5 updated\n",
      "Time 64.0 updated\n",
      "Time 64.5 updated\n",
      "Time 65.0 updated\n",
      "Time 65.5 updated\n",
      "Time 66.0 updated\n",
      "Time 66.5 updated\n",
      "Time 67.0 updated\n",
      "Time 67.5 updated\n",
      "Time 68.0 updated\n",
      "Time 68.5 updated\n",
      "Time 69.0 updated\n",
      "Time 69.5 updated\n",
      "Time 70.0 updated\n",
      "Time 70.5 updated\n",
      "Time 71.0 updated\n",
      "Time 71.5 updated\n",
      "Time 72.0 updated\n",
      "Time 72.5 updated\n",
      "Time 73.0 updated\n",
      "Time 73.5 updated\n",
      "Time 74.0 updated\n",
      "Time 74.5 updated\n",
      "Time 75.0 updated\n",
      "Time 75.5 updated\n",
      "Time 76.0 updated\n",
      "Time 76.5 updated\n",
      "Time 77.0 updated\n",
      "Time 77.5 updated\n",
      "Time 78.0 updated\n",
      "Time 78.5 updated\n",
      "Time 79.0 updated\n",
      "Time 79.5 updated\n",
      "Time 80.0 updated\n",
      "Time 80.5 updated\n",
      "Time 81.0 updated\n",
      "Time 81.5 updated\n",
      "Time 82.0 updated\n",
      "Time 82.5 updated\n",
      "Time 83.0 updated\n",
      "Time 83.5 updated\n",
      "Time 84.0 updated\n",
      "Time 84.5 updated\n",
      "Time 85.0 updated\n",
      "Time 85.5 updated\n",
      "Time 86.0 updated\n",
      "Time 86.5 updated\n",
      "Time 87.0 updated\n",
      "Time 87.5 updated\n",
      "Time 88.0 updated\n",
      "Time 88.5 updated\n",
      "Time 89.0 updated\n",
      "Time 89.5 updated\n",
      "Time 90.0 updated\n",
      "Time 90.5 updated\n",
      "Time 91.0 updated\n",
      "Time 91.5 updated\n",
      "Time 92.0 updated\n",
      "Time 92.5 updated\n",
      "Time 93.0 updated\n",
      "Time 93.5 updated\n",
      "Time 94.0 updated\n",
      "Time 94.5 updated\n",
      "Time 95.0 updated\n",
      "Time 95.5 updated\n",
      "Time 96.0 updated\n",
      "Time 96.5 updated\n",
      "Time 97.0 updated\n",
      "Time 97.5 updated\n",
      "Time 98.0 updated\n",
      "Time 98.5 updated\n",
      "Time 99.0 updated\n",
      "Time 99.5 updated\n",
      "Time 100.0 updated\n",
      "Time 100.5 updated\n",
      "Time 101.0 updated\n",
      "Time 101.5 updated\n",
      "Time 102.0 updated\n",
      "Time 102.5 updated\n",
      "Time 103.0 updated\n",
      "Time 103.5 updated\n",
      "Time 104.0 updated\n",
      "Time 104.5 updated\n",
      "Time 105.0 updated\n",
      "Time 105.5 updated\n",
      "Time 106.0 updated\n",
      "Time 106.5 updated\n",
      "Time 107.0 updated\n",
      "Time 107.5 updated\n",
      "Time 108.0 updated\n",
      "Time 108.5 updated\n",
      "Time 109.0 updated\n",
      "Time 109.5 updated\n",
      "Time 110.0 updated\n",
      "Time 110.5 updated\n",
      "Time 111.0 updated\n",
      "Time 111.5 updated\n",
      "Time 112.0 updated\n",
      "Time 112.5 updated\n",
      "Time 113.0 updated\n",
      "Time 113.5 updated\n",
      "Time 114.0 updated\n",
      "Time 114.5 updated\n",
      "Time 115.0 updated\n",
      "Time 115.5 updated\n",
      "Time 116.0 updated\n",
      "Time 116.5 updated\n",
      "Time 117.0 updated\n",
      "Time 117.5 updated\n",
      "Time 118.0 updated\n",
      "Time 118.5 updated\n",
      "Time 119.0 updated\n",
      "Time 119.5 updated\n",
      "Time 120.0 updated\n",
      "Time 120.5 updated\n",
      "Time 121.0 updated\n",
      "Time 121.5 updated\n",
      "Time 122.0 updated\n",
      "Time 122.5 updated\n",
      "Time 123.0 updated\n",
      "Time 123.5 updated\n",
      "Time 124.0 updated\n",
      "Time 124.5 updated\n",
      "Time 125.0 updated\n",
      "Time 125.5 updated\n",
      "Time 126.0 updated\n",
      "Time 126.5 updated\n",
      "Time 127.0 updated\n",
      "Time 127.5 updated\n",
      "Time 128.0 updated\n",
      "Time 128.5 updated\n",
      "Time 129.0 updated\n",
      "Time 129.5 updated\n",
      "Time 130.0 updated\n",
      "Time 130.5 updated\n",
      "Time 131.0 updated\n",
      "Time 131.5 updated\n",
      "Time 132.0 updated\n",
      "Time 132.5 updated\n",
      "Time 133.0 updated\n",
      "Time 133.5 updated\n",
      "Time 134.0 updated\n",
      "Time 134.5 updated\n",
      "Time 135.0 updated\n",
      "Time 135.5 updated\n",
      "Time 136.0 updated\n",
      "Time 136.5 updated\n",
      "Time 137.0 updated\n",
      "Time 137.5 updated\n",
      "Time 138.0 updated\n",
      "Time 138.5 updated\n",
      "Time 139.0 updated\n",
      "Time 139.5 updated\n",
      "Time 140.0 updated\n",
      "Time 140.5 updated\n",
      "Time 141.0 updated\n",
      "Time 141.5 updated\n",
      "Time 142.0 updated\n",
      "Time 142.5 updated\n",
      "Time 143.0 updated\n",
      "Time 143.5 updated\n",
      "Time 144.0 updated\n",
      "Time 144.5 updated\n",
      "Time 145.0 updated\n",
      "Time 145.5 updated\n",
      "Time 146.0 updated\n",
      "Time 146.5 updated\n",
      "Time 147.0 updated\n",
      "Time 147.5 updated\n",
      "Time 148.0 updated\n",
      "Time 148.5 updated\n",
      "Time 149.0 updated\n",
      "Time 149.5 updated\n",
      "Time 150.0 updated\n",
      "Time 150.5 updated\n",
      "Time 151.0 updated\n",
      "Time 151.5 updated\n",
      "Time 152.0 updated\n",
      "Time 152.5 updated\n",
      "Time 153.0 updated\n",
      "Time 153.5 updated\n",
      "Time 154.0 updated\n",
      "Time 154.5 updated\n",
      "Time 155.0 updated\n",
      "Time 155.5 updated\n",
      "Time 156.0 updated\n",
      "Time 156.5 updated\n",
      "Time 157.0 updated\n",
      "Time 157.5 updated\n",
      "Time 158.0 updated\n",
      "Time 158.5 updated\n",
      "Time 159.0 updated\n",
      "Time 159.5 updated\n",
      "Time 160.0 updated\n",
      "Time 160.5 updated\n",
      "Time 161.0 updated\n",
      "Time 161.5 updated\n",
      "Time 162.0 updated\n",
      "Time 162.5 updated\n",
      "Time 163.0 updated\n",
      "Time 163.5 updated\n",
      "Time 164.0 updated\n",
      "Time 164.5 updated\n",
      "Time 165.0 updated\n",
      "Time 165.5 updated\n",
      "Time 166.0 updated\n",
      "Time 166.5 updated\n",
      "Time 167.0 updated\n",
      "Time 167.5 updated\n",
      "Time 168.0 updated\n",
      "Time 168.5 updated\n",
      "Time 169.0 updated\n",
      "Time 169.5 updated\n",
      "Time 170.0 updated\n",
      "Time 170.5 updated\n",
      "Time 171.0 updated\n",
      "Time 171.5 updated\n",
      "Time 172.0 updated\n",
      "Time 172.5 updated\n",
      "Time 173.0 updated\n",
      "Time 173.5 updated\n",
      "Time 174.0 updated\n",
      "Time 174.5 updated\n",
      "Time 175.0 updated\n",
      "Time 175.5 updated\n",
      "Time 176.0 updated\n",
      "Time 176.5 updated\n",
      "Time 177.0 updated\n",
      "Time 177.5 updated\n",
      "Time 178.0 updated\n",
      "Time 178.5 updated\n",
      "Time 179.0 updated\n",
      "Time 179.5 updated\n",
      "Time 180.0 updated\n",
      "Time 180.5 updated\n",
      "Time 181.0 updated\n",
      "Time 181.5 updated\n",
      "Time 182.0 updated\n",
      "Time 182.5 updated\n",
      "Time 183.0 updated\n",
      "Time 183.5 updated\n",
      "Time 184.0 updated\n",
      "Time 184.5 updated\n",
      "Time 185.0 updated\n",
      "Time 185.5 updated\n",
      "Time 186.0 updated\n",
      "Time 186.5 updated\n",
      "Time 187.0 updated\n",
      "Time 187.5 updated\n",
      "Time 188.0 updated\n",
      "Time 188.5 updated\n",
      "Time 189.0 updated\n",
      "Time 189.5 updated\n",
      "Time 190.0 updated\n",
      "Time 190.5 updated\n",
      "Time 191.0 updated\n",
      "Time 191.5 updated\n",
      "Time 192.0 updated\n",
      "Time 192.5 updated\n",
      "Time 193.0 updated\n",
      "Time 193.5 updated\n",
      "Time 194.0 updated\n",
      "Time 194.5 updated\n",
      "Time 195.0 updated\n",
      "Time 195.5 updated\n",
      "Time 196.0 updated\n",
      "Time 196.5 updated\n",
      "Time 197.0 updated\n",
      "Time 197.5 updated\n",
      "Time 198.0 updated\n",
      "Time 198.5 updated\n",
      "Time 199.0 updated\n",
      "Time 199.5 updated\n",
      "Time 200.0 updated\n",
      "Time 200.5 updated\n",
      "Time 201.0 updated\n",
      "Time 201.5 updated\n",
      "Time 202.0 updated\n",
      "Time 202.5 updated\n",
      "Time 203.0 updated\n",
      "Time 203.5 updated\n",
      "Time 204.0 updated\n",
      "Time 204.5 updated\n",
      "Time 205.0 updated\n",
      "Time 205.5 updated\n",
      "Time 206.0 updated\n",
      "Time 206.5 updated\n",
      "Time 207.0 updated\n",
      "Time 207.5 updated\n",
      "Time 208.0 updated\n",
      "Time 208.5 updated\n",
      "Time 209.0 updated\n",
      "Time 209.5 updated\n",
      "Time 210.0 updated\n",
      "Time 210.5 updated\n",
      "Time 211.0 updated\n",
      "Time 211.5 updated\n",
      "Time 212.0 updated\n",
      "Time 212.5 updated\n",
      "Time 213.0 updated\n",
      "Time 213.5 updated\n",
      "Time 214.0 updated\n",
      "Time 214.5 updated\n",
      "Time 215.0 updated\n",
      "Time 215.5 updated\n",
      "Time 216.0 updated\n",
      "Time 216.5 updated\n",
      "Time 217.0 updated\n",
      "Time 217.5 updated\n",
      "Time 218.0 updated\n",
      "Time 218.5 updated\n",
      "Time 219.0 updated\n",
      "Time 219.5 updated\n",
      "Time 220.0 updated\n",
      "Time 220.5 updated\n",
      "Time 221.0 updated\n",
      "Time 221.5 updated\n",
      "Time 222.0 updated\n",
      "Time 222.5 updated\n",
      "Time 223.0 updated\n",
      "Time 223.5 updated\n",
      "Time 224.0 updated\n",
      "Time 224.5 updated\n",
      "Time 225.0 updated\n",
      "Time 225.5 updated\n",
      "Time 226.0 updated\n",
      "Time 226.5 updated\n",
      "Time 227.0 updated\n",
      "Time 227.5 updated\n",
      "Time 228.0 updated\n",
      "Time 228.5 updated\n",
      "Time 229.0 updated\n",
      "Time 229.5 updated\n",
      "Time 230.0 updated\n",
      "Time 230.5 updated\n",
      "Time 231.0 updated\n",
      "Time 231.5 updated\n",
      "Time 232.0 updated\n",
      "Time 232.5 updated\n",
      "Time 233.0 updated\n",
      "Time 233.5 updated\n",
      "Time 234.0 updated\n",
      "Time 234.5 updated\n",
      "Time 235.0 updated\n",
      "Time 235.5 updated\n",
      "Time 236.0 updated\n",
      "Time 236.5 updated\n",
      "Time 237.0 updated\n",
      "Time 237.5 updated\n",
      "Time 238.0 updated\n",
      "Time 238.5 updated\n",
      "Time 239.0 updated\n",
      "Time 239.5 updated\n",
      "Time 240.0 updated\n",
      "Time 240.5 updated\n",
      "Time 241.0 updated\n",
      "Time 241.5 updated\n",
      "Time 242.0 updated\n",
      "Time 242.5 updated\n",
      "Time 243.0 updated\n",
      "Time 243.5 updated\n",
      "Time 244.0 updated\n",
      "Time 244.5 updated\n",
      "Time 245.0 updated\n",
      "Time 245.5 updated\n",
      "Time 246.0 updated\n",
      "Time 246.5 updated\n",
      "Time 247.0 updated\n",
      "Time 247.5 updated\n",
      "Time 248.0 updated\n",
      "Time 248.5 updated\n",
      "Time 249.0 updated\n",
      "Time 249.5 updated\n",
      "Time 250.0 updated\n",
      "Time 250.5 updated\n",
      "Time 251.0 updated\n",
      "Time 251.5 updated\n",
      "Time 252.0 updated\n",
      "Time 252.5 updated\n",
      "Time 253.0 updated\n",
      "Time 253.5 updated\n",
      "Time 254.0 updated\n",
      "Time 254.5 updated\n",
      "Time 255.0 updated\n",
      "Time 255.5 updated\n",
      "Time 256.0 updated\n",
      "Time 256.5 updated\n",
      "Time 257.0 updated\n",
      "Time 257.5 updated\n",
      "Time 258.0 updated\n",
      "Time 258.5 updated\n",
      "Time 259.0 updated\n",
      "Time 259.5 updated\n",
      "Time 260.0 updated\n",
      "Time 260.5 updated\n",
      "Time 261.0 updated\n",
      "Time 261.5 updated\n",
      "Time 262.0 updated\n",
      "Time 262.5 updated\n",
      "Time 263.0 updated\n",
      "Time 263.5 updated\n",
      "Time 264.0 updated\n",
      "Time 264.5 updated\n",
      "Time 265.0 updated\n",
      "Time 265.5 updated\n",
      "Time 266.0 updated\n",
      "Time 266.5 updated\n",
      "Time 267.0 updated\n",
      "Time 267.5 updated\n",
      "Time 268.0 updated\n",
      "Time 268.5 updated\n",
      "Time 269.0 updated\n",
      "Time 269.5 updated\n",
      "Time 270.0 updated\n",
      "Time 270.5 updated\n",
      "Time 271.0 updated\n",
      "Time 271.5 updated\n",
      "Time 272.0 updated\n",
      "Time 272.5 updated\n",
      "Time 273.0 updated\n",
      "Time 273.5 updated\n",
      "Time 274.0 updated\n",
      "Time 274.5 updated\n",
      "Time 275.0 updated\n",
      "Time 275.5 updated\n",
      "Time 276.0 updated\n",
      "Time 276.5 updated\n",
      "Time 277.0 updated\n",
      "Time 277.5 updated\n",
      "Time 278.0 updated\n",
      "Time 278.5 updated\n",
      "Time 279.0 updated\n",
      "Time 279.5 updated\n",
      "Time 280.0 updated\n",
      "Time 280.5 updated\n",
      "Time 281.0 updated\n",
      "Time 281.5 updated\n",
      "Time 282.0 updated\n",
      "Time 282.5 updated\n",
      "Time 283.0 updated\n",
      "Time 283.5 updated\n",
      "Time 284.0 updated\n",
      "Time 284.5 updated\n",
      "Time 285.0 updated\n",
      "Time 285.5 updated\n",
      "Time 286.0 updated\n",
      "Time 286.5 updated\n",
      "Time 287.0 updated\n",
      "Time 287.5 updated\n",
      "Time 288.0 updated\n",
      "Time 288.5 updated\n",
      "Time 289.0 updated\n",
      "Time 289.5 updated\n",
      "Time 290.0 updated\n",
      "Time 290.5 updated\n",
      "Time 291.0 updated\n",
      "Time 291.5 updated\n",
      "Time 292.0 updated\n",
      "Time 292.5 updated\n",
      "Time 293.0 updated\n",
      "Time 293.5 updated\n",
      "Time 294.0 updated\n",
      "Time 294.5 updated\n",
      "Time 295.0 updated\n",
      "Time 295.5 updated\n",
      "Time 296.0 updated\n",
      "Time 296.5 updated\n",
      "Time 297.0 updated\n",
      "Time 297.5 updated\n",
      "Time 298.0 updated\n",
      "Time 298.5 updated\n",
      "Time 299.0 updated\n",
      "Time 299.5 updated\n",
      "Time 300.0 updated\n",
      "Time 300.5 updated\n",
      "Time 301.0 updated\n",
      "Time 301.5 updated\n",
      "Time 302.0 updated\n",
      "Time 302.5 updated\n",
      "Time 303.0 updated\n",
      "Time 303.5 updated\n",
      "Time 304.0 updated\n",
      "Time 304.5 updated\n",
      "Time 305.0 updated\n",
      "Time 305.5 updated\n",
      "Time 306.0 updated\n",
      "Time 306.5 updated\n",
      "Time 307.0 updated\n",
      "Time 307.5 updated\n",
      "Time 308.0 updated\n",
      "Time 308.5 updated\n",
      "Time 309.0 updated\n",
      "Time 309.5 updated\n",
      "Time 310.0 updated\n",
      "Time 310.5 updated\n",
      "Time 311.0 updated\n",
      "Time 311.5 updated\n",
      "Time 312.0 updated\n",
      "Time 312.5 updated\n",
      "Time 313.0 updated\n",
      "Time 313.5 updated\n",
      "Time 314.0 updated\n",
      "Time 314.5 updated\n",
      "Time 315.0 updated\n",
      "Time 315.5 updated\n",
      "Time 316.0 updated\n",
      "Time 316.5 updated\n",
      "Time 317.0 updated\n",
      "Time 317.5 updated\n",
      "Time 318.0 updated\n",
      "Time 318.5 updated\n",
      "Time 319.0 updated\n",
      "Time 319.5 updated\n",
      "Time 320.0 updated\n",
      "Time 320.5 updated\n",
      "Time 321.0 updated\n",
      "Time 321.5 updated\n",
      "Time 322.0 updated\n",
      "Time 322.5 updated\n",
      "Time 323.0 updated\n",
      "Time 323.5 updated\n",
      "Time 324.0 updated\n",
      "Time 324.5 updated\n",
      "Time 325.0 updated\n",
      "Time 325.5 updated\n",
      "Time 326.0 updated\n",
      "Time 326.5 updated\n",
      "Time 327.0 updated\n",
      "Time 327.5 updated\n",
      "Time 328.0 updated\n",
      "Time 328.5 updated\n",
      "Time 329.0 updated\n",
      "Time 329.5 updated\n",
      "Time 330.0 updated\n",
      "Time 330.5 updated\n",
      "Time 331.0 updated\n",
      "Time 331.5 updated\n",
      "Time 332.0 updated\n",
      "Time 332.5 updated\n",
      "Time 333.0 updated\n",
      "Time 333.5 updated\n",
      "Time 334.0 updated\n",
      "Time 334.5 updated\n",
      "Time 335.0 updated\n",
      "Time 335.5 updated\n",
      "Time 336.0 updated\n",
      "Time 336.5 updated\n",
      "Time 337.0 updated\n",
      "Time 337.5 updated\n",
      "Time 338.0 updated\n",
      "Time 338.5 updated\n",
      "Time 339.0 updated\n",
      "Time 339.5 updated\n",
      "Time 340.0 updated\n",
      "Time 340.5 updated\n",
      "Time 341.0 updated\n",
      "Time 341.5 updated\n",
      "Time 342.0 updated\n",
      "Time 342.5 updated\n",
      "Time 343.0 updated\n",
      "Time 343.5 updated\n",
      "Time 344.0 updated\n",
      "Time 344.5 updated\n",
      "Time 345.0 updated\n",
      "Time 345.5 updated\n",
      "Time 346.0 updated\n",
      "Time 346.5 updated\n",
      "Time 347.0 updated\n",
      "Time 347.5 updated\n",
      "Time 348.0 updated\n",
      "Time 348.5 updated\n",
      "Time 349.0 updated\n",
      "Time 349.5 updated\n",
      "Time 350.0 updated\n",
      "Time 350.5 updated\n",
      "Time 351.0 updated\n",
      "Time 351.5 updated\n",
      "Time 352.0 updated\n",
      "Time 352.5 updated\n",
      "Time 353.0 updated\n",
      "Time 353.5 updated\n",
      "Time 354.0 updated\n",
      "Time 354.5 updated\n",
      "Time 355.0 updated\n",
      "Time 355.5 updated\n",
      "Time 356.0 updated\n",
      "Time 356.5 updated\n",
      "Time 357.0 updated\n",
      "Time 357.5 updated\n",
      "Time 358.0 updated\n",
      "Time 358.5 updated\n",
      "Time 359.0 updated\n",
      "Time 359.5 updated\n",
      "Time 360.0 updated\n",
      "Time 360.5 updated\n",
      "Time 361.0 updated\n",
      "Time 361.5 updated\n",
      "Time 362.0 updated\n",
      "Time 362.5 updated\n",
      "Time 363.0 updated\n",
      "Time 363.5 updated\n",
      "Time 364.0 updated\n",
      "Time 364.5 updated\n",
      "Time 365.0 updated\n",
      "Time 365.5 updated\n",
      "Time 366.0 updated\n",
      "Time 366.5 updated\n",
      "Time 367.0 updated\n",
      "Time 367.5 updated\n",
      "Time 368.0 updated\n",
      "Time 368.5 updated\n",
      "Time 369.0 updated\n",
      "Time 369.5 updated\n",
      "Time 370.0 updated\n",
      "Time 370.5 updated\n",
      "Time 371.0 updated\n",
      "Time 371.5 updated\n",
      "Time 372.0 updated\n",
      "Time 372.5 updated\n",
      "Time 373.0 updated\n",
      "Time 373.5 updated\n",
      "Time 374.0 updated\n",
      "Time 374.5 updated\n",
      "Time 375.0 updated\n",
      "Time 375.5 updated\n",
      "Time 376.0 updated\n",
      "Time 376.5 updated\n",
      "Time 377.0 updated\n",
      "Time 377.5 updated\n",
      "Time 378.0 updated\n",
      "Time 378.5 updated\n",
      "Time 379.0 updated\n",
      "Time 379.5 updated\n",
      "Time 380.0 updated\n",
      "Time 380.5 updated\n",
      "Time 381.0 updated\n",
      "Time 381.5 updated\n",
      "Time 382.0 updated\n",
      "Time 382.5 updated\n",
      "Time 383.0 updated\n",
      "Time 383.5 updated\n",
      "Time 384.0 updated\n",
      "Time 384.5 updated\n",
      "Time 385.0 updated\n",
      "Time 385.5 updated\n",
      "Time 386.0 updated\n",
      "Time 386.5 updated\n",
      "Time 387.0 updated\n",
      "Time 387.5 updated\n",
      "Time 388.0 updated\n",
      "Time 388.5 updated\n",
      "Time 389.0 updated\n",
      "Time 389.5 updated\n",
      "Time 390.0 updated\n",
      "Time 390.5 updated\n",
      "Time 391.0 updated\n",
      "Time 391.5 updated\n",
      "Time 392.0 updated\n",
      "Time 392.5 updated\n",
      "Time 393.0 updated\n",
      "Time 393.5 updated\n",
      "Time 394.0 updated\n",
      "Time 394.5 updated\n",
      "Time 395.0 updated\n",
      "Time 395.5 updated\n",
      "Time 396.0 updated\n",
      "Time 396.5 updated\n",
      "Time 397.0 updated\n",
      "Time 397.5 updated\n",
      "Time 398.0 updated\n",
      "Time 398.5 updated\n",
      "Time 399.0 updated\n",
      "Time 399.5 updated\n",
      "Time 400.0 updated\n",
      "Time 400.5 updated\n",
      "Time 401.0 updated\n",
      "Time 401.5 updated\n",
      "Time 402.0 updated\n",
      "Time 402.5 updated\n",
      "Time 403.0 updated\n",
      "Time 403.5 updated\n",
      "Time 404.0 updated\n",
      "Time 404.5 updated\n",
      "Time 405.0 updated\n",
      "Time 405.5 updated\n",
      "Time 406.0 updated\n",
      "Time 406.5 updated\n",
      "Time 407.0 updated\n",
      "Time 407.5 updated\n",
      "Time 408.0 updated\n",
      "Time 408.5 updated\n",
      "Time 409.0 updated\n",
      "Time 409.5 updated\n",
      "Time 410.0 updated\n",
      "Time 410.5 updated\n",
      "Time 411.0 updated\n",
      "Time 411.5 updated\n",
      "Time 412.0 updated\n",
      "Time 412.5 updated\n",
      "Time 413.0 updated\n",
      "Time 413.5 updated\n",
      "Time 414.0 updated\n",
      "Time 414.5 updated\n",
      "Time 415.0 updated\n",
      "Time 415.5 updated\n",
      "Time 416.0 updated\n",
      "Time 416.5 updated\n",
      "Time 417.0 updated\n",
      "Time 417.5 updated\n",
      "Time 418.0 updated\n",
      "Time 418.5 updated\n",
      "Time 419.0 updated\n",
      "Time 419.5 updated\n",
      "Time 420.0 updated\n",
      "Time 420.5 updated\n",
      "Time 421.0 updated\n",
      "Time 421.5 updated\n",
      "Time 422.0 updated\n",
      "Time 422.5 updated\n",
      "Time 423.0 updated\n",
      "Time 423.5 updated\n",
      "Time 424.0 updated\n",
      "Time 424.5 updated\n",
      "Time 425.0 updated\n",
      "Time 425.5 updated\n",
      "Time 426.0 updated\n",
      "Time 426.5 updated\n",
      "Time 427.0 updated\n",
      "Time 427.5 updated\n",
      "Time 428.0 updated\n",
      "Time 428.5 updated\n",
      "Time 429.0 updated\n",
      "Time 429.5 updated\n",
      "Time 430.0 updated\n",
      "Time 430.5 updated\n",
      "Time 431.0 updated\n",
      "Time 431.5 updated\n",
      "Time 432.0 updated\n",
      "Time 432.5 updated\n",
      "Time 433.0 updated\n",
      "Time 433.5 updated\n",
      "Time 434.0 updated\n",
      "Time 434.5 updated\n",
      "Time 435.0 updated\n",
      "Time 435.5 updated\n",
      "Time 436.0 updated\n",
      "Time 436.5 updated\n",
      "Time 437.0 updated\n",
      "Time 437.5 updated\n",
      "Time 438.0 updated\n",
      "Time 438.5 updated\n",
      "Time 439.0 updated\n",
      "Time 439.5 updated\n",
      "Time 440.0 updated\n",
      "Time 440.5 updated\n",
      "Time 441.0 updated\n",
      "Time 441.5 updated\n",
      "Time 442.0 updated\n",
      "Time 442.5 updated\n",
      "Time 443.0 updated\n",
      "Time 443.5 updated\n",
      "Time 444.0 updated\n",
      "Time 444.5 updated\n",
      "Time 445.0 updated\n",
      "Time 445.5 updated\n",
      "Time 446.0 updated\n",
      "Time 446.5 updated\n",
      "Time 447.0 updated\n",
      "Time 447.5 updated\n",
      "Time 448.0 updated\n",
      "Time 448.5 updated\n",
      "Time 449.0 updated\n",
      "Time 449.5 updated\n",
      "Time 450.0 updated\n",
      "Time 450.5 updated\n",
      "Time 451.0 updated\n",
      "Time 451.5 updated\n",
      "Time 452.0 updated\n",
      "Time 452.5 updated\n",
      "Time 453.0 updated\n",
      "Time 453.5 updated\n",
      "Time 454.0 updated\n",
      "Time 454.5 updated\n",
      "Time 455.0 updated\n",
      "Time 455.5 updated\n",
      "Time 456.0 updated\n",
      "Time 456.5 updated\n",
      "Time 457.0 updated\n",
      "Time 457.5 updated\n",
      "Time 458.0 updated\n",
      "Time 458.5 updated\n",
      "Time 459.0 updated\n",
      "Time 459.5 updated\n",
      "Time 460.0 updated\n",
      "Time 460.5 updated\n",
      "Time 461.0 updated\n",
      "Time 461.5 updated\n",
      "Time 462.0 updated\n",
      "Time 462.5 updated\n",
      "Time 463.0 updated\n",
      "Time 463.5 updated\n",
      "Time 464.0 updated\n",
      "Time 464.5 updated\n",
      "Time 465.0 updated\n",
      "Time 465.5 updated\n",
      "Time 466.0 updated\n",
      "Time 466.5 updated\n",
      "Time 467.0 updated\n",
      "Time 467.5 updated\n",
      "Time 468.0 updated\n",
      "Time 468.5 updated\n",
      "Time 469.0 updated\n",
      "Time 469.5 updated\n",
      "Time 470.0 updated\n",
      "Time 470.5 updated\n",
      "Time 471.0 updated\n",
      "Time 471.5 updated\n",
      "Time 472.0 updated\n",
      "Time 472.5 updated\n",
      "Time 473.0 updated\n",
      "Time 473.5 updated\n",
      "Time 474.0 updated\n",
      "Time 474.5 updated\n",
      "Time 475.0 updated\n",
      "Time 475.5 updated\n",
      "Time 476.0 updated\n",
      "Time 476.5 updated\n",
      "Time 477.0 updated\n",
      "Time 477.5 updated\n",
      "Time 478.0 updated\n",
      "Time 478.5 updated\n",
      "Time 479.0 updated\n",
      "Time 479.5 updated\n",
      "Time 480.0 updated\n",
      "Time 480.5 updated\n",
      "Time 481.0 updated\n",
      "Time 481.5 updated\n",
      "Time 482.0 updated\n",
      "Time 482.5 updated\n",
      "Time 483.0 updated\n",
      "Time 483.5 updated\n",
      "Time 484.0 updated\n",
      "Time 484.5 updated\n",
      "Time 485.0 updated\n",
      "Time 485.5 updated\n",
      "Time 486.0 updated\n",
      "Time 486.5 updated\n",
      "Time 487.0 updated\n",
      "Time 487.5 updated\n",
      "Time 488.0 updated\n",
      "Time 488.5 updated\n",
      "Time 489.0 updated\n",
      "Time 489.5 updated\n",
      "Time 490.0 updated\n",
      "Time 490.5 updated\n",
      "Time 491.0 updated\n",
      "Time 491.5 updated\n",
      "Time 492.0 updated\n",
      "Time 492.5 updated\n",
      "Time 493.0 updated\n",
      "Time 493.5 updated\n",
      "Time 494.0 updated\n",
      "Time 494.5 updated\n",
      "Time 495.0 updated\n",
      "Time 495.5 updated\n",
      "Time 496.0 updated\n",
      "Time 496.5 updated\n",
      "Time 497.0 updated\n",
      "Time 497.5 updated\n",
      "Time 498.0 updated\n",
      "Time 498.5 updated\n",
      "Time 499.0 updated\n",
      "Time 499.5 updated\n",
      "Time 500.0 updated\n",
      "Time 500.5 updated\n",
      "Time 501.0 updated\n",
      "Time 501.5 updated\n",
      "Time 502.0 updated\n",
      "Time 502.5 updated\n",
      "Time 503.0 updated\n",
      "Time 503.5 updated\n",
      "Time 504.0 updated\n",
      "Time 504.5 updated\n",
      "Time 505.0 updated\n",
      "Time 505.5 updated\n",
      "Time 506.0 updated\n",
      "Time 506.5 updated\n",
      "Time 507.0 updated\n",
      "Time 507.5 updated\n",
      "Time 508.0 updated\n",
      "Time 508.5 updated\n",
      "Time 509.0 updated\n",
      "Time 509.5 updated\n",
      "Time 510.0 updated\n",
      "Time 510.5 updated\n",
      "Time 511.0 updated\n",
      "Time 511.5 updated\n",
      "Time 512.0 updated\n",
      "Time 512.5 updated\n",
      "Time 513.0 updated\n",
      "Time 513.5 updated\n",
      "Time 514.0 updated\n",
      "Time 514.5 updated\n",
      "Time 515.0 updated\n",
      "Time 515.5 updated\n",
      "Time 516.0 updated\n",
      "Time 516.5 updated\n",
      "Time 517.0 updated\n",
      "Time 517.5 updated\n",
      "Time 518.0 updated\n",
      "Time 518.5 updated\n",
      "Time 519.0 updated\n",
      "Time 519.5 updated\n",
      "Time 520.0 updated\n",
      "Time 520.5 updated\n",
      "Time 521.0 updated\n",
      "Time 521.5 updated\n",
      "Time 522.0 updated\n",
      "Time 522.5 updated\n",
      "Time 523.0 updated\n",
      "Time 523.5 updated\n",
      "Time 524.0 updated\n",
      "Time 524.5 updated\n",
      "Time 525.0 updated\n",
      "Time 525.5 updated\n",
      "Time 526.0 updated\n",
      "Time 526.5 updated\n",
      "Time 527.0 updated\n",
      "Time 527.5 updated\n",
      "Time 528.0 updated\n",
      "Time 528.5 updated\n",
      "Time 529.0 updated\n",
      "Time 529.5 updated\n",
      "Time 530.0 updated\n",
      "Time 530.5 updated\n",
      "Time 531.0 updated\n",
      "Time 531.5 updated\n",
      "Time 532.0 updated\n",
      "Time 532.5 updated\n",
      "Time 533.0 updated\n",
      "Time 533.5 updated\n",
      "Time 534.0 updated\n",
      "Time 534.5 updated\n",
      "Time 535.0 updated\n",
      "Time 535.5 updated\n",
      "Time 536.0 updated\n",
      "Time 536.5 updated\n",
      "Time 537.0 updated\n",
      "Time 537.5 updated\n",
      "Time 538.0 updated\n",
      "Time 538.5 updated\n",
      "Time 539.0 updated\n",
      "Time 539.5 updated\n",
      "Time 540.0 updated\n",
      "Time 540.5 updated\n",
      "Time 541.0 updated\n",
      "Time 541.5 updated\n",
      "Time 542.0 updated\n",
      "Time 542.5 updated\n",
      "Time 543.0 updated\n",
      "Time 543.5 updated\n",
      "Time 544.0 updated\n",
      "Time 544.5 updated\n",
      "Time 545.0 updated\n",
      "Time 545.5 updated\n",
      "Time 546.0 updated\n",
      "Time 546.5 updated\n",
      "Time 547.0 updated\n",
      "Time 547.5 updated\n",
      "Time 548.0 updated\n",
      "Time 548.5 updated\n",
      "Time 549.0 updated\n",
      "Time 549.5 updated\n",
      "Time 550.0 updated\n",
      "Time 550.5 updated\n",
      "Time 551.0 updated\n",
      "Time 551.5 updated\n",
      "Time 552.0 updated\n",
      "Time 552.5 updated\n",
      "Time 553.0 updated\n",
      "Time 553.5 updated\n",
      "Time 554.0 updated\n",
      "Time 554.5 updated\n",
      "Time 555.0 updated\n",
      "Time 555.5 updated\n",
      "Time 556.0 updated\n",
      "Time 556.5 updated\n",
      "Time 557.0 updated\n",
      "Time 557.5 updated\n",
      "Time 558.0 updated\n",
      "Time 558.5 updated\n",
      "Time 559.0 updated\n",
      "Time 559.5 updated\n",
      "Time 560.0 updated\n",
      "Time 560.5 updated\n",
      "Time 561.0 updated\n",
      "Time 561.5 updated\n",
      "Time 562.0 updated\n",
      "Time 562.5 updated\n",
      "Time 563.0 updated\n",
      "Time 563.5 updated\n",
      "Time 564.0 updated\n",
      "Time 564.5 updated\n",
      "Time 565.0 updated\n",
      "Time 565.5 updated\n",
      "Time 566.0 updated\n",
      "Time 566.5 updated\n",
      "Time 567.0 updated\n",
      "Time 567.5 updated\n",
      "Time 568.0 updated\n",
      "Time 568.5 updated\n",
      "Time 569.0 updated\n",
      "Time 569.5 updated\n",
      "Time 570.0 updated\n",
      "Time 570.5 updated\n",
      "Time 571.0 updated\n",
      "Time 571.5 updated\n",
      "Time 572.0 updated\n",
      "Time 572.5 updated\n",
      "Time 573.0 updated\n",
      "Time 573.5 updated\n",
      "Time 574.0 updated\n",
      "Time 574.5 updated\n",
      "Time 575.0 updated\n",
      "Time 575.5 updated\n",
      "Time 576.0 updated\n",
      "Time 576.5 updated\n",
      "Time 577.0 updated\n",
      "Time 577.5 updated\n",
      "Time 578.0 updated\n",
      "Time 578.5 updated\n",
      "Time 579.0 updated\n",
      "Time 579.5 updated\n",
      "Time 580.0 updated\n",
      "Time 580.5 updated\n",
      "Time 581.0 updated\n",
      "Time 581.5 updated\n",
      "Time 582.0 updated\n",
      "Time 582.5 updated\n",
      "Time 583.0 updated\n",
      "Time 583.5 updated\n",
      "Time 584.0 updated\n",
      "Time 584.5 updated\n",
      "Time 585.0 updated\n",
      "Time 585.5 updated\n",
      "Time 586.0 updated\n",
      "Time 586.5 updated\n",
      "Time 587.0 updated\n",
      "Time 587.5 updated\n",
      "Time 588.0 updated\n",
      "Time 588.5 updated\n",
      "Time 589.0 updated\n",
      "Time 589.5 updated\n",
      "Time 590.0 updated\n",
      "Time 590.5 updated\n",
      "Time 591.0 updated\n",
      "Time 591.5 updated\n",
      "Time 592.0 updated\n",
      "Time 592.5 updated\n",
      "Time 593.0 updated\n",
      "Time 593.5 updated\n",
      "Time 594.0 updated\n",
      "Time 594.5 updated\n",
      "Time 595.0 updated\n",
      "Time 595.5 updated\n",
      "Time 596.0 updated\n",
      "Time 596.5 updated\n",
      "Time 597.0 updated\n",
      "Time 597.5 updated\n",
      "Time 598.0 updated\n",
      "Time 598.5 updated\n",
      "Time 599.0 updated\n",
      "Time 599.5 updated\n",
      "Time 600.0 updated\n",
      "Time 600.5 updated\n",
      "Time 601.0 updated\n",
      "Time 601.5 updated\n",
      "Time 602.0 updated\n",
      "Time 602.5 updated\n",
      "Time 603.0 updated\n",
      "Time 603.5 updated\n",
      "Time 604.0 updated\n",
      "Time 604.5 updated\n",
      "Time 605.0 updated\n",
      "Time 605.5 updated\n",
      "Time 606.0 updated\n",
      "Time 606.5 updated\n",
      "Time 607.0 updated\n",
      "Time 607.5 updated\n",
      "Time 608.0 updated\n",
      "Time 608.5 updated\n",
      "Time 609.0 updated\n",
      "Time 609.5 updated\n",
      "Time 610.0 updated\n",
      "Time 610.5 updated\n",
      "Time 611.0 updated\n",
      "Time 611.5 updated\n",
      "Time 612.0 updated\n",
      "Time 612.5 updated\n",
      "Time 613.0 updated\n",
      "Time 613.5 updated\n",
      "Time 614.0 updated\n",
      "Time 614.5 updated\n",
      "Time 615.0 updated\n",
      "Time 615.5 updated\n",
      "Time 616.0 updated\n",
      "Time 616.5 updated\n",
      "Time 617.0 updated\n",
      "Time 617.5 updated\n",
      "Time 618.0 updated\n",
      "Time 618.5 updated\n",
      "Time 619.0 updated\n",
      "Time 619.5 updated\n",
      "Time 620.0 updated\n",
      "Time 620.5 updated\n",
      "Time 621.0 updated\n",
      "Time 621.5 updated\n",
      "Time 622.0 updated\n",
      "Time 622.5 updated\n",
      "Time 623.0 updated\n",
      "Time 623.5 updated\n",
      "Time 624.0 updated\n",
      "Time 624.5 updated\n",
      "Time 625.0 updated\n",
      "Time 625.5 updated\n",
      "Time 626.0 updated\n",
      "Time 626.5 updated\n",
      "Time 627.0 updated\n",
      "Time 627.5 updated\n",
      "Time 628.0 updated\n",
      "Time 628.5 updated\n",
      "Time 629.0 updated\n",
      "Time 629.5 updated\n",
      "Time 630.0 updated\n",
      "Time 630.5 updated\n",
      "Time 631.0 updated\n",
      "Time 631.5 updated\n",
      "Time 632.0 updated\n",
      "Time 632.5 updated\n",
      "Time 633.0 updated\n",
      "Time 633.5 updated\n",
      "Time 634.0 updated\n",
      "Time 634.5 updated\n",
      "Time 635.0 updated\n",
      "Time 635.5 updated\n",
      "Time 636.0 updated\n",
      "Time 636.5 updated\n",
      "Time 637.0 updated\n",
      "Time 637.5 updated\n",
      "Time 638.0 updated\n",
      "Time 638.5 updated\n",
      "Time 639.0 updated\n",
      "Time 639.5 updated\n",
      "Time 640.0 updated\n",
      "Time 640.5 updated\n",
      "Time 641.0 updated\n",
      "Time 641.5 updated\n",
      "Time 642.0 updated\n",
      "Time 642.5 updated\n",
      "Time 643.0 updated\n",
      "Time 643.5 updated\n",
      "Time 644.0 updated\n",
      "Time 644.5 updated\n",
      "Time 645.0 updated\n",
      "Time 645.5 updated\n",
      "Time 646.0 updated\n",
      "Time 646.5 updated\n",
      "Time 647.0 updated\n",
      "Time 647.5 updated\n",
      "Time 648.0 updated\n",
      "Time 648.5 updated\n",
      "Time 649.0 updated\n",
      "Time 649.5 updated\n",
      "Time 650.0 updated\n",
      "Time 650.5 updated\n",
      "Time 651.0 updated\n",
      "Time 651.5 updated\n",
      "Time 652.0 updated\n",
      "Time 652.5 updated\n",
      "Time 653.0 updated\n",
      "Time 653.5 updated\n",
      "Time 654.0 updated\n",
      "Time 654.5 updated\n",
      "Time 655.0 updated\n",
      "Time 655.5 updated\n",
      "Time 656.0 updated\n",
      "Time 656.5 updated\n",
      "Time 657.0 updated\n",
      "Time 657.5 updated\n",
      "Time 658.0 updated\n",
      "Time 658.5 updated\n",
      "Time 659.0 updated\n",
      "Time 659.5 updated\n",
      "Time 660.0 updated\n",
      "Time 660.5 updated\n",
      "Time 661.0 updated\n",
      "Time 661.5 updated\n",
      "Time 662.0 updated\n",
      "Time 662.5 updated\n",
      "Time 663.0 updated\n",
      "Time 663.5 updated\n",
      "Time 664.0 updated\n",
      "Time 664.5 updated\n",
      "Time 665.0 updated\n",
      "Time 665.5 updated\n",
      "Time 666.0 updated\n",
      "Time 666.5 updated\n",
      "Time 667.0 updated\n",
      "Time 667.5 updated\n",
      "Time 668.0 updated\n",
      "Time 668.5 updated\n",
      "Time 669.0 updated\n",
      "Time 669.5 updated\n",
      "Time 670.0 updated\n",
      "Time 670.5 updated\n",
      "Time 671.0 updated\n",
      "Time 671.5 updated\n",
      "Time 672.0 updated\n",
      "Time 672.5 updated\n",
      "Time 673.0 updated\n",
      "Time 673.5 updated\n",
      "Time 674.0 updated\n",
      "Time 674.5 updated\n",
      "Time 675.0 updated\n",
      "Time 675.5 updated\n",
      "Time 676.0 updated\n",
      "Time 676.5 updated\n",
      "Time 677.0 updated\n",
      "Time 677.5 updated\n",
      "Time 678.0 updated\n",
      "Time 678.5 updated\n",
      "Time 679.0 updated\n",
      "Time 679.5 updated\n",
      "Time 680.0 updated\n",
      "Time 680.5 updated\n",
      "Time 681.0 updated\n",
      "Time 681.5 updated\n",
      "Time 682.0 updated\n",
      "Time 682.5 updated\n",
      "Time 683.0 updated\n",
      "Time 683.5 updated\n",
      "Time 684.0 updated\n",
      "Time 684.5 updated\n",
      "Time 685.0 updated\n",
      "Time 685.5 updated\n",
      "Time 686.0 updated\n",
      "Time 686.5 updated\n",
      "Time 687.0 updated\n",
      "Time 687.5 updated\n",
      "Time 688.0 updated\n",
      "Time 688.5 updated\n",
      "Time 689.0 updated\n",
      "Time 689.5 updated\n",
      "Time 690.0 updated\n",
      "Time 690.5 updated\n",
      "Time 691.0 updated\n",
      "Time 691.5 updated\n",
      "Time 692.0 updated\n",
      "Time 692.5 updated\n",
      "Time 693.0 updated\n",
      "Time 693.5 updated\n",
      "Time 694.0 updated\n",
      "Time 694.5 updated\n",
      "Time 695.0 updated\n",
      "Time 695.5 updated\n",
      "Time 696.0 updated\n",
      "Time 696.5 updated\n",
      "Time 697.0 updated\n",
      "Time 697.5 updated\n",
      "Time 698.0 updated\n",
      "Time 698.5 updated\n",
      "Time 699.0 updated\n",
      "Time 699.5 updated\n",
      "Time 700.0 updated\n",
      "Time 700.5 updated\n",
      "Time 701.0 updated\n",
      "Time 701.5 updated\n",
      "Time 702.0 updated\n",
      "Time 702.5 updated\n",
      "Time 703.0 updated\n",
      "Time 703.5 updated\n",
      "Time 704.0 updated\n",
      "Time 704.5 updated\n",
      "Time 705.0 updated\n",
      "Time 705.5 updated\n",
      "Time 706.0 updated\n",
      "Time 706.5 updated\n",
      "Time 707.0 updated\n",
      "Time 707.5 updated\n",
      "Time 708.0 updated\n",
      "Time 708.5 updated\n",
      "Time 709.0 updated\n",
      "Time 709.5 updated\n",
      "Time 710.0 updated\n",
      "Time 710.5 updated\n",
      "Time 711.0 updated\n",
      "Time 711.5 updated\n",
      "Time 712.0 updated\n",
      "Time 712.5 updated\n",
      "Time 713.0 updated\n",
      "Time 713.5 updated\n",
      "Time 714.0 updated\n",
      "Time 714.5 updated\n",
      "Time 715.0 updated\n",
      "Time 715.5 updated\n",
      "Time 716.0 updated\n",
      "Time 716.5 updated\n",
      "Time 717.0 updated\n",
      "Time 717.5 updated\n",
      "Time 718.0 updated\n",
      "Time 718.5 updated\n",
      "Time 719.0 updated\n",
      "Time 719.5 updated\n",
      "Time 720.0 updated\n",
      "Time 720.5 updated\n",
      "Time 721.0 updated\n",
      "Time 721.5 updated\n",
      "Time 722.0 updated\n",
      "Time 722.5 updated\n",
      "Time 723.0 updated\n",
      "Time 723.5 updated\n",
      "Time 724.0 updated\n",
      "Time 724.5 updated\n",
      "Time 725.0 updated\n",
      "Time 725.5 updated\n",
      "Time 726.0 updated\n",
      "Time 726.5 updated\n",
      "Time 727.0 updated\n",
      "Time 727.5 updated\n",
      "Time 728.0 updated\n",
      "Time 728.5 updated\n",
      "Time 729.0 updated\n",
      "Time 729.5 updated\n",
      "Time 730.0 updated\n",
      "Time 730.5 updated\n",
      "Time 731.0 updated\n",
      "Time 731.5 updated\n",
      "Time 732.0 updated\n",
      "Time 732.5 updated\n",
      "Time 733.0 updated\n",
      "Time 733.5 updated\n",
      "Time 734.0 updated\n",
      "Time 734.5 updated\n",
      "Time 735.0 updated\n",
      "Time 735.5 updated\n",
      "Time 736.0 updated\n",
      "Time 736.5 updated\n",
      "Time 737.0 updated\n",
      "Time 737.5 updated\n",
      "Time 738.0 updated\n",
      "Time 738.5 updated\n",
      "Time 739.0 updated\n",
      "Time 739.5 updated\n",
      "Time 740.0 updated\n",
      "Time 740.5 updated\n",
      "Time 741.0 updated\n",
      "Time 741.5 updated\n",
      "Time 742.0 updated\n",
      "Time 742.5 updated\n",
      "Time 743.0 updated\n",
      "Time 743.5 updated\n",
      "Time 744.0 updated\n",
      "Time 744.5 updated\n",
      "Time 745.0 updated\n",
      "Time 745.5 updated\n",
      "Time 746.0 updated\n",
      "Time 746.5 updated\n",
      "Time 747.0 updated\n",
      "Time 747.5 updated\n",
      "Time 748.0 updated\n",
      "Time 748.5 updated\n",
      "Time 749.0 updated\n",
      "Time 749.5 updated\n",
      "Time 750.0 updated\n",
      "Time 750.5 updated\n",
      "Time 751.0 updated\n",
      "Time 751.5 updated\n",
      "Time 752.0 updated\n",
      "Time 752.5 updated\n",
      "Time 753.0 updated\n",
      "Time 753.5 updated\n",
      "Time 754.0 updated\n",
      "Time 754.5 updated\n",
      "Time 755.0 updated\n",
      "Time 755.5 updated\n",
      "Time 756.0 updated\n",
      "Time 756.5 updated\n",
      "Time 757.0 updated\n",
      "Time 757.5 updated\n",
      "Time 758.0 updated\n",
      "Time 758.5 updated\n",
      "Time 759.0 updated\n",
      "Time 759.5 updated\n",
      "Time 760.0 updated\n",
      "Time 760.5 updated\n",
      "Time 761.0 updated\n",
      "Time 761.5 updated\n",
      "Time 762.0 updated\n",
      "Time 762.5 updated\n",
      "Time 763.0 updated\n",
      "Time 763.5 updated\n",
      "Time 764.0 updated\n",
      "Time 764.5 updated\n",
      "Time 765.0 updated\n",
      "Time 765.5 updated\n",
      "Time 766.0 updated\n",
      "Time 766.5 updated\n",
      "Time 767.0 updated\n",
      "Time 767.5 updated\n",
      "Time 768.0 updated\n",
      "Time 768.5 updated\n",
      "Time 769.0 updated\n",
      "Time 769.5 updated\n",
      "Time 770.0 updated\n",
      "Time 770.5 updated\n",
      "Time 771.0 updated\n",
      "Time 771.5 updated\n",
      "Time 772.0 updated\n",
      "Time 772.5 updated\n",
      "Time 773.0 updated\n",
      "Time 773.5 updated\n",
      "Time 774.0 updated\n",
      "Time 774.5 updated\n",
      "Time 775.0 updated\n",
      "Time 775.5 updated\n",
      "Time 776.0 updated\n",
      "Time 776.5 updated\n",
      "Time 777.0 updated\n",
      "Time 777.5 updated\n",
      "Time 778.0 updated\n",
      "Time 778.5 updated\n",
      "Time 779.0 updated\n",
      "Time 779.5 updated\n",
      "Time 780.0 updated\n",
      "Time 780.5 updated\n",
      "Time 781.0 updated\n",
      "Time 781.5 updated\n",
      "Time 782.0 updated\n",
      "Time 782.5 updated\n",
      "Time 783.0 updated\n",
      "Time 783.5 updated\n",
      "Time 784.0 updated\n",
      "Time 784.5 updated\n",
      "Time 785.0 updated\n",
      "Time 785.5 updated\n",
      "Time 786.0 updated\n",
      "Time 786.5 updated\n",
      "Time 787.0 updated\n",
      "Time 787.5 updated\n",
      "Time 788.0 updated\n",
      "Time 788.5 updated\n",
      "Time 789.0 updated\n",
      "Time 789.5 updated\n",
      "Time 790.0 updated\n",
      "Time 790.5 updated\n",
      "Time 791.0 updated\n",
      "Time 791.5 updated\n",
      "Time 792.0 updated\n",
      "Time 792.5 updated\n",
      "Time 793.0 updated\n",
      "Time 793.5 updated\n",
      "Time 794.0 updated\n",
      "Time 794.5 updated\n",
      "Time 795.0 updated\n",
      "Time 795.5 updated\n",
      "Time 796.0 updated\n",
      "Time 796.5 updated\n",
      "Time 797.0 updated\n",
      "Time 797.5 updated\n",
      "Time 798.0 updated\n",
      "Time 798.5 updated\n",
      "Time 799.0 updated\n",
      "Time 799.5 updated\n",
      "Time 800.0 updated\n",
      "Time 800.5 updated\n",
      "Time 801.0 updated\n",
      "Time 801.5 updated\n",
      "Time 802.0 updated\n",
      "Time 802.5 updated\n",
      "Time 803.0 updated\n",
      "Time 803.5 updated\n",
      "Time 804.0 updated\n",
      "Time 804.5 updated\n",
      "Time 805.0 updated\n",
      "Time 805.5 updated\n",
      "Time 806.0 updated\n",
      "Time 806.5 updated\n",
      "Time 807.0 updated\n",
      "Time 807.5 updated\n",
      "Time 808.0 updated\n",
      "Time 808.5 updated\n",
      "Time 809.0 updated\n",
      "Time 809.5 updated\n",
      "Time 810.0 updated\n",
      "Time 810.5 updated\n",
      "Time 811.0 updated\n",
      "Time 811.5 updated\n",
      "Time 812.0 updated\n",
      "Time 812.5 updated\n",
      "Time 813.0 updated\n",
      "Time 813.5 updated\n",
      "Time 814.0 updated\n",
      "Time 814.5 updated\n",
      "Time 815.0 updated\n",
      "Time 815.5 updated\n",
      "Time 816.0 updated\n",
      "Time 816.5 updated\n",
      "Time 817.0 updated\n",
      "Time 817.5 updated\n",
      "Time 818.0 updated\n",
      "Time 818.5 updated\n",
      "Time 819.0 updated\n",
      "Time 819.5 updated\n",
      "Time 820.0 updated\n",
      "Time 820.5 updated\n",
      "Time 821.0 updated\n",
      "Time 821.5 updated\n",
      "Time 822.0 updated\n",
      "Time 822.5 updated\n",
      "Time 823.0 updated\n",
      "Time 823.5 updated\n",
      "Time 824.0 updated\n",
      "Time 824.5 updated\n",
      "Time 825.0 updated\n",
      "Time 825.5 updated\n",
      "Time 826.0 updated\n",
      "Time 826.5 updated\n",
      "Time 827.0 updated\n",
      "Time 827.5 updated\n",
      "Time 828.0 updated\n",
      "Time 828.5 updated\n",
      "Time 829.0 updated\n",
      "Time 829.5 updated\n",
      "Time 830.0 updated\n",
      "Time 830.5 updated\n",
      "Time 831.0 updated\n",
      "Time 831.5 updated\n",
      "Time 832.0 updated\n",
      "Time 832.5 updated\n",
      "Time 833.0 updated\n",
      "Time 833.5 updated\n",
      "Time 834.0 updated\n",
      "Time 834.5 updated\n",
      "Time 835.0 updated\n",
      "Time 835.5 updated\n",
      "Time 836.0 updated\n",
      "Time 836.5 updated\n",
      "Time 837.0 updated\n",
      "Time 837.5 updated\n",
      "Time 838.0 updated\n",
      "Time 838.5 updated\n",
      "Time 839.0 updated\n",
      "Time 839.5 updated\n",
      "Time 840.0 updated\n",
      "Time 840.5 updated\n",
      "Time 841.0 updated\n",
      "Time 841.5 updated\n",
      "Time 842.0 updated\n",
      "Time 842.5 updated\n",
      "Time 843.0 updated\n",
      "Time 843.5 updated\n",
      "Time 844.0 updated\n",
      "Time 844.5 updated\n",
      "Time 845.0 updated\n",
      "Time 845.5 updated\n",
      "Time 846.0 updated\n",
      "Time 846.5 updated\n",
      "Time 847.0 updated\n",
      "Time 847.5 updated\n",
      "Time 848.0 updated\n",
      "Time 848.5 updated\n",
      "Time 849.0 updated\n",
      "Time 849.5 updated\n",
      "Time 850.0 updated\n",
      "Time 850.5 updated\n",
      "Time 851.0 updated\n",
      "Time 851.5 updated\n",
      "Time 852.0 updated\n",
      "Time 852.5 updated\n",
      "Time 853.0 updated\n",
      "Time 853.5 updated\n",
      "Time 854.0 updated\n",
      "Time 854.5 updated\n",
      "Time 855.0 updated\n",
      "Time 855.5 updated\n",
      "Time 856.0 updated\n",
      "Time 856.5 updated\n",
      "Time 857.0 updated\n",
      "Time 857.5 updated\n",
      "Time 858.0 updated\n",
      "Time 858.5 updated\n",
      "Time 859.0 updated\n",
      "Time 859.5 updated\n",
      "Time 860.0 updated\n",
      "Time 860.5 updated\n",
      "Time 861.0 updated\n",
      "Time 861.5 updated\n",
      "Time 862.0 updated\n",
      "Time 862.5 updated\n",
      "Time 863.0 updated\n",
      "Time 863.5 updated\n",
      "Time 864.0 updated\n",
      "Time 864.5 updated\n",
      "Time 865.0 updated\n",
      "Time 865.5 updated\n",
      "Time 866.0 updated\n",
      "Time 866.5 updated\n",
      "Time 867.0 updated\n",
      "Time 867.5 updated\n",
      "Time 868.0 updated\n",
      "Time 868.5 updated\n",
      "Time 869.0 updated\n",
      "Time 869.5 updated\n",
      "Time 870.0 updated\n",
      "Time 870.5 updated\n",
      "Time 871.0 updated\n",
      "Time 871.5 updated\n",
      "Time 872.0 updated\n",
      "Time 872.5 updated\n",
      "Time 873.0 updated\n",
      "Time 873.5 updated\n",
      "Time 874.0 updated\n",
      "Time 874.5 updated\n",
      "Time 875.0 updated\n",
      "Time 875.5 updated\n",
      "Time 876.0 updated\n",
      "Time 876.5 updated\n",
      "Time 877.0 updated\n",
      "Time 877.5 updated\n",
      "Time 878.0 updated\n",
      "Time 878.5 updated\n",
      "Time 879.0 updated\n",
      "Time 879.5 updated\n",
      "Time 880.0 updated\n",
      "Time 880.5 updated\n",
      "Time 881.0 updated\n",
      "Time 881.5 updated\n",
      "Time 882.0 updated\n",
      "Time 882.5 updated\n",
      "Time 883.0 updated\n",
      "Time 883.5 updated\n",
      "Time 884.0 updated\n",
      "Time 884.5 updated\n",
      "Time 885.0 updated\n",
      "Time 885.5 updated\n",
      "Time 886.0 updated\n",
      "Time 886.5 updated\n",
      "Time 887.0 updated\n",
      "Time 887.5 updated\n",
      "Time 888.0 updated\n",
      "Time 888.5 updated\n",
      "Time 889.0 updated\n",
      "Time 889.5 updated\n",
      "Time 890.0 updated\n",
      "Time 890.5 updated\n",
      "Time 891.0 updated\n",
      "Time 891.5 updated\n",
      "Time 892.0 updated\n",
      "Time 892.5 updated\n",
      "Time 893.0 updated\n",
      "Time 893.5 updated\n",
      "Time 894.0 updated\n",
      "Time 894.5 updated\n",
      "Time 895.0 updated\n",
      "Time 895.5 updated\n",
      "Time 896.0 updated\n",
      "Time 896.5 updated\n",
      "Time 897.0 updated\n",
      "Time 897.5 updated\n",
      "Time 898.0 updated\n",
      "Time 898.5 updated\n",
      "Time 899.0 updated\n",
      "Time 899.5 updated\n",
      "Time 900.0 updated\n",
      "Time 900.5 updated\n",
      "Time 901.0 updated\n",
      "Time 901.5 updated\n",
      "Time 902.0 updated\n",
      "Time 902.5 updated\n",
      "Time 903.0 updated\n",
      "Time 903.5 updated\n",
      "Time 904.0 updated\n",
      "Time 904.5 updated\n",
      "Time 905.0 updated\n",
      "Time 905.5 updated\n",
      "Time 906.0 updated\n",
      "Time 906.5 updated\n",
      "Time 907.0 updated\n",
      "Time 907.5 updated\n",
      "Time 908.0 updated\n",
      "Time 908.5 updated\n",
      "Time 909.0 updated\n",
      "Time 909.5 updated\n",
      "Time 910.0 updated\n",
      "Time 910.5 updated\n",
      "Time 911.0 updated\n",
      "Time 911.5 updated\n",
      "Time 912.0 updated\n",
      "Time 912.5 updated\n",
      "Time 913.0 updated\n",
      "Time 913.5 updated\n",
      "Time 914.0 updated\n",
      "Time 914.5 updated\n",
      "Time 915.0 updated\n",
      "Time 915.5 updated\n",
      "Time 916.0 updated\n",
      "Time 916.5 updated\n",
      "Time 917.0 updated\n",
      "Time 917.5 updated\n",
      "Time 918.0 updated\n",
      "Time 918.5 updated\n",
      "Time 919.0 updated\n",
      "Time 919.5 updated\n",
      "Time 920.0 updated\n",
      "Time 920.5 updated\n",
      "Time 921.0 updated\n",
      "Time 921.5 updated\n",
      "Time 922.0 updated\n",
      "Time 922.5 updated\n",
      "Time 923.0 updated\n",
      "Time 923.5 updated\n",
      "Time 924.0 updated\n",
      "Time 924.5 updated\n",
      "Time 925.0 updated\n",
      "Time 925.5 updated\n",
      "Time 926.0 updated\n",
      "Time 926.5 updated\n",
      "Time 927.0 updated\n",
      "Time 927.5 updated\n",
      "Time 928.0 updated\n",
      "Time 928.5 updated\n",
      "Time 929.0 updated\n",
      "Time 929.5 updated\n",
      "Time 930.0 updated\n",
      "Time 930.5 updated\n",
      "Time 931.0 updated\n",
      "Time 931.5 updated\n",
      "Time 932.0 updated\n",
      "Time 932.5 updated\n",
      "Time 933.0 updated\n",
      "Time 933.5 updated\n",
      "Time 934.0 updated\n",
      "Time 934.5 updated\n",
      "Time 935.0 updated\n",
      "Time 935.5 updated\n",
      "Time 936.0 updated\n",
      "Time 936.5 updated\n",
      "Time 937.0 updated\n",
      "Time 937.5 updated\n",
      "Time 938.0 updated\n",
      "Time 938.5 updated\n",
      "Time 939.0 updated\n",
      "Time 939.5 updated\n",
      "Time 940.0 updated\n",
      "Time 940.5 updated\n",
      "Time 941.0 updated\n",
      "Time 941.5 updated\n",
      "Time 942.0 updated\n",
      "Time 942.5 updated\n",
      "Time 943.0 updated\n",
      "Time 943.5 updated\n",
      "Time 944.0 updated\n",
      "Time 944.5 updated\n",
      "Time 945.0 updated\n",
      "Time 945.5 updated\n",
      "Time 946.0 updated\n",
      "Time 946.5 updated\n",
      "Time 947.0 updated\n",
      "Time 947.5 updated\n",
      "Time 948.0 updated\n",
      "Time 948.5 updated\n",
      "Time 949.0 updated\n",
      "Time 949.5 updated\n",
      "Time 950.0 updated\n",
      "Time 950.5 updated\n",
      "Time 951.0 updated\n",
      "Time 951.5 updated\n",
      "Time 952.0 updated\n",
      "Time 952.5 updated\n",
      "Time 953.0 updated\n",
      "Time 953.5 updated\n",
      "Time 954.0 updated\n",
      "Time 954.5 updated\n",
      "Time 955.0 updated\n",
      "Time 955.5 updated\n",
      "Time 956.0 updated\n",
      "Time 956.5 updated\n",
      "Time 957.0 updated\n",
      "Time 957.5 updated\n",
      "Time 958.0 updated\n",
      "Time 958.5 updated\n",
      "Time 959.0 updated\n",
      "Time 959.5 updated\n",
      "Time 960.0 updated\n",
      "Time 960.5 updated\n",
      "Time 961.0 updated\n",
      "Time 961.5 updated\n",
      "Time 962.0 updated\n",
      "Time 962.5 updated\n",
      "Time 963.0 updated\n",
      "Time 963.5 updated\n",
      "Time 964.0 updated\n",
      "Time 964.5 updated\n",
      "Time 965.0 updated\n",
      "Time 965.5 updated\n",
      "Time 966.0 updated\n",
      "Time 966.5 updated\n",
      "Time 967.0 updated\n",
      "Time 967.5 updated\n",
      "Time 968.0 updated\n",
      "Time 968.5 updated\n",
      "Time 969.0 updated\n",
      "Time 969.5 updated\n",
      "Time 970.0 updated\n",
      "Time 970.5 updated\n",
      "Time 971.0 updated\n",
      "Time 971.5 updated\n",
      "Time 972.0 updated\n",
      "Time 972.5 updated\n",
      "Time 973.0 updated\n",
      "Time 973.5 updated\n",
      "Time 974.0 updated\n",
      "Time 974.5 updated\n",
      "Time 975.0 updated\n",
      "Time 975.5 updated\n",
      "Time 976.0 updated\n",
      "Time 976.5 updated\n",
      "Time 977.0 updated\n",
      "Time 977.5 updated\n",
      "Time 978.0 updated\n",
      "Time 978.5 updated\n",
      "Time 979.0 updated\n",
      "Time 979.5 updated\n",
      "Time 980.0 updated\n",
      "Time 980.5 updated\n",
      "Time 981.0 updated\n",
      "Time 981.5 updated\n",
      "Time 982.0 updated\n",
      "Time 982.5 updated\n",
      "Time 983.0 updated\n",
      "Time 983.5 updated\n",
      "Time 984.0 updated\n",
      "Time 984.5 updated\n",
      "Time 985.0 updated\n",
      "Time 985.5 updated\n",
      "Time 986.0 updated\n",
      "Time 986.5 updated\n",
      "Time 987.0 updated\n",
      "Time 987.5 updated\n",
      "Time 988.0 updated\n",
      "Time 988.5 updated\n",
      "Time 989.0 updated\n",
      "Time 989.5 updated\n",
      "Time 990.0 updated\n",
      "Time 990.5 updated\n",
      "Time 991.0 updated\n",
      "Time 991.5 updated\n",
      "Time 992.0 updated\n",
      "Time 992.5 updated\n",
      "Time 993.0 updated\n",
      "Time 993.5 updated\n",
      "Time 994.0 updated\n",
      "Time 994.5 updated\n",
      "Time 995.0 updated\n",
      "Time 995.5 updated\n",
      "Time 996.0 updated\n",
      "Time 996.5 updated\n",
      "Time 997.0 updated\n",
      "Time 997.5 updated\n",
      "Time 998.0 updated\n",
      "Time 998.5 updated\n",
      "Time 999.0 updated\n",
      "Time 999.5 updated\n",
      "Time 1000.0 updated\n",
      "Time 1000.5 updated\n",
      "Time 1001.0 updated\n",
      "Time 1001.5 updated\n",
      "Time 1002.0 updated\n",
      "Time 1002.5 updated\n",
      "Time 1003.0 updated\n",
      "Time 1003.5 updated\n",
      "Time 1004.0 updated\n",
      "Time 1004.5 updated\n",
      "Time 1005.0 updated\n",
      "Time 1005.5 updated\n",
      "Time 1006.0 updated\n",
      "Time 1006.5 updated\n",
      "Time 1007.0 updated\n",
      "Time 1007.5 updated\n",
      "Time 1008.0 updated\n",
      "Time 1008.5 updated\n",
      "Time 1009.0 updated\n",
      "Time 1009.5 updated\n",
      "Time 1010.0 updated\n",
      "Time 1010.5 updated\n",
      "Time 1011.0 updated\n",
      "Time 1011.5 updated\n",
      "Time 1012.0 updated\n",
      "Time 1012.5 updated\n",
      "Time 1013.0 updated\n",
      "Time 1013.5 updated\n",
      "Time 1014.0 updated\n",
      "Time 1014.5 updated\n",
      "Time 1015.0 updated\n",
      "Time 1015.5 updated\n",
      "Time 1016.0 updated\n",
      "Time 1016.5 updated\n",
      "Time 1017.0 updated\n",
      "Time 1017.5 updated\n",
      "Time 1018.0 updated\n",
      "Time 1018.5 updated\n",
      "Time 1019.0 updated\n",
      "Time 1019.5 updated\n",
      "Time 1020.0 updated\n",
      "Time 1020.5 updated\n",
      "Time 1021.0 updated\n",
      "Time 1021.5 updated\n",
      "Time 1022.0 updated\n",
      "Time 1022.5 updated\n",
      "Time 1023.0 updated\n",
      "Time 1023.5 updated\n",
      "Time 1024.0 updated\n",
      "Time 1024.5 updated\n",
      "Time 1025.0 updated\n",
      "Time 1025.5 updated\n",
      "Time 1026.0 updated\n",
      "Time 1026.5 updated\n",
      "Time 1027.0 updated\n",
      "Time 1027.5 updated\n",
      "Time 1028.0 updated\n",
      "Time 1028.5 updated\n",
      "Time 1029.0 updated\n",
      "Time 1029.5 updated\n",
      "Time 1030.0 updated\n",
      "Time 1030.5 updated\n",
      "Time 1031.0 updated\n",
      "Time 1031.5 updated\n",
      "Time 1032.0 updated\n",
      "Time 1032.5 updated\n",
      "Time 1033.0 updated\n",
      "Time 1033.5 updated\n",
      "Time 1034.0 updated\n",
      "Time 1034.5 updated\n",
      "Time 1035.0 updated\n",
      "Time 1035.5 updated\n",
      "Time 1036.0 updated\n",
      "Time 1036.5 updated\n",
      "Time 1037.0 updated\n",
      "Time 1037.5 updated\n",
      "Time 1038.0 updated\n",
      "Time 1038.5 updated\n",
      "Time 1039.0 updated\n",
      "Time 1039.5 updated\n",
      "Time 1040.0 updated\n",
      "Time 1040.5 updated\n",
      "Time 1041.0 updated\n",
      "Time 1041.5 updated\n",
      "Time 1042.0 updated\n",
      "Time 1042.5 updated\n",
      "Time 1043.0 updated\n",
      "Time 1043.5 updated\n",
      "Time 1044.0 updated\n",
      "Time 1044.5 updated\n",
      "Time 1045.0 updated\n",
      "Time 1045.5 updated\n",
      "Time 1046.0 updated\n",
      "Time 1046.5 updated\n",
      "Time 1047.0 updated\n",
      "Time 1047.5 updated\n",
      "Time 1048.0 updated\n",
      "Time 1048.5 updated\n",
      "Time 1049.0 updated\n",
      "Time 1049.5 updated\n",
      "Time 1050.0 updated\n",
      "Time 1050.5 updated\n",
      "Time 1051.0 updated\n",
      "Time 1051.5 updated\n",
      "Time 1052.0 updated\n",
      "Time 1052.5 updated\n",
      "Time 1053.0 updated\n",
      "Time 1053.5 updated\n",
      "Time 1054.0 updated\n",
      "Time 1054.5 updated\n",
      "Time 1055.0 updated\n",
      "Time 1055.5 updated\n",
      "Time 1056.0 updated\n",
      "Time 1056.5 updated\n",
      "Time 1057.0 updated\n",
      "Time 1057.5 updated\n",
      "Time 1058.0 updated\n",
      "Time 1058.5 updated\n",
      "Time 1059.0 updated\n",
      "Time 1059.5 updated\n",
      "Time 1060.0 updated\n",
      "Time 1060.5 updated\n",
      "Time 1061.0 updated\n",
      "Time 1061.5 updated\n",
      "Time 1062.0 updated\n",
      "Time 1062.5 updated\n",
      "Time 1063.0 updated\n",
      "Time 1063.5 updated\n",
      "Time 1064.0 updated\n",
      "Time 1064.5 updated\n",
      "Time 1065.0 updated\n",
      "Time 1065.5 updated\n",
      "Time 1066.0 updated\n",
      "Time 1066.5 updated\n",
      "Time 1067.0 updated\n",
      "Time 1067.5 updated\n",
      "Time 1068.0 updated\n",
      "Time 1068.5 updated\n",
      "Time 1069.0 updated\n",
      "Time 1069.5 updated\n",
      "Time 1070.0 updated\n",
      "Time 1070.5 updated\n",
      "Time 1071.0 updated\n",
      "Time 1071.5 updated\n",
      "Time 1072.0 updated\n",
      "Time 1072.5 updated\n",
      "Time 1073.0 updated\n",
      "Time 1073.5 updated\n",
      "Time 1074.0 updated\n",
      "Time 1074.5 updated\n",
      "Time 1075.0 updated\n",
      "Time 1075.5 updated\n",
      "Time 1076.0 updated\n",
      "Time 1076.5 updated\n",
      "Time 1077.0 updated\n",
      "Time 1077.5 updated\n",
      "Time 1078.0 updated\n",
      "Time 1078.5 updated\n",
      "Time 1079.0 updated\n",
      "Time 1079.5 updated\n",
      "Time 1080.0 updated\n",
      "Time 1080.5 updated\n",
      "Time 1081.0 updated\n",
      "Time 1081.5 updated\n",
      "Time 1082.0 updated\n",
      "Time 1082.5 updated\n",
      "Time 1083.0 updated\n",
      "Time 1083.5 updated\n",
      "Time 1084.0 updated\n",
      "Time 1084.5 updated\n",
      "Time 1085.0 updated\n",
      "Time 1085.5 updated\n",
      "Time 1086.0 updated\n",
      "Time 1086.5 updated\n",
      "Time 1087.0 updated\n",
      "Time 1087.5 updated\n",
      "Time 1088.0 updated\n",
      "Time 1088.5 updated\n",
      "Time 1089.0 updated\n",
      "Time 1089.5 updated\n",
      "Time 1090.0 updated\n",
      "Time 1090.5 updated\n",
      "Time 1091.0 updated\n",
      "Time 1091.5 updated\n",
      "Time 1092.0 updated\n",
      "Time 1092.5 updated\n",
      "Time 1093.0 updated\n",
      "Time 1093.5 updated\n",
      "Time 1094.0 updated\n",
      "Time 1094.5 updated\n",
      "Time 1095.0 updated\n",
      "Time 1095.5 updated\n",
      "Time 1096.0 updated\n",
      "Time 1096.5 updated\n",
      "Time 1097.0 updated\n",
      "Time 1097.5 updated\n",
      "Time 1098.0 updated\n",
      "Time 1098.5 updated\n",
      "Time 1099.0 updated\n",
      "Time 1099.5 updated\n",
      "Time 1100.0 updated\n",
      "Time 1100.5 updated\n",
      "Time 1101.0 updated\n",
      "Time 1101.5 updated\n",
      "Time 1102.0 updated\n",
      "Time 1102.5 updated\n",
      "Time 1103.0 updated\n",
      "Time 1103.5 updated\n",
      "Time 1104.0 updated\n",
      "Time 1104.5 updated\n",
      "Time 1105.0 updated\n",
      "Time 1105.5 updated\n",
      "Time 1106.0 updated\n",
      "Time 1106.5 updated\n",
      "Time 1107.0 updated\n",
      "Time 1107.5 updated\n",
      "Time 1108.0 updated\n",
      "Time 1108.5 updated\n",
      "Time 1109.0 updated\n",
      "Time 1109.5 updated\n",
      "Time 1110.0 updated\n",
      "Time 1110.5 updated\n",
      "Time 1111.0 updated\n",
      "Time 1111.5 updated\n",
      "Time 1112.0 updated\n",
      "Time 1112.5 updated\n",
      "Time 1113.0 updated\n",
      "Time 1113.5 updated\n",
      "Time 1114.0 updated\n",
      "Time 1114.5 updated\n",
      "Time 1115.0 updated\n",
      "Time 1115.5 updated\n",
      "Time 1116.0 updated\n",
      "Time 1116.5 updated\n",
      "Time 1117.0 updated\n",
      "Time 1117.5 updated\n",
      "Time 1118.0 updated\n",
      "Time 1118.5 updated\n",
      "Time 1119.0 updated\n",
      "Time 1119.5 updated\n",
      "Time 1120.0 updated\n",
      "Time 1120.5 updated\n",
      "Time 1121.0 updated\n",
      "Time 1121.5 updated\n",
      "Time 1122.0 updated\n",
      "Time 1122.5 updated\n",
      "Time 1123.0 updated\n",
      "Time 1123.5 updated\n",
      "Time 1124.0 updated\n",
      "Time 1124.5 updated\n",
      "Time 1125.0 updated\n",
      "Time 1125.5 updated\n",
      "Time 1126.0 updated\n",
      "Time 1126.5 updated\n",
      "Time 1127.0 updated\n",
      "Time 1127.5 updated\n",
      "Time 1128.0 updated\n",
      "Time 1128.5 updated\n",
      "Time 1129.0 updated\n",
      "Time 1129.5 updated\n",
      "Time 1130.0 updated\n",
      "Time 1130.5 updated\n",
      "Time 1131.0 updated\n",
      "Time 1131.5 updated\n",
      "Time 1132.0 updated\n",
      "Time 1132.5 updated\n",
      "Time 1133.0 updated\n",
      "Time 1133.5 updated\n",
      "Time 1134.0 updated\n",
      "Time 1134.5 updated\n",
      "Time 1135.0 updated\n",
      "Time 1135.5 updated\n",
      "Time 1136.0 updated\n",
      "Time 1136.5 updated\n",
      "Time 1137.0 updated\n",
      "Time 1137.5 updated\n",
      "Time 1138.0 updated\n",
      "Time 1138.5 updated\n",
      "Time 1139.0 updated\n",
      "Time 1139.5 updated\n",
      "Time 1140.0 updated\n",
      "Time 1140.5 updated\n",
      "Time 1141.0 updated\n",
      "Time 1141.5 updated\n",
      "Time 1142.0 updated\n",
      "Time 1142.5 updated\n",
      "Time 1143.0 updated\n",
      "Time 1143.5 updated\n",
      "Time 1144.0 updated\n",
      "Time 1144.5 updated\n",
      "Time 1145.0 updated\n",
      "Time 1145.5 updated\n",
      "Time 1146.0 updated\n",
      "Time 1146.5 updated\n",
      "Time 1147.0 updated\n",
      "Time 1147.5 updated\n",
      "Time 1148.0 updated\n",
      "Time 1148.5 updated\n",
      "Time 1149.0 updated\n",
      "Time 1149.5 updated\n",
      "Time 1150.0 updated\n",
      "Time 1150.5 updated\n",
      "Time 1151.0 updated\n",
      "Time 1151.5 updated\n",
      "Time 1152.0 updated\n",
      "Time 1152.5 updated\n",
      "Time 1153.0 updated\n",
      "Time 1153.5 updated\n",
      "Time 1154.0 updated\n",
      "Time 1154.5 updated\n",
      "Time 1155.0 updated\n",
      "Time 1155.5 updated\n",
      "Time 1156.0 updated\n",
      "Time 1156.5 updated\n",
      "Time 1157.0 updated\n",
      "Time 1157.5 updated\n",
      "Time 1158.0 updated\n",
      "Time 1158.5 updated\n",
      "Time 1159.0 updated\n",
      "Time 1159.5 updated\n",
      "Time 1160.0 updated\n",
      "Time 1160.5 updated\n",
      "Time 1161.0 updated\n",
      "Time 1161.5 updated\n",
      "Time 1162.0 updated\n",
      "Time 1162.5 updated\n",
      "Time 1163.0 updated\n",
      "Time 1163.5 updated\n",
      "Time 1164.0 updated\n",
      "Time 1164.5 updated\n",
      "Time 1165.0 updated\n",
      "Time 1165.5 updated\n",
      "Time 1166.0 updated\n",
      "Time 1166.5 updated\n",
      "Time 1167.0 updated\n",
      "Time 1167.5 updated\n",
      "Time 1168.0 updated\n",
      "Time 1168.5 updated\n",
      "Time 1169.0 updated\n",
      "Time 1169.5 updated\n",
      "Time 1170.0 updated\n",
      "Time 1170.5 updated\n",
      "Time 1171.0 updated\n",
      "Time 1171.5 updated\n",
      "Time 1172.0 updated\n",
      "Time 1172.5 updated\n",
      "Time 1173.0 updated\n",
      "Time 1173.5 updated\n",
      "Time 1174.0 updated\n",
      "Time 1174.5 updated\n",
      "Time 1175.0 updated\n",
      "Time 1175.5 updated\n",
      "Time 1176.0 updated\n",
      "Time 1176.5 updated\n",
      "Time 1177.0 updated\n",
      "Time 1177.5 updated\n",
      "Time 1178.0 updated\n",
      "Time 1178.5 updated\n",
      "Time 1179.0 updated\n",
      "Time 1179.5 updated\n",
      "Time 1180.0 updated\n",
      "Time 1180.5 updated\n",
      "Time 1181.0 updated\n",
      "Time 1181.5 updated\n",
      "Time 1182.0 updated\n",
      "Time 1182.5 updated\n",
      "Time 1183.0 updated\n",
      "Time 1183.5 updated\n",
      "Time 1184.0 updated\n",
      "Time 1184.5 updated\n",
      "Time 1185.0 updated\n",
      "Time 1185.5 updated\n",
      "Time 1186.0 updated\n",
      "Time 1186.5 updated\n",
      "Time 1187.0 updated\n",
      "Time 1187.5 updated\n",
      "Time 1188.0 updated\n",
      "Time 1188.5 updated\n",
      "Time 1189.0 updated\n",
      "Time 1189.5 updated\n",
      "Time 1190.0 updated\n",
      "Time 1190.5 updated\n",
      "Time 1191.0 updated\n",
      "Time 1191.5 updated\n",
      "Time 1192.0 updated\n",
      "Time 1192.5 updated\n",
      "Time 1193.0 updated\n",
      "Time 1193.5 updated\n",
      "Time 1194.0 updated\n",
      "Time 1194.5 updated\n",
      "Time 1195.0 updated\n",
      "Time 1195.5 updated\n",
      "Time 1196.0 updated\n",
      "Time 1196.5 updated\n",
      "Time 1197.0 updated\n",
      "Time 1197.5 updated\n",
      "Time 1198.0 updated\n",
      "Time 1198.5 updated\n",
      "Time 1199.0 updated\n",
      "Time 1199.5 updated\n",
      "Time 1200.0 updated\n",
      "Time 1200.5 updated\n",
      "Time 1201.0 updated\n",
      "Time 1201.5 updated\n",
      "Time 1202.0 updated\n",
      "Time 1202.5 updated\n",
      "Time 1203.0 updated\n",
      "Time 1203.5 updated\n",
      "Time 1204.0 updated\n",
      "Time 1204.5 updated\n",
      "Time 1205.0 updated\n",
      "Time 1205.5 updated\n",
      "Time 1206.0 updated\n",
      "Time 1206.5 updated\n",
      "Time 1207.0 updated\n",
      "Time 1207.5 updated\n",
      "Time 1208.0 updated\n",
      "Time 1208.5 updated\n",
      "Time 1209.0 updated\n",
      "Time 1209.5 updated\n",
      "Time 1210.0 updated\n",
      "Time 1210.5 updated\n",
      "Time 1211.0 updated\n",
      "Time 1211.5 updated\n",
      "Time 1212.0 updated\n",
      "Time 1212.5 updated\n",
      "Time 1213.0 updated\n",
      "Time 1213.5 updated\n",
      "Time 1214.0 updated\n",
      "Time 1214.5 updated\n",
      "Time 1215.0 updated\n",
      "Time 1215.5 updated\n",
      "Time 1216.0 updated\n",
      "Time 1216.5 updated\n",
      "Time 1217.0 updated\n",
      "Time 1217.5 updated\n",
      "Time 1218.0 updated\n",
      "Time 1218.5 updated\n",
      "Time 1219.0 updated\n",
      "Time 1219.5 updated\n",
      "Time 1220.0 updated\n",
      "Time 1220.5 updated\n",
      "Time 1221.0 updated\n",
      "Time 1221.5 updated\n",
      "Time 1222.0 updated\n",
      "Time 1222.5 updated\n",
      "Time 1223.0 updated\n",
      "Time 1223.5 updated\n",
      "Time 1224.0 updated\n",
      "Time 1224.5 updated\n",
      "Time 1225.0 updated\n",
      "Time 1225.5 updated\n",
      "Time 1226.0 updated\n",
      "Time 1226.5 updated\n",
      "Time 1227.0 updated\n",
      "Time 1227.5 updated\n",
      "Time 1228.0 updated\n",
      "Time 1228.5 updated\n",
      "Time 1229.0 updated\n",
      "Time 1229.5 updated\n",
      "Time 1230.0 updated\n",
      "Time 1230.5 updated\n",
      "Time 1231.0 updated\n",
      "Time 1231.5 updated\n",
      "Time 1232.0 updated\n",
      "Time 1232.5 updated\n",
      "Time 1233.0 updated\n",
      "Time 1233.5 updated\n",
      "Time 1234.0 updated\n",
      "Time 1234.5 updated\n",
      "Time 1235.0 updated\n",
      "Time 1235.5 updated\n",
      "Time 1236.0 updated\n",
      "Time 1236.5 updated\n",
      "Time 1237.0 updated\n",
      "Time 1237.5 updated\n",
      "Time 1238.0 updated\n",
      "Time 1238.5 updated\n",
      "Time 1239.0 updated\n",
      "Time 1239.5 updated\n",
      "Time 1240.0 updated\n",
      "Time 1240.5 updated\n",
      "Time 1241.0 updated\n",
      "Time 1241.5 updated\n",
      "Time 1242.0 updated\n",
      "Time 1242.5 updated\n",
      "Time 1243.0 updated\n",
      "Time 1243.5 updated\n",
      "Time 1244.0 updated\n",
      "Time 1244.5 updated\n",
      "Time 1245.0 updated\n",
      "Time 1245.5 updated\n",
      "Time 1246.0 updated\n",
      "Time 1246.5 updated\n",
      "Time 1247.0 updated\n",
      "Time 1247.5 updated\n",
      "Time 1248.0 updated\n",
      "Time 1248.5 updated\n",
      "Time 1249.0 updated\n",
      "Time 1249.5 updated\n",
      "Time 1250.0 updated\n",
      "Time 1250.5 updated\n",
      "Time 1251.0 updated\n",
      "Time 1251.5 updated\n",
      "Time 1252.0 updated\n",
      "Time 1252.5 updated\n",
      "Time 1253.0 updated\n",
      "Time 1253.5 updated\n",
      "Time 1254.0 updated\n",
      "Time 1254.5 updated\n",
      "Time 1255.0 updated\n",
      "Time 1255.5 updated\n",
      "Time 1256.0 updated\n",
      "Time 1256.5 updated\n",
      "Time 1257.0 updated\n",
      "Time 1257.5 updated\n",
      "Time 1258.0 updated\n",
      "Time 1258.5 updated\n",
      "Time 1259.0 updated\n",
      "Time 1259.5 updated\n",
      "Time 1260.0 updated\n",
      "Time 1260.5 updated\n",
      "Time 1261.0 updated\n",
      "Time 1261.5 updated\n",
      "Time 1262.0 updated\n",
      "Time 1262.5 updated\n",
      "Time 1263.0 updated\n",
      "Time 1263.5 updated\n",
      "Time 1264.0 updated\n",
      "Time 1264.5 updated\n",
      "Time 1265.0 updated\n",
      "Time 1265.5 updated\n",
      "Time 1266.0 updated\n",
      "Time 1266.5 updated\n",
      "Time 1267.0 updated\n",
      "Time 1267.5 updated\n",
      "Time 1268.0 updated\n",
      "Time 1268.5 updated\n",
      "Time 1269.0 updated\n",
      "Time 1269.5 updated\n",
      "Time 1270.0 updated\n",
      "Time 1270.5 updated\n",
      "Time 1271.0 updated\n",
      "Time 1271.5 updated\n",
      "Time 1272.0 updated\n",
      "Time 1272.5 updated\n",
      "Time 1273.0 updated\n",
      "Time 1273.5 updated\n",
      "Time 1274.0 updated\n",
      "Time 1274.5 updated\n",
      "Time 1275.0 updated\n",
      "Time 1275.5 updated\n",
      "Time 1276.0 updated\n",
      "Time 1276.5 updated\n",
      "Time 1277.0 updated\n",
      "Time 1277.5 updated\n",
      "Time 1278.0 updated\n",
      "Time 1278.5 updated\n",
      "Time 1279.0 updated\n",
      "Time 1279.5 updated\n",
      "Time 1280.0 updated\n",
      "Time 1280.5 updated\n",
      "Time 1281.0 updated\n",
      "Time 1281.5 updated\n",
      "Time 1282.0 updated\n",
      "Time 1282.5 updated\n",
      "Time 1283.0 updated\n",
      "Time 1283.5 updated\n",
      "Time 1284.0 updated\n",
      "Time 1284.5 updated\n",
      "Time 1285.0 updated\n",
      "Time 1285.5 updated\n",
      "Time 1286.0 updated\n",
      "Time 1286.5 updated\n",
      "Time 1287.0 updated\n",
      "Time 1287.5 updated\n",
      "Time 1288.0 updated\n",
      "Time 1288.5 updated\n",
      "Time 1289.0 updated\n",
      "Time 1289.5 updated\n",
      "Time 1290.0 updated\n",
      "Time 1290.5 updated\n",
      "Time 1291.0 updated\n",
      "Time 1291.5 updated\n",
      "Time 1292.0 updated\n",
      "Time 1292.5 updated\n",
      "Time 1293.0 updated\n",
      "Time 1293.5 updated\n",
      "Time 1294.0 updated\n",
      "Time 1294.5 updated\n",
      "Time 1295.0 updated\n",
      "Time 1295.5 updated\n",
      "Time 1296.0 updated\n",
      "Time 1296.5 updated\n",
      "Time 1297.0 updated\n",
      "Time 1297.5 updated\n",
      "Time 1298.0 updated\n",
      "Time 1298.5 updated\n",
      "Time 1299.0 updated\n",
      "Time 1299.5 updated\n",
      "Time 1300.0 updated\n",
      "Time 1300.5 updated\n",
      "Time 1301.0 updated\n",
      "Time 1301.5 updated\n",
      "Time 1302.0 updated\n",
      "Time 1302.5 updated\n",
      "Time 1303.0 updated\n",
      "Time 1303.5 updated\n",
      "Time 1304.0 updated\n",
      "Time 1304.5 updated\n",
      "Time 1305.0 updated\n",
      "Time 1305.5 updated\n",
      "Time 1306.0 updated\n",
      "Time 1306.5 updated\n",
      "Time 1307.0 updated\n",
      "Time 1307.5 updated\n",
      "Time 1308.0 updated\n",
      "Time 1308.5 updated\n",
      "Time 1309.0 updated\n",
      "Time 1309.5 updated\n",
      "Time 1310.0 updated\n",
      "Time 1310.5 updated\n",
      "Time 1311.0 updated\n",
      "Time 1311.5 updated\n",
      "Time 1312.0 updated\n",
      "Time 1312.5 updated\n",
      "Time 1313.0 updated\n",
      "Time 1313.5 updated\n",
      "Time 1314.0 updated\n",
      "Time 1314.5 updated\n",
      "Time 1315.0 updated\n",
      "Time 1315.5 updated\n",
      "Time 1316.0 updated\n",
      "Time 1316.5 updated\n",
      "Time 1317.0 updated\n",
      "Time 1317.5 updated\n",
      "Time 1318.0 updated\n",
      "Time 1318.5 updated\n",
      "Time 1319.0 updated\n",
      "Time 1319.5 updated\n",
      "Time 1320.0 updated\n",
      "Time 1320.5 updated\n",
      "Time 1321.0 updated\n",
      "Time 1321.5 updated\n",
      "Time 1322.0 updated\n",
      "Time 1322.5 updated\n",
      "Time 1323.0 updated\n",
      "Time 1323.5 updated\n",
      "Time 1324.0 updated\n",
      "Time 1324.5 updated\n",
      "Time 1325.0 updated\n",
      "Time 1325.5 updated\n",
      "Time 1326.0 updated\n",
      "Time 1326.5 updated\n",
      "Time 1327.0 updated\n",
      "Time 1327.5 updated\n",
      "Time 1328.0 updated\n",
      "Time 1328.5 updated\n",
      "Time 1329.0 updated\n",
      "Time 1329.5 updated\n",
      "Time 1330.0 updated\n",
      "Time 1330.5 updated\n",
      "Time 1331.0 updated\n",
      "Time 1331.5 updated\n",
      "Time 1332.0 updated\n",
      "Time 1332.5 updated\n",
      "Time 1333.0 updated\n",
      "Time 1333.5 updated\n",
      "Time 1334.0 updated\n",
      "Time 1334.5 updated\n",
      "Time 1335.0 updated\n",
      "Time 1335.5 updated\n",
      "Time 1336.0 updated\n",
      "Time 1336.5 updated\n",
      "Time 1337.0 updated\n",
      "Time 1337.5 updated\n",
      "Time 1338.0 updated\n",
      "Time 1338.5 updated\n",
      "Time 1339.0 updated\n",
      "Time 1339.5 updated\n",
      "Time 1340.0 updated\n",
      "Time 1340.5 updated\n",
      "Time 1341.0 updated\n",
      "Time 1341.5 updated\n",
      "Time 1342.0 updated\n",
      "Time 1342.5 updated\n",
      "Time 1343.0 updated\n",
      "Time 1343.5 updated\n",
      "Time 1344.0 updated\n",
      "Time 1344.5 updated\n",
      "Time 1345.0 updated\n",
      "Time 1345.5 updated\n",
      "Time 1346.0 updated\n",
      "Time 1346.5 updated\n",
      "Time 1347.0 updated\n",
      "Time 1347.5 updated\n",
      "Time 1348.0 updated\n",
      "Time 1348.5 updated\n",
      "Time 1349.0 updated\n",
      "Time 1349.5 updated\n",
      "Time 1350.0 updated\n",
      "Time 1350.5 updated\n",
      "Time 1351.0 updated\n",
      "Time 1351.5 updated\n",
      "Time 1352.0 updated\n",
      "Time 1352.5 updated\n",
      "Time 1353.0 updated\n",
      "Time 1353.5 updated\n",
      "Time 1354.0 updated\n",
      "Time 1354.5 updated\n",
      "Time 1355.0 updated\n",
      "Time 1355.5 updated\n",
      "Time 1356.0 updated\n",
      "Time 1356.5 updated\n",
      "Time 1357.0 updated\n",
      "Time 1357.5 updated\n",
      "Time 1358.0 updated\n",
      "Time 1358.5 updated\n",
      "Time 1359.0 updated\n",
      "Time 1359.5 updated\n",
      "Time 1360.0 updated\n",
      "Time 1360.5 updated\n",
      "Time 1361.0 updated\n",
      "Time 1361.5 updated\n",
      "Time 1362.0 updated\n",
      "Time 1362.5 updated\n",
      "Time 1363.0 updated\n",
      "Time 1363.5 updated\n",
      "Time 1364.0 updated\n",
      "Time 1364.5 updated\n",
      "Time 1365.0 updated\n",
      "Time 1365.5 updated\n",
      "Time 1366.0 updated\n",
      "Time 1366.5 updated\n",
      "Time 1367.0 updated\n",
      "Time 1367.5 updated\n",
      "Time 1368.0 updated\n",
      "Time 1368.5 updated\n",
      "Time 1369.0 updated\n",
      "Time 1369.5 updated\n",
      "Time 1370.0 updated\n",
      "Time 1370.5 updated\n",
      "Time 1371.0 updated\n",
      "Time 1371.5 updated\n",
      "Time 1372.0 updated\n",
      "Time 1372.5 updated\n",
      "Time 1373.0 updated\n",
      "Time 1373.5 updated\n",
      "Time 1374.0 updated\n",
      "Time 1374.5 updated\n",
      "Time 1375.0 updated\n",
      "Time 1375.5 updated\n",
      "Time 1376.0 updated\n",
      "Time 1376.5 updated\n",
      "Time 1377.0 updated\n",
      "Time 1377.5 updated\n",
      "Time 1378.0 updated\n",
      "Time 1378.5 updated\n",
      "Time 1379.0 updated\n",
      "Time 1379.5 updated\n",
      "Time 1380.0 updated\n",
      "Time 1380.5 updated\n",
      "Time 1381.0 updated\n",
      "Time 1381.5 updated\n",
      "Time 1382.0 updated\n",
      "Time 1382.5 updated\n",
      "Time 1383.0 updated\n",
      "Time 1383.5 updated\n",
      "Time 1384.0 updated\n",
      "Time 1384.5 updated\n",
      "Time 1385.0 updated\n",
      "Time 1385.5 updated\n",
      "Time 1386.0 updated\n",
      "Time 1386.5 updated\n",
      "Time 1387.0 updated\n",
      "Time 1387.5 updated\n",
      "Time 1388.0 updated\n",
      "Time 1388.5 updated\n",
      "Time 1389.0 updated\n",
      "Time 1389.5 updated\n",
      "Time 1390.0 updated\n",
      "Time 1390.5 updated\n",
      "Time 1391.0 updated\n",
      "Time 1391.5 updated\n",
      "Time 1392.0 updated\n",
      "Time 1392.5 updated\n",
      "Time 1393.0 updated\n",
      "Time 1393.5 updated\n",
      "Time 1394.0 updated\n",
      "Time 1394.5 updated\n",
      "Time 1395.0 updated\n",
      "Time 1395.5 updated\n",
      "Time 1396.0 updated\n",
      "Time 1396.5 updated\n",
      "Time 1397.0 updated\n",
      "Time 1397.5 updated\n",
      "Time 1398.0 updated\n",
      "Time 1398.5 updated\n",
      "Time 1399.0 updated\n",
      "Time 1399.5 updated\n",
      "Time 1400.0 updated\n",
      "Time 1400.5 updated\n"
     ]
    }
   ],
   "source": [
    "diff_sample_2 = generation(model, 250, n, 0.5, 4 * n, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4092ef01-f08c-473d-b756-d3e4c921e068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_sample = torch.concat([diff_sample_1, diff_sample_2], dim=0)\n",
    "fn = torch.norm(diff_sample, p='fro', dim=(1, 2))\n",
    "\n",
    "fig, ax = plt.subplots()  # width, height in inches\n",
    "# Plot histogram\n",
    "ax.hist(fn.detach().cpu().numpy(),\n",
    "        bins='fd',                     # Freedman–Diaconis rule\n",
    "        density=True,                  # or density=True for area=1\n",
    "        facecolor='lightblue',                # light gray\n",
    "        edgecolor='black', \n",
    "        alpha=0.6,\n",
    "        linewidth=0.5, label=\"Diffusion samples\")\n",
    "\n",
    "# Clean up spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "# Ticks outside\n",
    "ax.tick_params(direction='out', width=0.5)\n",
    "\n",
    "# Labels (match your manuscript font & size)\n",
    "ax.set_xlabel('Frobenius norm', fontsize=9)\n",
    "ax.set_ylabel('density', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('histogram_1.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3fd9a101-ce00-4783-9874-12ff8a7b8b20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.5 updated\n",
      "Time 1.0 updated\n",
      "Time 1.5 updated\n",
      "Time 2.0 updated\n",
      "Time 2.5 updated\n",
      "Time 3.0 updated\n",
      "Time 3.5 updated\n",
      "Time 4.0 updated\n",
      "Time 4.5 updated\n",
      "Time 5.0 updated\n",
      "Time 5.5 updated\n",
      "Time 6.0 updated\n",
      "Time 6.5 updated\n",
      "Time 7.0 updated\n",
      "Time 7.5 updated\n",
      "Time 8.0 updated\n",
      "Time 8.5 updated\n",
      "Time 9.0 updated\n",
      "Time 9.5 updated\n",
      "Time 10.0 updated\n",
      "Time 10.5 updated\n",
      "Time 11.0 updated\n",
      "Time 11.5 updated\n",
      "Time 12.0 updated\n",
      "Time 12.5 updated\n",
      "Time 13.0 updated\n",
      "Time 13.5 updated\n",
      "Time 14.0 updated\n",
      "Time 14.5 updated\n",
      "Time 15.0 updated\n",
      "Time 15.5 updated\n",
      "Time 16.0 updated\n",
      "Time 16.5 updated\n",
      "Time 17.0 updated\n",
      "Time 17.5 updated\n",
      "Time 18.0 updated\n",
      "Time 18.5 updated\n",
      "Time 19.0 updated\n",
      "Time 19.5 updated\n",
      "Time 20.0 updated\n",
      "Time 20.5 updated\n",
      "Time 21.0 updated\n",
      "Time 21.5 updated\n",
      "Time 22.0 updated\n",
      "Time 22.5 updated\n",
      "Time 23.0 updated\n",
      "Time 23.5 updated\n",
      "Time 24.0 updated\n",
      "Time 24.5 updated\n",
      "Time 25.0 updated\n",
      "Time 25.5 updated\n",
      "Time 26.0 updated\n",
      "Time 26.5 updated\n",
      "Time 27.0 updated\n",
      "Time 27.5 updated\n",
      "Time 28.0 updated\n",
      "Time 28.5 updated\n",
      "Time 29.0 updated\n",
      "Time 29.5 updated\n",
      "Time 30.0 updated\n",
      "Time 30.5 updated\n",
      "Time 31.0 updated\n",
      "Time 31.5 updated\n",
      "Time 32.0 updated\n",
      "Time 32.5 updated\n",
      "Time 33.0 updated\n",
      "Time 33.5 updated\n",
      "Time 34.0 updated\n",
      "Time 34.5 updated\n",
      "Time 35.0 updated\n",
      "Time 35.5 updated\n",
      "Time 36.0 updated\n",
      "Time 36.5 updated\n",
      "Time 37.0 updated\n",
      "Time 37.5 updated\n",
      "Time 38.0 updated\n",
      "Time 38.5 updated\n",
      "Time 39.0 updated\n",
      "Time 39.5 updated\n",
      "Time 40.0 updated\n",
      "Time 40.5 updated\n",
      "Time 41.0 updated\n",
      "Time 41.5 updated\n",
      "Time 42.0 updated\n",
      "Time 42.5 updated\n",
      "Time 43.0 updated\n",
      "Time 43.5 updated\n",
      "Time 44.0 updated\n",
      "Time 44.5 updated\n",
      "Time 45.0 updated\n",
      "Time 45.5 updated\n",
      "Time 46.0 updated\n",
      "Time 46.5 updated\n",
      "Time 47.0 updated\n",
      "Time 47.5 updated\n",
      "Time 48.0 updated\n",
      "Time 48.5 updated\n",
      "Time 49.0 updated\n",
      "Time 49.5 updated\n",
      "Time 50.0 updated\n",
      "Time 50.5 updated\n",
      "Time 51.0 updated\n",
      "Time 51.5 updated\n",
      "Time 52.0 updated\n",
      "Time 52.5 updated\n",
      "Time 53.0 updated\n",
      "Time 53.5 updated\n",
      "Time 54.0 updated\n",
      "Time 54.5 updated\n",
      "Time 55.0 updated\n",
      "Time 55.5 updated\n",
      "Time 56.0 updated\n",
      "Time 56.5 updated\n",
      "Time 57.0 updated\n",
      "Time 57.5 updated\n",
      "Time 58.0 updated\n",
      "Time 58.5 updated\n",
      "Time 59.0 updated\n",
      "Time 59.5 updated\n",
      "Time 60.0 updated\n",
      "Time 60.5 updated\n",
      "Time 61.0 updated\n",
      "Time 61.5 updated\n",
      "Time 62.0 updated\n",
      "Time 62.5 updated\n",
      "Time 63.0 updated\n",
      "Time 63.5 updated\n",
      "Time 64.0 updated\n",
      "Time 64.5 updated\n",
      "Time 65.0 updated\n",
      "Time 65.5 updated\n",
      "Time 66.0 updated\n",
      "Time 66.5 updated\n",
      "Time 67.0 updated\n",
      "Time 67.5 updated\n",
      "Time 68.0 updated\n",
      "Time 68.5 updated\n",
      "Time 69.0 updated\n",
      "Time 69.5 updated\n",
      "Time 70.0 updated\n",
      "Time 70.5 updated\n",
      "Time 71.0 updated\n",
      "Time 71.5 updated\n",
      "Time 72.0 updated\n",
      "Time 72.5 updated\n",
      "Time 73.0 updated\n",
      "Time 73.5 updated\n",
      "Time 74.0 updated\n",
      "Time 74.5 updated\n",
      "Time 75.0 updated\n",
      "Time 75.5 updated\n",
      "Time 76.0 updated\n",
      "Time 76.5 updated\n",
      "Time 77.0 updated\n",
      "Time 77.5 updated\n",
      "Time 78.0 updated\n",
      "Time 78.5 updated\n",
      "Time 79.0 updated\n",
      "Time 79.5 updated\n",
      "Time 80.0 updated\n",
      "Time 80.5 updated\n",
      "Time 81.0 updated\n",
      "Time 81.5 updated\n",
      "Time 82.0 updated\n",
      "Time 82.5 updated\n",
      "Time 83.0 updated\n",
      "Time 83.5 updated\n",
      "Time 84.0 updated\n",
      "Time 84.5 updated\n",
      "Time 85.0 updated\n",
      "Time 85.5 updated\n",
      "Time 86.0 updated\n",
      "Time 86.5 updated\n",
      "Time 87.0 updated\n",
      "Time 87.5 updated\n",
      "Time 88.0 updated\n",
      "Time 88.5 updated\n",
      "Time 89.0 updated\n",
      "Time 89.5 updated\n",
      "Time 90.0 updated\n",
      "Time 90.5 updated\n",
      "Time 91.0 updated\n",
      "Time 91.5 updated\n",
      "Time 92.0 updated\n",
      "Time 92.5 updated\n",
      "Time 93.0 updated\n",
      "Time 93.5 updated\n",
      "Time 94.0 updated\n",
      "Time 94.5 updated\n",
      "Time 95.0 updated\n",
      "Time 95.5 updated\n",
      "Time 96.0 updated\n",
      "Time 96.5 updated\n",
      "Time 97.0 updated\n",
      "Time 97.5 updated\n",
      "Time 98.0 updated\n",
      "Time 98.5 updated\n",
      "Time 99.0 updated\n",
      "Time 99.5 updated\n",
      "Time 100.0 updated\n",
      "Time 100.5 updated\n",
      "Time 101.0 updated\n",
      "Time 101.5 updated\n",
      "Time 102.0 updated\n",
      "Time 102.5 updated\n",
      "Time 103.0 updated\n",
      "Time 103.5 updated\n",
      "Time 104.0 updated\n",
      "Time 104.5 updated\n",
      "Time 105.0 updated\n",
      "Time 105.5 updated\n",
      "Time 106.0 updated\n",
      "Time 106.5 updated\n",
      "Time 107.0 updated\n",
      "Time 107.5 updated\n",
      "Time 108.0 updated\n",
      "Time 108.5 updated\n",
      "Time 109.0 updated\n",
      "Time 109.5 updated\n",
      "Time 110.0 updated\n",
      "Time 110.5 updated\n",
      "Time 111.0 updated\n",
      "Time 111.5 updated\n",
      "Time 112.0 updated\n",
      "Time 112.5 updated\n",
      "Time 113.0 updated\n",
      "Time 113.5 updated\n",
      "Time 114.0 updated\n",
      "Time 114.5 updated\n",
      "Time 115.0 updated\n",
      "Time 115.5 updated\n",
      "Time 116.0 updated\n",
      "Time 116.5 updated\n",
      "Time 117.0 updated\n",
      "Time 117.5 updated\n",
      "Time 118.0 updated\n",
      "Time 118.5 updated\n",
      "Time 119.0 updated\n",
      "Time 119.5 updated\n",
      "Time 120.0 updated\n",
      "Time 120.5 updated\n",
      "Time 121.0 updated\n",
      "Time 121.5 updated\n",
      "Time 122.0 updated\n",
      "Time 122.5 updated\n",
      "Time 123.0 updated\n",
      "Time 123.5 updated\n",
      "Time 124.0 updated\n",
      "Time 124.5 updated\n",
      "Time 125.0 updated\n",
      "Time 125.5 updated\n",
      "Time 126.0 updated\n",
      "Time 126.5 updated\n",
      "Time 127.0 updated\n",
      "Time 127.5 updated\n",
      "Time 128.0 updated\n",
      "Time 128.5 updated\n",
      "Time 129.0 updated\n",
      "Time 129.5 updated\n",
      "Time 130.0 updated\n",
      "Time 130.5 updated\n",
      "Time 131.0 updated\n",
      "Time 131.5 updated\n",
      "Time 132.0 updated\n",
      "Time 132.5 updated\n",
      "Time 133.0 updated\n",
      "Time 133.5 updated\n",
      "Time 134.0 updated\n",
      "Time 134.5 updated\n",
      "Time 135.0 updated\n",
      "Time 135.5 updated\n",
      "Time 136.0 updated\n",
      "Time 136.5 updated\n",
      "Time 137.0 updated\n",
      "Time 137.5 updated\n",
      "Time 138.0 updated\n",
      "Time 138.5 updated\n",
      "Time 139.0 updated\n",
      "Time 139.5 updated\n",
      "Time 140.0 updated\n",
      "Time 140.5 updated\n",
      "Time 141.0 updated\n",
      "Time 141.5 updated\n",
      "Time 142.0 updated\n",
      "Time 142.5 updated\n",
      "Time 143.0 updated\n",
      "Time 143.5 updated\n",
      "Time 144.0 updated\n",
      "Time 144.5 updated\n",
      "Time 145.0 updated\n",
      "Time 145.5 updated\n",
      "Time 146.0 updated\n",
      "Time 146.5 updated\n",
      "Time 147.0 updated\n",
      "Time 147.5 updated\n",
      "Time 148.0 updated\n",
      "Time 148.5 updated\n",
      "Time 149.0 updated\n",
      "Time 149.5 updated\n",
      "Time 150.0 updated\n",
      "Time 150.5 updated\n",
      "Time 151.0 updated\n",
      "Time 151.5 updated\n",
      "Time 152.0 updated\n",
      "Time 152.5 updated\n",
      "Time 153.0 updated\n",
      "Time 153.5 updated\n",
      "Time 154.0 updated\n",
      "Time 154.5 updated\n",
      "Time 155.0 updated\n",
      "Time 155.5 updated\n",
      "Time 156.0 updated\n",
      "Time 156.5 updated\n",
      "Time 157.0 updated\n",
      "Time 157.5 updated\n",
      "Time 158.0 updated\n",
      "Time 158.5 updated\n",
      "Time 159.0 updated\n",
      "Time 159.5 updated\n",
      "Time 160.0 updated\n",
      "Time 160.5 updated\n",
      "Time 161.0 updated\n",
      "Time 161.5 updated\n",
      "Time 162.0 updated\n",
      "Time 162.5 updated\n",
      "Time 163.0 updated\n",
      "Time 163.5 updated\n",
      "Time 164.0 updated\n",
      "Time 164.5 updated\n",
      "Time 165.0 updated\n",
      "Time 165.5 updated\n",
      "Time 166.0 updated\n",
      "Time 166.5 updated\n",
      "Time 167.0 updated\n",
      "Time 167.5 updated\n",
      "Time 168.0 updated\n",
      "Time 168.5 updated\n",
      "Time 169.0 updated\n",
      "Time 169.5 updated\n",
      "Time 170.0 updated\n",
      "Time 170.5 updated\n",
      "Time 171.0 updated\n",
      "Time 171.5 updated\n",
      "Time 172.0 updated\n",
      "Time 172.5 updated\n",
      "Time 173.0 updated\n",
      "Time 173.5 updated\n",
      "Time 174.0 updated\n",
      "Time 174.5 updated\n",
      "Time 175.0 updated\n",
      "Time 175.5 updated\n",
      "Time 176.0 updated\n",
      "Time 176.5 updated\n",
      "Time 177.0 updated\n",
      "Time 177.5 updated\n",
      "Time 178.0 updated\n",
      "Time 178.5 updated\n",
      "Time 179.0 updated\n",
      "Time 179.5 updated\n",
      "Time 180.0 updated\n",
      "Time 180.5 updated\n",
      "Time 181.0 updated\n",
      "Time 181.5 updated\n",
      "Time 182.0 updated\n",
      "Time 182.5 updated\n",
      "Time 183.0 updated\n",
      "Time 183.5 updated\n",
      "Time 184.0 updated\n",
      "Time 184.5 updated\n",
      "Time 185.0 updated\n",
      "Time 185.5 updated\n",
      "Time 186.0 updated\n",
      "Time 186.5 updated\n",
      "Time 187.0 updated\n",
      "Time 187.5 updated\n",
      "Time 188.0 updated\n",
      "Time 188.5 updated\n",
      "Time 189.0 updated\n",
      "Time 189.5 updated\n",
      "Time 190.0 updated\n",
      "Time 190.5 updated\n",
      "Time 191.0 updated\n",
      "Time 191.5 updated\n",
      "Time 192.0 updated\n",
      "Time 192.5 updated\n",
      "Time 193.0 updated\n",
      "Time 193.5 updated\n",
      "Time 194.0 updated\n",
      "Time 194.5 updated\n",
      "Time 195.0 updated\n",
      "Time 195.5 updated\n",
      "Time 196.0 updated\n",
      "Time 196.5 updated\n",
      "Time 197.0 updated\n",
      "Time 197.5 updated\n",
      "Time 198.0 updated\n",
      "Time 198.5 updated\n",
      "Time 199.0 updated\n",
      "Time 199.5 updated\n",
      "Time 200.0 updated\n",
      "Time 200.5 updated\n",
      "Time 201.0 updated\n",
      "Time 201.5 updated\n",
      "Time 202.0 updated\n",
      "Time 202.5 updated\n",
      "Time 203.0 updated\n",
      "Time 203.5 updated\n",
      "Time 204.0 updated\n",
      "Time 204.5 updated\n",
      "Time 205.0 updated\n",
      "Time 205.5 updated\n",
      "Time 206.0 updated\n",
      "Time 206.5 updated\n",
      "Time 207.0 updated\n",
      "Time 207.5 updated\n",
      "Time 208.0 updated\n",
      "Time 208.5 updated\n",
      "Time 209.0 updated\n",
      "Time 209.5 updated\n",
      "Time 210.0 updated\n",
      "Time 210.5 updated\n",
      "Time 211.0 updated\n",
      "Time 211.5 updated\n",
      "Time 212.0 updated\n",
      "Time 212.5 updated\n",
      "Time 213.0 updated\n",
      "Time 213.5 updated\n",
      "Time 214.0 updated\n",
      "Time 214.5 updated\n",
      "Time 215.0 updated\n",
      "Time 215.5 updated\n",
      "Time 216.0 updated\n",
      "Time 216.5 updated\n",
      "Time 217.0 updated\n",
      "Time 217.5 updated\n",
      "Time 218.0 updated\n",
      "Time 218.5 updated\n",
      "Time 219.0 updated\n",
      "Time 219.5 updated\n",
      "Time 220.0 updated\n",
      "Time 220.5 updated\n",
      "Time 221.0 updated\n",
      "Time 221.5 updated\n",
      "Time 222.0 updated\n",
      "Time 222.5 updated\n",
      "Time 223.0 updated\n",
      "Time 223.5 updated\n",
      "Time 224.0 updated\n",
      "Time 224.5 updated\n",
      "Time 225.0 updated\n",
      "Time 225.5 updated\n",
      "Time 226.0 updated\n",
      "Time 226.5 updated\n",
      "Time 227.0 updated\n",
      "Time 227.5 updated\n",
      "Time 228.0 updated\n",
      "Time 228.5 updated\n",
      "Time 229.0 updated\n",
      "Time 229.5 updated\n",
      "Time 230.0 updated\n",
      "Time 230.5 updated\n",
      "Time 231.0 updated\n",
      "Time 231.5 updated\n",
      "Time 232.0 updated\n",
      "Time 232.5 updated\n",
      "Time 233.0 updated\n",
      "Time 233.5 updated\n",
      "Time 234.0 updated\n",
      "Time 234.5 updated\n",
      "Time 235.0 updated\n",
      "Time 235.5 updated\n",
      "Time 236.0 updated\n",
      "Time 236.5 updated\n",
      "Time 237.0 updated\n",
      "Time 237.5 updated\n",
      "Time 238.0 updated\n",
      "Time 238.5 updated\n",
      "Time 239.0 updated\n",
      "Time 239.5 updated\n",
      "Time 240.0 updated\n",
      "Time 240.5 updated\n",
      "Time 241.0 updated\n",
      "Time 241.5 updated\n",
      "Time 242.0 updated\n",
      "Time 242.5 updated\n",
      "Time 243.0 updated\n",
      "Time 243.5 updated\n",
      "Time 244.0 updated\n",
      "Time 244.5 updated\n",
      "Time 245.0 updated\n",
      "Time 245.5 updated\n",
      "Time 246.0 updated\n",
      "Time 246.5 updated\n",
      "Time 247.0 updated\n",
      "Time 247.5 updated\n",
      "Time 248.0 updated\n",
      "Time 248.5 updated\n",
      "Time 249.0 updated\n",
      "Time 249.5 updated\n",
      "Time 250.0 updated\n",
      "Time 250.5 updated\n",
      "Time 251.0 updated\n",
      "Time 251.5 updated\n",
      "Time 252.0 updated\n",
      "Time 252.5 updated\n",
      "Time 253.0 updated\n",
      "Time 253.5 updated\n",
      "Time 254.0 updated\n",
      "Time 254.5 updated\n",
      "Time 255.0 updated\n",
      "Time 255.5 updated\n",
      "Time 256.0 updated\n",
      "Time 256.5 updated\n",
      "Time 257.0 updated\n",
      "Time 257.5 updated\n",
      "Time 258.0 updated\n",
      "Time 258.5 updated\n",
      "Time 259.0 updated\n",
      "Time 259.5 updated\n",
      "Time 260.0 updated\n",
      "Time 260.5 updated\n",
      "Time 261.0 updated\n",
      "Time 261.5 updated\n",
      "Time 262.0 updated\n",
      "Time 262.5 updated\n",
      "Time 263.0 updated\n",
      "Time 263.5 updated\n",
      "Time 264.0 updated\n",
      "Time 264.5 updated\n",
      "Time 265.0 updated\n",
      "Time 265.5 updated\n",
      "Time 266.0 updated\n",
      "Time 266.5 updated\n",
      "Time 267.0 updated\n",
      "Time 267.5 updated\n",
      "Time 268.0 updated\n",
      "Time 268.5 updated\n",
      "Time 269.0 updated\n",
      "Time 269.5 updated\n",
      "Time 270.0 updated\n",
      "Time 270.5 updated\n",
      "Time 271.0 updated\n",
      "Time 271.5 updated\n",
      "Time 272.0 updated\n",
      "Time 272.5 updated\n",
      "Time 273.0 updated\n",
      "Time 273.5 updated\n",
      "Time 274.0 updated\n",
      "Time 274.5 updated\n",
      "Time 275.0 updated\n",
      "Time 275.5 updated\n",
      "Time 276.0 updated\n",
      "Time 276.5 updated\n",
      "Time 277.0 updated\n",
      "Time 277.5 updated\n",
      "Time 278.0 updated\n",
      "Time 278.5 updated\n",
      "Time 279.0 updated\n",
      "Time 279.5 updated\n",
      "Time 280.0 updated\n",
      "Time 280.5 updated\n",
      "Time 281.0 updated\n",
      "Time 281.5 updated\n",
      "Time 282.0 updated\n",
      "Time 282.5 updated\n",
      "Time 283.0 updated\n",
      "Time 283.5 updated\n",
      "Time 284.0 updated\n",
      "Time 284.5 updated\n",
      "Time 285.0 updated\n",
      "Time 285.5 updated\n",
      "Time 286.0 updated\n",
      "Time 286.5 updated\n",
      "Time 287.0 updated\n",
      "Time 287.5 updated\n",
      "Time 288.0 updated\n",
      "Time 288.5 updated\n",
      "Time 289.0 updated\n",
      "Time 289.5 updated\n",
      "Time 290.0 updated\n",
      "Time 290.5 updated\n",
      "Time 291.0 updated\n",
      "Time 291.5 updated\n",
      "Time 292.0 updated\n",
      "Time 292.5 updated\n",
      "Time 293.0 updated\n",
      "Time 293.5 updated\n",
      "Time 294.0 updated\n",
      "Time 294.5 updated\n",
      "Time 295.0 updated\n",
      "Time 295.5 updated\n",
      "Time 296.0 updated\n",
      "Time 296.5 updated\n",
      "Time 297.0 updated\n",
      "Time 297.5 updated\n",
      "Time 298.0 updated\n",
      "Time 298.5 updated\n",
      "Time 299.0 updated\n",
      "Time 299.5 updated\n",
      "Time 300.0 updated\n",
      "Time 300.5 updated\n",
      "Time 301.0 updated\n",
      "Time 301.5 updated\n",
      "Time 302.0 updated\n",
      "Time 302.5 updated\n",
      "Time 303.0 updated\n",
      "Time 303.5 updated\n",
      "Time 304.0 updated\n",
      "Time 304.5 updated\n",
      "Time 305.0 updated\n",
      "Time 305.5 updated\n",
      "Time 306.0 updated\n",
      "Time 306.5 updated\n",
      "Time 307.0 updated\n",
      "Time 307.5 updated\n",
      "Time 308.0 updated\n",
      "Time 308.5 updated\n",
      "Time 309.0 updated\n",
      "Time 309.5 updated\n",
      "Time 310.0 updated\n",
      "Time 310.5 updated\n",
      "Time 311.0 updated\n",
      "Time 311.5 updated\n",
      "Time 312.0 updated\n",
      "Time 312.5 updated\n",
      "Time 313.0 updated\n",
      "Time 313.5 updated\n",
      "Time 314.0 updated\n",
      "Time 314.5 updated\n",
      "Time 315.0 updated\n",
      "Time 315.5 updated\n",
      "Time 316.0 updated\n",
      "Time 316.5 updated\n",
      "Time 317.0 updated\n",
      "Time 317.5 updated\n",
      "Time 318.0 updated\n",
      "Time 318.5 updated\n",
      "Time 319.0 updated\n",
      "Time 319.5 updated\n",
      "Time 320.0 updated\n",
      "Time 320.5 updated\n",
      "Time 321.0 updated\n",
      "Time 321.5 updated\n",
      "Time 322.0 updated\n",
      "Time 322.5 updated\n",
      "Time 323.0 updated\n",
      "Time 323.5 updated\n",
      "Time 324.0 updated\n",
      "Time 324.5 updated\n",
      "Time 325.0 updated\n",
      "Time 325.5 updated\n",
      "Time 326.0 updated\n",
      "Time 326.5 updated\n",
      "Time 327.0 updated\n",
      "Time 327.5 updated\n",
      "Time 328.0 updated\n",
      "Time 328.5 updated\n",
      "Time 329.0 updated\n",
      "Time 329.5 updated\n",
      "Time 330.0 updated\n",
      "Time 330.5 updated\n",
      "Time 331.0 updated\n",
      "Time 331.5 updated\n",
      "Time 332.0 updated\n",
      "Time 332.5 updated\n",
      "Time 333.0 updated\n",
      "Time 333.5 updated\n",
      "Time 334.0 updated\n",
      "Time 334.5 updated\n",
      "Time 335.0 updated\n",
      "Time 335.5 updated\n",
      "Time 336.0 updated\n",
      "Time 336.5 updated\n",
      "Time 337.0 updated\n",
      "Time 337.5 updated\n",
      "Time 338.0 updated\n",
      "Time 338.5 updated\n",
      "Time 339.0 updated\n",
      "Time 339.5 updated\n",
      "Time 340.0 updated\n",
      "Time 340.5 updated\n",
      "Time 341.0 updated\n",
      "Time 341.5 updated\n",
      "Time 342.0 updated\n",
      "Time 342.5 updated\n",
      "Time 343.0 updated\n",
      "Time 343.5 updated\n",
      "Time 344.0 updated\n",
      "Time 344.5 updated\n",
      "Time 345.0 updated\n",
      "Time 345.5 updated\n",
      "Time 346.0 updated\n",
      "Time 346.5 updated\n",
      "Time 347.0 updated\n",
      "Time 347.5 updated\n",
      "Time 348.0 updated\n",
      "Time 348.5 updated\n",
      "Time 349.0 updated\n",
      "Time 349.5 updated\n",
      "Time 350.0 updated\n",
      "Time 350.5 updated\n",
      "Time 351.0 updated\n",
      "Time 351.5 updated\n",
      "Time 352.0 updated\n",
      "Time 352.5 updated\n",
      "Time 353.0 updated\n",
      "Time 353.5 updated\n",
      "Time 354.0 updated\n",
      "Time 354.5 updated\n",
      "Time 355.0 updated\n",
      "Time 355.5 updated\n",
      "Time 356.0 updated\n",
      "Time 356.5 updated\n",
      "Time 357.0 updated\n",
      "Time 357.5 updated\n",
      "Time 358.0 updated\n",
      "Time 358.5 updated\n",
      "Time 359.0 updated\n",
      "Time 359.5 updated\n",
      "Time 360.0 updated\n",
      "Time 360.5 updated\n",
      "Time 361.0 updated\n",
      "Time 361.5 updated\n",
      "Time 362.0 updated\n",
      "Time 362.5 updated\n",
      "Time 363.0 updated\n",
      "Time 363.5 updated\n",
      "Time 364.0 updated\n",
      "Time 364.5 updated\n",
      "Time 365.0 updated\n",
      "Time 365.5 updated\n",
      "Time 366.0 updated\n",
      "Time 366.5 updated\n",
      "Time 367.0 updated\n",
      "Time 367.5 updated\n",
      "Time 368.0 updated\n",
      "Time 368.5 updated\n",
      "Time 369.0 updated\n",
      "Time 369.5 updated\n",
      "Time 370.0 updated\n",
      "Time 370.5 updated\n",
      "Time 371.0 updated\n",
      "Time 371.5 updated\n",
      "Time 372.0 updated\n",
      "Time 372.5 updated\n",
      "Time 373.0 updated\n",
      "Time 373.5 updated\n",
      "Time 374.0 updated\n",
      "Time 374.5 updated\n",
      "Time 375.0 updated\n",
      "Time 375.5 updated\n",
      "Time 376.0 updated\n",
      "Time 376.5 updated\n",
      "Time 377.0 updated\n",
      "Time 377.5 updated\n",
      "Time 378.0 updated\n",
      "Time 378.5 updated\n",
      "Time 379.0 updated\n",
      "Time 379.5 updated\n",
      "Time 380.0 updated\n",
      "Time 380.5 updated\n",
      "Time 381.0 updated\n",
      "Time 381.5 updated\n",
      "Time 382.0 updated\n",
      "Time 382.5 updated\n",
      "Time 383.0 updated\n",
      "Time 383.5 updated\n",
      "Time 384.0 updated\n",
      "Time 384.5 updated\n",
      "Time 385.0 updated\n",
      "Time 385.5 updated\n",
      "Time 386.0 updated\n",
      "Time 386.5 updated\n",
      "Time 387.0 updated\n",
      "Time 387.5 updated\n",
      "Time 388.0 updated\n",
      "Time 388.5 updated\n",
      "Time 389.0 updated\n",
      "Time 389.5 updated\n",
      "Time 390.0 updated\n",
      "Time 390.5 updated\n",
      "Time 391.0 updated\n",
      "Time 391.5 updated\n",
      "Time 392.0 updated\n",
      "Time 392.5 updated\n",
      "Time 393.0 updated\n",
      "Time 393.5 updated\n",
      "Time 394.0 updated\n",
      "Time 394.5 updated\n",
      "Time 395.0 updated\n",
      "Time 395.5 updated\n",
      "Time 396.0 updated\n",
      "Time 396.5 updated\n",
      "Time 397.0 updated\n",
      "Time 397.5 updated\n",
      "Time 398.0 updated\n",
      "Time 398.5 updated\n",
      "Time 399.0 updated\n",
      "Time 399.5 updated\n",
      "Time 400.0 updated\n",
      "Time 400.5 updated\n",
      "Time 401.0 updated\n",
      "Time 401.5 updated\n",
      "Time 402.0 updated\n",
      "Time 402.5 updated\n",
      "Time 403.0 updated\n",
      "Time 403.5 updated\n",
      "Time 404.0 updated\n",
      "Time 404.5 updated\n",
      "Time 405.0 updated\n",
      "Time 405.5 updated\n",
      "Time 406.0 updated\n",
      "Time 406.5 updated\n",
      "Time 407.0 updated\n",
      "Time 407.5 updated\n",
      "Time 408.0 updated\n",
      "Time 408.5 updated\n",
      "Time 409.0 updated\n",
      "Time 409.5 updated\n",
      "Time 410.0 updated\n",
      "Time 410.5 updated\n",
      "Time 411.0 updated\n",
      "Time 411.5 updated\n",
      "Time 412.0 updated\n",
      "Time 412.5 updated\n",
      "Time 413.0 updated\n",
      "Time 413.5 updated\n",
      "Time 414.0 updated\n",
      "Time 414.5 updated\n",
      "Time 415.0 updated\n",
      "Time 415.5 updated\n",
      "Time 416.0 updated\n",
      "Time 416.5 updated\n",
      "Time 417.0 updated\n",
      "Time 417.5 updated\n",
      "Time 418.0 updated\n",
      "Time 418.5 updated\n",
      "Time 419.0 updated\n",
      "Time 419.5 updated\n",
      "Time 420.0 updated\n",
      "Time 420.5 updated\n",
      "Time 421.0 updated\n",
      "Time 421.5 updated\n",
      "Time 422.0 updated\n",
      "Time 422.5 updated\n",
      "Time 423.0 updated\n",
      "Time 423.5 updated\n",
      "Time 424.0 updated\n",
      "Time 424.5 updated\n",
      "Time 425.0 updated\n",
      "Time 425.5 updated\n",
      "Time 426.0 updated\n",
      "Time 426.5 updated\n",
      "Time 427.0 updated\n",
      "Time 427.5 updated\n",
      "Time 428.0 updated\n",
      "Time 428.5 updated\n",
      "Time 429.0 updated\n",
      "Time 429.5 updated\n",
      "Time 430.0 updated\n",
      "Time 430.5 updated\n",
      "Time 431.0 updated\n",
      "Time 431.5 updated\n",
      "Time 432.0 updated\n",
      "Time 432.5 updated\n",
      "Time 433.0 updated\n",
      "Time 433.5 updated\n",
      "Time 434.0 updated\n",
      "Time 434.5 updated\n",
      "Time 435.0 updated\n",
      "Time 435.5 updated\n",
      "Time 436.0 updated\n",
      "Time 436.5 updated\n",
      "Time 437.0 updated\n",
      "Time 437.5 updated\n",
      "Time 438.0 updated\n",
      "Time 438.5 updated\n",
      "Time 439.0 updated\n",
      "Time 439.5 updated\n",
      "Time 440.0 updated\n",
      "Time 440.5 updated\n",
      "Time 441.0 updated\n",
      "Time 441.5 updated\n",
      "Time 442.0 updated\n",
      "Time 442.5 updated\n",
      "Time 443.0 updated\n",
      "Time 443.5 updated\n",
      "Time 444.0 updated\n",
      "Time 444.5 updated\n",
      "Time 445.0 updated\n",
      "Time 445.5 updated\n",
      "Time 446.0 updated\n",
      "Time 446.5 updated\n",
      "Time 447.0 updated\n",
      "Time 447.5 updated\n",
      "Time 448.0 updated\n",
      "Time 448.5 updated\n",
      "Time 449.0 updated\n",
      "Time 449.5 updated\n",
      "Time 450.0 updated\n",
      "Time 450.5 updated\n",
      "Time 451.0 updated\n",
      "Time 451.5 updated\n",
      "Time 452.0 updated\n",
      "Time 452.5 updated\n",
      "Time 453.0 updated\n",
      "Time 453.5 updated\n",
      "Time 454.0 updated\n",
      "Time 454.5 updated\n",
      "Time 455.0 updated\n",
      "Time 455.5 updated\n",
      "Time 456.0 updated\n",
      "Time 456.5 updated\n",
      "Time 457.0 updated\n",
      "Time 457.5 updated\n",
      "Time 458.0 updated\n",
      "Time 458.5 updated\n",
      "Time 459.0 updated\n",
      "Time 459.5 updated\n",
      "Time 460.0 updated\n",
      "Time 460.5 updated\n",
      "Time 461.0 updated\n",
      "Time 461.5 updated\n",
      "Time 462.0 updated\n",
      "Time 462.5 updated\n",
      "Time 463.0 updated\n",
      "Time 463.5 updated\n",
      "Time 464.0 updated\n",
      "Time 464.5 updated\n",
      "Time 465.0 updated\n",
      "Time 465.5 updated\n",
      "Time 466.0 updated\n",
      "Time 466.5 updated\n",
      "Time 467.0 updated\n",
      "Time 467.5 updated\n",
      "Time 468.0 updated\n",
      "Time 468.5 updated\n",
      "Time 469.0 updated\n",
      "Time 469.5 updated\n",
      "Time 470.0 updated\n",
      "Time 470.5 updated\n",
      "Time 471.0 updated\n",
      "Time 471.5 updated\n",
      "Time 472.0 updated\n",
      "Time 472.5 updated\n",
      "Time 473.0 updated\n",
      "Time 473.5 updated\n",
      "Time 474.0 updated\n",
      "Time 474.5 updated\n",
      "Time 475.0 updated\n",
      "Time 475.5 updated\n",
      "Time 476.0 updated\n",
      "Time 476.5 updated\n",
      "Time 477.0 updated\n",
      "Time 477.5 updated\n",
      "Time 478.0 updated\n",
      "Time 478.5 updated\n",
      "Time 479.0 updated\n",
      "Time 479.5 updated\n",
      "Time 480.0 updated\n",
      "Time 480.5 updated\n",
      "Time 481.0 updated\n",
      "Time 481.5 updated\n",
      "Time 482.0 updated\n",
      "Time 482.5 updated\n",
      "Time 483.0 updated\n",
      "Time 483.5 updated\n",
      "Time 484.0 updated\n",
      "Time 484.5 updated\n",
      "Time 485.0 updated\n",
      "Time 485.5 updated\n",
      "Time 486.0 updated\n",
      "Time 486.5 updated\n",
      "Time 487.0 updated\n",
      "Time 487.5 updated\n",
      "Time 488.0 updated\n",
      "Time 488.5 updated\n",
      "Time 489.0 updated\n",
      "Time 489.5 updated\n",
      "Time 490.0 updated\n",
      "Time 490.5 updated\n",
      "Time 491.0 updated\n",
      "Time 491.5 updated\n",
      "Time 492.0 updated\n",
      "Time 492.5 updated\n",
      "Time 493.0 updated\n",
      "Time 493.5 updated\n",
      "Time 494.0 updated\n",
      "Time 494.5 updated\n",
      "Time 495.0 updated\n",
      "Time 495.5 updated\n",
      "Time 496.0 updated\n",
      "Time 496.5 updated\n",
      "Time 497.0 updated\n",
      "Time 497.5 updated\n",
      "Time 498.0 updated\n",
      "Time 498.5 updated\n",
      "Time 499.0 updated\n",
      "Time 499.5 updated\n",
      "Time 500.0 updated\n",
      "Time 500.5 updated\n",
      "Time 501.0 updated\n",
      "Time 501.5 updated\n",
      "Time 502.0 updated\n",
      "Time 502.5 updated\n",
      "Time 503.0 updated\n",
      "Time 503.5 updated\n",
      "Time 504.0 updated\n",
      "Time 504.5 updated\n",
      "Time 505.0 updated\n",
      "Time 505.5 updated\n",
      "Time 506.0 updated\n",
      "Time 506.5 updated\n",
      "Time 507.0 updated\n",
      "Time 507.5 updated\n",
      "Time 508.0 updated\n",
      "Time 508.5 updated\n",
      "Time 509.0 updated\n",
      "Time 509.5 updated\n",
      "Time 510.0 updated\n",
      "Time 510.5 updated\n",
      "Time 511.0 updated\n",
      "Time 511.5 updated\n",
      "Time 512.0 updated\n",
      "Time 512.5 updated\n",
      "Time 513.0 updated\n",
      "Time 513.5 updated\n",
      "Time 514.0 updated\n",
      "Time 514.5 updated\n",
      "Time 515.0 updated\n",
      "Time 515.5 updated\n",
      "Time 516.0 updated\n",
      "Time 516.5 updated\n",
      "Time 517.0 updated\n",
      "Time 517.5 updated\n",
      "Time 518.0 updated\n",
      "Time 518.5 updated\n",
      "Time 519.0 updated\n",
      "Time 519.5 updated\n",
      "Time 520.0 updated\n",
      "Time 520.5 updated\n",
      "Time 521.0 updated\n",
      "Time 521.5 updated\n",
      "Time 522.0 updated\n",
      "Time 522.5 updated\n",
      "Time 523.0 updated\n",
      "Time 523.5 updated\n",
      "Time 524.0 updated\n",
      "Time 524.5 updated\n",
      "Time 525.0 updated\n",
      "Time 525.5 updated\n",
      "Time 526.0 updated\n",
      "Time 526.5 updated\n",
      "Time 527.0 updated\n",
      "Time 527.5 updated\n",
      "Time 528.0 updated\n",
      "Time 528.5 updated\n",
      "Time 529.0 updated\n",
      "Time 529.5 updated\n",
      "Time 530.0 updated\n",
      "Time 530.5 updated\n",
      "Time 531.0 updated\n",
      "Time 531.5 updated\n",
      "Time 532.0 updated\n",
      "Time 532.5 updated\n",
      "Time 533.0 updated\n",
      "Time 533.5 updated\n",
      "Time 534.0 updated\n",
      "Time 534.5 updated\n",
      "Time 535.0 updated\n",
      "Time 535.5 updated\n",
      "Time 536.0 updated\n",
      "Time 536.5 updated\n",
      "Time 537.0 updated\n",
      "Time 537.5 updated\n",
      "Time 538.0 updated\n",
      "Time 538.5 updated\n",
      "Time 539.0 updated\n",
      "Time 539.5 updated\n",
      "Time 540.0 updated\n",
      "Time 540.5 updated\n",
      "Time 541.0 updated\n",
      "Time 541.5 updated\n",
      "Time 542.0 updated\n",
      "Time 542.5 updated\n",
      "Time 543.0 updated\n",
      "Time 543.5 updated\n",
      "Time 544.0 updated\n",
      "Time 544.5 updated\n",
      "Time 545.0 updated\n",
      "Time 545.5 updated\n",
      "Time 546.0 updated\n",
      "Time 546.5 updated\n",
      "Time 547.0 updated\n",
      "Time 547.5 updated\n",
      "Time 548.0 updated\n",
      "Time 548.5 updated\n",
      "Time 549.0 updated\n",
      "Time 549.5 updated\n",
      "Time 550.0 updated\n",
      "Time 550.5 updated\n",
      "Time 551.0 updated\n",
      "Time 551.5 updated\n",
      "Time 552.0 updated\n",
      "Time 552.5 updated\n",
      "Time 553.0 updated\n",
      "Time 553.5 updated\n",
      "Time 554.0 updated\n",
      "Time 554.5 updated\n",
      "Time 555.0 updated\n",
      "Time 555.5 updated\n",
      "Time 556.0 updated\n",
      "Time 556.5 updated\n",
      "Time 557.0 updated\n",
      "Time 557.5 updated\n",
      "Time 558.0 updated\n",
      "Time 558.5 updated\n",
      "Time 559.0 updated\n",
      "Time 559.5 updated\n",
      "Time 560.0 updated\n",
      "Time 560.5 updated\n",
      "Time 561.0 updated\n",
      "Time 561.5 updated\n",
      "Time 562.0 updated\n",
      "Time 562.5 updated\n",
      "Time 563.0 updated\n",
      "Time 563.5 updated\n",
      "Time 564.0 updated\n",
      "Time 564.5 updated\n",
      "Time 565.0 updated\n",
      "Time 565.5 updated\n",
      "Time 566.0 updated\n",
      "Time 566.5 updated\n",
      "Time 567.0 updated\n",
      "Time 567.5 updated\n",
      "Time 568.0 updated\n",
      "Time 568.5 updated\n",
      "Time 569.0 updated\n",
      "Time 569.5 updated\n",
      "Time 570.0 updated\n",
      "Time 570.5 updated\n",
      "Time 571.0 updated\n",
      "Time 571.5 updated\n",
      "Time 572.0 updated\n",
      "Time 572.5 updated\n",
      "Time 573.0 updated\n",
      "Time 573.5 updated\n",
      "Time 574.0 updated\n",
      "Time 574.5 updated\n",
      "Time 575.0 updated\n",
      "Time 575.5 updated\n",
      "Time 576.0 updated\n",
      "Time 576.5 updated\n",
      "Time 577.0 updated\n",
      "Time 577.5 updated\n",
      "Time 578.0 updated\n",
      "Time 578.5 updated\n",
      "Time 579.0 updated\n",
      "Time 579.5 updated\n",
      "Time 580.0 updated\n",
      "Time 580.5 updated\n",
      "Time 581.0 updated\n",
      "Time 581.5 updated\n",
      "Time 582.0 updated\n",
      "Time 582.5 updated\n",
      "Time 583.0 updated\n",
      "Time 583.5 updated\n",
      "Time 584.0 updated\n",
      "Time 584.5 updated\n",
      "Time 585.0 updated\n",
      "Time 585.5 updated\n",
      "Time 586.0 updated\n",
      "Time 586.5 updated\n",
      "Time 587.0 updated\n",
      "Time 587.5 updated\n",
      "Time 588.0 updated\n",
      "Time 588.5 updated\n",
      "Time 589.0 updated\n",
      "Time 589.5 updated\n",
      "Time 590.0 updated\n",
      "Time 590.5 updated\n",
      "Time 591.0 updated\n",
      "Time 591.5 updated\n",
      "Time 592.0 updated\n",
      "Time 592.5 updated\n",
      "Time 593.0 updated\n",
      "Time 593.5 updated\n",
      "Time 594.0 updated\n",
      "Time 594.5 updated\n",
      "Time 595.0 updated\n",
      "Time 595.5 updated\n",
      "Time 596.0 updated\n",
      "Time 596.5 updated\n",
      "Time 597.0 updated\n",
      "Time 597.5 updated\n",
      "Time 598.0 updated\n",
      "Time 598.5 updated\n",
      "Time 599.0 updated\n",
      "Time 599.5 updated\n",
      "Time 600.0 updated\n",
      "Time 600.5 updated\n",
      "Time 601.0 updated\n",
      "Time 601.5 updated\n",
      "Time 602.0 updated\n",
      "Time 602.5 updated\n",
      "Time 603.0 updated\n",
      "Time 603.5 updated\n",
      "Time 604.0 updated\n",
      "Time 604.5 updated\n",
      "Time 605.0 updated\n",
      "Time 605.5 updated\n",
      "Time 606.0 updated\n",
      "Time 606.5 updated\n",
      "Time 607.0 updated\n",
      "Time 607.5 updated\n",
      "Time 608.0 updated\n",
      "Time 608.5 updated\n",
      "Time 609.0 updated\n",
      "Time 609.5 updated\n",
      "Time 610.0 updated\n",
      "Time 610.5 updated\n",
      "Time 611.0 updated\n",
      "Time 611.5 updated\n",
      "Time 612.0 updated\n",
      "Time 612.5 updated\n",
      "Time 613.0 updated\n",
      "Time 613.5 updated\n",
      "Time 614.0 updated\n",
      "Time 614.5 updated\n",
      "Time 615.0 updated\n",
      "Time 615.5 updated\n",
      "Time 616.0 updated\n",
      "Time 616.5 updated\n",
      "Time 617.0 updated\n",
      "Time 617.5 updated\n",
      "Time 618.0 updated\n",
      "Time 618.5 updated\n",
      "Time 619.0 updated\n",
      "Time 619.5 updated\n",
      "Time 620.0 updated\n",
      "Time 620.5 updated\n",
      "Time 621.0 updated\n",
      "Time 621.5 updated\n",
      "Time 622.0 updated\n",
      "Time 622.5 updated\n",
      "Time 623.0 updated\n",
      "Time 623.5 updated\n",
      "Time 624.0 updated\n",
      "Time 624.5 updated\n",
      "Time 625.0 updated\n",
      "Time 625.5 updated\n",
      "Time 626.0 updated\n",
      "Time 626.5 updated\n",
      "Time 627.0 updated\n",
      "Time 627.5 updated\n",
      "Time 628.0 updated\n",
      "Time 628.5 updated\n",
      "Time 629.0 updated\n",
      "Time 629.5 updated\n",
      "Time 630.0 updated\n",
      "Time 630.5 updated\n",
      "Time 631.0 updated\n",
      "Time 631.5 updated\n",
      "Time 632.0 updated\n",
      "Time 632.5 updated\n",
      "Time 633.0 updated\n",
      "Time 633.5 updated\n",
      "Time 634.0 updated\n",
      "Time 634.5 updated\n",
      "Time 635.0 updated\n",
      "Time 635.5 updated\n",
      "Time 636.0 updated\n",
      "Time 636.5 updated\n",
      "Time 637.0 updated\n",
      "Time 637.5 updated\n",
      "Time 638.0 updated\n",
      "Time 638.5 updated\n",
      "Time 639.0 updated\n",
      "Time 639.5 updated\n",
      "Time 640.0 updated\n",
      "Time 640.5 updated\n",
      "Time 641.0 updated\n",
      "Time 641.5 updated\n",
      "Time 642.0 updated\n",
      "Time 642.5 updated\n",
      "Time 643.0 updated\n",
      "Time 643.5 updated\n",
      "Time 644.0 updated\n",
      "Time 644.5 updated\n",
      "Time 645.0 updated\n",
      "Time 645.5 updated\n",
      "Time 646.0 updated\n",
      "Time 646.5 updated\n",
      "Time 647.0 updated\n",
      "Time 647.5 updated\n",
      "Time 648.0 updated\n",
      "Time 648.5 updated\n",
      "Time 649.0 updated\n",
      "Time 649.5 updated\n",
      "Time 650.0 updated\n",
      "Time 650.5 updated\n",
      "Time 651.0 updated\n",
      "Time 651.5 updated\n",
      "Time 652.0 updated\n",
      "Time 652.5 updated\n",
      "Time 653.0 updated\n",
      "Time 653.5 updated\n",
      "Time 654.0 updated\n",
      "Time 654.5 updated\n",
      "Time 655.0 updated\n",
      "Time 655.5 updated\n",
      "Time 656.0 updated\n",
      "Time 656.5 updated\n",
      "Time 657.0 updated\n",
      "Time 657.5 updated\n",
      "Time 658.0 updated\n",
      "Time 658.5 updated\n",
      "Time 659.0 updated\n",
      "Time 659.5 updated\n",
      "Time 660.0 updated\n",
      "Time 660.5 updated\n",
      "Time 661.0 updated\n",
      "Time 661.5 updated\n",
      "Time 662.0 updated\n",
      "Time 662.5 updated\n",
      "Time 663.0 updated\n",
      "Time 663.5 updated\n",
      "Time 664.0 updated\n",
      "Time 664.5 updated\n",
      "Time 665.0 updated\n",
      "Time 665.5 updated\n",
      "Time 666.0 updated\n",
      "Time 666.5 updated\n",
      "Time 667.0 updated\n",
      "Time 667.5 updated\n",
      "Time 668.0 updated\n",
      "Time 668.5 updated\n",
      "Time 669.0 updated\n",
      "Time 669.5 updated\n",
      "Time 670.0 updated\n",
      "Time 670.5 updated\n",
      "Time 671.0 updated\n",
      "Time 671.5 updated\n",
      "Time 672.0 updated\n",
      "Time 672.5 updated\n",
      "Time 673.0 updated\n",
      "Time 673.5 updated\n",
      "Time 674.0 updated\n",
      "Time 674.5 updated\n",
      "Time 675.0 updated\n",
      "Time 675.5 updated\n",
      "Time 676.0 updated\n",
      "Time 676.5 updated\n",
      "Time 677.0 updated\n",
      "Time 677.5 updated\n",
      "Time 678.0 updated\n",
      "Time 678.5 updated\n",
      "Time 679.0 updated\n",
      "Time 679.5 updated\n",
      "Time 680.0 updated\n",
      "Time 680.5 updated\n",
      "Time 681.0 updated\n",
      "Time 681.5 updated\n",
      "Time 682.0 updated\n",
      "Time 682.5 updated\n",
      "Time 683.0 updated\n",
      "Time 683.5 updated\n",
      "Time 684.0 updated\n",
      "Time 684.5 updated\n",
      "Time 685.0 updated\n",
      "Time 685.5 updated\n",
      "Time 686.0 updated\n",
      "Time 686.5 updated\n",
      "Time 687.0 updated\n",
      "Time 687.5 updated\n",
      "Time 688.0 updated\n",
      "Time 688.5 updated\n",
      "Time 689.0 updated\n",
      "Time 689.5 updated\n",
      "Time 690.0 updated\n",
      "Time 690.5 updated\n",
      "Time 691.0 updated\n",
      "Time 691.5 updated\n",
      "Time 692.0 updated\n",
      "Time 692.5 updated\n",
      "Time 693.0 updated\n",
      "Time 693.5 updated\n",
      "Time 694.0 updated\n",
      "Time 694.5 updated\n",
      "Time 695.0 updated\n",
      "Time 695.5 updated\n",
      "Time 696.0 updated\n",
      "Time 696.5 updated\n",
      "Time 697.0 updated\n",
      "Time 697.5 updated\n",
      "Time 698.0 updated\n",
      "Time 698.5 updated\n",
      "Time 699.0 updated\n",
      "Time 699.5 updated\n",
      "Time 700.0 updated\n",
      "Time 700.5 updated\n",
      "Time 701.0 updated\n",
      "Time 701.5 updated\n",
      "Time 702.0 updated\n",
      "Time 702.5 updated\n",
      "Time 703.0 updated\n",
      "Time 703.5 updated\n",
      "Time 704.0 updated\n",
      "Time 704.5 updated\n",
      "Time 705.0 updated\n",
      "Time 705.5 updated\n",
      "Time 706.0 updated\n",
      "Time 706.5 updated\n",
      "Time 707.0 updated\n",
      "Time 707.5 updated\n",
      "Time 708.0 updated\n",
      "Time 708.5 updated\n",
      "Time 709.0 updated\n",
      "Time 709.5 updated\n",
      "Time 710.0 updated\n",
      "Time 710.5 updated\n",
      "Time 711.0 updated\n",
      "Time 711.5 updated\n",
      "Time 712.0 updated\n",
      "Time 712.5 updated\n",
      "Time 713.0 updated\n",
      "Time 713.5 updated\n",
      "Time 714.0 updated\n",
      "Time 714.5 updated\n",
      "Time 715.0 updated\n",
      "Time 715.5 updated\n",
      "Time 716.0 updated\n",
      "Time 716.5 updated\n",
      "Time 717.0 updated\n",
      "Time 717.5 updated\n",
      "Time 718.0 updated\n",
      "Time 718.5 updated\n",
      "Time 719.0 updated\n",
      "Time 719.5 updated\n",
      "Time 720.0 updated\n",
      "Time 720.5 updated\n",
      "Time 721.0 updated\n",
      "Time 721.5 updated\n",
      "Time 722.0 updated\n",
      "Time 722.5 updated\n",
      "Time 723.0 updated\n",
      "Time 723.5 updated\n",
      "Time 724.0 updated\n",
      "Time 724.5 updated\n",
      "Time 725.0 updated\n",
      "Time 725.5 updated\n",
      "Time 726.0 updated\n",
      "Time 726.5 updated\n",
      "Time 727.0 updated\n",
      "Time 727.5 updated\n",
      "Time 728.0 updated\n",
      "Time 728.5 updated\n",
      "Time 729.0 updated\n",
      "Time 729.5 updated\n",
      "Time 730.0 updated\n",
      "Time 730.5 updated\n",
      "Time 731.0 updated\n",
      "Time 731.5 updated\n",
      "Time 732.0 updated\n",
      "Time 732.5 updated\n",
      "Time 733.0 updated\n",
      "Time 733.5 updated\n",
      "Time 734.0 updated\n",
      "Time 734.5 updated\n",
      "Time 735.0 updated\n",
      "Time 735.5 updated\n",
      "Time 736.0 updated\n",
      "Time 736.5 updated\n",
      "Time 737.0 updated\n",
      "Time 737.5 updated\n",
      "Time 738.0 updated\n",
      "Time 738.5 updated\n",
      "Time 739.0 updated\n",
      "Time 739.5 updated\n",
      "Time 740.0 updated\n",
      "Time 740.5 updated\n",
      "Time 741.0 updated\n",
      "Time 741.5 updated\n",
      "Time 742.0 updated\n",
      "Time 742.5 updated\n",
      "Time 743.0 updated\n",
      "Time 743.5 updated\n",
      "Time 744.0 updated\n",
      "Time 744.5 updated\n",
      "Time 745.0 updated\n",
      "Time 745.5 updated\n",
      "Time 746.0 updated\n",
      "Time 746.5 updated\n",
      "Time 747.0 updated\n",
      "Time 747.5 updated\n",
      "Time 748.0 updated\n",
      "Time 748.5 updated\n",
      "Time 749.0 updated\n",
      "Time 749.5 updated\n",
      "Time 750.0 updated\n",
      "Time 750.5 updated\n",
      "Time 751.0 updated\n",
      "Time 751.5 updated\n",
      "Time 752.0 updated\n",
      "Time 752.5 updated\n",
      "Time 753.0 updated\n",
      "Time 753.5 updated\n",
      "Time 754.0 updated\n",
      "Time 754.5 updated\n",
      "Time 755.0 updated\n",
      "Time 755.5 updated\n",
      "Time 756.0 updated\n",
      "Time 756.5 updated\n",
      "Time 757.0 updated\n",
      "Time 757.5 updated\n",
      "Time 758.0 updated\n",
      "Time 758.5 updated\n",
      "Time 759.0 updated\n",
      "Time 759.5 updated\n",
      "Time 760.0 updated\n",
      "Time 760.5 updated\n",
      "Time 761.0 updated\n",
      "Time 761.5 updated\n",
      "Time 762.0 updated\n",
      "Time 762.5 updated\n",
      "Time 763.0 updated\n",
      "Time 763.5 updated\n",
      "Time 764.0 updated\n",
      "Time 764.5 updated\n",
      "Time 765.0 updated\n",
      "Time 765.5 updated\n",
      "Time 766.0 updated\n",
      "Time 766.5 updated\n",
      "Time 767.0 updated\n",
      "Time 767.5 updated\n",
      "Time 768.0 updated\n",
      "Time 768.5 updated\n",
      "Time 769.0 updated\n",
      "Time 769.5 updated\n",
      "Time 770.0 updated\n",
      "Time 770.5 updated\n",
      "Time 771.0 updated\n",
      "Time 771.5 updated\n",
      "Time 772.0 updated\n",
      "Time 772.5 updated\n",
      "Time 773.0 updated\n",
      "Time 773.5 updated\n",
      "Time 774.0 updated\n",
      "Time 774.5 updated\n",
      "Time 775.0 updated\n",
      "Time 775.5 updated\n",
      "Time 776.0 updated\n",
      "Time 776.5 updated\n",
      "Time 777.0 updated\n",
      "Time 777.5 updated\n",
      "Time 778.0 updated\n",
      "Time 778.5 updated\n",
      "Time 779.0 updated\n",
      "Time 779.5 updated\n",
      "Time 780.0 updated\n",
      "Time 780.5 updated\n",
      "Time 781.0 updated\n",
      "Time 781.5 updated\n",
      "Time 782.0 updated\n",
      "Time 782.5 updated\n",
      "Time 783.0 updated\n",
      "Time 783.5 updated\n",
      "Time 784.0 updated\n",
      "Time 784.5 updated\n",
      "Time 785.0 updated\n",
      "Time 785.5 updated\n",
      "Time 786.0 updated\n",
      "Time 786.5 updated\n",
      "Time 787.0 updated\n",
      "Time 787.5 updated\n",
      "Time 788.0 updated\n",
      "Time 788.5 updated\n",
      "Time 789.0 updated\n",
      "Time 789.5 updated\n",
      "Time 790.0 updated\n",
      "Time 790.5 updated\n",
      "Time 791.0 updated\n",
      "Time 791.5 updated\n",
      "Time 792.0 updated\n",
      "Time 792.5 updated\n",
      "Time 793.0 updated\n",
      "Time 793.5 updated\n",
      "Time 794.0 updated\n",
      "Time 794.5 updated\n",
      "Time 795.0 updated\n",
      "Time 795.5 updated\n",
      "Time 796.0 updated\n",
      "Time 796.5 updated\n",
      "Time 797.0 updated\n",
      "Time 797.5 updated\n",
      "Time 798.0 updated\n",
      "Time 798.5 updated\n",
      "Time 799.0 updated\n",
      "Time 799.5 updated\n",
      "Time 800.0 updated\n",
      "Time 800.5 updated\n",
      "Time 801.0 updated\n",
      "Time 801.5 updated\n",
      "Time 802.0 updated\n",
      "Time 802.5 updated\n",
      "Time 803.0 updated\n",
      "Time 803.5 updated\n",
      "Time 804.0 updated\n",
      "Time 804.5 updated\n",
      "Time 805.0 updated\n",
      "Time 805.5 updated\n",
      "Time 806.0 updated\n",
      "Time 806.5 updated\n",
      "Time 807.0 updated\n",
      "Time 807.5 updated\n",
      "Time 808.0 updated\n",
      "Time 808.5 updated\n",
      "Time 809.0 updated\n",
      "Time 809.5 updated\n",
      "Time 810.0 updated\n",
      "Time 810.5 updated\n",
      "Time 811.0 updated\n",
      "Time 811.5 updated\n",
      "Time 812.0 updated\n",
      "Time 812.5 updated\n",
      "Time 813.0 updated\n",
      "Time 813.5 updated\n",
      "Time 814.0 updated\n",
      "Time 814.5 updated\n",
      "Time 815.0 updated\n",
      "Time 815.5 updated\n",
      "Time 816.0 updated\n",
      "Time 816.5 updated\n",
      "Time 817.0 updated\n",
      "Time 817.5 updated\n",
      "Time 818.0 updated\n",
      "Time 818.5 updated\n",
      "Time 819.0 updated\n",
      "Time 819.5 updated\n",
      "Time 820.0 updated\n",
      "Time 820.5 updated\n",
      "Time 821.0 updated\n",
      "Time 821.5 updated\n",
      "Time 822.0 updated\n",
      "Time 822.5 updated\n",
      "Time 823.0 updated\n",
      "Time 823.5 updated\n",
      "Time 824.0 updated\n",
      "Time 824.5 updated\n",
      "Time 825.0 updated\n",
      "Time 825.5 updated\n",
      "Time 826.0 updated\n",
      "Time 826.5 updated\n",
      "Time 827.0 updated\n",
      "Time 827.5 updated\n",
      "Time 828.0 updated\n",
      "Time 828.5 updated\n",
      "Time 829.0 updated\n",
      "Time 829.5 updated\n",
      "Time 830.0 updated\n",
      "Time 830.5 updated\n",
      "Time 831.0 updated\n",
      "Time 831.5 updated\n",
      "Time 832.0 updated\n",
      "Time 832.5 updated\n",
      "Time 833.0 updated\n",
      "Time 833.5 updated\n",
      "Time 834.0 updated\n",
      "Time 834.5 updated\n",
      "Time 835.0 updated\n",
      "Time 835.5 updated\n",
      "Time 836.0 updated\n",
      "Time 836.5 updated\n",
      "Time 837.0 updated\n",
      "Time 837.5 updated\n",
      "Time 838.0 updated\n",
      "Time 838.5 updated\n",
      "Time 839.0 updated\n",
      "Time 839.5 updated\n",
      "Time 840.0 updated\n",
      "Time 840.5 updated\n",
      "Time 841.0 updated\n",
      "Time 841.5 updated\n",
      "Time 842.0 updated\n",
      "Time 842.5 updated\n",
      "Time 843.0 updated\n",
      "Time 843.5 updated\n",
      "Time 844.0 updated\n",
      "Time 844.5 updated\n",
      "Time 845.0 updated\n",
      "Time 845.5 updated\n",
      "Time 846.0 updated\n",
      "Time 846.5 updated\n",
      "Time 847.0 updated\n",
      "Time 847.5 updated\n",
      "Time 848.0 updated\n",
      "Time 848.5 updated\n",
      "Time 849.0 updated\n",
      "Time 849.5 updated\n",
      "Time 850.0 updated\n",
      "Time 850.5 updated\n",
      "Time 851.0 updated\n",
      "Time 851.5 updated\n",
      "Time 852.0 updated\n",
      "Time 852.5 updated\n",
      "Time 853.0 updated\n",
      "Time 853.5 updated\n",
      "Time 854.0 updated\n",
      "Time 854.5 updated\n",
      "Time 855.0 updated\n",
      "Time 855.5 updated\n",
      "Time 856.0 updated\n",
      "Time 856.5 updated\n",
      "Time 857.0 updated\n",
      "Time 857.5 updated\n",
      "Time 858.0 updated\n",
      "Time 858.5 updated\n",
      "Time 859.0 updated\n",
      "Time 859.5 updated\n",
      "Time 860.0 updated\n",
      "Time 860.5 updated\n",
      "Time 861.0 updated\n",
      "Time 861.5 updated\n",
      "Time 862.0 updated\n",
      "Time 862.5 updated\n",
      "Time 863.0 updated\n",
      "Time 863.5 updated\n",
      "Time 864.0 updated\n",
      "Time 864.5 updated\n",
      "Time 865.0 updated\n",
      "Time 865.5 updated\n",
      "Time 866.0 updated\n",
      "Time 866.5 updated\n",
      "Time 867.0 updated\n",
      "Time 867.5 updated\n",
      "Time 868.0 updated\n",
      "Time 868.5 updated\n",
      "Time 869.0 updated\n",
      "Time 869.5 updated\n",
      "Time 870.0 updated\n",
      "Time 870.5 updated\n",
      "Time 871.0 updated\n",
      "Time 871.5 updated\n",
      "Time 872.0 updated\n",
      "Time 872.5 updated\n",
      "Time 873.0 updated\n",
      "Time 873.5 updated\n",
      "Time 874.0 updated\n",
      "Time 874.5 updated\n",
      "Time 875.0 updated\n",
      "Time 875.5 updated\n",
      "Time 876.0 updated\n",
      "Time 876.5 updated\n",
      "Time 877.0 updated\n",
      "Time 877.5 updated\n",
      "Time 878.0 updated\n",
      "Time 878.5 updated\n",
      "Time 879.0 updated\n",
      "Time 879.5 updated\n",
      "Time 880.0 updated\n",
      "Time 880.5 updated\n",
      "Time 881.0 updated\n",
      "Time 881.5 updated\n",
      "Time 882.0 updated\n",
      "Time 882.5 updated\n",
      "Time 883.0 updated\n",
      "Time 883.5 updated\n",
      "Time 884.0 updated\n",
      "Time 884.5 updated\n",
      "Time 885.0 updated\n",
      "Time 885.5 updated\n",
      "Time 886.0 updated\n",
      "Time 886.5 updated\n",
      "Time 887.0 updated\n",
      "Time 887.5 updated\n",
      "Time 888.0 updated\n",
      "Time 888.5 updated\n",
      "Time 889.0 updated\n",
      "Time 889.5 updated\n",
      "Time 890.0 updated\n",
      "Time 890.5 updated\n",
      "Time 891.0 updated\n",
      "Time 891.5 updated\n",
      "Time 892.0 updated\n",
      "Time 892.5 updated\n",
      "Time 893.0 updated\n",
      "Time 893.5 updated\n",
      "Time 894.0 updated\n",
      "Time 894.5 updated\n",
      "Time 895.0 updated\n",
      "Time 895.5 updated\n",
      "Time 896.0 updated\n",
      "Time 896.5 updated\n",
      "Time 897.0 updated\n",
      "Time 897.5 updated\n",
      "Time 898.0 updated\n",
      "Time 898.5 updated\n",
      "Time 899.0 updated\n",
      "Time 899.5 updated\n",
      "Time 900.0 updated\n",
      "Time 900.5 updated\n",
      "Time 901.0 updated\n",
      "Time 901.5 updated\n",
      "Time 902.0 updated\n",
      "Time 902.5 updated\n",
      "Time 903.0 updated\n",
      "Time 903.5 updated\n",
      "Time 904.0 updated\n",
      "Time 904.5 updated\n",
      "Time 905.0 updated\n",
      "Time 905.5 updated\n",
      "Time 906.0 updated\n",
      "Time 906.5 updated\n",
      "Time 907.0 updated\n",
      "Time 907.5 updated\n",
      "Time 908.0 updated\n",
      "Time 908.5 updated\n",
      "Time 909.0 updated\n",
      "Time 909.5 updated\n",
      "Time 910.0 updated\n",
      "Time 910.5 updated\n",
      "Time 911.0 updated\n",
      "Time 911.5 updated\n",
      "Time 912.0 updated\n",
      "Time 912.5 updated\n",
      "Time 913.0 updated\n",
      "Time 913.5 updated\n",
      "Time 914.0 updated\n",
      "Time 914.5 updated\n",
      "Time 915.0 updated\n",
      "Time 915.5 updated\n",
      "Time 916.0 updated\n",
      "Time 916.5 updated\n",
      "Time 917.0 updated\n",
      "Time 917.5 updated\n",
      "Time 918.0 updated\n",
      "Time 918.5 updated\n",
      "Time 919.0 updated\n",
      "Time 919.5 updated\n",
      "Time 920.0 updated\n",
      "Time 920.5 updated\n",
      "Time 921.0 updated\n",
      "Time 921.5 updated\n",
      "Time 922.0 updated\n",
      "Time 922.5 updated\n",
      "Time 923.0 updated\n",
      "Time 923.5 updated\n",
      "Time 924.0 updated\n",
      "Time 924.5 updated\n",
      "Time 925.0 updated\n",
      "Time 925.5 updated\n",
      "Time 926.0 updated\n",
      "Time 926.5 updated\n",
      "Time 927.0 updated\n",
      "Time 927.5 updated\n",
      "Time 928.0 updated\n",
      "Time 928.5 updated\n",
      "Time 929.0 updated\n",
      "Time 929.5 updated\n",
      "Time 930.0 updated\n",
      "Time 930.5 updated\n",
      "Time 931.0 updated\n",
      "Time 931.5 updated\n",
      "Time 932.0 updated\n",
      "Time 932.5 updated\n",
      "Time 933.0 updated\n",
      "Time 933.5 updated\n",
      "Time 934.0 updated\n",
      "Time 934.5 updated\n",
      "Time 935.0 updated\n",
      "Time 935.5 updated\n",
      "Time 936.0 updated\n",
      "Time 936.5 updated\n",
      "Time 937.0 updated\n",
      "Time 937.5 updated\n",
      "Time 938.0 updated\n",
      "Time 938.5 updated\n",
      "Time 939.0 updated\n",
      "Time 939.5 updated\n",
      "Time 940.0 updated\n",
      "Time 940.5 updated\n",
      "Time 941.0 updated\n",
      "Time 941.5 updated\n",
      "Time 942.0 updated\n",
      "Time 942.5 updated\n",
      "Time 943.0 updated\n",
      "Time 943.5 updated\n",
      "Time 944.0 updated\n",
      "Time 944.5 updated\n",
      "Time 945.0 updated\n",
      "Time 945.5 updated\n",
      "Time 946.0 updated\n",
      "Time 946.5 updated\n",
      "Time 947.0 updated\n",
      "Time 947.5 updated\n",
      "Time 948.0 updated\n",
      "Time 948.5 updated\n",
      "Time 949.0 updated\n",
      "Time 949.5 updated\n",
      "Time 950.0 updated\n",
      "Time 950.5 updated\n",
      "Time 951.0 updated\n",
      "Time 951.5 updated\n",
      "Time 952.0 updated\n",
      "Time 952.5 updated\n",
      "Time 953.0 updated\n",
      "Time 953.5 updated\n",
      "Time 954.0 updated\n",
      "Time 954.5 updated\n",
      "Time 955.0 updated\n",
      "Time 955.5 updated\n",
      "Time 956.0 updated\n",
      "Time 956.5 updated\n",
      "Time 957.0 updated\n",
      "Time 957.5 updated\n",
      "Time 958.0 updated\n",
      "Time 958.5 updated\n",
      "Time 959.0 updated\n",
      "Time 959.5 updated\n",
      "Time 960.0 updated\n",
      "Time 960.5 updated\n",
      "Time 961.0 updated\n",
      "Time 961.5 updated\n",
      "Time 962.0 updated\n",
      "Time 962.5 updated\n",
      "Time 963.0 updated\n",
      "Time 963.5 updated\n",
      "Time 964.0 updated\n",
      "Time 964.5 updated\n",
      "Time 965.0 updated\n",
      "Time 965.5 updated\n",
      "Time 966.0 updated\n",
      "Time 966.5 updated\n",
      "Time 967.0 updated\n",
      "Time 967.5 updated\n",
      "Time 968.0 updated\n",
      "Time 968.5 updated\n",
      "Time 969.0 updated\n",
      "Time 969.5 updated\n",
      "Time 970.0 updated\n",
      "Time 970.5 updated\n",
      "Time 971.0 updated\n",
      "Time 971.5 updated\n",
      "Time 972.0 updated\n",
      "Time 972.5 updated\n",
      "Time 973.0 updated\n",
      "Time 973.5 updated\n",
      "Time 974.0 updated\n",
      "Time 974.5 updated\n",
      "Time 975.0 updated\n",
      "Time 975.5 updated\n",
      "Time 976.0 updated\n",
      "Time 976.5 updated\n",
      "Time 977.0 updated\n",
      "Time 977.5 updated\n",
      "Time 978.0 updated\n",
      "Time 978.5 updated\n",
      "Time 979.0 updated\n",
      "Time 979.5 updated\n",
      "Time 980.0 updated\n",
      "Time 980.5 updated\n",
      "Time 981.0 updated\n",
      "Time 981.5 updated\n",
      "Time 982.0 updated\n",
      "Time 982.5 updated\n",
      "Time 983.0 updated\n",
      "Time 983.5 updated\n",
      "Time 984.0 updated\n",
      "Time 984.5 updated\n",
      "Time 985.0 updated\n",
      "Time 985.5 updated\n",
      "Time 986.0 updated\n",
      "Time 986.5 updated\n",
      "Time 987.0 updated\n",
      "Time 987.5 updated\n",
      "Time 988.0 updated\n",
      "Time 988.5 updated\n",
      "Time 989.0 updated\n",
      "Time 989.5 updated\n",
      "Time 990.0 updated\n",
      "Time 990.5 updated\n",
      "Time 991.0 updated\n",
      "Time 991.5 updated\n",
      "Time 992.0 updated\n",
      "Time 992.5 updated\n",
      "Time 993.0 updated\n",
      "Time 993.5 updated\n",
      "Time 994.0 updated\n",
      "Time 994.5 updated\n",
      "Time 995.0 updated\n",
      "Time 995.5 updated\n",
      "Time 996.0 updated\n",
      "Time 996.5 updated\n",
      "Time 997.0 updated\n",
      "Time 997.5 updated\n",
      "Time 998.0 updated\n",
      "Time 998.5 updated\n",
      "Time 999.0 updated\n",
      "Time 999.5 updated\n",
      "Time 1000.0 updated\n",
      "Time 1000.5 updated\n",
      "Time 1001.0 updated\n",
      "Time 1001.5 updated\n",
      "Time 1002.0 updated\n",
      "Time 1002.5 updated\n",
      "Time 1003.0 updated\n",
      "Time 1003.5 updated\n",
      "Time 1004.0 updated\n",
      "Time 1004.5 updated\n",
      "Time 1005.0 updated\n",
      "Time 1005.5 updated\n",
      "Time 1006.0 updated\n",
      "Time 1006.5 updated\n",
      "Time 1007.0 updated\n",
      "Time 1007.5 updated\n",
      "Time 1008.0 updated\n",
      "Time 1008.5 updated\n",
      "Time 1009.0 updated\n",
      "Time 1009.5 updated\n",
      "Time 1010.0 updated\n",
      "Time 1010.5 updated\n",
      "Time 1011.0 updated\n",
      "Time 1011.5 updated\n",
      "Time 1012.0 updated\n",
      "Time 1012.5 updated\n",
      "Time 1013.0 updated\n",
      "Time 1013.5 updated\n",
      "Time 1014.0 updated\n",
      "Time 1014.5 updated\n",
      "Time 1015.0 updated\n",
      "Time 1015.5 updated\n",
      "Time 1016.0 updated\n",
      "Time 1016.5 updated\n",
      "Time 1017.0 updated\n",
      "Time 1017.5 updated\n",
      "Time 1018.0 updated\n",
      "Time 1018.5 updated\n",
      "Time 1019.0 updated\n",
      "Time 1019.5 updated\n",
      "Time 1020.0 updated\n",
      "Time 1020.5 updated\n",
      "Time 1021.0 updated\n",
      "Time 1021.5 updated\n",
      "Time 1022.0 updated\n",
      "Time 1022.5 updated\n",
      "Time 1023.0 updated\n",
      "Time 1023.5 updated\n",
      "Time 1024.0 updated\n",
      "Time 1024.5 updated\n",
      "Time 1025.0 updated\n",
      "Time 1025.5 updated\n",
      "Time 1026.0 updated\n",
      "Time 1026.5 updated\n",
      "Time 1027.0 updated\n",
      "Time 1027.5 updated\n",
      "Time 1028.0 updated\n",
      "Time 1028.5 updated\n",
      "Time 1029.0 updated\n",
      "Time 1029.5 updated\n",
      "Time 1030.0 updated\n",
      "Time 1030.5 updated\n",
      "Time 1031.0 updated\n",
      "Time 1031.5 updated\n",
      "Time 1032.0 updated\n",
      "Time 1032.5 updated\n",
      "Time 1033.0 updated\n",
      "Time 1033.5 updated\n",
      "Time 1034.0 updated\n",
      "Time 1034.5 updated\n",
      "Time 1035.0 updated\n",
      "Time 1035.5 updated\n",
      "Time 1036.0 updated\n",
      "Time 1036.5 updated\n",
      "Time 1037.0 updated\n",
      "Time 1037.5 updated\n",
      "Time 1038.0 updated\n",
      "Time 1038.5 updated\n",
      "Time 1039.0 updated\n",
      "Time 1039.5 updated\n",
      "Time 1040.0 updated\n",
      "Time 1040.5 updated\n",
      "Time 1041.0 updated\n",
      "Time 1041.5 updated\n",
      "Time 1042.0 updated\n",
      "Time 1042.5 updated\n",
      "Time 1043.0 updated\n",
      "Time 1043.5 updated\n",
      "Time 1044.0 updated\n",
      "Time 1044.5 updated\n",
      "Time 1045.0 updated\n",
      "Time 1045.5 updated\n",
      "Time 1046.0 updated\n",
      "Time 1046.5 updated\n",
      "Time 1047.0 updated\n",
      "Time 1047.5 updated\n",
      "Time 1048.0 updated\n",
      "Time 1048.5 updated\n",
      "Time 1049.0 updated\n",
      "Time 1049.5 updated\n",
      "Time 1050.0 updated\n",
      "Time 1050.5 updated\n",
      "Time 1051.0 updated\n",
      "Time 1051.5 updated\n",
      "Time 1052.0 updated\n",
      "Time 1052.5 updated\n",
      "Time 1053.0 updated\n",
      "Time 1053.5 updated\n",
      "Time 1054.0 updated\n",
      "Time 1054.5 updated\n",
      "Time 1055.0 updated\n",
      "Time 1055.5 updated\n",
      "Time 1056.0 updated\n",
      "Time 1056.5 updated\n",
      "Time 1057.0 updated\n",
      "Time 1057.5 updated\n",
      "Time 1058.0 updated\n",
      "Time 1058.5 updated\n",
      "Time 1059.0 updated\n",
      "Time 1059.5 updated\n",
      "Time 1060.0 updated\n",
      "Time 1060.5 updated\n",
      "Time 1061.0 updated\n",
      "Time 1061.5 updated\n",
      "Time 1062.0 updated\n",
      "Time 1062.5 updated\n",
      "Time 1063.0 updated\n",
      "Time 1063.5 updated\n",
      "Time 1064.0 updated\n",
      "Time 1064.5 updated\n",
      "Time 1065.0 updated\n",
      "Time 1065.5 updated\n",
      "Time 1066.0 updated\n",
      "Time 1066.5 updated\n",
      "Time 1067.0 updated\n",
      "Time 1067.5 updated\n",
      "Time 1068.0 updated\n",
      "Time 1068.5 updated\n",
      "Time 1069.0 updated\n",
      "Time 1069.5 updated\n",
      "Time 1070.0 updated\n",
      "Time 1070.5 updated\n",
      "Time 1071.0 updated\n",
      "Time 1071.5 updated\n",
      "Time 1072.0 updated\n",
      "Time 1072.5 updated\n",
      "Time 1073.0 updated\n",
      "Time 1073.5 updated\n",
      "Time 1074.0 updated\n",
      "Time 1074.5 updated\n",
      "Time 1075.0 updated\n",
      "Time 1075.5 updated\n",
      "Time 1076.0 updated\n",
      "Time 1076.5 updated\n",
      "Time 1077.0 updated\n",
      "Time 1077.5 updated\n",
      "Time 1078.0 updated\n",
      "Time 1078.5 updated\n",
      "Time 1079.0 updated\n",
      "Time 1079.5 updated\n",
      "Time 1080.0 updated\n",
      "Time 1080.5 updated\n",
      "Time 1081.0 updated\n",
      "Time 1081.5 updated\n",
      "Time 1082.0 updated\n",
      "Time 1082.5 updated\n",
      "Time 1083.0 updated\n",
      "Time 1083.5 updated\n",
      "Time 1084.0 updated\n",
      "Time 1084.5 updated\n",
      "Time 1085.0 updated\n",
      "Time 1085.5 updated\n",
      "Time 1086.0 updated\n",
      "Time 1086.5 updated\n",
      "Time 1087.0 updated\n",
      "Time 1087.5 updated\n",
      "Time 1088.0 updated\n",
      "Time 1088.5 updated\n",
      "Time 1089.0 updated\n",
      "Time 1089.5 updated\n",
      "Time 1090.0 updated\n",
      "Time 1090.5 updated\n",
      "Time 1091.0 updated\n",
      "Time 1091.5 updated\n",
      "Time 1092.0 updated\n",
      "Time 1092.5 updated\n",
      "Time 1093.0 updated\n",
      "Time 1093.5 updated\n",
      "Time 1094.0 updated\n",
      "Time 1094.5 updated\n",
      "Time 1095.0 updated\n",
      "Time 1095.5 updated\n",
      "Time 1096.0 updated\n",
      "Time 1096.5 updated\n",
      "Time 1097.0 updated\n",
      "Time 1097.5 updated\n",
      "Time 1098.0 updated\n",
      "Time 1098.5 updated\n",
      "Time 1099.0 updated\n",
      "Time 1099.5 updated\n",
      "Time 1100.0 updated\n",
      "Time 1100.5 updated\n",
      "Time 1101.0 updated\n",
      "Time 1101.5 updated\n",
      "Time 1102.0 updated\n",
      "Time 1102.5 updated\n",
      "Time 1103.0 updated\n",
      "Time 1103.5 updated\n",
      "Time 1104.0 updated\n",
      "Time 1104.5 updated\n",
      "Time 1105.0 updated\n",
      "Time 1105.5 updated\n",
      "Time 1106.0 updated\n",
      "Time 1106.5 updated\n",
      "Time 1107.0 updated\n",
      "Time 1107.5 updated\n",
      "Time 1108.0 updated\n",
      "Time 1108.5 updated\n",
      "Time 1109.0 updated\n",
      "Time 1109.5 updated\n",
      "Time 1110.0 updated\n",
      "Time 1110.5 updated\n",
      "Time 1111.0 updated\n",
      "Time 1111.5 updated\n",
      "Time 1112.0 updated\n",
      "Time 1112.5 updated\n",
      "Time 1113.0 updated\n",
      "Time 1113.5 updated\n",
      "Time 1114.0 updated\n",
      "Time 1114.5 updated\n",
      "Time 1115.0 updated\n",
      "Time 1115.5 updated\n",
      "Time 1116.0 updated\n",
      "Time 1116.5 updated\n",
      "Time 1117.0 updated\n",
      "Time 1117.5 updated\n",
      "Time 1118.0 updated\n",
      "Time 1118.5 updated\n",
      "Time 1119.0 updated\n",
      "Time 1119.5 updated\n",
      "Time 1120.0 updated\n",
      "Time 1120.5 updated\n",
      "Time 1121.0 updated\n",
      "Time 1121.5 updated\n",
      "Time 1122.0 updated\n",
      "Time 1122.5 updated\n",
      "Time 1123.0 updated\n",
      "Time 1123.5 updated\n",
      "Time 1124.0 updated\n",
      "Time 1124.5 updated\n",
      "Time 1125.0 updated\n",
      "Time 1125.5 updated\n",
      "Time 1126.0 updated\n",
      "Time 1126.5 updated\n",
      "Time 1127.0 updated\n",
      "Time 1127.5 updated\n",
      "Time 1128.0 updated\n",
      "Time 1128.5 updated\n",
      "Time 1129.0 updated\n",
      "Time 1129.5 updated\n",
      "Time 1130.0 updated\n",
      "Time 1130.5 updated\n",
      "Time 1131.0 updated\n",
      "Time 1131.5 updated\n",
      "Time 1132.0 updated\n",
      "Time 1132.5 updated\n",
      "Time 1133.0 updated\n",
      "Time 1133.5 updated\n",
      "Time 1134.0 updated\n",
      "Time 1134.5 updated\n",
      "Time 1135.0 updated\n",
      "Time 1135.5 updated\n",
      "Time 1136.0 updated\n",
      "Time 1136.5 updated\n",
      "Time 1137.0 updated\n",
      "Time 1137.5 updated\n",
      "Time 1138.0 updated\n",
      "Time 1138.5 updated\n",
      "Time 1139.0 updated\n",
      "Time 1139.5 updated\n",
      "Time 1140.0 updated\n",
      "Time 1140.5 updated\n",
      "Time 1141.0 updated\n",
      "Time 1141.5 updated\n",
      "Time 1142.0 updated\n",
      "Time 1142.5 updated\n",
      "Time 1143.0 updated\n",
      "Time 1143.5 updated\n",
      "Time 1144.0 updated\n",
      "Time 1144.5 updated\n",
      "Time 1145.0 updated\n",
      "Time 1145.5 updated\n",
      "Time 1146.0 updated\n",
      "Time 1146.5 updated\n",
      "Time 1147.0 updated\n",
      "Time 1147.5 updated\n",
      "Time 1148.0 updated\n",
      "Time 1148.5 updated\n",
      "Time 1149.0 updated\n",
      "Time 1149.5 updated\n",
      "Time 1150.0 updated\n",
      "Time 1150.5 updated\n",
      "Time 1151.0 updated\n",
      "Time 1151.5 updated\n",
      "Time 1152.0 updated\n",
      "Time 1152.5 updated\n",
      "Time 1153.0 updated\n",
      "Time 1153.5 updated\n",
      "Time 1154.0 updated\n",
      "Time 1154.5 updated\n",
      "Time 1155.0 updated\n",
      "Time 1155.5 updated\n",
      "Time 1156.0 updated\n",
      "Time 1156.5 updated\n",
      "Time 1157.0 updated\n",
      "Time 1157.5 updated\n",
      "Time 1158.0 updated\n",
      "Time 1158.5 updated\n",
      "Time 1159.0 updated\n",
      "Time 1159.5 updated\n",
      "Time 1160.0 updated\n",
      "Time 1160.5 updated\n",
      "Time 1161.0 updated\n",
      "Time 1161.5 updated\n",
      "Time 1162.0 updated\n",
      "Time 1162.5 updated\n",
      "Time 1163.0 updated\n",
      "Time 1163.5 updated\n",
      "Time 1164.0 updated\n",
      "Time 1164.5 updated\n",
      "Time 1165.0 updated\n",
      "Time 1165.5 updated\n",
      "Time 1166.0 updated\n",
      "Time 1166.5 updated\n",
      "Time 1167.0 updated\n",
      "Time 1167.5 updated\n",
      "Time 1168.0 updated\n",
      "Time 1168.5 updated\n",
      "Time 1169.0 updated\n",
      "Time 1169.5 updated\n",
      "Time 1170.0 updated\n",
      "Time 1170.5 updated\n",
      "Time 1171.0 updated\n",
      "Time 1171.5 updated\n",
      "Time 1172.0 updated\n",
      "Time 1172.5 updated\n",
      "Time 1173.0 updated\n",
      "Time 1173.5 updated\n",
      "Time 1174.0 updated\n",
      "Time 1174.5 updated\n",
      "Time 1175.0 updated\n",
      "Time 1175.5 updated\n",
      "Time 1176.0 updated\n",
      "Time 1176.5 updated\n",
      "Time 1177.0 updated\n",
      "Time 1177.5 updated\n",
      "Time 1178.0 updated\n",
      "Time 1178.5 updated\n",
      "Time 1179.0 updated\n",
      "Time 1179.5 updated\n",
      "Time 1180.0 updated\n",
      "Time 1180.5 updated\n",
      "Time 1181.0 updated\n",
      "Time 1181.5 updated\n",
      "Time 1182.0 updated\n",
      "Time 1182.5 updated\n",
      "Time 1183.0 updated\n",
      "Time 1183.5 updated\n",
      "Time 1184.0 updated\n",
      "Time 1184.5 updated\n",
      "Time 1185.0 updated\n",
      "Time 1185.5 updated\n",
      "Time 1186.0 updated\n",
      "Time 1186.5 updated\n",
      "Time 1187.0 updated\n",
      "Time 1187.5 updated\n",
      "Time 1188.0 updated\n",
      "Time 1188.5 updated\n",
      "Time 1189.0 updated\n",
      "Time 1189.5 updated\n",
      "Time 1190.0 updated\n",
      "Time 1190.5 updated\n",
      "Time 1191.0 updated\n",
      "Time 1191.5 updated\n",
      "Time 1192.0 updated\n",
      "Time 1192.5 updated\n",
      "Time 1193.0 updated\n",
      "Time 1193.5 updated\n",
      "Time 1194.0 updated\n",
      "Time 1194.5 updated\n",
      "Time 1195.0 updated\n",
      "Time 1195.5 updated\n",
      "Time 1196.0 updated\n",
      "Time 1196.5 updated\n",
      "Time 1197.0 updated\n",
      "Time 1197.5 updated\n",
      "Time 1198.0 updated\n",
      "Time 1198.5 updated\n",
      "Time 1199.0 updated\n",
      "Time 1199.5 updated\n",
      "Time 1200.0 updated\n",
      "Time 1200.5 updated\n",
      "Time 1201.0 updated\n",
      "Time 1201.5 updated\n",
      "Time 1202.0 updated\n",
      "Time 1202.5 updated\n",
      "Time 1203.0 updated\n",
      "Time 1203.5 updated\n",
      "Time 1204.0 updated\n",
      "Time 1204.5 updated\n",
      "Time 1205.0 updated\n",
      "Time 1205.5 updated\n",
      "Time 1206.0 updated\n",
      "Time 1206.5 updated\n",
      "Time 1207.0 updated\n",
      "Time 1207.5 updated\n",
      "Time 1208.0 updated\n",
      "Time 1208.5 updated\n",
      "Time 1209.0 updated\n",
      "Time 1209.5 updated\n",
      "Time 1210.0 updated\n",
      "Time 1210.5 updated\n",
      "Time 1211.0 updated\n",
      "Time 1211.5 updated\n",
      "Time 1212.0 updated\n",
      "Time 1212.5 updated\n",
      "Time 1213.0 updated\n",
      "Time 1213.5 updated\n",
      "Time 1214.0 updated\n",
      "Time 1214.5 updated\n",
      "Time 1215.0 updated\n",
      "Time 1215.5 updated\n",
      "Time 1216.0 updated\n",
      "Time 1216.5 updated\n",
      "Time 1217.0 updated\n",
      "Time 1217.5 updated\n",
      "Time 1218.0 updated\n",
      "Time 1218.5 updated\n",
      "Time 1219.0 updated\n",
      "Time 1219.5 updated\n",
      "Time 1220.0 updated\n",
      "Time 1220.5 updated\n",
      "Time 1221.0 updated\n",
      "Time 1221.5 updated\n",
      "Time 1222.0 updated\n",
      "Time 1222.5 updated\n",
      "Time 1223.0 updated\n",
      "Time 1223.5 updated\n",
      "Time 1224.0 updated\n",
      "Time 1224.5 updated\n",
      "Time 1225.0 updated\n",
      "Time 1225.5 updated\n",
      "Time 1226.0 updated\n",
      "Time 1226.5 updated\n",
      "Time 1227.0 updated\n",
      "Time 1227.5 updated\n",
      "Time 1228.0 updated\n",
      "Time 1228.5 updated\n",
      "Time 1229.0 updated\n",
      "Time 1229.5 updated\n",
      "Time 1230.0 updated\n",
      "Time 1230.5 updated\n",
      "Time 1231.0 updated\n",
      "Time 1231.5 updated\n",
      "Time 1232.0 updated\n",
      "Time 1232.5 updated\n",
      "Time 1233.0 updated\n",
      "Time 1233.5 updated\n",
      "Time 1234.0 updated\n",
      "Time 1234.5 updated\n",
      "Time 1235.0 updated\n",
      "Time 1235.5 updated\n",
      "Time 1236.0 updated\n",
      "Time 1236.5 updated\n",
      "Time 1237.0 updated\n",
      "Time 1237.5 updated\n",
      "Time 1238.0 updated\n",
      "Time 1238.5 updated\n",
      "Time 1239.0 updated\n",
      "Time 1239.5 updated\n",
      "Time 1240.0 updated\n",
      "Time 1240.5 updated\n",
      "Time 1241.0 updated\n",
      "Time 1241.5 updated\n",
      "Time 1242.0 updated\n",
      "Time 1242.5 updated\n",
      "Time 1243.0 updated\n",
      "Time 1243.5 updated\n",
      "Time 1244.0 updated\n",
      "Time 1244.5 updated\n",
      "Time 1245.0 updated\n",
      "Time 1245.5 updated\n",
      "Time 1246.0 updated\n",
      "Time 1246.5 updated\n",
      "Time 1247.0 updated\n",
      "Time 1247.5 updated\n",
      "Time 1248.0 updated\n",
      "Time 1248.5 updated\n",
      "Time 1249.0 updated\n",
      "Time 1249.5 updated\n",
      "Time 1250.0 updated\n",
      "Time 1250.5 updated\n",
      "Time 1251.0 updated\n",
      "Time 1251.5 updated\n",
      "Time 1252.0 updated\n",
      "Time 1252.5 updated\n",
      "Time 1253.0 updated\n",
      "Time 1253.5 updated\n",
      "Time 1254.0 updated\n",
      "Time 1254.5 updated\n",
      "Time 1255.0 updated\n",
      "Time 1255.5 updated\n",
      "Time 1256.0 updated\n",
      "Time 1256.5 updated\n",
      "Time 1257.0 updated\n",
      "Time 1257.5 updated\n",
      "Time 1258.0 updated\n",
      "Time 1258.5 updated\n",
      "Time 1259.0 updated\n",
      "Time 1259.5 updated\n",
      "Time 1260.0 updated\n",
      "Time 1260.5 updated\n",
      "Time 1261.0 updated\n",
      "Time 1261.5 updated\n",
      "Time 1262.0 updated\n",
      "Time 1262.5 updated\n",
      "Time 1263.0 updated\n",
      "Time 1263.5 updated\n",
      "Time 1264.0 updated\n",
      "Time 1264.5 updated\n",
      "Time 1265.0 updated\n",
      "Time 1265.5 updated\n",
      "Time 1266.0 updated\n",
      "Time 1266.5 updated\n",
      "Time 1267.0 updated\n",
      "Time 1267.5 updated\n",
      "Time 1268.0 updated\n",
      "Time 1268.5 updated\n",
      "Time 1269.0 updated\n",
      "Time 1269.5 updated\n",
      "Time 1270.0 updated\n",
      "Time 1270.5 updated\n",
      "Time 1271.0 updated\n",
      "Time 1271.5 updated\n",
      "Time 1272.0 updated\n",
      "Time 1272.5 updated\n",
      "Time 1273.0 updated\n",
      "Time 1273.5 updated\n",
      "Time 1274.0 updated\n",
      "Time 1274.5 updated\n",
      "Time 1275.0 updated\n",
      "Time 1275.5 updated\n",
      "Time 1276.0 updated\n",
      "Time 1276.5 updated\n",
      "Time 1277.0 updated\n",
      "Time 1277.5 updated\n",
      "Time 1278.0 updated\n",
      "Time 1278.5 updated\n",
      "Time 1279.0 updated\n",
      "Time 1279.5 updated\n",
      "Time 1280.0 updated\n",
      "Time 1280.5 updated\n",
      "Time 1281.0 updated\n",
      "Time 1281.5 updated\n",
      "Time 1282.0 updated\n",
      "Time 1282.5 updated\n",
      "Time 1283.0 updated\n",
      "Time 1283.5 updated\n",
      "Time 1284.0 updated\n",
      "Time 1284.5 updated\n",
      "Time 1285.0 updated\n",
      "Time 1285.5 updated\n",
      "Time 1286.0 updated\n",
      "Time 1286.5 updated\n",
      "Time 1287.0 updated\n",
      "Time 1287.5 updated\n",
      "Time 1288.0 updated\n",
      "Time 1288.5 updated\n",
      "Time 1289.0 updated\n",
      "Time 1289.5 updated\n",
      "Time 1290.0 updated\n",
      "Time 1290.5 updated\n",
      "Time 1291.0 updated\n",
      "Time 1291.5 updated\n",
      "Time 1292.0 updated\n",
      "Time 1292.5 updated\n",
      "Time 1293.0 updated\n",
      "Time 1293.5 updated\n",
      "Time 1294.0 updated\n",
      "Time 1294.5 updated\n",
      "Time 1295.0 updated\n",
      "Time 1295.5 updated\n",
      "Time 1296.0 updated\n",
      "Time 1296.5 updated\n",
      "Time 1297.0 updated\n",
      "Time 1297.5 updated\n",
      "Time 1298.0 updated\n",
      "Time 1298.5 updated\n",
      "Time 1299.0 updated\n",
      "Time 1299.5 updated\n",
      "Time 1300.0 updated\n",
      "Time 1300.5 updated\n",
      "Time 1301.0 updated\n",
      "Time 1301.5 updated\n",
      "Time 1302.0 updated\n",
      "Time 1302.5 updated\n",
      "Time 1303.0 updated\n",
      "Time 1303.5 updated\n",
      "Time 1304.0 updated\n",
      "Time 1304.5 updated\n",
      "Time 1305.0 updated\n",
      "Time 1305.5 updated\n",
      "Time 1306.0 updated\n",
      "Time 1306.5 updated\n",
      "Time 1307.0 updated\n",
      "Time 1307.5 updated\n",
      "Time 1308.0 updated\n",
      "Time 1308.5 updated\n",
      "Time 1309.0 updated\n",
      "Time 1309.5 updated\n",
      "Time 1310.0 updated\n",
      "Time 1310.5 updated\n",
      "Time 1311.0 updated\n",
      "Time 1311.5 updated\n",
      "Time 1312.0 updated\n",
      "Time 1312.5 updated\n",
      "Time 1313.0 updated\n",
      "Time 1313.5 updated\n",
      "Time 1314.0 updated\n",
      "Time 1314.5 updated\n",
      "Time 1315.0 updated\n",
      "Time 1315.5 updated\n",
      "Time 1316.0 updated\n",
      "Time 1316.5 updated\n",
      "Time 1317.0 updated\n",
      "Time 1317.5 updated\n",
      "Time 1318.0 updated\n",
      "Time 1318.5 updated\n",
      "Time 1319.0 updated\n",
      "Time 1319.5 updated\n",
      "Time 1320.0 updated\n",
      "Time 1320.5 updated\n",
      "Time 1321.0 updated\n",
      "Time 1321.5 updated\n",
      "Time 1322.0 updated\n",
      "Time 1322.5 updated\n",
      "Time 1323.0 updated\n",
      "Time 1323.5 updated\n",
      "Time 1324.0 updated\n",
      "Time 1324.5 updated\n",
      "Time 1325.0 updated\n",
      "Time 1325.5 updated\n",
      "Time 1326.0 updated\n",
      "Time 1326.5 updated\n",
      "Time 1327.0 updated\n",
      "Time 1327.5 updated\n",
      "Time 1328.0 updated\n",
      "Time 1328.5 updated\n",
      "Time 1329.0 updated\n",
      "Time 1329.5 updated\n",
      "Time 1330.0 updated\n",
      "Time 1330.5 updated\n",
      "Time 1331.0 updated\n",
      "Time 1331.5 updated\n",
      "Time 1332.0 updated\n",
      "Time 1332.5 updated\n",
      "Time 1333.0 updated\n",
      "Time 1333.5 updated\n",
      "Time 1334.0 updated\n",
      "Time 1334.5 updated\n",
      "Time 1335.0 updated\n",
      "Time 1335.5 updated\n",
      "Time 1336.0 updated\n",
      "Time 1336.5 updated\n",
      "Time 1337.0 updated\n",
      "Time 1337.5 updated\n",
      "Time 1338.0 updated\n",
      "Time 1338.5 updated\n",
      "Time 1339.0 updated\n",
      "Time 1339.5 updated\n",
      "Time 1340.0 updated\n",
      "Time 1340.5 updated\n",
      "Time 1341.0 updated\n",
      "Time 1341.5 updated\n",
      "Time 1342.0 updated\n",
      "Time 1342.5 updated\n",
      "Time 1343.0 updated\n",
      "Time 1343.5 updated\n",
      "Time 1344.0 updated\n",
      "Time 1344.5 updated\n",
      "Time 1345.0 updated\n",
      "Time 1345.5 updated\n",
      "Time 1346.0 updated\n",
      "Time 1346.5 updated\n",
      "Time 1347.0 updated\n",
      "Time 1347.5 updated\n",
      "Time 1348.0 updated\n",
      "Time 1348.5 updated\n",
      "Time 1349.0 updated\n",
      "Time 1349.5 updated\n",
      "Time 1350.0 updated\n",
      "Time 1350.5 updated\n",
      "Time 1351.0 updated\n",
      "Time 1351.5 updated\n",
      "Time 1352.0 updated\n",
      "Time 1352.5 updated\n",
      "Time 1353.0 updated\n",
      "Time 1353.5 updated\n",
      "Time 1354.0 updated\n",
      "Time 1354.5 updated\n",
      "Time 1355.0 updated\n",
      "Time 1355.5 updated\n",
      "Time 1356.0 updated\n",
      "Time 1356.5 updated\n",
      "Time 1357.0 updated\n",
      "Time 1357.5 updated\n",
      "Time 1358.0 updated\n",
      "Time 1358.5 updated\n",
      "Time 1359.0 updated\n",
      "Time 1359.5 updated\n",
      "Time 1360.0 updated\n",
      "Time 1360.5 updated\n",
      "Time 1361.0 updated\n",
      "Time 1361.5 updated\n",
      "Time 1362.0 updated\n",
      "Time 1362.5 updated\n",
      "Time 1363.0 updated\n",
      "Time 1363.5 updated\n",
      "Time 1364.0 updated\n",
      "Time 1364.5 updated\n",
      "Time 1365.0 updated\n",
      "Time 1365.5 updated\n",
      "Time 1366.0 updated\n",
      "Time 1366.5 updated\n",
      "Time 1367.0 updated\n",
      "Time 1367.5 updated\n",
      "Time 1368.0 updated\n",
      "Time 1368.5 updated\n",
      "Time 1369.0 updated\n",
      "Time 1369.5 updated\n",
      "Time 1370.0 updated\n",
      "Time 1370.5 updated\n",
      "Time 1371.0 updated\n",
      "Time 1371.5 updated\n",
      "Time 1372.0 updated\n",
      "Time 1372.5 updated\n",
      "Time 1373.0 updated\n",
      "Time 1373.5 updated\n",
      "Time 1374.0 updated\n",
      "Time 1374.5 updated\n",
      "Time 1375.0 updated\n",
      "Time 1375.5 updated\n",
      "Time 1376.0 updated\n",
      "Time 1376.5 updated\n",
      "Time 1377.0 updated\n",
      "Time 1377.5 updated\n",
      "Time 1378.0 updated\n",
      "Time 1378.5 updated\n",
      "Time 1379.0 updated\n",
      "Time 1379.5 updated\n",
      "Time 1380.0 updated\n",
      "Time 1380.5 updated\n",
      "Time 1381.0 updated\n",
      "Time 1381.5 updated\n",
      "Time 1382.0 updated\n",
      "Time 1382.5 updated\n",
      "Time 1383.0 updated\n",
      "Time 1383.5 updated\n",
      "Time 1384.0 updated\n",
      "Time 1384.5 updated\n",
      "Time 1385.0 updated\n",
      "Time 1385.5 updated\n",
      "Time 1386.0 updated\n",
      "Time 1386.5 updated\n",
      "Time 1387.0 updated\n",
      "Time 1387.5 updated\n",
      "Time 1388.0 updated\n",
      "Time 1388.5 updated\n",
      "Time 1389.0 updated\n",
      "Time 1389.5 updated\n",
      "Time 1390.0 updated\n",
      "Time 1390.5 updated\n",
      "Time 1391.0 updated\n",
      "Time 1391.5 updated\n",
      "Time 1392.0 updated\n",
      "Time 1392.5 updated\n",
      "Time 1393.0 updated\n",
      "Time 1393.5 updated\n",
      "Time 1394.0 updated\n",
      "Time 1394.5 updated\n",
      "Time 1395.0 updated\n",
      "Time 1395.5 updated\n",
      "Time 1396.0 updated\n",
      "Time 1396.5 updated\n",
      "Time 1397.0 updated\n",
      "Time 1397.5 updated\n",
      "Time 1398.0 updated\n",
      "Time 1398.5 updated\n",
      "Time 1399.0 updated\n",
      "Time 1399.5 updated\n",
      "Time 1400.0 updated\n",
      "Time 1400.5 updated\n"
     ]
    }
   ],
   "source": [
    "model = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"model_564395852.pth\", weights_only=False))\n",
    "\n",
    "# Generate\n",
    "diff_sample_1 = generation(model, 250, n, 0.5, 4 * n, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "ffbf4334-b5c3-4a4c-ae87-072b6b74d174",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.5 updated\n",
      "Time 1.0 updated\n",
      "Time 1.5 updated\n",
      "Time 2.0 updated\n",
      "Time 2.5 updated\n",
      "Time 3.0 updated\n",
      "Time 3.5 updated\n",
      "Time 4.0 updated\n",
      "Time 4.5 updated\n",
      "Time 5.0 updated\n",
      "Time 5.5 updated\n",
      "Time 6.0 updated\n",
      "Time 6.5 updated\n",
      "Time 7.0 updated\n",
      "Time 7.5 updated\n",
      "Time 8.0 updated\n",
      "Time 8.5 updated\n",
      "Time 9.0 updated\n",
      "Time 9.5 updated\n",
      "Time 10.0 updated\n",
      "Time 10.5 updated\n",
      "Time 11.0 updated\n",
      "Time 11.5 updated\n",
      "Time 12.0 updated\n",
      "Time 12.5 updated\n",
      "Time 13.0 updated\n",
      "Time 13.5 updated\n",
      "Time 14.0 updated\n",
      "Time 14.5 updated\n",
      "Time 15.0 updated\n",
      "Time 15.5 updated\n",
      "Time 16.0 updated\n",
      "Time 16.5 updated\n",
      "Time 17.0 updated\n",
      "Time 17.5 updated\n",
      "Time 18.0 updated\n",
      "Time 18.5 updated\n",
      "Time 19.0 updated\n",
      "Time 19.5 updated\n",
      "Time 20.0 updated\n",
      "Time 20.5 updated\n",
      "Time 21.0 updated\n",
      "Time 21.5 updated\n",
      "Time 22.0 updated\n",
      "Time 22.5 updated\n",
      "Time 23.0 updated\n",
      "Time 23.5 updated\n",
      "Time 24.0 updated\n",
      "Time 24.5 updated\n",
      "Time 25.0 updated\n",
      "Time 25.5 updated\n",
      "Time 26.0 updated\n",
      "Time 26.5 updated\n",
      "Time 27.0 updated\n",
      "Time 27.5 updated\n",
      "Time 28.0 updated\n",
      "Time 28.5 updated\n",
      "Time 29.0 updated\n",
      "Time 29.5 updated\n",
      "Time 30.0 updated\n",
      "Time 30.5 updated\n",
      "Time 31.0 updated\n",
      "Time 31.5 updated\n",
      "Time 32.0 updated\n",
      "Time 32.5 updated\n",
      "Time 33.0 updated\n",
      "Time 33.5 updated\n",
      "Time 34.0 updated\n",
      "Time 34.5 updated\n",
      "Time 35.0 updated\n",
      "Time 35.5 updated\n",
      "Time 36.0 updated\n",
      "Time 36.5 updated\n",
      "Time 37.0 updated\n",
      "Time 37.5 updated\n",
      "Time 38.0 updated\n",
      "Time 38.5 updated\n",
      "Time 39.0 updated\n",
      "Time 39.5 updated\n",
      "Time 40.0 updated\n",
      "Time 40.5 updated\n",
      "Time 41.0 updated\n",
      "Time 41.5 updated\n",
      "Time 42.0 updated\n",
      "Time 42.5 updated\n",
      "Time 43.0 updated\n",
      "Time 43.5 updated\n",
      "Time 44.0 updated\n",
      "Time 44.5 updated\n",
      "Time 45.0 updated\n",
      "Time 45.5 updated\n",
      "Time 46.0 updated\n",
      "Time 46.5 updated\n",
      "Time 47.0 updated\n",
      "Time 47.5 updated\n",
      "Time 48.0 updated\n",
      "Time 48.5 updated\n",
      "Time 49.0 updated\n",
      "Time 49.5 updated\n",
      "Time 50.0 updated\n",
      "Time 50.5 updated\n",
      "Time 51.0 updated\n",
      "Time 51.5 updated\n",
      "Time 52.0 updated\n",
      "Time 52.5 updated\n",
      "Time 53.0 updated\n",
      "Time 53.5 updated\n",
      "Time 54.0 updated\n",
      "Time 54.5 updated\n",
      "Time 55.0 updated\n",
      "Time 55.5 updated\n",
      "Time 56.0 updated\n",
      "Time 56.5 updated\n",
      "Time 57.0 updated\n",
      "Time 57.5 updated\n",
      "Time 58.0 updated\n",
      "Time 58.5 updated\n",
      "Time 59.0 updated\n",
      "Time 59.5 updated\n",
      "Time 60.0 updated\n",
      "Time 60.5 updated\n",
      "Time 61.0 updated\n",
      "Time 61.5 updated\n",
      "Time 62.0 updated\n",
      "Time 62.5 updated\n",
      "Time 63.0 updated\n",
      "Time 63.5 updated\n",
      "Time 64.0 updated\n",
      "Time 64.5 updated\n",
      "Time 65.0 updated\n",
      "Time 65.5 updated\n",
      "Time 66.0 updated\n",
      "Time 66.5 updated\n",
      "Time 67.0 updated\n",
      "Time 67.5 updated\n",
      "Time 68.0 updated\n",
      "Time 68.5 updated\n",
      "Time 69.0 updated\n",
      "Time 69.5 updated\n",
      "Time 70.0 updated\n",
      "Time 70.5 updated\n",
      "Time 71.0 updated\n",
      "Time 71.5 updated\n",
      "Time 72.0 updated\n",
      "Time 72.5 updated\n",
      "Time 73.0 updated\n",
      "Time 73.5 updated\n",
      "Time 74.0 updated\n",
      "Time 74.5 updated\n",
      "Time 75.0 updated\n",
      "Time 75.5 updated\n",
      "Time 76.0 updated\n",
      "Time 76.5 updated\n",
      "Time 77.0 updated\n",
      "Time 77.5 updated\n",
      "Time 78.0 updated\n",
      "Time 78.5 updated\n",
      "Time 79.0 updated\n",
      "Time 79.5 updated\n",
      "Time 80.0 updated\n",
      "Time 80.5 updated\n",
      "Time 81.0 updated\n",
      "Time 81.5 updated\n",
      "Time 82.0 updated\n",
      "Time 82.5 updated\n",
      "Time 83.0 updated\n",
      "Time 83.5 updated\n",
      "Time 84.0 updated\n",
      "Time 84.5 updated\n",
      "Time 85.0 updated\n",
      "Time 85.5 updated\n",
      "Time 86.0 updated\n",
      "Time 86.5 updated\n",
      "Time 87.0 updated\n",
      "Time 87.5 updated\n",
      "Time 88.0 updated\n",
      "Time 88.5 updated\n",
      "Time 89.0 updated\n",
      "Time 89.5 updated\n",
      "Time 90.0 updated\n",
      "Time 90.5 updated\n",
      "Time 91.0 updated\n",
      "Time 91.5 updated\n",
      "Time 92.0 updated\n",
      "Time 92.5 updated\n",
      "Time 93.0 updated\n",
      "Time 93.5 updated\n",
      "Time 94.0 updated\n",
      "Time 94.5 updated\n",
      "Time 95.0 updated\n",
      "Time 95.5 updated\n",
      "Time 96.0 updated\n",
      "Time 96.5 updated\n",
      "Time 97.0 updated\n",
      "Time 97.5 updated\n",
      "Time 98.0 updated\n",
      "Time 98.5 updated\n",
      "Time 99.0 updated\n",
      "Time 99.5 updated\n",
      "Time 100.0 updated\n",
      "Time 100.5 updated\n",
      "Time 101.0 updated\n",
      "Time 101.5 updated\n",
      "Time 102.0 updated\n",
      "Time 102.5 updated\n",
      "Time 103.0 updated\n",
      "Time 103.5 updated\n",
      "Time 104.0 updated\n",
      "Time 104.5 updated\n",
      "Time 105.0 updated\n",
      "Time 105.5 updated\n",
      "Time 106.0 updated\n",
      "Time 106.5 updated\n",
      "Time 107.0 updated\n",
      "Time 107.5 updated\n",
      "Time 108.0 updated\n",
      "Time 108.5 updated\n",
      "Time 109.0 updated\n",
      "Time 109.5 updated\n",
      "Time 110.0 updated\n",
      "Time 110.5 updated\n",
      "Time 111.0 updated\n",
      "Time 111.5 updated\n",
      "Time 112.0 updated\n",
      "Time 112.5 updated\n",
      "Time 113.0 updated\n",
      "Time 113.5 updated\n",
      "Time 114.0 updated\n",
      "Time 114.5 updated\n",
      "Time 115.0 updated\n",
      "Time 115.5 updated\n",
      "Time 116.0 updated\n",
      "Time 116.5 updated\n",
      "Time 117.0 updated\n",
      "Time 117.5 updated\n",
      "Time 118.0 updated\n",
      "Time 118.5 updated\n",
      "Time 119.0 updated\n",
      "Time 119.5 updated\n",
      "Time 120.0 updated\n",
      "Time 120.5 updated\n",
      "Time 121.0 updated\n",
      "Time 121.5 updated\n",
      "Time 122.0 updated\n",
      "Time 122.5 updated\n",
      "Time 123.0 updated\n",
      "Time 123.5 updated\n",
      "Time 124.0 updated\n",
      "Time 124.5 updated\n",
      "Time 125.0 updated\n",
      "Time 125.5 updated\n",
      "Time 126.0 updated\n",
      "Time 126.5 updated\n",
      "Time 127.0 updated\n",
      "Time 127.5 updated\n",
      "Time 128.0 updated\n",
      "Time 128.5 updated\n",
      "Time 129.0 updated\n",
      "Time 129.5 updated\n",
      "Time 130.0 updated\n",
      "Time 130.5 updated\n",
      "Time 131.0 updated\n",
      "Time 131.5 updated\n",
      "Time 132.0 updated\n",
      "Time 132.5 updated\n",
      "Time 133.0 updated\n",
      "Time 133.5 updated\n",
      "Time 134.0 updated\n",
      "Time 134.5 updated\n",
      "Time 135.0 updated\n",
      "Time 135.5 updated\n",
      "Time 136.0 updated\n",
      "Time 136.5 updated\n",
      "Time 137.0 updated\n",
      "Time 137.5 updated\n",
      "Time 138.0 updated\n",
      "Time 138.5 updated\n",
      "Time 139.0 updated\n",
      "Time 139.5 updated\n",
      "Time 140.0 updated\n",
      "Time 140.5 updated\n",
      "Time 141.0 updated\n",
      "Time 141.5 updated\n",
      "Time 142.0 updated\n",
      "Time 142.5 updated\n",
      "Time 143.0 updated\n",
      "Time 143.5 updated\n",
      "Time 144.0 updated\n",
      "Time 144.5 updated\n",
      "Time 145.0 updated\n",
      "Time 145.5 updated\n",
      "Time 146.0 updated\n",
      "Time 146.5 updated\n",
      "Time 147.0 updated\n",
      "Time 147.5 updated\n",
      "Time 148.0 updated\n",
      "Time 148.5 updated\n",
      "Time 149.0 updated\n",
      "Time 149.5 updated\n",
      "Time 150.0 updated\n",
      "Time 150.5 updated\n",
      "Time 151.0 updated\n",
      "Time 151.5 updated\n",
      "Time 152.0 updated\n",
      "Time 152.5 updated\n",
      "Time 153.0 updated\n",
      "Time 153.5 updated\n",
      "Time 154.0 updated\n",
      "Time 154.5 updated\n",
      "Time 155.0 updated\n",
      "Time 155.5 updated\n",
      "Time 156.0 updated\n",
      "Time 156.5 updated\n",
      "Time 157.0 updated\n",
      "Time 157.5 updated\n",
      "Time 158.0 updated\n",
      "Time 158.5 updated\n",
      "Time 159.0 updated\n",
      "Time 159.5 updated\n",
      "Time 160.0 updated\n",
      "Time 160.5 updated\n",
      "Time 161.0 updated\n",
      "Time 161.5 updated\n",
      "Time 162.0 updated\n",
      "Time 162.5 updated\n",
      "Time 163.0 updated\n",
      "Time 163.5 updated\n",
      "Time 164.0 updated\n",
      "Time 164.5 updated\n",
      "Time 165.0 updated\n",
      "Time 165.5 updated\n",
      "Time 166.0 updated\n",
      "Time 166.5 updated\n",
      "Time 167.0 updated\n",
      "Time 167.5 updated\n",
      "Time 168.0 updated\n",
      "Time 168.5 updated\n",
      "Time 169.0 updated\n",
      "Time 169.5 updated\n",
      "Time 170.0 updated\n",
      "Time 170.5 updated\n",
      "Time 171.0 updated\n",
      "Time 171.5 updated\n",
      "Time 172.0 updated\n",
      "Time 172.5 updated\n",
      "Time 173.0 updated\n",
      "Time 173.5 updated\n",
      "Time 174.0 updated\n",
      "Time 174.5 updated\n",
      "Time 175.0 updated\n",
      "Time 175.5 updated\n",
      "Time 176.0 updated\n",
      "Time 176.5 updated\n",
      "Time 177.0 updated\n",
      "Time 177.5 updated\n",
      "Time 178.0 updated\n",
      "Time 178.5 updated\n",
      "Time 179.0 updated\n",
      "Time 179.5 updated\n",
      "Time 180.0 updated\n",
      "Time 180.5 updated\n",
      "Time 181.0 updated\n",
      "Time 181.5 updated\n",
      "Time 182.0 updated\n",
      "Time 182.5 updated\n",
      "Time 183.0 updated\n",
      "Time 183.5 updated\n",
      "Time 184.0 updated\n",
      "Time 184.5 updated\n",
      "Time 185.0 updated\n",
      "Time 185.5 updated\n",
      "Time 186.0 updated\n",
      "Time 186.5 updated\n",
      "Time 187.0 updated\n",
      "Time 187.5 updated\n",
      "Time 188.0 updated\n",
      "Time 188.5 updated\n",
      "Time 189.0 updated\n",
      "Time 189.5 updated\n",
      "Time 190.0 updated\n",
      "Time 190.5 updated\n",
      "Time 191.0 updated\n",
      "Time 191.5 updated\n",
      "Time 192.0 updated\n",
      "Time 192.5 updated\n",
      "Time 193.0 updated\n",
      "Time 193.5 updated\n",
      "Time 194.0 updated\n",
      "Time 194.5 updated\n",
      "Time 195.0 updated\n",
      "Time 195.5 updated\n",
      "Time 196.0 updated\n",
      "Time 196.5 updated\n",
      "Time 197.0 updated\n",
      "Time 197.5 updated\n",
      "Time 198.0 updated\n",
      "Time 198.5 updated\n",
      "Time 199.0 updated\n",
      "Time 199.5 updated\n",
      "Time 200.0 updated\n",
      "Time 200.5 updated\n",
      "Time 201.0 updated\n",
      "Time 201.5 updated\n",
      "Time 202.0 updated\n",
      "Time 202.5 updated\n",
      "Time 203.0 updated\n",
      "Time 203.5 updated\n",
      "Time 204.0 updated\n",
      "Time 204.5 updated\n",
      "Time 205.0 updated\n",
      "Time 205.5 updated\n",
      "Time 206.0 updated\n",
      "Time 206.5 updated\n",
      "Time 207.0 updated\n",
      "Time 207.5 updated\n",
      "Time 208.0 updated\n",
      "Time 208.5 updated\n",
      "Time 209.0 updated\n",
      "Time 209.5 updated\n",
      "Time 210.0 updated\n",
      "Time 210.5 updated\n",
      "Time 211.0 updated\n",
      "Time 211.5 updated\n",
      "Time 212.0 updated\n",
      "Time 212.5 updated\n",
      "Time 213.0 updated\n",
      "Time 213.5 updated\n",
      "Time 214.0 updated\n",
      "Time 214.5 updated\n",
      "Time 215.0 updated\n",
      "Time 215.5 updated\n",
      "Time 216.0 updated\n",
      "Time 216.5 updated\n",
      "Time 217.0 updated\n",
      "Time 217.5 updated\n",
      "Time 218.0 updated\n",
      "Time 218.5 updated\n",
      "Time 219.0 updated\n",
      "Time 219.5 updated\n",
      "Time 220.0 updated\n",
      "Time 220.5 updated\n",
      "Time 221.0 updated\n",
      "Time 221.5 updated\n",
      "Time 222.0 updated\n",
      "Time 222.5 updated\n",
      "Time 223.0 updated\n",
      "Time 223.5 updated\n",
      "Time 224.0 updated\n",
      "Time 224.5 updated\n",
      "Time 225.0 updated\n",
      "Time 225.5 updated\n",
      "Time 226.0 updated\n",
      "Time 226.5 updated\n",
      "Time 227.0 updated\n",
      "Time 227.5 updated\n",
      "Time 228.0 updated\n",
      "Time 228.5 updated\n",
      "Time 229.0 updated\n",
      "Time 229.5 updated\n",
      "Time 230.0 updated\n",
      "Time 230.5 updated\n",
      "Time 231.0 updated\n",
      "Time 231.5 updated\n",
      "Time 232.0 updated\n",
      "Time 232.5 updated\n",
      "Time 233.0 updated\n",
      "Time 233.5 updated\n",
      "Time 234.0 updated\n",
      "Time 234.5 updated\n",
      "Time 235.0 updated\n",
      "Time 235.5 updated\n",
      "Time 236.0 updated\n",
      "Time 236.5 updated\n",
      "Time 237.0 updated\n",
      "Time 237.5 updated\n",
      "Time 238.0 updated\n",
      "Time 238.5 updated\n",
      "Time 239.0 updated\n",
      "Time 239.5 updated\n",
      "Time 240.0 updated\n",
      "Time 240.5 updated\n",
      "Time 241.0 updated\n",
      "Time 241.5 updated\n",
      "Time 242.0 updated\n",
      "Time 242.5 updated\n",
      "Time 243.0 updated\n",
      "Time 243.5 updated\n",
      "Time 244.0 updated\n",
      "Time 244.5 updated\n",
      "Time 245.0 updated\n",
      "Time 245.5 updated\n",
      "Time 246.0 updated\n",
      "Time 246.5 updated\n",
      "Time 247.0 updated\n",
      "Time 247.5 updated\n",
      "Time 248.0 updated\n",
      "Time 248.5 updated\n",
      "Time 249.0 updated\n",
      "Time 249.5 updated\n",
      "Time 250.0 updated\n",
      "Time 250.5 updated\n",
      "Time 251.0 updated\n",
      "Time 251.5 updated\n",
      "Time 252.0 updated\n",
      "Time 252.5 updated\n",
      "Time 253.0 updated\n",
      "Time 253.5 updated\n",
      "Time 254.0 updated\n",
      "Time 254.5 updated\n",
      "Time 255.0 updated\n",
      "Time 255.5 updated\n",
      "Time 256.0 updated\n",
      "Time 256.5 updated\n",
      "Time 257.0 updated\n",
      "Time 257.5 updated\n",
      "Time 258.0 updated\n",
      "Time 258.5 updated\n",
      "Time 259.0 updated\n",
      "Time 259.5 updated\n",
      "Time 260.0 updated\n",
      "Time 260.5 updated\n",
      "Time 261.0 updated\n",
      "Time 261.5 updated\n",
      "Time 262.0 updated\n",
      "Time 262.5 updated\n",
      "Time 263.0 updated\n",
      "Time 263.5 updated\n",
      "Time 264.0 updated\n",
      "Time 264.5 updated\n",
      "Time 265.0 updated\n",
      "Time 265.5 updated\n",
      "Time 266.0 updated\n",
      "Time 266.5 updated\n",
      "Time 267.0 updated\n",
      "Time 267.5 updated\n",
      "Time 268.0 updated\n",
      "Time 268.5 updated\n",
      "Time 269.0 updated\n",
      "Time 269.5 updated\n",
      "Time 270.0 updated\n",
      "Time 270.5 updated\n",
      "Time 271.0 updated\n",
      "Time 271.5 updated\n",
      "Time 272.0 updated\n",
      "Time 272.5 updated\n",
      "Time 273.0 updated\n",
      "Time 273.5 updated\n",
      "Time 274.0 updated\n",
      "Time 274.5 updated\n",
      "Time 275.0 updated\n",
      "Time 275.5 updated\n",
      "Time 276.0 updated\n",
      "Time 276.5 updated\n",
      "Time 277.0 updated\n",
      "Time 277.5 updated\n",
      "Time 278.0 updated\n",
      "Time 278.5 updated\n",
      "Time 279.0 updated\n",
      "Time 279.5 updated\n",
      "Time 280.0 updated\n",
      "Time 280.5 updated\n",
      "Time 281.0 updated\n",
      "Time 281.5 updated\n",
      "Time 282.0 updated\n",
      "Time 282.5 updated\n",
      "Time 283.0 updated\n",
      "Time 283.5 updated\n",
      "Time 284.0 updated\n",
      "Time 284.5 updated\n",
      "Time 285.0 updated\n",
      "Time 285.5 updated\n",
      "Time 286.0 updated\n",
      "Time 286.5 updated\n",
      "Time 287.0 updated\n",
      "Time 287.5 updated\n",
      "Time 288.0 updated\n",
      "Time 288.5 updated\n",
      "Time 289.0 updated\n",
      "Time 289.5 updated\n",
      "Time 290.0 updated\n",
      "Time 290.5 updated\n",
      "Time 291.0 updated\n",
      "Time 291.5 updated\n",
      "Time 292.0 updated\n",
      "Time 292.5 updated\n",
      "Time 293.0 updated\n",
      "Time 293.5 updated\n",
      "Time 294.0 updated\n",
      "Time 294.5 updated\n",
      "Time 295.0 updated\n",
      "Time 295.5 updated\n",
      "Time 296.0 updated\n",
      "Time 296.5 updated\n",
      "Time 297.0 updated\n",
      "Time 297.5 updated\n",
      "Time 298.0 updated\n",
      "Time 298.5 updated\n",
      "Time 299.0 updated\n",
      "Time 299.5 updated\n",
      "Time 300.0 updated\n",
      "Time 300.5 updated\n",
      "Time 301.0 updated\n",
      "Time 301.5 updated\n",
      "Time 302.0 updated\n",
      "Time 302.5 updated\n",
      "Time 303.0 updated\n",
      "Time 303.5 updated\n",
      "Time 304.0 updated\n",
      "Time 304.5 updated\n",
      "Time 305.0 updated\n",
      "Time 305.5 updated\n",
      "Time 306.0 updated\n",
      "Time 306.5 updated\n",
      "Time 307.0 updated\n",
      "Time 307.5 updated\n",
      "Time 308.0 updated\n",
      "Time 308.5 updated\n",
      "Time 309.0 updated\n",
      "Time 309.5 updated\n",
      "Time 310.0 updated\n",
      "Time 310.5 updated\n",
      "Time 311.0 updated\n",
      "Time 311.5 updated\n",
      "Time 312.0 updated\n",
      "Time 312.5 updated\n",
      "Time 313.0 updated\n",
      "Time 313.5 updated\n",
      "Time 314.0 updated\n",
      "Time 314.5 updated\n",
      "Time 315.0 updated\n",
      "Time 315.5 updated\n",
      "Time 316.0 updated\n",
      "Time 316.5 updated\n",
      "Time 317.0 updated\n",
      "Time 317.5 updated\n",
      "Time 318.0 updated\n",
      "Time 318.5 updated\n",
      "Time 319.0 updated\n",
      "Time 319.5 updated\n",
      "Time 320.0 updated\n",
      "Time 320.5 updated\n",
      "Time 321.0 updated\n",
      "Time 321.5 updated\n",
      "Time 322.0 updated\n",
      "Time 322.5 updated\n",
      "Time 323.0 updated\n",
      "Time 323.5 updated\n",
      "Time 324.0 updated\n",
      "Time 324.5 updated\n",
      "Time 325.0 updated\n",
      "Time 325.5 updated\n",
      "Time 326.0 updated\n",
      "Time 326.5 updated\n",
      "Time 327.0 updated\n",
      "Time 327.5 updated\n",
      "Time 328.0 updated\n",
      "Time 328.5 updated\n",
      "Time 329.0 updated\n",
      "Time 329.5 updated\n",
      "Time 330.0 updated\n",
      "Time 330.5 updated\n",
      "Time 331.0 updated\n",
      "Time 331.5 updated\n",
      "Time 332.0 updated\n",
      "Time 332.5 updated\n",
      "Time 333.0 updated\n",
      "Time 333.5 updated\n",
      "Time 334.0 updated\n",
      "Time 334.5 updated\n",
      "Time 335.0 updated\n",
      "Time 335.5 updated\n",
      "Time 336.0 updated\n",
      "Time 336.5 updated\n",
      "Time 337.0 updated\n",
      "Time 337.5 updated\n",
      "Time 338.0 updated\n",
      "Time 338.5 updated\n",
      "Time 339.0 updated\n",
      "Time 339.5 updated\n",
      "Time 340.0 updated\n",
      "Time 340.5 updated\n",
      "Time 341.0 updated\n",
      "Time 341.5 updated\n",
      "Time 342.0 updated\n",
      "Time 342.5 updated\n",
      "Time 343.0 updated\n",
      "Time 343.5 updated\n",
      "Time 344.0 updated\n",
      "Time 344.5 updated\n",
      "Time 345.0 updated\n",
      "Time 345.5 updated\n",
      "Time 346.0 updated\n",
      "Time 346.5 updated\n",
      "Time 347.0 updated\n",
      "Time 347.5 updated\n",
      "Time 348.0 updated\n",
      "Time 348.5 updated\n",
      "Time 349.0 updated\n",
      "Time 349.5 updated\n",
      "Time 350.0 updated\n",
      "Time 350.5 updated\n",
      "Time 351.0 updated\n",
      "Time 351.5 updated\n",
      "Time 352.0 updated\n",
      "Time 352.5 updated\n",
      "Time 353.0 updated\n",
      "Time 353.5 updated\n",
      "Time 354.0 updated\n",
      "Time 354.5 updated\n",
      "Time 355.0 updated\n",
      "Time 355.5 updated\n",
      "Time 356.0 updated\n",
      "Time 356.5 updated\n",
      "Time 357.0 updated\n",
      "Time 357.5 updated\n",
      "Time 358.0 updated\n",
      "Time 358.5 updated\n",
      "Time 359.0 updated\n",
      "Time 359.5 updated\n",
      "Time 360.0 updated\n",
      "Time 360.5 updated\n",
      "Time 361.0 updated\n",
      "Time 361.5 updated\n",
      "Time 362.0 updated\n",
      "Time 362.5 updated\n",
      "Time 363.0 updated\n",
      "Time 363.5 updated\n",
      "Time 364.0 updated\n",
      "Time 364.5 updated\n",
      "Time 365.0 updated\n",
      "Time 365.5 updated\n",
      "Time 366.0 updated\n",
      "Time 366.5 updated\n",
      "Time 367.0 updated\n",
      "Time 367.5 updated\n",
      "Time 368.0 updated\n",
      "Time 368.5 updated\n",
      "Time 369.0 updated\n",
      "Time 369.5 updated\n",
      "Time 370.0 updated\n",
      "Time 370.5 updated\n",
      "Time 371.0 updated\n",
      "Time 371.5 updated\n",
      "Time 372.0 updated\n",
      "Time 372.5 updated\n",
      "Time 373.0 updated\n",
      "Time 373.5 updated\n",
      "Time 374.0 updated\n",
      "Time 374.5 updated\n",
      "Time 375.0 updated\n",
      "Time 375.5 updated\n",
      "Time 376.0 updated\n",
      "Time 376.5 updated\n",
      "Time 377.0 updated\n",
      "Time 377.5 updated\n",
      "Time 378.0 updated\n",
      "Time 378.5 updated\n",
      "Time 379.0 updated\n",
      "Time 379.5 updated\n",
      "Time 380.0 updated\n",
      "Time 380.5 updated\n",
      "Time 381.0 updated\n",
      "Time 381.5 updated\n",
      "Time 382.0 updated\n",
      "Time 382.5 updated\n",
      "Time 383.0 updated\n",
      "Time 383.5 updated\n",
      "Time 384.0 updated\n",
      "Time 384.5 updated\n",
      "Time 385.0 updated\n",
      "Time 385.5 updated\n",
      "Time 386.0 updated\n",
      "Time 386.5 updated\n",
      "Time 387.0 updated\n",
      "Time 387.5 updated\n",
      "Time 388.0 updated\n",
      "Time 388.5 updated\n",
      "Time 389.0 updated\n",
      "Time 389.5 updated\n",
      "Time 390.0 updated\n",
      "Time 390.5 updated\n",
      "Time 391.0 updated\n",
      "Time 391.5 updated\n",
      "Time 392.0 updated\n",
      "Time 392.5 updated\n",
      "Time 393.0 updated\n",
      "Time 393.5 updated\n",
      "Time 394.0 updated\n",
      "Time 394.5 updated\n",
      "Time 395.0 updated\n",
      "Time 395.5 updated\n",
      "Time 396.0 updated\n",
      "Time 396.5 updated\n",
      "Time 397.0 updated\n",
      "Time 397.5 updated\n",
      "Time 398.0 updated\n",
      "Time 398.5 updated\n",
      "Time 399.0 updated\n",
      "Time 399.5 updated\n",
      "Time 400.0 updated\n",
      "Time 400.5 updated\n",
      "Time 401.0 updated\n",
      "Time 401.5 updated\n",
      "Time 402.0 updated\n",
      "Time 402.5 updated\n",
      "Time 403.0 updated\n",
      "Time 403.5 updated\n",
      "Time 404.0 updated\n",
      "Time 404.5 updated\n",
      "Time 405.0 updated\n",
      "Time 405.5 updated\n",
      "Time 406.0 updated\n",
      "Time 406.5 updated\n",
      "Time 407.0 updated\n",
      "Time 407.5 updated\n",
      "Time 408.0 updated\n",
      "Time 408.5 updated\n",
      "Time 409.0 updated\n",
      "Time 409.5 updated\n",
      "Time 410.0 updated\n",
      "Time 410.5 updated\n",
      "Time 411.0 updated\n",
      "Time 411.5 updated\n",
      "Time 412.0 updated\n",
      "Time 412.5 updated\n",
      "Time 413.0 updated\n",
      "Time 413.5 updated\n",
      "Time 414.0 updated\n",
      "Time 414.5 updated\n",
      "Time 415.0 updated\n",
      "Time 415.5 updated\n",
      "Time 416.0 updated\n",
      "Time 416.5 updated\n",
      "Time 417.0 updated\n",
      "Time 417.5 updated\n",
      "Time 418.0 updated\n",
      "Time 418.5 updated\n",
      "Time 419.0 updated\n",
      "Time 419.5 updated\n",
      "Time 420.0 updated\n",
      "Time 420.5 updated\n",
      "Time 421.0 updated\n",
      "Time 421.5 updated\n",
      "Time 422.0 updated\n",
      "Time 422.5 updated\n",
      "Time 423.0 updated\n",
      "Time 423.5 updated\n",
      "Time 424.0 updated\n",
      "Time 424.5 updated\n",
      "Time 425.0 updated\n",
      "Time 425.5 updated\n",
      "Time 426.0 updated\n",
      "Time 426.5 updated\n",
      "Time 427.0 updated\n",
      "Time 427.5 updated\n",
      "Time 428.0 updated\n",
      "Time 428.5 updated\n",
      "Time 429.0 updated\n",
      "Time 429.5 updated\n",
      "Time 430.0 updated\n",
      "Time 430.5 updated\n",
      "Time 431.0 updated\n",
      "Time 431.5 updated\n",
      "Time 432.0 updated\n",
      "Time 432.5 updated\n",
      "Time 433.0 updated\n",
      "Time 433.5 updated\n",
      "Time 434.0 updated\n",
      "Time 434.5 updated\n",
      "Time 435.0 updated\n",
      "Time 435.5 updated\n",
      "Time 436.0 updated\n",
      "Time 436.5 updated\n",
      "Time 437.0 updated\n",
      "Time 437.5 updated\n",
      "Time 438.0 updated\n",
      "Time 438.5 updated\n",
      "Time 439.0 updated\n",
      "Time 439.5 updated\n",
      "Time 440.0 updated\n",
      "Time 440.5 updated\n",
      "Time 441.0 updated\n",
      "Time 441.5 updated\n",
      "Time 442.0 updated\n",
      "Time 442.5 updated\n",
      "Time 443.0 updated\n",
      "Time 443.5 updated\n",
      "Time 444.0 updated\n",
      "Time 444.5 updated\n",
      "Time 445.0 updated\n",
      "Time 445.5 updated\n",
      "Time 446.0 updated\n",
      "Time 446.5 updated\n",
      "Time 447.0 updated\n",
      "Time 447.5 updated\n",
      "Time 448.0 updated\n",
      "Time 448.5 updated\n",
      "Time 449.0 updated\n",
      "Time 449.5 updated\n",
      "Time 450.0 updated\n",
      "Time 450.5 updated\n",
      "Time 451.0 updated\n",
      "Time 451.5 updated\n",
      "Time 452.0 updated\n",
      "Time 452.5 updated\n",
      "Time 453.0 updated\n",
      "Time 453.5 updated\n",
      "Time 454.0 updated\n",
      "Time 454.5 updated\n",
      "Time 455.0 updated\n",
      "Time 455.5 updated\n",
      "Time 456.0 updated\n",
      "Time 456.5 updated\n",
      "Time 457.0 updated\n",
      "Time 457.5 updated\n",
      "Time 458.0 updated\n",
      "Time 458.5 updated\n",
      "Time 459.0 updated\n",
      "Time 459.5 updated\n",
      "Time 460.0 updated\n",
      "Time 460.5 updated\n",
      "Time 461.0 updated\n",
      "Time 461.5 updated\n",
      "Time 462.0 updated\n",
      "Time 462.5 updated\n",
      "Time 463.0 updated\n",
      "Time 463.5 updated\n",
      "Time 464.0 updated\n",
      "Time 464.5 updated\n",
      "Time 465.0 updated\n",
      "Time 465.5 updated\n",
      "Time 466.0 updated\n",
      "Time 466.5 updated\n",
      "Time 467.0 updated\n",
      "Time 467.5 updated\n",
      "Time 468.0 updated\n",
      "Time 468.5 updated\n",
      "Time 469.0 updated\n",
      "Time 469.5 updated\n",
      "Time 470.0 updated\n",
      "Time 470.5 updated\n",
      "Time 471.0 updated\n",
      "Time 471.5 updated\n",
      "Time 472.0 updated\n",
      "Time 472.5 updated\n",
      "Time 473.0 updated\n",
      "Time 473.5 updated\n",
      "Time 474.0 updated\n",
      "Time 474.5 updated\n",
      "Time 475.0 updated\n",
      "Time 475.5 updated\n",
      "Time 476.0 updated\n",
      "Time 476.5 updated\n",
      "Time 477.0 updated\n",
      "Time 477.5 updated\n",
      "Time 478.0 updated\n",
      "Time 478.5 updated\n",
      "Time 479.0 updated\n",
      "Time 479.5 updated\n",
      "Time 480.0 updated\n",
      "Time 480.5 updated\n",
      "Time 481.0 updated\n",
      "Time 481.5 updated\n",
      "Time 482.0 updated\n",
      "Time 482.5 updated\n",
      "Time 483.0 updated\n",
      "Time 483.5 updated\n",
      "Time 484.0 updated\n",
      "Time 484.5 updated\n",
      "Time 485.0 updated\n",
      "Time 485.5 updated\n",
      "Time 486.0 updated\n",
      "Time 486.5 updated\n",
      "Time 487.0 updated\n",
      "Time 487.5 updated\n",
      "Time 488.0 updated\n",
      "Time 488.5 updated\n",
      "Time 489.0 updated\n",
      "Time 489.5 updated\n",
      "Time 490.0 updated\n",
      "Time 490.5 updated\n",
      "Time 491.0 updated\n",
      "Time 491.5 updated\n",
      "Time 492.0 updated\n",
      "Time 492.5 updated\n",
      "Time 493.0 updated\n",
      "Time 493.5 updated\n",
      "Time 494.0 updated\n",
      "Time 494.5 updated\n",
      "Time 495.0 updated\n",
      "Time 495.5 updated\n",
      "Time 496.0 updated\n",
      "Time 496.5 updated\n",
      "Time 497.0 updated\n",
      "Time 497.5 updated\n",
      "Time 498.0 updated\n",
      "Time 498.5 updated\n",
      "Time 499.0 updated\n",
      "Time 499.5 updated\n",
      "Time 500.0 updated\n",
      "Time 500.5 updated\n",
      "Time 501.0 updated\n",
      "Time 501.5 updated\n",
      "Time 502.0 updated\n",
      "Time 502.5 updated\n",
      "Time 503.0 updated\n",
      "Time 503.5 updated\n",
      "Time 504.0 updated\n",
      "Time 504.5 updated\n",
      "Time 505.0 updated\n",
      "Time 505.5 updated\n",
      "Time 506.0 updated\n",
      "Time 506.5 updated\n",
      "Time 507.0 updated\n",
      "Time 507.5 updated\n",
      "Time 508.0 updated\n",
      "Time 508.5 updated\n",
      "Time 509.0 updated\n",
      "Time 509.5 updated\n",
      "Time 510.0 updated\n",
      "Time 510.5 updated\n",
      "Time 511.0 updated\n",
      "Time 511.5 updated\n",
      "Time 512.0 updated\n",
      "Time 512.5 updated\n",
      "Time 513.0 updated\n",
      "Time 513.5 updated\n",
      "Time 514.0 updated\n",
      "Time 514.5 updated\n",
      "Time 515.0 updated\n",
      "Time 515.5 updated\n",
      "Time 516.0 updated\n",
      "Time 516.5 updated\n",
      "Time 517.0 updated\n",
      "Time 517.5 updated\n",
      "Time 518.0 updated\n",
      "Time 518.5 updated\n",
      "Time 519.0 updated\n",
      "Time 519.5 updated\n",
      "Time 520.0 updated\n",
      "Time 520.5 updated\n",
      "Time 521.0 updated\n",
      "Time 521.5 updated\n",
      "Time 522.0 updated\n",
      "Time 522.5 updated\n",
      "Time 523.0 updated\n",
      "Time 523.5 updated\n",
      "Time 524.0 updated\n",
      "Time 524.5 updated\n",
      "Time 525.0 updated\n",
      "Time 525.5 updated\n",
      "Time 526.0 updated\n",
      "Time 526.5 updated\n",
      "Time 527.0 updated\n",
      "Time 527.5 updated\n",
      "Time 528.0 updated\n",
      "Time 528.5 updated\n",
      "Time 529.0 updated\n",
      "Time 529.5 updated\n",
      "Time 530.0 updated\n",
      "Time 530.5 updated\n",
      "Time 531.0 updated\n",
      "Time 531.5 updated\n",
      "Time 532.0 updated\n",
      "Time 532.5 updated\n",
      "Time 533.0 updated\n",
      "Time 533.5 updated\n",
      "Time 534.0 updated\n",
      "Time 534.5 updated\n",
      "Time 535.0 updated\n",
      "Time 535.5 updated\n",
      "Time 536.0 updated\n",
      "Time 536.5 updated\n",
      "Time 537.0 updated\n",
      "Time 537.5 updated\n",
      "Time 538.0 updated\n",
      "Time 538.5 updated\n",
      "Time 539.0 updated\n",
      "Time 539.5 updated\n",
      "Time 540.0 updated\n",
      "Time 540.5 updated\n",
      "Time 541.0 updated\n",
      "Time 541.5 updated\n",
      "Time 542.0 updated\n",
      "Time 542.5 updated\n",
      "Time 543.0 updated\n",
      "Time 543.5 updated\n",
      "Time 544.0 updated\n",
      "Time 544.5 updated\n",
      "Time 545.0 updated\n",
      "Time 545.5 updated\n",
      "Time 546.0 updated\n",
      "Time 546.5 updated\n",
      "Time 547.0 updated\n",
      "Time 547.5 updated\n",
      "Time 548.0 updated\n",
      "Time 548.5 updated\n",
      "Time 549.0 updated\n",
      "Time 549.5 updated\n",
      "Time 550.0 updated\n",
      "Time 550.5 updated\n",
      "Time 551.0 updated\n",
      "Time 551.5 updated\n",
      "Time 552.0 updated\n",
      "Time 552.5 updated\n",
      "Time 553.0 updated\n",
      "Time 553.5 updated\n",
      "Time 554.0 updated\n",
      "Time 554.5 updated\n",
      "Time 555.0 updated\n",
      "Time 555.5 updated\n",
      "Time 556.0 updated\n",
      "Time 556.5 updated\n",
      "Time 557.0 updated\n",
      "Time 557.5 updated\n",
      "Time 558.0 updated\n",
      "Time 558.5 updated\n",
      "Time 559.0 updated\n",
      "Time 559.5 updated\n",
      "Time 560.0 updated\n",
      "Time 560.5 updated\n",
      "Time 561.0 updated\n",
      "Time 561.5 updated\n",
      "Time 562.0 updated\n",
      "Time 562.5 updated\n",
      "Time 563.0 updated\n",
      "Time 563.5 updated\n",
      "Time 564.0 updated\n",
      "Time 564.5 updated\n",
      "Time 565.0 updated\n",
      "Time 565.5 updated\n",
      "Time 566.0 updated\n",
      "Time 566.5 updated\n",
      "Time 567.0 updated\n",
      "Time 567.5 updated\n",
      "Time 568.0 updated\n",
      "Time 568.5 updated\n",
      "Time 569.0 updated\n",
      "Time 569.5 updated\n",
      "Time 570.0 updated\n",
      "Time 570.5 updated\n",
      "Time 571.0 updated\n",
      "Time 571.5 updated\n",
      "Time 572.0 updated\n",
      "Time 572.5 updated\n",
      "Time 573.0 updated\n",
      "Time 573.5 updated\n",
      "Time 574.0 updated\n",
      "Time 574.5 updated\n",
      "Time 575.0 updated\n",
      "Time 575.5 updated\n",
      "Time 576.0 updated\n",
      "Time 576.5 updated\n",
      "Time 577.0 updated\n",
      "Time 577.5 updated\n",
      "Time 578.0 updated\n",
      "Time 578.5 updated\n",
      "Time 579.0 updated\n",
      "Time 579.5 updated\n",
      "Time 580.0 updated\n",
      "Time 580.5 updated\n",
      "Time 581.0 updated\n",
      "Time 581.5 updated\n",
      "Time 582.0 updated\n",
      "Time 582.5 updated\n",
      "Time 583.0 updated\n",
      "Time 583.5 updated\n",
      "Time 584.0 updated\n",
      "Time 584.5 updated\n",
      "Time 585.0 updated\n",
      "Time 585.5 updated\n",
      "Time 586.0 updated\n",
      "Time 586.5 updated\n",
      "Time 587.0 updated\n",
      "Time 587.5 updated\n",
      "Time 588.0 updated\n",
      "Time 588.5 updated\n",
      "Time 589.0 updated\n",
      "Time 589.5 updated\n",
      "Time 590.0 updated\n",
      "Time 590.5 updated\n",
      "Time 591.0 updated\n",
      "Time 591.5 updated\n",
      "Time 592.0 updated\n",
      "Time 592.5 updated\n",
      "Time 593.0 updated\n",
      "Time 593.5 updated\n",
      "Time 594.0 updated\n",
      "Time 594.5 updated\n",
      "Time 595.0 updated\n",
      "Time 595.5 updated\n",
      "Time 596.0 updated\n",
      "Time 596.5 updated\n",
      "Time 597.0 updated\n",
      "Time 597.5 updated\n",
      "Time 598.0 updated\n",
      "Time 598.5 updated\n",
      "Time 599.0 updated\n",
      "Time 599.5 updated\n",
      "Time 600.0 updated\n",
      "Time 600.5 updated\n",
      "Time 601.0 updated\n",
      "Time 601.5 updated\n",
      "Time 602.0 updated\n",
      "Time 602.5 updated\n",
      "Time 603.0 updated\n",
      "Time 603.5 updated\n",
      "Time 604.0 updated\n",
      "Time 604.5 updated\n",
      "Time 605.0 updated\n",
      "Time 605.5 updated\n",
      "Time 606.0 updated\n",
      "Time 606.5 updated\n",
      "Time 607.0 updated\n",
      "Time 607.5 updated\n",
      "Time 608.0 updated\n",
      "Time 608.5 updated\n",
      "Time 609.0 updated\n",
      "Time 609.5 updated\n",
      "Time 610.0 updated\n",
      "Time 610.5 updated\n",
      "Time 611.0 updated\n",
      "Time 611.5 updated\n",
      "Time 612.0 updated\n",
      "Time 612.5 updated\n",
      "Time 613.0 updated\n",
      "Time 613.5 updated\n",
      "Time 614.0 updated\n",
      "Time 614.5 updated\n",
      "Time 615.0 updated\n",
      "Time 615.5 updated\n",
      "Time 616.0 updated\n",
      "Time 616.5 updated\n",
      "Time 617.0 updated\n",
      "Time 617.5 updated\n",
      "Time 618.0 updated\n",
      "Time 618.5 updated\n",
      "Time 619.0 updated\n",
      "Time 619.5 updated\n",
      "Time 620.0 updated\n",
      "Time 620.5 updated\n",
      "Time 621.0 updated\n",
      "Time 621.5 updated\n",
      "Time 622.0 updated\n",
      "Time 622.5 updated\n",
      "Time 623.0 updated\n",
      "Time 623.5 updated\n",
      "Time 624.0 updated\n",
      "Time 624.5 updated\n",
      "Time 625.0 updated\n",
      "Time 625.5 updated\n",
      "Time 626.0 updated\n",
      "Time 626.5 updated\n",
      "Time 627.0 updated\n",
      "Time 627.5 updated\n",
      "Time 628.0 updated\n",
      "Time 628.5 updated\n",
      "Time 629.0 updated\n",
      "Time 629.5 updated\n",
      "Time 630.0 updated\n",
      "Time 630.5 updated\n",
      "Time 631.0 updated\n",
      "Time 631.5 updated\n",
      "Time 632.0 updated\n",
      "Time 632.5 updated\n",
      "Time 633.0 updated\n",
      "Time 633.5 updated\n",
      "Time 634.0 updated\n",
      "Time 634.5 updated\n",
      "Time 635.0 updated\n",
      "Time 635.5 updated\n",
      "Time 636.0 updated\n",
      "Time 636.5 updated\n",
      "Time 637.0 updated\n",
      "Time 637.5 updated\n",
      "Time 638.0 updated\n",
      "Time 638.5 updated\n",
      "Time 639.0 updated\n",
      "Time 639.5 updated\n",
      "Time 640.0 updated\n",
      "Time 640.5 updated\n",
      "Time 641.0 updated\n",
      "Time 641.5 updated\n",
      "Time 642.0 updated\n",
      "Time 642.5 updated\n",
      "Time 643.0 updated\n",
      "Time 643.5 updated\n",
      "Time 644.0 updated\n",
      "Time 644.5 updated\n",
      "Time 645.0 updated\n",
      "Time 645.5 updated\n",
      "Time 646.0 updated\n",
      "Time 646.5 updated\n",
      "Time 647.0 updated\n",
      "Time 647.5 updated\n",
      "Time 648.0 updated\n",
      "Time 648.5 updated\n",
      "Time 649.0 updated\n",
      "Time 649.5 updated\n",
      "Time 650.0 updated\n",
      "Time 650.5 updated\n",
      "Time 651.0 updated\n",
      "Time 651.5 updated\n",
      "Time 652.0 updated\n",
      "Time 652.5 updated\n",
      "Time 653.0 updated\n",
      "Time 653.5 updated\n",
      "Time 654.0 updated\n",
      "Time 654.5 updated\n",
      "Time 655.0 updated\n",
      "Time 655.5 updated\n",
      "Time 656.0 updated\n",
      "Time 656.5 updated\n",
      "Time 657.0 updated\n",
      "Time 657.5 updated\n",
      "Time 658.0 updated\n",
      "Time 658.5 updated\n",
      "Time 659.0 updated\n",
      "Time 659.5 updated\n",
      "Time 660.0 updated\n",
      "Time 660.5 updated\n",
      "Time 661.0 updated\n",
      "Time 661.5 updated\n",
      "Time 662.0 updated\n",
      "Time 662.5 updated\n",
      "Time 663.0 updated\n",
      "Time 663.5 updated\n",
      "Time 664.0 updated\n",
      "Time 664.5 updated\n",
      "Time 665.0 updated\n",
      "Time 665.5 updated\n",
      "Time 666.0 updated\n",
      "Time 666.5 updated\n",
      "Time 667.0 updated\n",
      "Time 667.5 updated\n",
      "Time 668.0 updated\n",
      "Time 668.5 updated\n",
      "Time 669.0 updated\n",
      "Time 669.5 updated\n",
      "Time 670.0 updated\n",
      "Time 670.5 updated\n",
      "Time 671.0 updated\n",
      "Time 671.5 updated\n",
      "Time 672.0 updated\n",
      "Time 672.5 updated\n",
      "Time 673.0 updated\n",
      "Time 673.5 updated\n",
      "Time 674.0 updated\n",
      "Time 674.5 updated\n",
      "Time 675.0 updated\n",
      "Time 675.5 updated\n",
      "Time 676.0 updated\n",
      "Time 676.5 updated\n",
      "Time 677.0 updated\n",
      "Time 677.5 updated\n",
      "Time 678.0 updated\n",
      "Time 678.5 updated\n",
      "Time 679.0 updated\n",
      "Time 679.5 updated\n",
      "Time 680.0 updated\n",
      "Time 680.5 updated\n",
      "Time 681.0 updated\n",
      "Time 681.5 updated\n",
      "Time 682.0 updated\n",
      "Time 682.5 updated\n",
      "Time 683.0 updated\n",
      "Time 683.5 updated\n",
      "Time 684.0 updated\n",
      "Time 684.5 updated\n",
      "Time 685.0 updated\n",
      "Time 685.5 updated\n",
      "Time 686.0 updated\n",
      "Time 686.5 updated\n",
      "Time 687.0 updated\n",
      "Time 687.5 updated\n",
      "Time 688.0 updated\n",
      "Time 688.5 updated\n",
      "Time 689.0 updated\n",
      "Time 689.5 updated\n",
      "Time 690.0 updated\n",
      "Time 690.5 updated\n",
      "Time 691.0 updated\n",
      "Time 691.5 updated\n",
      "Time 692.0 updated\n",
      "Time 692.5 updated\n",
      "Time 693.0 updated\n",
      "Time 693.5 updated\n",
      "Time 694.0 updated\n",
      "Time 694.5 updated\n",
      "Time 695.0 updated\n",
      "Time 695.5 updated\n",
      "Time 696.0 updated\n",
      "Time 696.5 updated\n",
      "Time 697.0 updated\n",
      "Time 697.5 updated\n",
      "Time 698.0 updated\n",
      "Time 698.5 updated\n",
      "Time 699.0 updated\n",
      "Time 699.5 updated\n",
      "Time 700.0 updated\n",
      "Time 700.5 updated\n",
      "Time 701.0 updated\n",
      "Time 701.5 updated\n",
      "Time 702.0 updated\n",
      "Time 702.5 updated\n",
      "Time 703.0 updated\n",
      "Time 703.5 updated\n",
      "Time 704.0 updated\n",
      "Time 704.5 updated\n",
      "Time 705.0 updated\n",
      "Time 705.5 updated\n",
      "Time 706.0 updated\n",
      "Time 706.5 updated\n",
      "Time 707.0 updated\n",
      "Time 707.5 updated\n",
      "Time 708.0 updated\n",
      "Time 708.5 updated\n",
      "Time 709.0 updated\n",
      "Time 709.5 updated\n",
      "Time 710.0 updated\n",
      "Time 710.5 updated\n",
      "Time 711.0 updated\n",
      "Time 711.5 updated\n",
      "Time 712.0 updated\n",
      "Time 712.5 updated\n",
      "Time 713.0 updated\n",
      "Time 713.5 updated\n",
      "Time 714.0 updated\n",
      "Time 714.5 updated\n",
      "Time 715.0 updated\n",
      "Time 715.5 updated\n",
      "Time 716.0 updated\n",
      "Time 716.5 updated\n",
      "Time 717.0 updated\n",
      "Time 717.5 updated\n",
      "Time 718.0 updated\n",
      "Time 718.5 updated\n",
      "Time 719.0 updated\n",
      "Time 719.5 updated\n",
      "Time 720.0 updated\n",
      "Time 720.5 updated\n",
      "Time 721.0 updated\n",
      "Time 721.5 updated\n",
      "Time 722.0 updated\n",
      "Time 722.5 updated\n",
      "Time 723.0 updated\n",
      "Time 723.5 updated\n",
      "Time 724.0 updated\n",
      "Time 724.5 updated\n",
      "Time 725.0 updated\n",
      "Time 725.5 updated\n",
      "Time 726.0 updated\n",
      "Time 726.5 updated\n",
      "Time 727.0 updated\n",
      "Time 727.5 updated\n",
      "Time 728.0 updated\n",
      "Time 728.5 updated\n",
      "Time 729.0 updated\n",
      "Time 729.5 updated\n",
      "Time 730.0 updated\n",
      "Time 730.5 updated\n",
      "Time 731.0 updated\n",
      "Time 731.5 updated\n",
      "Time 732.0 updated\n",
      "Time 732.5 updated\n",
      "Time 733.0 updated\n",
      "Time 733.5 updated\n",
      "Time 734.0 updated\n",
      "Time 734.5 updated\n",
      "Time 735.0 updated\n",
      "Time 735.5 updated\n",
      "Time 736.0 updated\n",
      "Time 736.5 updated\n",
      "Time 737.0 updated\n",
      "Time 737.5 updated\n",
      "Time 738.0 updated\n",
      "Time 738.5 updated\n",
      "Time 739.0 updated\n",
      "Time 739.5 updated\n",
      "Time 740.0 updated\n",
      "Time 740.5 updated\n",
      "Time 741.0 updated\n",
      "Time 741.5 updated\n",
      "Time 742.0 updated\n",
      "Time 742.5 updated\n",
      "Time 743.0 updated\n",
      "Time 743.5 updated\n",
      "Time 744.0 updated\n",
      "Time 744.5 updated\n",
      "Time 745.0 updated\n",
      "Time 745.5 updated\n",
      "Time 746.0 updated\n",
      "Time 746.5 updated\n",
      "Time 747.0 updated\n",
      "Time 747.5 updated\n",
      "Time 748.0 updated\n",
      "Time 748.5 updated\n",
      "Time 749.0 updated\n",
      "Time 749.5 updated\n",
      "Time 750.0 updated\n",
      "Time 750.5 updated\n",
      "Time 751.0 updated\n",
      "Time 751.5 updated\n",
      "Time 752.0 updated\n",
      "Time 752.5 updated\n",
      "Time 753.0 updated\n",
      "Time 753.5 updated\n",
      "Time 754.0 updated\n",
      "Time 754.5 updated\n",
      "Time 755.0 updated\n",
      "Time 755.5 updated\n",
      "Time 756.0 updated\n",
      "Time 756.5 updated\n",
      "Time 757.0 updated\n",
      "Time 757.5 updated\n",
      "Time 758.0 updated\n",
      "Time 758.5 updated\n",
      "Time 759.0 updated\n",
      "Time 759.5 updated\n",
      "Time 760.0 updated\n",
      "Time 760.5 updated\n",
      "Time 761.0 updated\n",
      "Time 761.5 updated\n",
      "Time 762.0 updated\n",
      "Time 762.5 updated\n",
      "Time 763.0 updated\n",
      "Time 763.5 updated\n",
      "Time 764.0 updated\n",
      "Time 764.5 updated\n",
      "Time 765.0 updated\n",
      "Time 765.5 updated\n",
      "Time 766.0 updated\n",
      "Time 766.5 updated\n",
      "Time 767.0 updated\n",
      "Time 767.5 updated\n",
      "Time 768.0 updated\n",
      "Time 768.5 updated\n",
      "Time 769.0 updated\n",
      "Time 769.5 updated\n",
      "Time 770.0 updated\n",
      "Time 770.5 updated\n",
      "Time 771.0 updated\n",
      "Time 771.5 updated\n",
      "Time 772.0 updated\n",
      "Time 772.5 updated\n",
      "Time 773.0 updated\n",
      "Time 773.5 updated\n",
      "Time 774.0 updated\n",
      "Time 774.5 updated\n",
      "Time 775.0 updated\n",
      "Time 775.5 updated\n",
      "Time 776.0 updated\n",
      "Time 776.5 updated\n",
      "Time 777.0 updated\n",
      "Time 777.5 updated\n",
      "Time 778.0 updated\n",
      "Time 778.5 updated\n",
      "Time 779.0 updated\n",
      "Time 779.5 updated\n",
      "Time 780.0 updated\n",
      "Time 780.5 updated\n",
      "Time 781.0 updated\n",
      "Time 781.5 updated\n",
      "Time 782.0 updated\n",
      "Time 782.5 updated\n",
      "Time 783.0 updated\n",
      "Time 783.5 updated\n",
      "Time 784.0 updated\n",
      "Time 784.5 updated\n",
      "Time 785.0 updated\n",
      "Time 785.5 updated\n",
      "Time 786.0 updated\n",
      "Time 786.5 updated\n",
      "Time 787.0 updated\n",
      "Time 787.5 updated\n",
      "Time 788.0 updated\n",
      "Time 788.5 updated\n",
      "Time 789.0 updated\n",
      "Time 789.5 updated\n",
      "Time 790.0 updated\n",
      "Time 790.5 updated\n",
      "Time 791.0 updated\n",
      "Time 791.5 updated\n",
      "Time 792.0 updated\n",
      "Time 792.5 updated\n",
      "Time 793.0 updated\n",
      "Time 793.5 updated\n",
      "Time 794.0 updated\n",
      "Time 794.5 updated\n",
      "Time 795.0 updated\n",
      "Time 795.5 updated\n",
      "Time 796.0 updated\n",
      "Time 796.5 updated\n",
      "Time 797.0 updated\n",
      "Time 797.5 updated\n",
      "Time 798.0 updated\n",
      "Time 798.5 updated\n",
      "Time 799.0 updated\n",
      "Time 799.5 updated\n",
      "Time 800.0 updated\n",
      "Time 800.5 updated\n",
      "Time 801.0 updated\n",
      "Time 801.5 updated\n",
      "Time 802.0 updated\n",
      "Time 802.5 updated\n",
      "Time 803.0 updated\n",
      "Time 803.5 updated\n",
      "Time 804.0 updated\n",
      "Time 804.5 updated\n",
      "Time 805.0 updated\n",
      "Time 805.5 updated\n",
      "Time 806.0 updated\n",
      "Time 806.5 updated\n",
      "Time 807.0 updated\n",
      "Time 807.5 updated\n",
      "Time 808.0 updated\n",
      "Time 808.5 updated\n",
      "Time 809.0 updated\n",
      "Time 809.5 updated\n",
      "Time 810.0 updated\n",
      "Time 810.5 updated\n",
      "Time 811.0 updated\n",
      "Time 811.5 updated\n",
      "Time 812.0 updated\n",
      "Time 812.5 updated\n",
      "Time 813.0 updated\n",
      "Time 813.5 updated\n",
      "Time 814.0 updated\n",
      "Time 814.5 updated\n",
      "Time 815.0 updated\n",
      "Time 815.5 updated\n",
      "Time 816.0 updated\n",
      "Time 816.5 updated\n",
      "Time 817.0 updated\n",
      "Time 817.5 updated\n",
      "Time 818.0 updated\n",
      "Time 818.5 updated\n",
      "Time 819.0 updated\n",
      "Time 819.5 updated\n",
      "Time 820.0 updated\n",
      "Time 820.5 updated\n",
      "Time 821.0 updated\n",
      "Time 821.5 updated\n",
      "Time 822.0 updated\n",
      "Time 822.5 updated\n",
      "Time 823.0 updated\n",
      "Time 823.5 updated\n",
      "Time 824.0 updated\n",
      "Time 824.5 updated\n",
      "Time 825.0 updated\n",
      "Time 825.5 updated\n",
      "Time 826.0 updated\n",
      "Time 826.5 updated\n",
      "Time 827.0 updated\n",
      "Time 827.5 updated\n",
      "Time 828.0 updated\n",
      "Time 828.5 updated\n",
      "Time 829.0 updated\n",
      "Time 829.5 updated\n",
      "Time 830.0 updated\n",
      "Time 830.5 updated\n",
      "Time 831.0 updated\n",
      "Time 831.5 updated\n",
      "Time 832.0 updated\n",
      "Time 832.5 updated\n",
      "Time 833.0 updated\n",
      "Time 833.5 updated\n",
      "Time 834.0 updated\n",
      "Time 834.5 updated\n",
      "Time 835.0 updated\n",
      "Time 835.5 updated\n",
      "Time 836.0 updated\n",
      "Time 836.5 updated\n",
      "Time 837.0 updated\n",
      "Time 837.5 updated\n",
      "Time 838.0 updated\n",
      "Time 838.5 updated\n",
      "Time 839.0 updated\n",
      "Time 839.5 updated\n",
      "Time 840.0 updated\n",
      "Time 840.5 updated\n",
      "Time 841.0 updated\n",
      "Time 841.5 updated\n",
      "Time 842.0 updated\n",
      "Time 842.5 updated\n",
      "Time 843.0 updated\n",
      "Time 843.5 updated\n",
      "Time 844.0 updated\n",
      "Time 844.5 updated\n",
      "Time 845.0 updated\n",
      "Time 845.5 updated\n",
      "Time 846.0 updated\n",
      "Time 846.5 updated\n",
      "Time 847.0 updated\n",
      "Time 847.5 updated\n",
      "Time 848.0 updated\n",
      "Time 848.5 updated\n",
      "Time 849.0 updated\n",
      "Time 849.5 updated\n",
      "Time 850.0 updated\n",
      "Time 850.5 updated\n",
      "Time 851.0 updated\n",
      "Time 851.5 updated\n",
      "Time 852.0 updated\n",
      "Time 852.5 updated\n",
      "Time 853.0 updated\n",
      "Time 853.5 updated\n",
      "Time 854.0 updated\n",
      "Time 854.5 updated\n",
      "Time 855.0 updated\n",
      "Time 855.5 updated\n",
      "Time 856.0 updated\n",
      "Time 856.5 updated\n",
      "Time 857.0 updated\n",
      "Time 857.5 updated\n",
      "Time 858.0 updated\n",
      "Time 858.5 updated\n",
      "Time 859.0 updated\n",
      "Time 859.5 updated\n",
      "Time 860.0 updated\n",
      "Time 860.5 updated\n",
      "Time 861.0 updated\n",
      "Time 861.5 updated\n",
      "Time 862.0 updated\n",
      "Time 862.5 updated\n",
      "Time 863.0 updated\n",
      "Time 863.5 updated\n",
      "Time 864.0 updated\n",
      "Time 864.5 updated\n",
      "Time 865.0 updated\n",
      "Time 865.5 updated\n",
      "Time 866.0 updated\n",
      "Time 866.5 updated\n",
      "Time 867.0 updated\n",
      "Time 867.5 updated\n",
      "Time 868.0 updated\n",
      "Time 868.5 updated\n",
      "Time 869.0 updated\n",
      "Time 869.5 updated\n",
      "Time 870.0 updated\n",
      "Time 870.5 updated\n",
      "Time 871.0 updated\n",
      "Time 871.5 updated\n",
      "Time 872.0 updated\n",
      "Time 872.5 updated\n",
      "Time 873.0 updated\n",
      "Time 873.5 updated\n",
      "Time 874.0 updated\n",
      "Time 874.5 updated\n",
      "Time 875.0 updated\n",
      "Time 875.5 updated\n",
      "Time 876.0 updated\n",
      "Time 876.5 updated\n",
      "Time 877.0 updated\n",
      "Time 877.5 updated\n",
      "Time 878.0 updated\n",
      "Time 878.5 updated\n",
      "Time 879.0 updated\n",
      "Time 879.5 updated\n",
      "Time 880.0 updated\n",
      "Time 880.5 updated\n",
      "Time 881.0 updated\n",
      "Time 881.5 updated\n",
      "Time 882.0 updated\n",
      "Time 882.5 updated\n",
      "Time 883.0 updated\n",
      "Time 883.5 updated\n",
      "Time 884.0 updated\n",
      "Time 884.5 updated\n",
      "Time 885.0 updated\n",
      "Time 885.5 updated\n",
      "Time 886.0 updated\n",
      "Time 886.5 updated\n",
      "Time 887.0 updated\n",
      "Time 887.5 updated\n",
      "Time 888.0 updated\n",
      "Time 888.5 updated\n",
      "Time 889.0 updated\n",
      "Time 889.5 updated\n",
      "Time 890.0 updated\n",
      "Time 890.5 updated\n",
      "Time 891.0 updated\n",
      "Time 891.5 updated\n",
      "Time 892.0 updated\n",
      "Time 892.5 updated\n",
      "Time 893.0 updated\n",
      "Time 893.5 updated\n",
      "Time 894.0 updated\n",
      "Time 894.5 updated\n",
      "Time 895.0 updated\n",
      "Time 895.5 updated\n",
      "Time 896.0 updated\n",
      "Time 896.5 updated\n",
      "Time 897.0 updated\n",
      "Time 897.5 updated\n",
      "Time 898.0 updated\n",
      "Time 898.5 updated\n",
      "Time 899.0 updated\n",
      "Time 899.5 updated\n",
      "Time 900.0 updated\n",
      "Time 900.5 updated\n",
      "Time 901.0 updated\n",
      "Time 901.5 updated\n",
      "Time 902.0 updated\n",
      "Time 902.5 updated\n",
      "Time 903.0 updated\n",
      "Time 903.5 updated\n",
      "Time 904.0 updated\n",
      "Time 904.5 updated\n",
      "Time 905.0 updated\n",
      "Time 905.5 updated\n",
      "Time 906.0 updated\n",
      "Time 906.5 updated\n",
      "Time 907.0 updated\n",
      "Time 907.5 updated\n",
      "Time 908.0 updated\n",
      "Time 908.5 updated\n",
      "Time 909.0 updated\n",
      "Time 909.5 updated\n",
      "Time 910.0 updated\n",
      "Time 910.5 updated\n",
      "Time 911.0 updated\n",
      "Time 911.5 updated\n",
      "Time 912.0 updated\n",
      "Time 912.5 updated\n",
      "Time 913.0 updated\n",
      "Time 913.5 updated\n",
      "Time 914.0 updated\n",
      "Time 914.5 updated\n",
      "Time 915.0 updated\n",
      "Time 915.5 updated\n",
      "Time 916.0 updated\n",
      "Time 916.5 updated\n",
      "Time 917.0 updated\n",
      "Time 917.5 updated\n",
      "Time 918.0 updated\n",
      "Time 918.5 updated\n",
      "Time 919.0 updated\n",
      "Time 919.5 updated\n",
      "Time 920.0 updated\n",
      "Time 920.5 updated\n",
      "Time 921.0 updated\n",
      "Time 921.5 updated\n",
      "Time 922.0 updated\n",
      "Time 922.5 updated\n",
      "Time 923.0 updated\n",
      "Time 923.5 updated\n",
      "Time 924.0 updated\n",
      "Time 924.5 updated\n",
      "Time 925.0 updated\n",
      "Time 925.5 updated\n",
      "Time 926.0 updated\n",
      "Time 926.5 updated\n",
      "Time 927.0 updated\n",
      "Time 927.5 updated\n",
      "Time 928.0 updated\n",
      "Time 928.5 updated\n",
      "Time 929.0 updated\n",
      "Time 929.5 updated\n",
      "Time 930.0 updated\n",
      "Time 930.5 updated\n",
      "Time 931.0 updated\n",
      "Time 931.5 updated\n",
      "Time 932.0 updated\n",
      "Time 932.5 updated\n",
      "Time 933.0 updated\n",
      "Time 933.5 updated\n",
      "Time 934.0 updated\n",
      "Time 934.5 updated\n",
      "Time 935.0 updated\n",
      "Time 935.5 updated\n",
      "Time 936.0 updated\n",
      "Time 936.5 updated\n",
      "Time 937.0 updated\n",
      "Time 937.5 updated\n",
      "Time 938.0 updated\n",
      "Time 938.5 updated\n",
      "Time 939.0 updated\n",
      "Time 939.5 updated\n",
      "Time 940.0 updated\n",
      "Time 940.5 updated\n",
      "Time 941.0 updated\n",
      "Time 941.5 updated\n",
      "Time 942.0 updated\n",
      "Time 942.5 updated\n",
      "Time 943.0 updated\n",
      "Time 943.5 updated\n",
      "Time 944.0 updated\n",
      "Time 944.5 updated\n",
      "Time 945.0 updated\n",
      "Time 945.5 updated\n",
      "Time 946.0 updated\n",
      "Time 946.5 updated\n",
      "Time 947.0 updated\n",
      "Time 947.5 updated\n",
      "Time 948.0 updated\n",
      "Time 948.5 updated\n",
      "Time 949.0 updated\n",
      "Time 949.5 updated\n",
      "Time 950.0 updated\n",
      "Time 950.5 updated\n",
      "Time 951.0 updated\n",
      "Time 951.5 updated\n",
      "Time 952.0 updated\n",
      "Time 952.5 updated\n",
      "Time 953.0 updated\n",
      "Time 953.5 updated\n",
      "Time 954.0 updated\n",
      "Time 954.5 updated\n",
      "Time 955.0 updated\n",
      "Time 955.5 updated\n",
      "Time 956.0 updated\n",
      "Time 956.5 updated\n",
      "Time 957.0 updated\n",
      "Time 957.5 updated\n",
      "Time 958.0 updated\n",
      "Time 958.5 updated\n",
      "Time 959.0 updated\n",
      "Time 959.5 updated\n",
      "Time 960.0 updated\n",
      "Time 960.5 updated\n",
      "Time 961.0 updated\n",
      "Time 961.5 updated\n",
      "Time 962.0 updated\n",
      "Time 962.5 updated\n",
      "Time 963.0 updated\n",
      "Time 963.5 updated\n",
      "Time 964.0 updated\n",
      "Time 964.5 updated\n",
      "Time 965.0 updated\n",
      "Time 965.5 updated\n",
      "Time 966.0 updated\n",
      "Time 966.5 updated\n",
      "Time 967.0 updated\n",
      "Time 967.5 updated\n",
      "Time 968.0 updated\n",
      "Time 968.5 updated\n",
      "Time 969.0 updated\n",
      "Time 969.5 updated\n",
      "Time 970.0 updated\n",
      "Time 970.5 updated\n",
      "Time 971.0 updated\n",
      "Time 971.5 updated\n",
      "Time 972.0 updated\n",
      "Time 972.5 updated\n",
      "Time 973.0 updated\n",
      "Time 973.5 updated\n",
      "Time 974.0 updated\n",
      "Time 974.5 updated\n",
      "Time 975.0 updated\n",
      "Time 975.5 updated\n",
      "Time 976.0 updated\n",
      "Time 976.5 updated\n",
      "Time 977.0 updated\n",
      "Time 977.5 updated\n",
      "Time 978.0 updated\n",
      "Time 978.5 updated\n",
      "Time 979.0 updated\n",
      "Time 979.5 updated\n",
      "Time 980.0 updated\n",
      "Time 980.5 updated\n",
      "Time 981.0 updated\n",
      "Time 981.5 updated\n",
      "Time 982.0 updated\n",
      "Time 982.5 updated\n",
      "Time 983.0 updated\n",
      "Time 983.5 updated\n",
      "Time 984.0 updated\n",
      "Time 984.5 updated\n",
      "Time 985.0 updated\n",
      "Time 985.5 updated\n",
      "Time 986.0 updated\n",
      "Time 986.5 updated\n",
      "Time 987.0 updated\n",
      "Time 987.5 updated\n",
      "Time 988.0 updated\n",
      "Time 988.5 updated\n",
      "Time 989.0 updated\n",
      "Time 989.5 updated\n",
      "Time 990.0 updated\n",
      "Time 990.5 updated\n",
      "Time 991.0 updated\n",
      "Time 991.5 updated\n",
      "Time 992.0 updated\n",
      "Time 992.5 updated\n",
      "Time 993.0 updated\n",
      "Time 993.5 updated\n",
      "Time 994.0 updated\n",
      "Time 994.5 updated\n",
      "Time 995.0 updated\n",
      "Time 995.5 updated\n",
      "Time 996.0 updated\n",
      "Time 996.5 updated\n",
      "Time 997.0 updated\n",
      "Time 997.5 updated\n",
      "Time 998.0 updated\n",
      "Time 998.5 updated\n",
      "Time 999.0 updated\n",
      "Time 999.5 updated\n",
      "Time 1000.0 updated\n",
      "Time 1000.5 updated\n",
      "Time 1001.0 updated\n",
      "Time 1001.5 updated\n",
      "Time 1002.0 updated\n",
      "Time 1002.5 updated\n",
      "Time 1003.0 updated\n",
      "Time 1003.5 updated\n",
      "Time 1004.0 updated\n",
      "Time 1004.5 updated\n",
      "Time 1005.0 updated\n",
      "Time 1005.5 updated\n",
      "Time 1006.0 updated\n",
      "Time 1006.5 updated\n",
      "Time 1007.0 updated\n",
      "Time 1007.5 updated\n",
      "Time 1008.0 updated\n",
      "Time 1008.5 updated\n",
      "Time 1009.0 updated\n",
      "Time 1009.5 updated\n",
      "Time 1010.0 updated\n",
      "Time 1010.5 updated\n",
      "Time 1011.0 updated\n",
      "Time 1011.5 updated\n",
      "Time 1012.0 updated\n",
      "Time 1012.5 updated\n",
      "Time 1013.0 updated\n",
      "Time 1013.5 updated\n",
      "Time 1014.0 updated\n",
      "Time 1014.5 updated\n",
      "Time 1015.0 updated\n",
      "Time 1015.5 updated\n",
      "Time 1016.0 updated\n",
      "Time 1016.5 updated\n",
      "Time 1017.0 updated\n",
      "Time 1017.5 updated\n",
      "Time 1018.0 updated\n",
      "Time 1018.5 updated\n",
      "Time 1019.0 updated\n",
      "Time 1019.5 updated\n",
      "Time 1020.0 updated\n",
      "Time 1020.5 updated\n",
      "Time 1021.0 updated\n",
      "Time 1021.5 updated\n",
      "Time 1022.0 updated\n",
      "Time 1022.5 updated\n",
      "Time 1023.0 updated\n",
      "Time 1023.5 updated\n",
      "Time 1024.0 updated\n",
      "Time 1024.5 updated\n",
      "Time 1025.0 updated\n",
      "Time 1025.5 updated\n",
      "Time 1026.0 updated\n",
      "Time 1026.5 updated\n",
      "Time 1027.0 updated\n",
      "Time 1027.5 updated\n",
      "Time 1028.0 updated\n",
      "Time 1028.5 updated\n",
      "Time 1029.0 updated\n",
      "Time 1029.5 updated\n",
      "Time 1030.0 updated\n",
      "Time 1030.5 updated\n",
      "Time 1031.0 updated\n",
      "Time 1031.5 updated\n",
      "Time 1032.0 updated\n",
      "Time 1032.5 updated\n",
      "Time 1033.0 updated\n",
      "Time 1033.5 updated\n",
      "Time 1034.0 updated\n",
      "Time 1034.5 updated\n",
      "Time 1035.0 updated\n",
      "Time 1035.5 updated\n",
      "Time 1036.0 updated\n",
      "Time 1036.5 updated\n",
      "Time 1037.0 updated\n",
      "Time 1037.5 updated\n",
      "Time 1038.0 updated\n",
      "Time 1038.5 updated\n",
      "Time 1039.0 updated\n",
      "Time 1039.5 updated\n",
      "Time 1040.0 updated\n",
      "Time 1040.5 updated\n",
      "Time 1041.0 updated\n",
      "Time 1041.5 updated\n",
      "Time 1042.0 updated\n",
      "Time 1042.5 updated\n",
      "Time 1043.0 updated\n",
      "Time 1043.5 updated\n",
      "Time 1044.0 updated\n",
      "Time 1044.5 updated\n",
      "Time 1045.0 updated\n",
      "Time 1045.5 updated\n",
      "Time 1046.0 updated\n",
      "Time 1046.5 updated\n",
      "Time 1047.0 updated\n",
      "Time 1047.5 updated\n",
      "Time 1048.0 updated\n",
      "Time 1048.5 updated\n",
      "Time 1049.0 updated\n",
      "Time 1049.5 updated\n",
      "Time 1050.0 updated\n",
      "Time 1050.5 updated\n",
      "Time 1051.0 updated\n",
      "Time 1051.5 updated\n",
      "Time 1052.0 updated\n",
      "Time 1052.5 updated\n",
      "Time 1053.0 updated\n",
      "Time 1053.5 updated\n",
      "Time 1054.0 updated\n",
      "Time 1054.5 updated\n",
      "Time 1055.0 updated\n",
      "Time 1055.5 updated\n",
      "Time 1056.0 updated\n",
      "Time 1056.5 updated\n",
      "Time 1057.0 updated\n",
      "Time 1057.5 updated\n",
      "Time 1058.0 updated\n",
      "Time 1058.5 updated\n",
      "Time 1059.0 updated\n",
      "Time 1059.5 updated\n",
      "Time 1060.0 updated\n",
      "Time 1060.5 updated\n",
      "Time 1061.0 updated\n",
      "Time 1061.5 updated\n",
      "Time 1062.0 updated\n",
      "Time 1062.5 updated\n",
      "Time 1063.0 updated\n",
      "Time 1063.5 updated\n",
      "Time 1064.0 updated\n",
      "Time 1064.5 updated\n",
      "Time 1065.0 updated\n",
      "Time 1065.5 updated\n",
      "Time 1066.0 updated\n",
      "Time 1066.5 updated\n",
      "Time 1067.0 updated\n",
      "Time 1067.5 updated\n",
      "Time 1068.0 updated\n",
      "Time 1068.5 updated\n",
      "Time 1069.0 updated\n",
      "Time 1069.5 updated\n",
      "Time 1070.0 updated\n",
      "Time 1070.5 updated\n",
      "Time 1071.0 updated\n",
      "Time 1071.5 updated\n",
      "Time 1072.0 updated\n",
      "Time 1072.5 updated\n",
      "Time 1073.0 updated\n",
      "Time 1073.5 updated\n",
      "Time 1074.0 updated\n",
      "Time 1074.5 updated\n",
      "Time 1075.0 updated\n",
      "Time 1075.5 updated\n",
      "Time 1076.0 updated\n",
      "Time 1076.5 updated\n",
      "Time 1077.0 updated\n",
      "Time 1077.5 updated\n",
      "Time 1078.0 updated\n",
      "Time 1078.5 updated\n",
      "Time 1079.0 updated\n",
      "Time 1079.5 updated\n",
      "Time 1080.0 updated\n",
      "Time 1080.5 updated\n",
      "Time 1081.0 updated\n",
      "Time 1081.5 updated\n",
      "Time 1082.0 updated\n",
      "Time 1082.5 updated\n",
      "Time 1083.0 updated\n",
      "Time 1083.5 updated\n",
      "Time 1084.0 updated\n",
      "Time 1084.5 updated\n",
      "Time 1085.0 updated\n",
      "Time 1085.5 updated\n",
      "Time 1086.0 updated\n",
      "Time 1086.5 updated\n",
      "Time 1087.0 updated\n",
      "Time 1087.5 updated\n",
      "Time 1088.0 updated\n",
      "Time 1088.5 updated\n",
      "Time 1089.0 updated\n",
      "Time 1089.5 updated\n",
      "Time 1090.0 updated\n",
      "Time 1090.5 updated\n",
      "Time 1091.0 updated\n",
      "Time 1091.5 updated\n",
      "Time 1092.0 updated\n",
      "Time 1092.5 updated\n",
      "Time 1093.0 updated\n",
      "Time 1093.5 updated\n",
      "Time 1094.0 updated\n",
      "Time 1094.5 updated\n",
      "Time 1095.0 updated\n",
      "Time 1095.5 updated\n",
      "Time 1096.0 updated\n",
      "Time 1096.5 updated\n",
      "Time 1097.0 updated\n",
      "Time 1097.5 updated\n",
      "Time 1098.0 updated\n",
      "Time 1098.5 updated\n",
      "Time 1099.0 updated\n",
      "Time 1099.5 updated\n",
      "Time 1100.0 updated\n",
      "Time 1100.5 updated\n",
      "Time 1101.0 updated\n",
      "Time 1101.5 updated\n",
      "Time 1102.0 updated\n",
      "Time 1102.5 updated\n",
      "Time 1103.0 updated\n",
      "Time 1103.5 updated\n",
      "Time 1104.0 updated\n",
      "Time 1104.5 updated\n",
      "Time 1105.0 updated\n",
      "Time 1105.5 updated\n",
      "Time 1106.0 updated\n",
      "Time 1106.5 updated\n",
      "Time 1107.0 updated\n",
      "Time 1107.5 updated\n",
      "Time 1108.0 updated\n",
      "Time 1108.5 updated\n",
      "Time 1109.0 updated\n",
      "Time 1109.5 updated\n",
      "Time 1110.0 updated\n",
      "Time 1110.5 updated\n",
      "Time 1111.0 updated\n",
      "Time 1111.5 updated\n",
      "Time 1112.0 updated\n",
      "Time 1112.5 updated\n",
      "Time 1113.0 updated\n",
      "Time 1113.5 updated\n",
      "Time 1114.0 updated\n",
      "Time 1114.5 updated\n",
      "Time 1115.0 updated\n",
      "Time 1115.5 updated\n",
      "Time 1116.0 updated\n",
      "Time 1116.5 updated\n",
      "Time 1117.0 updated\n",
      "Time 1117.5 updated\n",
      "Time 1118.0 updated\n",
      "Time 1118.5 updated\n",
      "Time 1119.0 updated\n",
      "Time 1119.5 updated\n",
      "Time 1120.0 updated\n",
      "Time 1120.5 updated\n",
      "Time 1121.0 updated\n",
      "Time 1121.5 updated\n",
      "Time 1122.0 updated\n",
      "Time 1122.5 updated\n",
      "Time 1123.0 updated\n",
      "Time 1123.5 updated\n",
      "Time 1124.0 updated\n",
      "Time 1124.5 updated\n",
      "Time 1125.0 updated\n",
      "Time 1125.5 updated\n",
      "Time 1126.0 updated\n",
      "Time 1126.5 updated\n",
      "Time 1127.0 updated\n",
      "Time 1127.5 updated\n",
      "Time 1128.0 updated\n",
      "Time 1128.5 updated\n",
      "Time 1129.0 updated\n",
      "Time 1129.5 updated\n",
      "Time 1130.0 updated\n",
      "Time 1130.5 updated\n",
      "Time 1131.0 updated\n",
      "Time 1131.5 updated\n",
      "Time 1132.0 updated\n",
      "Time 1132.5 updated\n",
      "Time 1133.0 updated\n",
      "Time 1133.5 updated\n",
      "Time 1134.0 updated\n",
      "Time 1134.5 updated\n",
      "Time 1135.0 updated\n",
      "Time 1135.5 updated\n",
      "Time 1136.0 updated\n",
      "Time 1136.5 updated\n",
      "Time 1137.0 updated\n",
      "Time 1137.5 updated\n",
      "Time 1138.0 updated\n",
      "Time 1138.5 updated\n",
      "Time 1139.0 updated\n",
      "Time 1139.5 updated\n",
      "Time 1140.0 updated\n",
      "Time 1140.5 updated\n",
      "Time 1141.0 updated\n",
      "Time 1141.5 updated\n",
      "Time 1142.0 updated\n",
      "Time 1142.5 updated\n",
      "Time 1143.0 updated\n",
      "Time 1143.5 updated\n",
      "Time 1144.0 updated\n",
      "Time 1144.5 updated\n",
      "Time 1145.0 updated\n",
      "Time 1145.5 updated\n",
      "Time 1146.0 updated\n",
      "Time 1146.5 updated\n",
      "Time 1147.0 updated\n",
      "Time 1147.5 updated\n",
      "Time 1148.0 updated\n",
      "Time 1148.5 updated\n",
      "Time 1149.0 updated\n",
      "Time 1149.5 updated\n",
      "Time 1150.0 updated\n",
      "Time 1150.5 updated\n",
      "Time 1151.0 updated\n",
      "Time 1151.5 updated\n",
      "Time 1152.0 updated\n",
      "Time 1152.5 updated\n",
      "Time 1153.0 updated\n",
      "Time 1153.5 updated\n",
      "Time 1154.0 updated\n",
      "Time 1154.5 updated\n",
      "Time 1155.0 updated\n",
      "Time 1155.5 updated\n",
      "Time 1156.0 updated\n",
      "Time 1156.5 updated\n",
      "Time 1157.0 updated\n",
      "Time 1157.5 updated\n",
      "Time 1158.0 updated\n",
      "Time 1158.5 updated\n",
      "Time 1159.0 updated\n",
      "Time 1159.5 updated\n",
      "Time 1160.0 updated\n",
      "Time 1160.5 updated\n",
      "Time 1161.0 updated\n",
      "Time 1161.5 updated\n",
      "Time 1162.0 updated\n",
      "Time 1162.5 updated\n",
      "Time 1163.0 updated\n",
      "Time 1163.5 updated\n",
      "Time 1164.0 updated\n",
      "Time 1164.5 updated\n",
      "Time 1165.0 updated\n",
      "Time 1165.5 updated\n",
      "Time 1166.0 updated\n",
      "Time 1166.5 updated\n",
      "Time 1167.0 updated\n",
      "Time 1167.5 updated\n",
      "Time 1168.0 updated\n",
      "Time 1168.5 updated\n",
      "Time 1169.0 updated\n",
      "Time 1169.5 updated\n",
      "Time 1170.0 updated\n",
      "Time 1170.5 updated\n",
      "Time 1171.0 updated\n",
      "Time 1171.5 updated\n",
      "Time 1172.0 updated\n",
      "Time 1172.5 updated\n",
      "Time 1173.0 updated\n",
      "Time 1173.5 updated\n",
      "Time 1174.0 updated\n",
      "Time 1174.5 updated\n",
      "Time 1175.0 updated\n",
      "Time 1175.5 updated\n",
      "Time 1176.0 updated\n",
      "Time 1176.5 updated\n",
      "Time 1177.0 updated\n",
      "Time 1177.5 updated\n",
      "Time 1178.0 updated\n",
      "Time 1178.5 updated\n",
      "Time 1179.0 updated\n",
      "Time 1179.5 updated\n",
      "Time 1180.0 updated\n",
      "Time 1180.5 updated\n",
      "Time 1181.0 updated\n",
      "Time 1181.5 updated\n",
      "Time 1182.0 updated\n",
      "Time 1182.5 updated\n",
      "Time 1183.0 updated\n",
      "Time 1183.5 updated\n",
      "Time 1184.0 updated\n",
      "Time 1184.5 updated\n",
      "Time 1185.0 updated\n",
      "Time 1185.5 updated\n",
      "Time 1186.0 updated\n",
      "Time 1186.5 updated\n",
      "Time 1187.0 updated\n",
      "Time 1187.5 updated\n",
      "Time 1188.0 updated\n",
      "Time 1188.5 updated\n",
      "Time 1189.0 updated\n",
      "Time 1189.5 updated\n",
      "Time 1190.0 updated\n",
      "Time 1190.5 updated\n",
      "Time 1191.0 updated\n",
      "Time 1191.5 updated\n",
      "Time 1192.0 updated\n",
      "Time 1192.5 updated\n",
      "Time 1193.0 updated\n",
      "Time 1193.5 updated\n",
      "Time 1194.0 updated\n",
      "Time 1194.5 updated\n",
      "Time 1195.0 updated\n",
      "Time 1195.5 updated\n",
      "Time 1196.0 updated\n",
      "Time 1196.5 updated\n",
      "Time 1197.0 updated\n",
      "Time 1197.5 updated\n",
      "Time 1198.0 updated\n",
      "Time 1198.5 updated\n",
      "Time 1199.0 updated\n",
      "Time 1199.5 updated\n",
      "Time 1200.0 updated\n",
      "Time 1200.5 updated\n",
      "Time 1201.0 updated\n",
      "Time 1201.5 updated\n",
      "Time 1202.0 updated\n",
      "Time 1202.5 updated\n",
      "Time 1203.0 updated\n",
      "Time 1203.5 updated\n",
      "Time 1204.0 updated\n",
      "Time 1204.5 updated\n",
      "Time 1205.0 updated\n",
      "Time 1205.5 updated\n",
      "Time 1206.0 updated\n",
      "Time 1206.5 updated\n",
      "Time 1207.0 updated\n",
      "Time 1207.5 updated\n",
      "Time 1208.0 updated\n",
      "Time 1208.5 updated\n",
      "Time 1209.0 updated\n",
      "Time 1209.5 updated\n",
      "Time 1210.0 updated\n",
      "Time 1210.5 updated\n",
      "Time 1211.0 updated\n",
      "Time 1211.5 updated\n",
      "Time 1212.0 updated\n",
      "Time 1212.5 updated\n",
      "Time 1213.0 updated\n",
      "Time 1213.5 updated\n",
      "Time 1214.0 updated\n",
      "Time 1214.5 updated\n",
      "Time 1215.0 updated\n",
      "Time 1215.5 updated\n",
      "Time 1216.0 updated\n",
      "Time 1216.5 updated\n",
      "Time 1217.0 updated\n",
      "Time 1217.5 updated\n",
      "Time 1218.0 updated\n",
      "Time 1218.5 updated\n",
      "Time 1219.0 updated\n",
      "Time 1219.5 updated\n",
      "Time 1220.0 updated\n",
      "Time 1220.5 updated\n",
      "Time 1221.0 updated\n",
      "Time 1221.5 updated\n",
      "Time 1222.0 updated\n",
      "Time 1222.5 updated\n",
      "Time 1223.0 updated\n",
      "Time 1223.5 updated\n",
      "Time 1224.0 updated\n",
      "Time 1224.5 updated\n",
      "Time 1225.0 updated\n",
      "Time 1225.5 updated\n",
      "Time 1226.0 updated\n",
      "Time 1226.5 updated\n",
      "Time 1227.0 updated\n",
      "Time 1227.5 updated\n",
      "Time 1228.0 updated\n",
      "Time 1228.5 updated\n",
      "Time 1229.0 updated\n",
      "Time 1229.5 updated\n",
      "Time 1230.0 updated\n",
      "Time 1230.5 updated\n",
      "Time 1231.0 updated\n",
      "Time 1231.5 updated\n",
      "Time 1232.0 updated\n",
      "Time 1232.5 updated\n",
      "Time 1233.0 updated\n",
      "Time 1233.5 updated\n",
      "Time 1234.0 updated\n",
      "Time 1234.5 updated\n",
      "Time 1235.0 updated\n",
      "Time 1235.5 updated\n",
      "Time 1236.0 updated\n",
      "Time 1236.5 updated\n",
      "Time 1237.0 updated\n",
      "Time 1237.5 updated\n",
      "Time 1238.0 updated\n",
      "Time 1238.5 updated\n",
      "Time 1239.0 updated\n",
      "Time 1239.5 updated\n",
      "Time 1240.0 updated\n",
      "Time 1240.5 updated\n",
      "Time 1241.0 updated\n",
      "Time 1241.5 updated\n",
      "Time 1242.0 updated\n",
      "Time 1242.5 updated\n",
      "Time 1243.0 updated\n",
      "Time 1243.5 updated\n",
      "Time 1244.0 updated\n",
      "Time 1244.5 updated\n",
      "Time 1245.0 updated\n",
      "Time 1245.5 updated\n",
      "Time 1246.0 updated\n",
      "Time 1246.5 updated\n",
      "Time 1247.0 updated\n",
      "Time 1247.5 updated\n",
      "Time 1248.0 updated\n",
      "Time 1248.5 updated\n",
      "Time 1249.0 updated\n",
      "Time 1249.5 updated\n",
      "Time 1250.0 updated\n",
      "Time 1250.5 updated\n",
      "Time 1251.0 updated\n",
      "Time 1251.5 updated\n",
      "Time 1252.0 updated\n",
      "Time 1252.5 updated\n",
      "Time 1253.0 updated\n",
      "Time 1253.5 updated\n",
      "Time 1254.0 updated\n",
      "Time 1254.5 updated\n",
      "Time 1255.0 updated\n",
      "Time 1255.5 updated\n",
      "Time 1256.0 updated\n",
      "Time 1256.5 updated\n",
      "Time 1257.0 updated\n",
      "Time 1257.5 updated\n",
      "Time 1258.0 updated\n",
      "Time 1258.5 updated\n",
      "Time 1259.0 updated\n",
      "Time 1259.5 updated\n",
      "Time 1260.0 updated\n",
      "Time 1260.5 updated\n",
      "Time 1261.0 updated\n",
      "Time 1261.5 updated\n",
      "Time 1262.0 updated\n",
      "Time 1262.5 updated\n",
      "Time 1263.0 updated\n",
      "Time 1263.5 updated\n",
      "Time 1264.0 updated\n",
      "Time 1264.5 updated\n",
      "Time 1265.0 updated\n",
      "Time 1265.5 updated\n",
      "Time 1266.0 updated\n",
      "Time 1266.5 updated\n",
      "Time 1267.0 updated\n",
      "Time 1267.5 updated\n",
      "Time 1268.0 updated\n",
      "Time 1268.5 updated\n",
      "Time 1269.0 updated\n",
      "Time 1269.5 updated\n",
      "Time 1270.0 updated\n",
      "Time 1270.5 updated\n",
      "Time 1271.0 updated\n",
      "Time 1271.5 updated\n",
      "Time 1272.0 updated\n",
      "Time 1272.5 updated\n",
      "Time 1273.0 updated\n",
      "Time 1273.5 updated\n",
      "Time 1274.0 updated\n",
      "Time 1274.5 updated\n",
      "Time 1275.0 updated\n",
      "Time 1275.5 updated\n",
      "Time 1276.0 updated\n",
      "Time 1276.5 updated\n",
      "Time 1277.0 updated\n",
      "Time 1277.5 updated\n",
      "Time 1278.0 updated\n",
      "Time 1278.5 updated\n",
      "Time 1279.0 updated\n",
      "Time 1279.5 updated\n",
      "Time 1280.0 updated\n",
      "Time 1280.5 updated\n",
      "Time 1281.0 updated\n",
      "Time 1281.5 updated\n",
      "Time 1282.0 updated\n",
      "Time 1282.5 updated\n",
      "Time 1283.0 updated\n",
      "Time 1283.5 updated\n",
      "Time 1284.0 updated\n",
      "Time 1284.5 updated\n",
      "Time 1285.0 updated\n",
      "Time 1285.5 updated\n",
      "Time 1286.0 updated\n",
      "Time 1286.5 updated\n",
      "Time 1287.0 updated\n",
      "Time 1287.5 updated\n",
      "Time 1288.0 updated\n",
      "Time 1288.5 updated\n",
      "Time 1289.0 updated\n",
      "Time 1289.5 updated\n",
      "Time 1290.0 updated\n",
      "Time 1290.5 updated\n",
      "Time 1291.0 updated\n",
      "Time 1291.5 updated\n",
      "Time 1292.0 updated\n",
      "Time 1292.5 updated\n",
      "Time 1293.0 updated\n",
      "Time 1293.5 updated\n",
      "Time 1294.0 updated\n",
      "Time 1294.5 updated\n",
      "Time 1295.0 updated\n",
      "Time 1295.5 updated\n",
      "Time 1296.0 updated\n",
      "Time 1296.5 updated\n",
      "Time 1297.0 updated\n",
      "Time 1297.5 updated\n",
      "Time 1298.0 updated\n",
      "Time 1298.5 updated\n",
      "Time 1299.0 updated\n",
      "Time 1299.5 updated\n",
      "Time 1300.0 updated\n",
      "Time 1300.5 updated\n",
      "Time 1301.0 updated\n",
      "Time 1301.5 updated\n",
      "Time 1302.0 updated\n",
      "Time 1302.5 updated\n",
      "Time 1303.0 updated\n",
      "Time 1303.5 updated\n",
      "Time 1304.0 updated\n",
      "Time 1304.5 updated\n",
      "Time 1305.0 updated\n",
      "Time 1305.5 updated\n",
      "Time 1306.0 updated\n",
      "Time 1306.5 updated\n",
      "Time 1307.0 updated\n",
      "Time 1307.5 updated\n",
      "Time 1308.0 updated\n",
      "Time 1308.5 updated\n",
      "Time 1309.0 updated\n",
      "Time 1309.5 updated\n",
      "Time 1310.0 updated\n",
      "Time 1310.5 updated\n",
      "Time 1311.0 updated\n",
      "Time 1311.5 updated\n",
      "Time 1312.0 updated\n",
      "Time 1312.5 updated\n",
      "Time 1313.0 updated\n",
      "Time 1313.5 updated\n",
      "Time 1314.0 updated\n",
      "Time 1314.5 updated\n",
      "Time 1315.0 updated\n",
      "Time 1315.5 updated\n",
      "Time 1316.0 updated\n",
      "Time 1316.5 updated\n",
      "Time 1317.0 updated\n",
      "Time 1317.5 updated\n",
      "Time 1318.0 updated\n",
      "Time 1318.5 updated\n",
      "Time 1319.0 updated\n",
      "Time 1319.5 updated\n",
      "Time 1320.0 updated\n",
      "Time 1320.5 updated\n",
      "Time 1321.0 updated\n",
      "Time 1321.5 updated\n",
      "Time 1322.0 updated\n",
      "Time 1322.5 updated\n",
      "Time 1323.0 updated\n",
      "Time 1323.5 updated\n",
      "Time 1324.0 updated\n",
      "Time 1324.5 updated\n",
      "Time 1325.0 updated\n",
      "Time 1325.5 updated\n",
      "Time 1326.0 updated\n",
      "Time 1326.5 updated\n",
      "Time 1327.0 updated\n",
      "Time 1327.5 updated\n",
      "Time 1328.0 updated\n",
      "Time 1328.5 updated\n",
      "Time 1329.0 updated\n",
      "Time 1329.5 updated\n",
      "Time 1330.0 updated\n",
      "Time 1330.5 updated\n",
      "Time 1331.0 updated\n",
      "Time 1331.5 updated\n",
      "Time 1332.0 updated\n",
      "Time 1332.5 updated\n",
      "Time 1333.0 updated\n",
      "Time 1333.5 updated\n",
      "Time 1334.0 updated\n",
      "Time 1334.5 updated\n",
      "Time 1335.0 updated\n",
      "Time 1335.5 updated\n",
      "Time 1336.0 updated\n",
      "Time 1336.5 updated\n",
      "Time 1337.0 updated\n",
      "Time 1337.5 updated\n",
      "Time 1338.0 updated\n",
      "Time 1338.5 updated\n",
      "Time 1339.0 updated\n",
      "Time 1339.5 updated\n",
      "Time 1340.0 updated\n",
      "Time 1340.5 updated\n",
      "Time 1341.0 updated\n",
      "Time 1341.5 updated\n",
      "Time 1342.0 updated\n",
      "Time 1342.5 updated\n",
      "Time 1343.0 updated\n",
      "Time 1343.5 updated\n",
      "Time 1344.0 updated\n",
      "Time 1344.5 updated\n",
      "Time 1345.0 updated\n",
      "Time 1345.5 updated\n",
      "Time 1346.0 updated\n",
      "Time 1346.5 updated\n",
      "Time 1347.0 updated\n",
      "Time 1347.5 updated\n",
      "Time 1348.0 updated\n",
      "Time 1348.5 updated\n",
      "Time 1349.0 updated\n",
      "Time 1349.5 updated\n",
      "Time 1350.0 updated\n",
      "Time 1350.5 updated\n",
      "Time 1351.0 updated\n",
      "Time 1351.5 updated\n",
      "Time 1352.0 updated\n",
      "Time 1352.5 updated\n",
      "Time 1353.0 updated\n",
      "Time 1353.5 updated\n",
      "Time 1354.0 updated\n",
      "Time 1354.5 updated\n",
      "Time 1355.0 updated\n",
      "Time 1355.5 updated\n",
      "Time 1356.0 updated\n",
      "Time 1356.5 updated\n",
      "Time 1357.0 updated\n",
      "Time 1357.5 updated\n",
      "Time 1358.0 updated\n",
      "Time 1358.5 updated\n",
      "Time 1359.0 updated\n",
      "Time 1359.5 updated\n",
      "Time 1360.0 updated\n",
      "Time 1360.5 updated\n",
      "Time 1361.0 updated\n",
      "Time 1361.5 updated\n",
      "Time 1362.0 updated\n",
      "Time 1362.5 updated\n",
      "Time 1363.0 updated\n",
      "Time 1363.5 updated\n",
      "Time 1364.0 updated\n",
      "Time 1364.5 updated\n",
      "Time 1365.0 updated\n",
      "Time 1365.5 updated\n",
      "Time 1366.0 updated\n",
      "Time 1366.5 updated\n",
      "Time 1367.0 updated\n",
      "Time 1367.5 updated\n",
      "Time 1368.0 updated\n",
      "Time 1368.5 updated\n",
      "Time 1369.0 updated\n",
      "Time 1369.5 updated\n",
      "Time 1370.0 updated\n",
      "Time 1370.5 updated\n",
      "Time 1371.0 updated\n",
      "Time 1371.5 updated\n",
      "Time 1372.0 updated\n",
      "Time 1372.5 updated\n",
      "Time 1373.0 updated\n",
      "Time 1373.5 updated\n",
      "Time 1374.0 updated\n",
      "Time 1374.5 updated\n",
      "Time 1375.0 updated\n",
      "Time 1375.5 updated\n",
      "Time 1376.0 updated\n",
      "Time 1376.5 updated\n",
      "Time 1377.0 updated\n",
      "Time 1377.5 updated\n",
      "Time 1378.0 updated\n",
      "Time 1378.5 updated\n",
      "Time 1379.0 updated\n",
      "Time 1379.5 updated\n",
      "Time 1380.0 updated\n",
      "Time 1380.5 updated\n",
      "Time 1381.0 updated\n",
      "Time 1381.5 updated\n",
      "Time 1382.0 updated\n",
      "Time 1382.5 updated\n",
      "Time 1383.0 updated\n",
      "Time 1383.5 updated\n",
      "Time 1384.0 updated\n",
      "Time 1384.5 updated\n",
      "Time 1385.0 updated\n",
      "Time 1385.5 updated\n",
      "Time 1386.0 updated\n",
      "Time 1386.5 updated\n",
      "Time 1387.0 updated\n",
      "Time 1387.5 updated\n",
      "Time 1388.0 updated\n",
      "Time 1388.5 updated\n",
      "Time 1389.0 updated\n",
      "Time 1389.5 updated\n",
      "Time 1390.0 updated\n",
      "Time 1390.5 updated\n",
      "Time 1391.0 updated\n",
      "Time 1391.5 updated\n",
      "Time 1392.0 updated\n",
      "Time 1392.5 updated\n",
      "Time 1393.0 updated\n",
      "Time 1393.5 updated\n",
      "Time 1394.0 updated\n",
      "Time 1394.5 updated\n",
      "Time 1395.0 updated\n",
      "Time 1395.5 updated\n",
      "Time 1396.0 updated\n",
      "Time 1396.5 updated\n",
      "Time 1397.0 updated\n",
      "Time 1397.5 updated\n",
      "Time 1398.0 updated\n",
      "Time 1398.5 updated\n",
      "Time 1399.0 updated\n",
      "Time 1399.5 updated\n",
      "Time 1400.0 updated\n",
      "Time 1400.5 updated\n"
     ]
    }
   ],
   "source": [
    "diff_sample_2 = generation(model, 250, n, 0.5, 4 * n, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "75b98cc7-8195-44d6-8acb-9e262108e614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_sample = torch.concat([diff_sample_1, diff_sample_2], dim=0)\n",
    "fn = torch.norm(diff_sample, p='fro', dim=(1, 2))\n",
    "\n",
    "fig, ax = plt.subplots()  # width, height in inches\n",
    "# Plot histogram\n",
    "ax.hist(fn.detach().cpu().numpy(),\n",
    "        bins='fd',                     # Freedman–Diaconis rule\n",
    "        density=True,                  # or density=True for area=1\n",
    "        facecolor='lightblue',                # light gray\n",
    "        edgecolor='black', \n",
    "        alpha=0.6,\n",
    "        linewidth=0.5, label=\"Diffusion samples\")\n",
    "\n",
    "# Clean up spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "# Ticks outside\n",
    "ax.tick_params(direction='out', width=0.5)\n",
    "\n",
    "# Labels (match your manuscript font & size)\n",
    "ax.set_xlabel('Frobenius norm', fontsize=9)\n",
    "ax.set_ylabel('density', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('histogram_2.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2210ea7-e3e5-4b18-a70a-233114e22312",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    # Make test dataset\n",
    "    N = 4000\n",
    "    n = 350\n",
    "    k = 20\n",
    "    \n",
    "    # Create dataset for training\n",
    "    test_data = sample_data(N, n, k, device=device)\n",
    "    test_dataset = SubmatrixDataset(test_data)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=250, shuffle=True)\n",
    "    \n",
    "    # Draw a loss curve\n",
    "    loss_pts = []\n",
    "    \n",
    "    # Make an array of time points\n",
    "    time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for it in range(len(time_array)):\n",
    "            loss = 0\n",
    "            \n",
    "            # Set time\n",
    "            t = time_array[it].item()\n",
    "        \n",
    "            # Evaluate model\n",
    "            for batch in test_dataloader:\n",
    "                loss_b = mc_loss_batch_fixed(model, batch, t, n, k, predict=True, device=device)\n",
    "                loss += loss_b.item() * batch.shape[0]\n",
    "    \n",
    "            # Store loss\n",
    "            loss_pts.append(loss / len(test_dataloader.dataset))\n",
    "            print(\"Iteration {} finished!\".format(it))\n",
    "\n",
    "    return loss_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d6ecadc-6ab7-4ba9-9a5c-7fd115fc815d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 finished!\n",
      "Iteration 1 finished!\n",
      "Iteration 2 finished!\n",
      "Iteration 3 finished!\n",
      "Iteration 4 finished!\n",
      "Iteration 5 finished!\n",
      "Iteration 6 finished!\n",
      "Iteration 7 finished!\n",
      "Iteration 8 finished!\n",
      "Iteration 9 finished!\n",
      "Iteration 10 finished!\n",
      "Iteration 11 finished!\n",
      "Iteration 12 finished!\n",
      "Iteration 13 finished!\n",
      "Iteration 14 finished!\n",
      "Iteration 15 finished!\n",
      "Iteration 16 finished!\n",
      "Iteration 17 finished!\n",
      "Iteration 18 finished!\n",
      "Iteration 19 finished!\n",
      "Iteration 20 finished!\n",
      "Iteration 21 finished!\n",
      "Iteration 22 finished!\n",
      "Iteration 23 finished!\n",
      "Iteration 24 finished!\n",
      "Iteration 25 finished!\n",
      "Iteration 26 finished!\n",
      "Iteration 27 finished!\n",
      "Iteration 28 finished!\n",
      "Iteration 29 finished!\n",
      "Iteration 30 finished!\n",
      "Iteration 31 finished!\n",
      "Iteration 32 finished!\n",
      "Iteration 33 finished!\n",
      "Iteration 34 finished!\n",
      "Iteration 35 finished!\n",
      "Iteration 36 finished!\n",
      "Iteration 37 finished!\n",
      "Iteration 38 finished!\n",
      "Iteration 39 finished!\n",
      "Iteration 40 finished!\n",
      "Iteration 41 finished!\n",
      "Iteration 42 finished!\n",
      "Iteration 43 finished!\n",
      "Iteration 44 finished!\n",
      "Iteration 45 finished!\n",
      "Iteration 46 finished!\n",
      "Iteration 47 finished!\n",
      "Iteration 48 finished!\n",
      "Iteration 49 finished!\n",
      "Iteration 50 finished!\n",
      "Iteration 51 finished!\n",
      "Iteration 52 finished!\n",
      "Iteration 53 finished!\n",
      "Iteration 54 finished!\n",
      "Iteration 55 finished!\n",
      "Iteration 56 finished!\n",
      "Iteration 57 finished!\n",
      "Iteration 58 finished!\n",
      "Iteration 59 finished!\n",
      "Iteration 60 finished!\n",
      "Iteration 61 finished!\n",
      "Iteration 62 finished!\n",
      "Iteration 63 finished!\n",
      "Iteration 64 finished!\n",
      "Iteration 65 finished!\n",
      "Iteration 66 finished!\n",
      "Iteration 67 finished!\n",
      "Iteration 68 finished!\n",
      "Iteration 69 finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5020885020494461,\n",
       " 0.5019984804093838,\n",
       " 0.5020536296069622,\n",
       " 0.5022800266742706,\n",
       " 0.5022178720682859,\n",
       " 0.5021618660539389,\n",
       " 0.5021905545145273,\n",
       " 0.5003505311906338,\n",
       " 0.4896172694861889,\n",
       " 0.4739218447357416,\n",
       " 0.4411539398133755,\n",
       " 0.39247821271419525,\n",
       " 0.3337413650006056,\n",
       " 0.2651530737057328,\n",
       " 0.20388762187212706,\n",
       " 0.15461339615285397,\n",
       " 0.11271554650738835,\n",
       " 0.079610510263592,\n",
       " 0.0532805051188916,\n",
       " 0.03935227554757148,\n",
       " 0.026435636449605227,\n",
       " 0.018036375869996846,\n",
       " 0.012284025084227324,\n",
       " 0.009803929933696054,\n",
       " 0.00805684803344775,\n",
       " 0.006480080111941788,\n",
       " 0.006901239539729431,\n",
       " 0.004729209627839737,\n",
       " 0.003990019060438499,\n",
       " 0.0029925947201263625,\n",
       " 0.003991233019405627,\n",
       " 0.0024484594295017814,\n",
       " 0.0030160561634602345,\n",
       " 0.0015557483112615955,\n",
       " 0.0016104837054626842,\n",
       " 0.0015522012582778189,\n",
       " 0.0017539857357178334,\n",
       " 0.0018643302717009647,\n",
       " 0.0012337385069258744,\n",
       " 0.0011296239262037489,\n",
       " 0.001625689652996698,\n",
       " 0.0015640726174979136,\n",
       " 0.000997534532757527,\n",
       " 0.002056469208469025,\n",
       " 0.0017923032852422693,\n",
       " 0.0018797333874545075,\n",
       " 0.001137806084670956,\n",
       " 0.0015167462934186915,\n",
       " 0.0017844826645614376,\n",
       " 0.00129967709540324,\n",
       " 0.0010338890034518045,\n",
       " 0.0018989319158890794,\n",
       " 0.0012626281571783693,\n",
       " 0.0016683088448417038,\n",
       " 0.001374935514718345,\n",
       " 0.001238388612136987,\n",
       " 0.001912478045824173,\n",
       " 0.0011372560477411753,\n",
       " 0.00048781212854009937,\n",
       " 0.0005524772579974524,\n",
       " 0.0012412334526743507,\n",
       " 0.0016237854540577246,\n",
       " 0.00046423493063230126,\n",
       " 0.000924517135899805,\n",
       " 0.0002316269778930291,\n",
       " 0.0019307196691897843,\n",
       " 0.0008334316310083523,\n",
       " 0.0018189469185472262,\n",
       " 0.0011251229454956047,\n",
       " 0.0012119893522140046]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b0e616a8-27e0-416e-8ee5-6420fe32a0f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_pts_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m time_array \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m1400\u001b[39m, \u001b[38;5;241m70\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(time_array\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m/\u001b[39m n, np\u001b[38;5;241m.\u001b[39marray(loss_pts_1), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph NN Denoiser\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(time_array\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m/\u001b[39m n, np\u001b[38;5;241m.\u001b[39marray(loss_ls_1), label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraph NN Denoiser pre-trained\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_pts_1' is not defined"
     ]
    }
   ],
   "source": [
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, np.array(loss_pts_1), label=\"Graph NN Denoiser\")\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, np.array(loss_ls_1), label=\"Graph NN Denoiser pre-trained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0c43d695-8ee5-4833-8db9-921cb7184fdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n"
     ]
    }
   ],
   "source": [
    "# Make test dataset\n",
    "N = 4000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "test_data = sample_data(N, n, k, device=device)\n",
    "test_dataset = SubmatrixDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=250, shuffle=True)\n",
    "\n",
    "# Load model\n",
    "model_350 = TestMPNN_3_a(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model_350.to(device)\n",
    "model_350.load_state_dict(torch.load(\"model_weights_opt_350.pth\", weights_only=False))\n",
    "\n",
    "# Draw a loss curve\n",
    "loss_ls_1 = []\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "    \n",
    "        # Evaluate model\n",
    "        for batch in test_dataloader:\n",
    "            loss_b = mc_loss_batch_fixed(model_350, batch, t, n, k, predict=True, device=device)\n",
    "            loss += loss_b.item() * batch.shape[0]\n",
    "\n",
    "            # Print\n",
    "            print(\"Batch {} finished!\".format(counter))\n",
    "            counter += 1\n",
    "\n",
    "        # Store loss\n",
    "        loss_ls_1.append(loss / len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43fe43df-c476-4af7-9df2-cb64917fb566",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.49184071086347103,\n",
       " 0.4917485285550356,\n",
       " 0.49180174618959427,\n",
       " 0.4920301903039217,\n",
       " 0.4919696170836687,\n",
       " 0.4919114410877228,\n",
       " 0.49180582351982594,\n",
       " 0.4894152209162712,\n",
       " 0.4828316941857338,\n",
       " 0.4644024521112442,\n",
       " 0.4297377746552229,\n",
       " 0.38629792258143425,\n",
       " 0.3301869314163923,\n",
       " 0.260413670912385,\n",
       " 0.20248392783105373,\n",
       " 0.14986747410148382,\n",
       " 0.10805582720786333,\n",
       " 0.07768315658904612,\n",
       " 0.05658283829689026,\n",
       " 0.03454471076838672,\n",
       " 0.02209648856660351,\n",
       " 0.018536123243393376,\n",
       " 0.013676238391781226,\n",
       " 0.012535405432572588,\n",
       " 0.00767348067893181,\n",
       " 0.00667822829564102,\n",
       " 0.004993184457816824,\n",
       " 0.006481169850303559,\n",
       " 0.0038570397555304226,\n",
       " 0.0039449159457944916,\n",
       " 0.0031131474427184003,\n",
       " 0.0018784997314469365,\n",
       " 0.0015788356006396498,\n",
       " 0.002813213779745638,\n",
       " 0.002445227249609161,\n",
       " 0.0014150493469742287,\n",
       " 0.001652131041055327,\n",
       " 0.0016261149310139444,\n",
       " 0.0007841777002113304,\n",
       " 0.002100876395843443,\n",
       " 0.0007271711087923904,\n",
       " 0.001405602803970396,\n",
       " 0.0017962892704872502,\n",
       " 0.0016572392897842292,\n",
       " 0.0011811207034497784,\n",
       " 0.0005793365266981709,\n",
       " 0.0010966514673782513,\n",
       " 0.0015194012521533296,\n",
       " 0.001503611211774114,\n",
       " 0.0010969073041451338,\n",
       " 0.0014974671655636485,\n",
       " 0.001092119230179378,\n",
       " 0.0014073660424855916,\n",
       " 0.000490115735829022,\n",
       " 0.00032264147705518553,\n",
       " 0.0007670733546092379,\n",
       " 0.0010973945358045967,\n",
       " 0.0007192500455630579,\n",
       " 0.000716612245014403,\n",
       " 0.0008415510196755349,\n",
       " 0.001262128415874031,\n",
       " 0.001704162164060108,\n",
       " 0.0015460767606327863,\n",
       " 0.0006209544415014534,\n",
       " 0.0006222464603524713,\n",
       " 0.0007617194942213246,\n",
       " 0.0011991833810043317,\n",
       " 0.00054659356396769,\n",
       " 0.0013724366438054858,\n",
       " 0.0008592208948812186]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "2a817460-4a0d-4aac-b8ab-e289b6ab9c6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "0.0008855383436525699\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[134], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m test_dataloader:\n\u001b[0;32m---> 25\u001b[0m     loss_b \u001b[38;5;241m=\u001b[39m mc_loss_batch_fixed(model, batch, t, n, k, predict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_b\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Print\u001b[39;00m\n",
      "File \u001b[0;32m/home/groups/montanar/vietvu01/SparseDiffusion/loss.py:136\u001b[0m, in \u001b[0;36mmc_loss_batch_fixed\u001b[0;34m(model, batch, t, n, k, predict, device)\u001b[0m\n\u001b[1;32m    133\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((out \u001b[38;5;241m-\u001b[39m batch_outer) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Return memory\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss \u001b[38;5;241m/\u001b[39m batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/cuda/memory.py:192\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 192\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_emptyCache()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Make test dataset\n",
    "N = 4000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "test_data = sample_data(N, n, k, device=device)\n",
    "test_dataset = SubmatrixDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=250, shuffle=True)\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "        t = 400000\n",
    "    \n",
    "        # Evaluate model\n",
    "        for batch in test_dataloader:\n",
    "            loss_b = mc_loss_batch_fixed(model, batch, t, n, k, predict=True, device=device)\n",
    "            loss += loss_b.item() * batch.shape[0]\n",
    "\n",
    "            # Print\n",
    "            print(\"Batch {} finished!\".format(counter))\n",
    "            counter += 1\n",
    "\n",
    "        # Store loss\n",
    "        print(loss / len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43d1539a-d5da-4186-aefd-66c52112b454",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 10\n",
      "Iteration 451, Batch: 0, Loss: 0.3171202838420868\n",
      "Iteration 451, Batch: 1, Loss: 0.31667184829711914\n",
      "Iteration 451, Batch: 2, Loss: 0.22980999946594238\n",
      "Iteration 451, Batch: 3, Loss: 0.24955077469348907\n",
      "Iteration 451, Batch: 4, Loss: 0.17116615176200867\n",
      "Iteration 451, Batch: 5, Loss: 0.12999339401721954\n",
      "Iteration 451, Batch: 6, Loss: 0.12718981504440308\n",
      "Iteration 451, Batch: 7, Loss: 0.12258017063140869\n",
      "Iteration 451, Batch: 8, Loss: 0.10189250111579895\n",
      "Iteration 451, Batch: 9, Loss: 0.10251332074403763\n",
      "Iteration 451, Batch: 10, Loss: 0.07048789411783218\n",
      "Iteration 451, Batch: 11, Loss: 0.0994030088186264\n",
      "Iteration 451, Batch: 12, Loss: 0.07229512184858322\n",
      "Iteration 451, Batch: 13, Loss: 0.09597458690404892\n",
      "Iteration 451, Batch: 14, Loss: 0.07843354344367981\n",
      "Iteration 451, Batch: 15, Loss: 0.061997316777706146\n",
      "Iteration 451, Batch: 16, Loss: 0.0790979191660881\n",
      "Iteration 451, Batch: 17, Loss: 0.0687742605805397\n",
      "Iteration 451, Batch: 18, Loss: 0.055865637958049774\n",
      "Iteration 451, Batch: 19, Loss: 0.09880506247282028\n",
      "Iteration 451, Batch: 20, Loss: 0.041713424026966095\n",
      "Iteration 451, Batch: 21, Loss: 0.05648195743560791\n",
      "Iteration 451, Batch: 22, Loss: 0.06696510314941406\n",
      "Iteration 451, Batch: 23, Loss: 0.07328353077173233\n",
      "Iteration 451, Batch: 24, Loss: 0.03877296298742294\n",
      "Iteration 451, Batch: 25, Loss: 0.037071049213409424\n",
      "Iteration 451, Batch: 26, Loss: 0.06741751730442047\n",
      "Iteration 451, Batch: 27, Loss: 0.09104464948177338\n",
      "Iteration 451, Batch: 28, Loss: 0.03839318826794624\n",
      "Iteration 451, Batch: 29, Loss: 0.051009416580200195\n",
      "Iteration 451, Batch: 30, Loss: 0.06321334838867188\n",
      "Iteration 451, Batch: 31, Loss: 0.04877617582678795\n",
      "Iteration 451, Batch: 32, Loss: 0.0539119616150856\n",
      "Iteration 451, Batch: 33, Loss: 0.06266917288303375\n",
      "Iteration 451, Batch: 34, Loss: 0.0471925213932991\n",
      "Iteration 451, Batch: 35, Loss: 0.04318666458129883\n",
      "Iteration 451, Batch: 36, Loss: 0.049910806119441986\n",
      "Iteration 451, Batch: 37, Loss: 0.04296741262078285\n",
      "Iteration 451, Batch: 38, Loss: 0.06998196244239807\n",
      "Iteration 451, Batch: 39, Loss: 0.04208778589963913\n",
      "Iteration 451, Batch: 40, Loss: 0.036226630210876465\n",
      "Iteration 451, Batch: 41, Loss: 0.028812991455197334\n",
      "Iteration 451, Batch: 42, Loss: 0.059906043112277985\n",
      "Iteration 451, Batch: 43, Loss: 0.06476108729839325\n",
      "Iteration 451, Batch: 44, Loss: 0.03211422264575958\n",
      "Iteration 451, Batch: 45, Loss: 0.084840327501297\n",
      "Iteration 451, Batch: 46, Loss: 0.04281628876924515\n",
      "Iteration 451, Batch: 47, Loss: 0.06355234235525131\n",
      "Iteration 451, Batch: 48, Loss: 0.03684144467115402\n",
      "Iteration 451, Batch: 49, Loss: 0.02634737826883793\n",
      "Number of layers: 10\n",
      "Iteration 452, Batch: 0, Loss: 0.05112825334072113\n",
      "Iteration 452, Batch: 1, Loss: 0.037916138768196106\n",
      "Iteration 452, Batch: 2, Loss: 0.02409958839416504\n",
      "Iteration 452, Batch: 3, Loss: 0.03339691087603569\n",
      "Iteration 452, Batch: 4, Loss: 0.023201482370495796\n",
      "Iteration 452, Batch: 5, Loss: 0.033840350806713104\n",
      "Iteration 452, Batch: 6, Loss: 0.03534448891878128\n",
      "Iteration 452, Batch: 7, Loss: 0.07993412017822266\n",
      "Iteration 452, Batch: 8, Loss: 0.05849210545420647\n",
      "Iteration 452, Batch: 9, Loss: 0.01600288413465023\n",
      "Iteration 452, Batch: 10, Loss: 0.03456760197877884\n",
      "Iteration 452, Batch: 11, Loss: 0.045408204197883606\n",
      "Iteration 452, Batch: 12, Loss: 0.04694606363773346\n",
      "Iteration 452, Batch: 13, Loss: 0.036601126194000244\n",
      "Iteration 452, Batch: 14, Loss: 0.05604797229170799\n",
      "Iteration 452, Batch: 15, Loss: 0.02568928338587284\n",
      "Iteration 452, Batch: 16, Loss: 0.056328438222408295\n",
      "Iteration 452, Batch: 17, Loss: 0.04276365786790848\n",
      "Iteration 452, Batch: 18, Loss: 0.05034680292010307\n",
      "Iteration 452, Batch: 19, Loss: 0.04827481135725975\n",
      "Iteration 452, Batch: 20, Loss: 0.03244733810424805\n",
      "Iteration 452, Batch: 21, Loss: 0.03914912790060043\n",
      "Iteration 452, Batch: 22, Loss: 0.034547869116067886\n",
      "Iteration 452, Batch: 23, Loss: 0.03733228147029877\n",
      "Iteration 452, Batch: 24, Loss: 0.04362781345844269\n",
      "Iteration 452, Batch: 25, Loss: 0.055938951671123505\n",
      "Iteration 452, Batch: 26, Loss: 0.06538710743188858\n",
      "Iteration 452, Batch: 27, Loss: 0.07598431408405304\n",
      "Iteration 452, Batch: 28, Loss: 0.061226069927215576\n",
      "Iteration 452, Batch: 29, Loss: 0.04270889237523079\n",
      "Iteration 452, Batch: 30, Loss: 0.04775773733854294\n",
      "Iteration 452, Batch: 31, Loss: 0.04546958953142166\n",
      "Iteration 452, Batch: 32, Loss: 0.03937934339046478\n",
      "Iteration 452, Batch: 33, Loss: 0.06422839313745499\n",
      "Iteration 452, Batch: 34, Loss: 0.05960142984986305\n",
      "Iteration 452, Batch: 35, Loss: 0.04964139312505722\n",
      "Iteration 452, Batch: 36, Loss: 0.04959098622202873\n",
      "Iteration 452, Batch: 37, Loss: 0.07983856648206711\n",
      "Iteration 452, Batch: 38, Loss: 0.04652590677142143\n",
      "Iteration 452, Batch: 39, Loss: 0.03123522736132145\n",
      "Iteration 452, Batch: 40, Loss: 0.04052676260471344\n",
      "Iteration 452, Batch: 41, Loss: 0.057808779180049896\n",
      "Iteration 452, Batch: 42, Loss: 0.044837139546871185\n",
      "Iteration 452, Batch: 43, Loss: 0.05834842473268509\n",
      "Iteration 452, Batch: 44, Loss: 0.02846648544073105\n",
      "Iteration 452, Batch: 45, Loss: 0.0462878979742527\n",
      "Iteration 452, Batch: 46, Loss: 0.047382477670907974\n",
      "Iteration 452, Batch: 47, Loss: 0.0598442368209362\n",
      "Iteration 452, Batch: 48, Loss: 0.05452959984540939\n",
      "Iteration 452, Batch: 49, Loss: 0.05293181166052818\n",
      "Number of layers: 10\n",
      "Iteration 453, Batch: 0, Loss: 0.07306408137083054\n",
      "Iteration 453, Batch: 1, Loss: 0.0501725859940052\n",
      "Iteration 453, Batch: 2, Loss: 0.03444569930434227\n",
      "Iteration 453, Batch: 3, Loss: 0.02213357575237751\n",
      "Iteration 453, Batch: 4, Loss: 0.04547155275940895\n",
      "Iteration 453, Batch: 5, Loss: 0.02596394531428814\n",
      "Iteration 453, Batch: 6, Loss: 0.036230240017175674\n",
      "Iteration 453, Batch: 7, Loss: 0.040713369846343994\n",
      "Iteration 453, Batch: 8, Loss: 0.0433073528110981\n",
      "Iteration 453, Batch: 9, Loss: 0.05177488178014755\n",
      "Iteration 453, Batch: 10, Loss: 0.0644109770655632\n",
      "Iteration 453, Batch: 11, Loss: 0.05627285689115524\n",
      "Iteration 453, Batch: 12, Loss: 0.04053826257586479\n",
      "Iteration 453, Batch: 13, Loss: 0.049436528235673904\n",
      "Iteration 453, Batch: 14, Loss: 0.04596441984176636\n",
      "Iteration 453, Batch: 15, Loss: 0.02375330962240696\n",
      "Iteration 453, Batch: 16, Loss: 0.0375780425965786\n",
      "Iteration 453, Batch: 17, Loss: 0.03592482581734657\n",
      "Iteration 453, Batch: 18, Loss: 0.04414726793766022\n",
      "Iteration 453, Batch: 19, Loss: 0.04519377648830414\n",
      "Iteration 453, Batch: 20, Loss: 0.043008532375097275\n",
      "Iteration 453, Batch: 21, Loss: 0.042838528752326965\n",
      "Iteration 453, Batch: 22, Loss: 0.020278293639421463\n",
      "Iteration 453, Batch: 23, Loss: 0.04121113568544388\n",
      "Iteration 453, Batch: 24, Loss: 0.04613789916038513\n",
      "Iteration 453, Batch: 25, Loss: 0.028708308935165405\n",
      "Iteration 453, Batch: 26, Loss: 0.04266245663166046\n",
      "Iteration 453, Batch: 27, Loss: 0.054197486490011215\n",
      "Iteration 453, Batch: 28, Loss: 0.03495362773537636\n",
      "Iteration 453, Batch: 29, Loss: 0.03908625617623329\n",
      "Iteration 453, Batch: 30, Loss: 0.04444991797208786\n",
      "Iteration 453, Batch: 31, Loss: 0.03951730951666832\n",
      "Iteration 453, Batch: 32, Loss: 0.05220039188861847\n",
      "Iteration 453, Batch: 33, Loss: 0.0558885857462883\n",
      "Iteration 453, Batch: 34, Loss: 0.050658199936151505\n",
      "Iteration 453, Batch: 35, Loss: 0.024195145815610886\n",
      "Iteration 453, Batch: 36, Loss: 0.05060158297419548\n",
      "Iteration 453, Batch: 37, Loss: 0.061342641711235046\n",
      "Iteration 453, Batch: 38, Loss: 0.021917881444096565\n",
      "Iteration 453, Batch: 39, Loss: 0.035040490329265594\n",
      "Iteration 453, Batch: 40, Loss: 0.03499603644013405\n",
      "Iteration 453, Batch: 41, Loss: 0.0356447696685791\n",
      "Iteration 453, Batch: 42, Loss: 0.0514773353934288\n",
      "Iteration 453, Batch: 43, Loss: 0.03250836208462715\n",
      "Iteration 453, Batch: 44, Loss: 0.05164725333452225\n",
      "Iteration 453, Batch: 45, Loss: 0.057943008840084076\n",
      "Iteration 453, Batch: 46, Loss: 0.06535376608371735\n",
      "Iteration 453, Batch: 47, Loss: 0.03454415872693062\n",
      "Iteration 453, Batch: 48, Loss: 0.04871805012226105\n",
      "Iteration 453, Batch: 49, Loss: 0.03476177155971527\n",
      "Number of layers: 10\n",
      "Iteration 454, Batch: 0, Loss: 0.05787532776594162\n",
      "Iteration 454, Batch: 1, Loss: 0.04297087714076042\n",
      "Iteration 454, Batch: 2, Loss: 0.038638707250356674\n",
      "Iteration 454, Batch: 3, Loss: 0.05518736690282822\n",
      "Iteration 454, Batch: 4, Loss: 0.038919709622859955\n",
      "Iteration 454, Batch: 5, Loss: 0.02151174284517765\n",
      "Iteration 454, Batch: 6, Loss: 0.02893959917128086\n",
      "Iteration 454, Batch: 7, Loss: 0.06654258817434311\n",
      "Iteration 454, Batch: 8, Loss: 0.0417708083987236\n",
      "Iteration 454, Batch: 9, Loss: 0.025367382913827896\n",
      "Iteration 454, Batch: 10, Loss: 0.05974999815225601\n",
      "Iteration 454, Batch: 11, Loss: 0.039004094898700714\n",
      "Iteration 454, Batch: 12, Loss: 0.055589206516742706\n",
      "Iteration 454, Batch: 13, Loss: 0.02678620256483555\n",
      "Iteration 454, Batch: 14, Loss: 0.06078195571899414\n",
      "Iteration 454, Batch: 15, Loss: 0.05060696601867676\n",
      "Iteration 454, Batch: 16, Loss: 0.0440070815384388\n",
      "Iteration 454, Batch: 17, Loss: 0.04499643296003342\n",
      "Iteration 454, Batch: 18, Loss: 0.045342616736888885\n",
      "Iteration 454, Batch: 19, Loss: 0.03244340047240257\n",
      "Iteration 454, Batch: 20, Loss: 0.06229550391435623\n",
      "Iteration 454, Batch: 21, Loss: 0.025207554921507835\n",
      "Iteration 454, Batch: 22, Loss: 0.051759984344244\n",
      "Iteration 454, Batch: 23, Loss: 0.04622877016663551\n",
      "Iteration 454, Batch: 24, Loss: 0.053239092230796814\n",
      "Iteration 454, Batch: 25, Loss: 0.05101267620921135\n",
      "Iteration 454, Batch: 26, Loss: 0.02219238318502903\n",
      "Iteration 454, Batch: 27, Loss: 0.027886614203453064\n",
      "Iteration 454, Batch: 28, Loss: 0.06206144019961357\n",
      "Iteration 454, Batch: 29, Loss: 0.049518607556819916\n",
      "Iteration 454, Batch: 30, Loss: 0.034378595650196075\n",
      "Iteration 454, Batch: 31, Loss: 0.019498873502016068\n",
      "Iteration 454, Batch: 32, Loss: 0.0408470518887043\n",
      "Iteration 454, Batch: 33, Loss: 0.03778655081987381\n",
      "Iteration 454, Batch: 34, Loss: 0.06212956830859184\n",
      "Iteration 454, Batch: 35, Loss: 0.030018562451004982\n",
      "Iteration 454, Batch: 36, Loss: 0.04828933626413345\n",
      "Iteration 454, Batch: 37, Loss: 0.0304657444357872\n",
      "Iteration 454, Batch: 38, Loss: 0.0585116371512413\n",
      "Iteration 454, Batch: 39, Loss: 0.054870638996362686\n",
      "Iteration 454, Batch: 40, Loss: 0.05243572220206261\n",
      "Iteration 454, Batch: 41, Loss: 0.04696425423026085\n",
      "Iteration 454, Batch: 42, Loss: 0.05581465736031532\n",
      "Iteration 454, Batch: 43, Loss: 0.05556483194231987\n",
      "Iteration 454, Batch: 44, Loss: 0.0207622479647398\n",
      "Iteration 454, Batch: 45, Loss: 0.03551671281456947\n",
      "Iteration 454, Batch: 46, Loss: 0.05211811512708664\n",
      "Iteration 454, Batch: 47, Loss: 0.047185301780700684\n",
      "Iteration 454, Batch: 48, Loss: 0.060883957892656326\n",
      "Iteration 454, Batch: 49, Loss: 0.03062203899025917\n",
      "Number of layers: 10\n",
      "Iteration 455, Batch: 0, Loss: 0.05418708547949791\n",
      "Iteration 455, Batch: 1, Loss: 0.04788794741034508\n",
      "Iteration 455, Batch: 2, Loss: 0.05207154154777527\n",
      "Iteration 455, Batch: 3, Loss: 0.0533457025885582\n",
      "Iteration 455, Batch: 4, Loss: 0.06075676903128624\n",
      "Iteration 455, Batch: 5, Loss: 0.05941294506192207\n",
      "Iteration 455, Batch: 6, Loss: 0.03402193635702133\n",
      "Iteration 455, Batch: 7, Loss: 0.06148854270577431\n",
      "Iteration 455, Batch: 8, Loss: 0.06699120998382568\n",
      "Iteration 455, Batch: 9, Loss: 0.06414663046598434\n",
      "Iteration 455, Batch: 10, Loss: 0.05043245106935501\n",
      "Iteration 455, Batch: 11, Loss: 0.07078875601291656\n",
      "Iteration 455, Batch: 12, Loss: 0.050284042954444885\n",
      "Iteration 455, Batch: 13, Loss: 0.070309579372406\n",
      "Iteration 455, Batch: 14, Loss: 0.06172063574194908\n",
      "Iteration 455, Batch: 15, Loss: 0.060374390333890915\n",
      "Iteration 455, Batch: 16, Loss: 0.07223878800868988\n",
      "Iteration 455, Batch: 17, Loss: 0.10896983742713928\n",
      "Iteration 455, Batch: 18, Loss: 0.10821182280778885\n",
      "Iteration 455, Batch: 19, Loss: 0.10098195821046829\n",
      "Iteration 455, Batch: 20, Loss: 0.11940384656190872\n",
      "Iteration 455, Batch: 21, Loss: 0.0806930735707283\n",
      "Iteration 455, Batch: 22, Loss: 0.0901852622628212\n",
      "Iteration 455, Batch: 23, Loss: 0.07405095547437668\n",
      "Iteration 455, Batch: 24, Loss: 0.09049282968044281\n",
      "Iteration 455, Batch: 25, Loss: 0.12336033582687378\n",
      "Iteration 455, Batch: 26, Loss: 0.08034335821866989\n",
      "Iteration 455, Batch: 27, Loss: 0.07931182533502579\n",
      "Iteration 455, Batch: 28, Loss: 0.08718524873256683\n",
      "Iteration 455, Batch: 29, Loss: 0.11202000081539154\n",
      "Iteration 455, Batch: 30, Loss: 0.10564195364713669\n",
      "Iteration 455, Batch: 31, Loss: 0.1254066377878189\n",
      "Iteration 455, Batch: 32, Loss: 0.1152365580201149\n",
      "Iteration 455, Batch: 33, Loss: 0.12616920471191406\n",
      "Iteration 455, Batch: 34, Loss: 0.1446351706981659\n",
      "Iteration 455, Batch: 35, Loss: 0.13169467449188232\n",
      "Iteration 455, Batch: 36, Loss: 0.10052820295095444\n",
      "Iteration 455, Batch: 37, Loss: 0.11610610783100128\n",
      "Iteration 455, Batch: 38, Loss: 0.11779353767633438\n",
      "Iteration 455, Batch: 39, Loss: 0.11582975089550018\n",
      "Iteration 455, Batch: 40, Loss: 0.18973632156848907\n",
      "Iteration 455, Batch: 41, Loss: 0.15365362167358398\n",
      "Iteration 455, Batch: 42, Loss: 0.08710906654596329\n",
      "Iteration 455, Batch: 43, Loss: 0.10183770954608917\n",
      "Iteration 455, Batch: 44, Loss: 0.1096796840429306\n",
      "Iteration 455, Batch: 45, Loss: 0.07442864030599594\n",
      "Iteration 455, Batch: 46, Loss: 0.0779310092329979\n",
      "Iteration 455, Batch: 47, Loss: 0.09755071252584457\n",
      "Iteration 455, Batch: 48, Loss: 0.09366331249475479\n",
      "Iteration 455, Batch: 49, Loss: 0.05909793823957443\n",
      "Number of layers: 10\n",
      "Iteration 456, Batch: 0, Loss: 0.09623004496097565\n",
      "Iteration 456, Batch: 1, Loss: 0.08951958268880844\n",
      "Iteration 456, Batch: 2, Loss: 0.053588371723890305\n",
      "Iteration 456, Batch: 3, Loss: 0.07891402393579483\n",
      "Iteration 456, Batch: 4, Loss: 0.0872877836227417\n",
      "Iteration 456, Batch: 5, Loss: 0.07720328122377396\n",
      "Iteration 456, Batch: 6, Loss: 0.0631340891122818\n",
      "Iteration 456, Batch: 7, Loss: 0.06636171787977219\n",
      "Iteration 456, Batch: 8, Loss: 0.05890626832842827\n",
      "Iteration 456, Batch: 9, Loss: 0.08302746713161469\n",
      "Iteration 456, Batch: 10, Loss: 0.08037863671779633\n",
      "Iteration 456, Batch: 11, Loss: 0.07453268766403198\n",
      "Iteration 456, Batch: 12, Loss: 0.08892752230167389\n",
      "Iteration 456, Batch: 13, Loss: 0.0884498730301857\n",
      "Iteration 456, Batch: 14, Loss: 0.08399725705385208\n",
      "Iteration 456, Batch: 15, Loss: 0.06205273047089577\n",
      "Iteration 456, Batch: 16, Loss: 0.10922719538211823\n",
      "Iteration 456, Batch: 17, Loss: 0.09311137348413467\n",
      "Iteration 456, Batch: 18, Loss: 0.15102100372314453\n",
      "Iteration 456, Batch: 19, Loss: 0.19403976202011108\n",
      "Iteration 456, Batch: 20, Loss: 0.18037104606628418\n",
      "Iteration 456, Batch: 21, Loss: 0.23089729249477386\n",
      "Iteration 456, Batch: 22, Loss: 0.1517002284526825\n",
      "Iteration 456, Batch: 23, Loss: 0.10349716991186142\n",
      "Iteration 456, Batch: 24, Loss: 0.12428764253854752\n",
      "Iteration 456, Batch: 25, Loss: 0.12055529654026031\n",
      "Iteration 456, Batch: 26, Loss: 0.10120219737291336\n",
      "Iteration 456, Batch: 27, Loss: 0.09914828836917877\n",
      "Iteration 456, Batch: 28, Loss: 0.11120662093162537\n",
      "Iteration 456, Batch: 29, Loss: 0.1274898648262024\n",
      "Iteration 456, Batch: 30, Loss: 0.11864614486694336\n",
      "Iteration 456, Batch: 31, Loss: 0.12108451873064041\n",
      "Iteration 456, Batch: 32, Loss: 0.0987071767449379\n",
      "Iteration 456, Batch: 33, Loss: 0.10785751044750214\n",
      "Iteration 456, Batch: 34, Loss: 0.128648579120636\n",
      "Iteration 456, Batch: 35, Loss: 0.12270358949899673\n",
      "Iteration 456, Batch: 36, Loss: 0.07805849611759186\n",
      "Iteration 456, Batch: 37, Loss: 0.11114484816789627\n",
      "Iteration 456, Batch: 38, Loss: 0.10836642980575562\n",
      "Iteration 456, Batch: 39, Loss: 0.10044761747121811\n",
      "Iteration 456, Batch: 40, Loss: 0.10489097237586975\n",
      "Iteration 456, Batch: 41, Loss: 0.06379220634698868\n",
      "Iteration 456, Batch: 42, Loss: 0.06475918740034103\n",
      "Iteration 456, Batch: 43, Loss: 0.09675785154104233\n",
      "Iteration 456, Batch: 44, Loss: 0.07703590393066406\n",
      "Iteration 456, Batch: 45, Loss: 0.08046962320804596\n",
      "Iteration 456, Batch: 46, Loss: 0.08597054332494736\n",
      "Iteration 456, Batch: 47, Loss: 0.1109938770532608\n",
      "Iteration 456, Batch: 48, Loss: 0.11629143357276917\n",
      "Iteration 456, Batch: 49, Loss: 0.11516450345516205\n",
      "Number of layers: 10\n",
      "Iteration 457, Batch: 0, Loss: 0.12818966805934906\n",
      "Iteration 457, Batch: 1, Loss: 0.14605307579040527\n",
      "Iteration 457, Batch: 2, Loss: 0.12118405848741531\n",
      "Iteration 457, Batch: 3, Loss: 0.12712715566158295\n",
      "Iteration 457, Batch: 4, Loss: 0.13663047552108765\n",
      "Iteration 457, Batch: 5, Loss: 0.11637048423290253\n",
      "Iteration 457, Batch: 6, Loss: 0.09368548542261124\n",
      "Iteration 457, Batch: 7, Loss: 0.1275925636291504\n",
      "Iteration 457, Batch: 8, Loss: 0.1071610227227211\n",
      "Iteration 457, Batch: 9, Loss: 0.09250164031982422\n",
      "Iteration 457, Batch: 10, Loss: 0.11295527964830399\n",
      "Iteration 457, Batch: 11, Loss: 0.11491882055997849\n",
      "Iteration 457, Batch: 12, Loss: 0.15438668429851532\n",
      "Iteration 457, Batch: 13, Loss: 0.13606609404087067\n",
      "Iteration 457, Batch: 14, Loss: 0.14988796412944794\n",
      "Iteration 457, Batch: 15, Loss: 0.10440398007631302\n",
      "Iteration 457, Batch: 16, Loss: 0.1173340305685997\n",
      "Iteration 457, Batch: 17, Loss: 0.10768820345401764\n",
      "Iteration 457, Batch: 18, Loss: 0.0978749468922615\n",
      "Iteration 457, Batch: 19, Loss: 0.10100378096103668\n",
      "Iteration 457, Batch: 20, Loss: 0.0819203332066536\n",
      "Iteration 457, Batch: 21, Loss: 0.10643410682678223\n",
      "Iteration 457, Batch: 22, Loss: 0.08486787974834442\n",
      "Iteration 457, Batch: 23, Loss: 0.1293380856513977\n",
      "Iteration 457, Batch: 24, Loss: 0.10663148760795593\n",
      "Iteration 457, Batch: 25, Loss: 0.12497510761022568\n",
      "Iteration 457, Batch: 26, Loss: 0.09862746298313141\n",
      "Iteration 457, Batch: 27, Loss: 0.10955613851547241\n",
      "Iteration 457, Batch: 28, Loss: 0.1212872862815857\n",
      "Iteration 457, Batch: 29, Loss: 0.11009734123945236\n",
      "Iteration 457, Batch: 30, Loss: 0.12721151113510132\n",
      "Iteration 457, Batch: 31, Loss: 0.1267266571521759\n",
      "Iteration 457, Batch: 32, Loss: 0.12093827873468399\n",
      "Iteration 457, Batch: 33, Loss: 0.14189453423023224\n",
      "Iteration 457, Batch: 34, Loss: 0.14052489399909973\n",
      "Iteration 457, Batch: 35, Loss: 0.11872483044862747\n",
      "Iteration 457, Batch: 36, Loss: 0.10761487483978271\n",
      "Iteration 457, Batch: 37, Loss: 0.14047226309776306\n",
      "Iteration 457, Batch: 38, Loss: 0.11340989172458649\n",
      "Iteration 457, Batch: 39, Loss: 0.11086499691009521\n",
      "Iteration 457, Batch: 40, Loss: 0.08606252819299698\n",
      "Iteration 457, Batch: 41, Loss: 0.11394798010587692\n",
      "Iteration 457, Batch: 42, Loss: 0.12413152307271957\n",
      "Iteration 457, Batch: 43, Loss: 0.11363925784826279\n",
      "Iteration 457, Batch: 44, Loss: 0.09063176065683365\n",
      "Iteration 457, Batch: 45, Loss: 0.12308812141418457\n",
      "Iteration 457, Batch: 46, Loss: 0.12986721098423004\n",
      "Iteration 457, Batch: 47, Loss: 0.07750751078128815\n",
      "Iteration 457, Batch: 48, Loss: 0.0903339684009552\n",
      "Iteration 457, Batch: 49, Loss: 0.07541786879301071\n",
      "Number of layers: 10\n",
      "Iteration 458, Batch: 0, Loss: 0.08302430808544159\n",
      "Iteration 458, Batch: 1, Loss: 0.08688436448574066\n",
      "Iteration 458, Batch: 2, Loss: 0.12209364026784897\n",
      "Iteration 458, Batch: 3, Loss: 0.08647007495164871\n",
      "Iteration 458, Batch: 4, Loss: 0.07686721533536911\n",
      "Iteration 458, Batch: 5, Loss: 0.06894742697477341\n",
      "Iteration 458, Batch: 6, Loss: 0.07471415400505066\n",
      "Iteration 458, Batch: 7, Loss: 0.06999539583921432\n",
      "Iteration 458, Batch: 8, Loss: 0.10288909822702408\n",
      "Iteration 458, Batch: 9, Loss: 0.06525580585002899\n",
      "Iteration 458, Batch: 10, Loss: 0.07696960121393204\n",
      "Iteration 458, Batch: 11, Loss: 0.09237578511238098\n",
      "Iteration 458, Batch: 12, Loss: 0.06801732629537582\n",
      "Iteration 458, Batch: 13, Loss: 0.07291483134031296\n",
      "Iteration 458, Batch: 14, Loss: 0.06550119072198868\n",
      "Iteration 458, Batch: 15, Loss: 0.09197443723678589\n",
      "Iteration 458, Batch: 16, Loss: 0.08358815312385559\n",
      "Iteration 458, Batch: 17, Loss: 0.06962661445140839\n",
      "Iteration 458, Batch: 18, Loss: 0.12433914840221405\n",
      "Iteration 458, Batch: 19, Loss: 0.08510488271713257\n",
      "Iteration 458, Batch: 20, Loss: 0.07804470509290695\n",
      "Iteration 458, Batch: 21, Loss: 0.10716728121042252\n",
      "Iteration 458, Batch: 22, Loss: 0.1283683329820633\n",
      "Iteration 458, Batch: 23, Loss: 0.10074266046285629\n",
      "Iteration 458, Batch: 24, Loss: 0.09338010847568512\n",
      "Iteration 458, Batch: 25, Loss: 0.09222128987312317\n",
      "Iteration 458, Batch: 26, Loss: 0.08397161215543747\n",
      "Iteration 458, Batch: 27, Loss: 0.14500223100185394\n",
      "Iteration 458, Batch: 28, Loss: 0.1313387006521225\n",
      "Iteration 458, Batch: 29, Loss: 0.10389924049377441\n",
      "Iteration 458, Batch: 30, Loss: 0.10950945317745209\n",
      "Iteration 458, Batch: 31, Loss: 0.08231501281261444\n",
      "Iteration 458, Batch: 32, Loss: 0.08945446461439133\n",
      "Iteration 458, Batch: 33, Loss: 0.10266074538230896\n",
      "Iteration 458, Batch: 34, Loss: 0.1347290277481079\n",
      "Iteration 458, Batch: 35, Loss: 0.07688767462968826\n",
      "Iteration 458, Batch: 36, Loss: 0.0706663578748703\n",
      "Iteration 458, Batch: 37, Loss: 0.11754781752824783\n",
      "Iteration 458, Batch: 38, Loss: 0.06691181659698486\n",
      "Iteration 458, Batch: 39, Loss: 0.08739922940731049\n",
      "Iteration 458, Batch: 40, Loss: 0.09899835288524628\n",
      "Iteration 458, Batch: 41, Loss: 0.09806607663631439\n",
      "Iteration 458, Batch: 42, Loss: 0.09882304817438126\n",
      "Iteration 458, Batch: 43, Loss: 0.1291343718767166\n",
      "Iteration 458, Batch: 44, Loss: 0.12911637127399445\n",
      "Iteration 458, Batch: 45, Loss: 0.10627225041389465\n",
      "Iteration 458, Batch: 46, Loss: 0.08653910458087921\n",
      "Iteration 458, Batch: 47, Loss: 0.10163214802742004\n",
      "Iteration 458, Batch: 48, Loss: 0.09347518533468246\n",
      "Iteration 458, Batch: 49, Loss: 0.1070033460855484\n",
      "Number of layers: 10\n",
      "Iteration 459, Batch: 0, Loss: 0.07855057716369629\n",
      "Iteration 459, Batch: 1, Loss: 0.07388746738433838\n",
      "Iteration 459, Batch: 2, Loss: 0.0965275764465332\n",
      "Iteration 459, Batch: 3, Loss: 0.11193488538265228\n",
      "Iteration 459, Batch: 4, Loss: 0.08963807672262192\n",
      "Iteration 459, Batch: 5, Loss: 0.08642899990081787\n",
      "Iteration 459, Batch: 6, Loss: 0.1215561106801033\n",
      "Iteration 459, Batch: 7, Loss: 0.13428445160388947\n",
      "Iteration 459, Batch: 8, Loss: 0.10999370366334915\n",
      "Iteration 459, Batch: 9, Loss: 0.10011153668165207\n",
      "Iteration 459, Batch: 10, Loss: 0.07239682227373123\n",
      "Iteration 459, Batch: 11, Loss: 0.0927821546792984\n",
      "Iteration 459, Batch: 12, Loss: 0.09618150442838669\n",
      "Iteration 459, Batch: 13, Loss: 0.10028789192438126\n",
      "Iteration 459, Batch: 14, Loss: 0.07508764415979385\n",
      "Iteration 459, Batch: 15, Loss: 0.11721135675907135\n",
      "Iteration 459, Batch: 16, Loss: 0.07418278604745865\n",
      "Iteration 459, Batch: 17, Loss: 0.05328948050737381\n",
      "Iteration 459, Batch: 18, Loss: 0.08676151931285858\n",
      "Iteration 459, Batch: 19, Loss: 0.1031344085931778\n",
      "Iteration 459, Batch: 20, Loss: 0.09405136853456497\n",
      "Iteration 459, Batch: 21, Loss: 0.13001102209091187\n",
      "Iteration 459, Batch: 22, Loss: 0.11229947209358215\n",
      "Iteration 459, Batch: 23, Loss: 0.12197376042604446\n",
      "Iteration 459, Batch: 24, Loss: 0.05663162097334862\n",
      "Iteration 459, Batch: 25, Loss: 0.09679637849330902\n",
      "Iteration 459, Batch: 26, Loss: 0.06955944746732712\n",
      "Iteration 459, Batch: 27, Loss: 0.10292888432741165\n",
      "Iteration 459, Batch: 28, Loss: 0.10863317549228668\n",
      "Iteration 459, Batch: 29, Loss: 0.07244257628917694\n",
      "Iteration 459, Batch: 30, Loss: 0.08742812275886536\n",
      "Iteration 459, Batch: 31, Loss: 0.09042491763830185\n",
      "Iteration 459, Batch: 32, Loss: 0.06993342936038971\n",
      "Iteration 459, Batch: 33, Loss: 0.09053022414445877\n",
      "Iteration 459, Batch: 34, Loss: 0.11874601989984512\n",
      "Iteration 459, Batch: 35, Loss: 0.07320333272218704\n",
      "Iteration 459, Batch: 36, Loss: 0.08853864669799805\n",
      "Iteration 459, Batch: 37, Loss: 0.1111057698726654\n",
      "Iteration 459, Batch: 38, Loss: 0.06923920661211014\n",
      "Iteration 459, Batch: 39, Loss: 0.07217788696289062\n",
      "Iteration 459, Batch: 40, Loss: 0.08060357719659805\n",
      "Iteration 459, Batch: 41, Loss: 0.09229201823472977\n",
      "Iteration 459, Batch: 42, Loss: 0.07865095138549805\n",
      "Iteration 459, Batch: 43, Loss: 0.0909438207745552\n",
      "Iteration 459, Batch: 44, Loss: 0.0541955828666687\n",
      "Iteration 459, Batch: 45, Loss: 0.08653372526168823\n",
      "Iteration 459, Batch: 46, Loss: 0.0649031400680542\n",
      "Iteration 459, Batch: 47, Loss: 0.08099882304668427\n",
      "Iteration 459, Batch: 48, Loss: 0.09260327368974686\n",
      "Iteration 459, Batch: 49, Loss: 0.07816676050424576\n",
      "Number of layers: 10\n",
      "Iteration 460, Batch: 0, Loss: 0.055240198969841\n",
      "Iteration 460, Batch: 1, Loss: 0.06386109441518784\n",
      "Iteration 460, Batch: 2, Loss: 0.07305178046226501\n",
      "Iteration 460, Batch: 3, Loss: 0.05282595381140709\n",
      "Iteration 460, Batch: 4, Loss: 0.05234590545296669\n",
      "Iteration 460, Batch: 5, Loss: 0.08010214567184448\n",
      "Iteration 460, Batch: 6, Loss: 0.0666913166642189\n",
      "Iteration 460, Batch: 7, Loss: 0.07168684899806976\n",
      "Iteration 460, Batch: 8, Loss: 0.0791618749499321\n",
      "Iteration 460, Batch: 9, Loss: 0.08429961651563644\n",
      "Iteration 460, Batch: 10, Loss: 0.09654935449361801\n",
      "Iteration 460, Batch: 11, Loss: 0.04658589884638786\n",
      "Iteration 460, Batch: 12, Loss: 0.091953806579113\n",
      "Iteration 460, Batch: 13, Loss: 0.09728443622589111\n",
      "Iteration 460, Batch: 14, Loss: 0.07793380320072174\n",
      "Iteration 460, Batch: 15, Loss: 0.07016477733850479\n",
      "Iteration 460, Batch: 16, Loss: 0.07472042739391327\n",
      "Iteration 460, Batch: 17, Loss: 0.07951103150844574\n",
      "Iteration 460, Batch: 18, Loss: 0.08052103966474533\n",
      "Iteration 460, Batch: 19, Loss: 0.04702548310160637\n",
      "Iteration 460, Batch: 20, Loss: 0.07048764079809189\n",
      "Iteration 460, Batch: 21, Loss: 0.1246228739619255\n",
      "Iteration 460, Batch: 22, Loss: 0.12882474064826965\n",
      "Iteration 460, Batch: 23, Loss: 0.12386071681976318\n",
      "Iteration 460, Batch: 24, Loss: 0.13382165133953094\n",
      "Iteration 460, Batch: 25, Loss: 0.10687175393104553\n",
      "Iteration 460, Batch: 26, Loss: 0.07557359337806702\n",
      "Iteration 460, Batch: 27, Loss: 0.09860355406999588\n",
      "Iteration 460, Batch: 28, Loss: 0.0980822741985321\n",
      "Iteration 460, Batch: 29, Loss: 0.08993043005466461\n",
      "Iteration 460, Batch: 30, Loss: 0.0667622834444046\n",
      "Iteration 460, Batch: 31, Loss: 0.05509832873940468\n",
      "Iteration 460, Batch: 32, Loss: 0.06267011165618896\n",
      "Iteration 460, Batch: 33, Loss: 0.07769926637411118\n",
      "Iteration 460, Batch: 34, Loss: 0.08033090084791183\n",
      "Iteration 460, Batch: 35, Loss: 0.068412184715271\n",
      "Iteration 460, Batch: 36, Loss: 0.08484479039907455\n",
      "Iteration 460, Batch: 37, Loss: 0.059778936207294464\n",
      "Iteration 460, Batch: 38, Loss: 0.11427459120750427\n",
      "Iteration 460, Batch: 39, Loss: 0.0839529037475586\n",
      "Iteration 460, Batch: 40, Loss: 0.10691646486520767\n",
      "Iteration 460, Batch: 41, Loss: 0.10892000049352646\n",
      "Iteration 460, Batch: 42, Loss: 0.05941286310553551\n",
      "Iteration 460, Batch: 43, Loss: 0.1066998839378357\n",
      "Iteration 460, Batch: 44, Loss: 0.09113515913486481\n",
      "Iteration 460, Batch: 45, Loss: 0.08037427812814713\n",
      "Iteration 460, Batch: 46, Loss: 0.08385836333036423\n",
      "Iteration 460, Batch: 47, Loss: 0.10896456241607666\n",
      "Iteration 460, Batch: 48, Loss: 0.09461311250925064\n",
      "Iteration 460, Batch: 49, Loss: 0.07986193150281906\n",
      "Number of layers: 10\n",
      "Iteration 461, Batch: 0, Loss: 0.0790301039814949\n",
      "Iteration 461, Batch: 1, Loss: 0.10618436336517334\n",
      "Iteration 461, Batch: 2, Loss: 0.0736236721277237\n",
      "Iteration 461, Batch: 3, Loss: 0.07784458249807358\n",
      "Iteration 461, Batch: 4, Loss: 0.09579329937696457\n",
      "Iteration 461, Batch: 5, Loss: 0.05497324466705322\n",
      "Iteration 461, Batch: 6, Loss: 0.09814349561929703\n",
      "Iteration 461, Batch: 7, Loss: 0.11646421998739243\n",
      "Iteration 461, Batch: 8, Loss: 0.09068193286657333\n",
      "Iteration 461, Batch: 9, Loss: 0.10028659552335739\n",
      "Iteration 461, Batch: 10, Loss: 0.09986523538827896\n",
      "Iteration 461, Batch: 11, Loss: 0.06735091656446457\n",
      "Iteration 461, Batch: 12, Loss: 0.10546552389860153\n",
      "Iteration 461, Batch: 13, Loss: 0.08361697942018509\n",
      "Iteration 461, Batch: 14, Loss: 0.07554267346858978\n",
      "Iteration 461, Batch: 15, Loss: 0.07875753194093704\n",
      "Iteration 461, Batch: 16, Loss: 0.09783847630023956\n",
      "Iteration 461, Batch: 17, Loss: 0.12394702434539795\n",
      "Iteration 461, Batch: 18, Loss: 0.08620467036962509\n",
      "Iteration 461, Batch: 19, Loss: 0.07930803298950195\n",
      "Iteration 461, Batch: 20, Loss: 0.05155764892697334\n",
      "Iteration 461, Batch: 21, Loss: 0.06574937701225281\n",
      "Iteration 461, Batch: 22, Loss: 0.09840649366378784\n",
      "Iteration 461, Batch: 23, Loss: 0.057532284408807755\n",
      "Iteration 461, Batch: 24, Loss: 0.087711863219738\n",
      "Iteration 461, Batch: 25, Loss: 0.0698721781373024\n",
      "Iteration 461, Batch: 26, Loss: 0.06975176930427551\n",
      "Iteration 461, Batch: 27, Loss: 0.06393847614526749\n",
      "Iteration 461, Batch: 28, Loss: 0.05971861258149147\n",
      "Iteration 461, Batch: 29, Loss: 0.05518677458167076\n",
      "Iteration 461, Batch: 30, Loss: 0.07510994374752045\n",
      "Iteration 461, Batch: 31, Loss: 0.05709368735551834\n",
      "Iteration 461, Batch: 32, Loss: 0.07420779764652252\n",
      "Iteration 461, Batch: 33, Loss: 0.06602750718593597\n",
      "Iteration 461, Batch: 34, Loss: 0.07293470203876495\n",
      "Iteration 461, Batch: 35, Loss: 0.06410729140043259\n",
      "Iteration 461, Batch: 36, Loss: 0.08091648668050766\n",
      "Iteration 461, Batch: 37, Loss: 0.09426607936620712\n",
      "Iteration 461, Batch: 38, Loss: 0.05166520178318024\n",
      "Iteration 461, Batch: 39, Loss: 0.07768619805574417\n",
      "Iteration 461, Batch: 40, Loss: 0.06078990921378136\n",
      "Iteration 461, Batch: 41, Loss: 0.041915006935596466\n",
      "Iteration 461, Batch: 42, Loss: 0.0751994401216507\n",
      "Iteration 461, Batch: 43, Loss: 0.06327836215496063\n",
      "Iteration 461, Batch: 44, Loss: 0.06984610855579376\n",
      "Iteration 461, Batch: 45, Loss: 0.09558238834142685\n",
      "Iteration 461, Batch: 46, Loss: 0.05955738574266434\n",
      "Iteration 461, Batch: 47, Loss: 0.06073492765426636\n",
      "Iteration 461, Batch: 48, Loss: 0.05853624641895294\n",
      "Iteration 461, Batch: 49, Loss: 0.06959447264671326\n",
      "Number of layers: 10\n",
      "Iteration 462, Batch: 0, Loss: 0.07240622490644455\n",
      "Iteration 462, Batch: 1, Loss: 0.09066192060709\n",
      "Iteration 462, Batch: 2, Loss: 0.08846055716276169\n",
      "Iteration 462, Batch: 3, Loss: 0.08010967820882797\n",
      "Iteration 462, Batch: 4, Loss: 0.11132180690765381\n",
      "Iteration 462, Batch: 5, Loss: 0.07318656891584396\n",
      "Iteration 462, Batch: 6, Loss: 0.06616605073213577\n",
      "Iteration 462, Batch: 7, Loss: 0.10105498880147934\n",
      "Iteration 462, Batch: 8, Loss: 0.08237352222204208\n",
      "Iteration 462, Batch: 9, Loss: 0.08205722272396088\n",
      "Iteration 462, Batch: 10, Loss: 0.09126954525709152\n",
      "Iteration 462, Batch: 11, Loss: 0.0871187373995781\n",
      "Iteration 462, Batch: 12, Loss: 0.09230874478816986\n",
      "Iteration 462, Batch: 13, Loss: 0.0693153664469719\n",
      "Iteration 462, Batch: 14, Loss: 0.05171719938516617\n",
      "Iteration 462, Batch: 15, Loss: 0.07860952615737915\n",
      "Iteration 462, Batch: 16, Loss: 0.08091654628515244\n",
      "Iteration 462, Batch: 17, Loss: 0.07191462069749832\n",
      "Iteration 462, Batch: 18, Loss: 0.06981723010540009\n",
      "Iteration 462, Batch: 19, Loss: 0.07391795516014099\n",
      "Iteration 462, Batch: 20, Loss: 0.07918054610490799\n",
      "Iteration 462, Batch: 21, Loss: 0.06796307116746902\n",
      "Iteration 462, Batch: 22, Loss: 0.0869801864027977\n",
      "Iteration 462, Batch: 23, Loss: 0.06147390231490135\n",
      "Iteration 462, Batch: 24, Loss: 0.08276940882205963\n",
      "Iteration 462, Batch: 25, Loss: 0.07376216351985931\n",
      "Iteration 462, Batch: 26, Loss: 0.06528709083795547\n",
      "Iteration 462, Batch: 27, Loss: 0.12056871503591537\n",
      "Iteration 462, Batch: 28, Loss: 0.11763161420822144\n",
      "Iteration 462, Batch: 29, Loss: 0.046977490186691284\n",
      "Iteration 462, Batch: 30, Loss: 0.04512231796979904\n",
      "Iteration 462, Batch: 31, Loss: 0.07388222962617874\n",
      "Iteration 462, Batch: 32, Loss: 0.07256191223859787\n",
      "Iteration 462, Batch: 33, Loss: 0.08959346264600754\n",
      "Iteration 462, Batch: 34, Loss: 0.07650215923786163\n",
      "Iteration 462, Batch: 35, Loss: 0.052838265895843506\n",
      "Iteration 462, Batch: 36, Loss: 0.06390794366598129\n",
      "Iteration 462, Batch: 37, Loss: 0.08062410354614258\n",
      "Iteration 462, Batch: 38, Loss: 0.0748857781291008\n",
      "Iteration 462, Batch: 39, Loss: 0.07643772661685944\n",
      "Iteration 462, Batch: 40, Loss: 0.08724023401737213\n",
      "Iteration 462, Batch: 41, Loss: 0.09701747447252274\n",
      "Iteration 462, Batch: 42, Loss: 0.07575412839651108\n",
      "Iteration 462, Batch: 43, Loss: 0.07989239692687988\n",
      "Iteration 462, Batch: 44, Loss: 0.10160575807094574\n",
      "Iteration 462, Batch: 45, Loss: 0.08363567292690277\n",
      "Iteration 462, Batch: 46, Loss: 0.07584790140390396\n",
      "Iteration 462, Batch: 47, Loss: 0.0585634708404541\n",
      "Iteration 462, Batch: 48, Loss: 0.07796468585729599\n",
      "Iteration 462, Batch: 49, Loss: 0.07507508993148804\n",
      "Number of layers: 10\n",
      "Iteration 463, Batch: 0, Loss: 0.07439049333333969\n",
      "Iteration 463, Batch: 1, Loss: 0.05777730792760849\n",
      "Iteration 463, Batch: 2, Loss: 0.08798946440219879\n",
      "Iteration 463, Batch: 3, Loss: 0.043629903346300125\n",
      "Iteration 463, Batch: 4, Loss: 0.062327317893505096\n",
      "Iteration 463, Batch: 5, Loss: 0.062137454748153687\n",
      "Iteration 463, Batch: 6, Loss: 0.07326914370059967\n",
      "Iteration 463, Batch: 7, Loss: 0.1065712422132492\n",
      "Iteration 463, Batch: 8, Loss: 0.0732494667172432\n",
      "Iteration 463, Batch: 9, Loss: 0.07462038099765778\n",
      "Iteration 463, Batch: 10, Loss: 0.08977431058883667\n",
      "Iteration 463, Batch: 11, Loss: 0.07309366017580032\n",
      "Iteration 463, Batch: 12, Loss: 0.09266767650842667\n",
      "Iteration 463, Batch: 13, Loss: 0.07352972775697708\n",
      "Iteration 463, Batch: 14, Loss: 0.09873370826244354\n",
      "Iteration 463, Batch: 15, Loss: 0.07720474153757095\n",
      "Iteration 463, Batch: 16, Loss: 0.06821884959936142\n",
      "Iteration 463, Batch: 17, Loss: 0.07781092822551727\n",
      "Iteration 463, Batch: 18, Loss: 0.07030034810304642\n",
      "Iteration 463, Batch: 19, Loss: 0.07447609305381775\n",
      "Iteration 463, Batch: 20, Loss: 0.06074673682451248\n",
      "Iteration 463, Batch: 21, Loss: 0.058857258409261703\n",
      "Iteration 463, Batch: 22, Loss: 0.07655762881040573\n",
      "Iteration 463, Batch: 23, Loss: 0.08808743208646774\n",
      "Iteration 463, Batch: 24, Loss: 0.08137280493974686\n",
      "Iteration 463, Batch: 25, Loss: 0.1061328649520874\n",
      "Iteration 463, Batch: 26, Loss: 0.08751600235700607\n",
      "Iteration 463, Batch: 27, Loss: 0.05721050500869751\n",
      "Iteration 463, Batch: 28, Loss: 0.08847478777170181\n",
      "Iteration 463, Batch: 29, Loss: 0.07154165953397751\n",
      "Iteration 463, Batch: 30, Loss: 0.08347724378108978\n",
      "Iteration 463, Batch: 31, Loss: 0.09040471911430359\n",
      "Iteration 463, Batch: 32, Loss: 0.06128242239356041\n",
      "Iteration 463, Batch: 33, Loss: 0.07066518813371658\n",
      "Iteration 463, Batch: 34, Loss: 0.06771458685398102\n",
      "Iteration 463, Batch: 35, Loss: 0.062347933650016785\n",
      "Iteration 463, Batch: 36, Loss: 0.04468783363699913\n",
      "Iteration 463, Batch: 37, Loss: 0.05339163541793823\n",
      "Iteration 463, Batch: 38, Loss: 0.046410903334617615\n",
      "Iteration 463, Batch: 39, Loss: 0.06259281188249588\n",
      "Iteration 463, Batch: 40, Loss: 0.062122736126184464\n",
      "Iteration 463, Batch: 41, Loss: 0.07273314148187637\n",
      "Iteration 463, Batch: 42, Loss: 0.07021713256835938\n",
      "Iteration 463, Batch: 43, Loss: 0.07248185575008392\n",
      "Iteration 463, Batch: 44, Loss: 0.06373053044080734\n",
      "Iteration 463, Batch: 45, Loss: 0.06607726216316223\n",
      "Iteration 463, Batch: 46, Loss: 0.04410184919834137\n",
      "Iteration 463, Batch: 47, Loss: 0.10368501394987106\n",
      "Iteration 463, Batch: 48, Loss: 0.09232326596975327\n",
      "Iteration 463, Batch: 49, Loss: 0.09969542920589447\n",
      "Number of layers: 10\n",
      "Iteration 464, Batch: 0, Loss: 0.06121893227100372\n",
      "Iteration 464, Batch: 1, Loss: 0.06747590750455856\n",
      "Iteration 464, Batch: 2, Loss: 0.06854813545942307\n",
      "Iteration 464, Batch: 3, Loss: 0.05030398815870285\n",
      "Iteration 464, Batch: 4, Loss: 0.06421493738889694\n",
      "Iteration 464, Batch: 5, Loss: 0.045482173562049866\n",
      "Iteration 464, Batch: 6, Loss: 0.056756600737571716\n",
      "Iteration 464, Batch: 7, Loss: 0.057345952838659286\n",
      "Iteration 464, Batch: 8, Loss: 0.07146468758583069\n",
      "Iteration 464, Batch: 9, Loss: 0.05075845122337341\n",
      "Iteration 464, Batch: 10, Loss: 0.07674220204353333\n",
      "Iteration 464, Batch: 11, Loss: 0.05517367273569107\n",
      "Iteration 464, Batch: 12, Loss: 0.07460779696702957\n",
      "Iteration 464, Batch: 13, Loss: 0.06679849326610565\n",
      "Iteration 464, Batch: 14, Loss: 0.08646528422832489\n",
      "Iteration 464, Batch: 15, Loss: 0.0738488957285881\n",
      "Iteration 464, Batch: 16, Loss: 0.08009787648916245\n",
      "Iteration 464, Batch: 17, Loss: 0.08526109158992767\n",
      "Iteration 464, Batch: 18, Loss: 0.06164299696683884\n",
      "Iteration 464, Batch: 19, Loss: 0.07681231200695038\n",
      "Iteration 464, Batch: 20, Loss: 0.08090493083000183\n",
      "Iteration 464, Batch: 21, Loss: 0.10360626131296158\n",
      "Iteration 464, Batch: 22, Loss: 0.04540983960032463\n",
      "Iteration 464, Batch: 23, Loss: 0.05767293646931648\n",
      "Iteration 464, Batch: 24, Loss: 0.06285464018583298\n",
      "Iteration 464, Batch: 25, Loss: 0.08550527691841125\n",
      "Iteration 464, Batch: 26, Loss: 0.05632892996072769\n",
      "Iteration 464, Batch: 27, Loss: 0.08283056318759918\n",
      "Iteration 464, Batch: 28, Loss: 0.06418583542108536\n",
      "Iteration 464, Batch: 29, Loss: 0.07918086647987366\n",
      "Iteration 464, Batch: 30, Loss: 0.053353700786828995\n",
      "Iteration 464, Batch: 31, Loss: 0.05222902074456215\n",
      "Iteration 464, Batch: 32, Loss: 0.0741397961974144\n",
      "Iteration 464, Batch: 33, Loss: 0.07927820831537247\n",
      "Iteration 464, Batch: 34, Loss: 0.0632341131567955\n",
      "Iteration 464, Batch: 35, Loss: 0.07911227643489838\n",
      "Iteration 464, Batch: 36, Loss: 0.062293604016304016\n",
      "Iteration 464, Batch: 37, Loss: 0.05590119585394859\n",
      "Iteration 464, Batch: 38, Loss: 0.05786331370472908\n",
      "Iteration 464, Batch: 39, Loss: 0.06937866657972336\n",
      "Iteration 464, Batch: 40, Loss: 0.0782749280333519\n",
      "Iteration 464, Batch: 41, Loss: 0.08544251322746277\n",
      "Iteration 464, Batch: 42, Loss: 0.07177386432886124\n",
      "Iteration 464, Batch: 43, Loss: 0.05131378024816513\n",
      "Iteration 464, Batch: 44, Loss: 0.09264016896486282\n",
      "Iteration 464, Batch: 45, Loss: 0.06161763519048691\n",
      "Iteration 464, Batch: 46, Loss: 0.05466986075043678\n",
      "Iteration 464, Batch: 47, Loss: 0.062485676258802414\n",
      "Iteration 464, Batch: 48, Loss: 0.06172255799174309\n",
      "Iteration 464, Batch: 49, Loss: 0.03677957132458687\n",
      "Number of layers: 10\n",
      "Iteration 465, Batch: 0, Loss: 0.044641464948654175\n",
      "Iteration 465, Batch: 1, Loss: 0.0560334287583828\n",
      "Iteration 465, Batch: 2, Loss: 0.05872862786054611\n",
      "Iteration 465, Batch: 3, Loss: 0.05336225777864456\n",
      "Iteration 465, Batch: 4, Loss: 0.06625517457723618\n",
      "Iteration 465, Batch: 5, Loss: 0.07630223035812378\n",
      "Iteration 465, Batch: 6, Loss: 0.0751284658908844\n",
      "Iteration 465, Batch: 7, Loss: 0.06473860889673233\n",
      "Iteration 465, Batch: 8, Loss: 0.04806463047862053\n",
      "Iteration 465, Batch: 9, Loss: 0.09705162048339844\n",
      "Iteration 465, Batch: 10, Loss: 0.06422412395477295\n",
      "Iteration 465, Batch: 11, Loss: 0.04929789900779724\n",
      "Iteration 465, Batch: 12, Loss: 0.07656797766685486\n",
      "Iteration 465, Batch: 13, Loss: 0.06150144338607788\n",
      "Iteration 465, Batch: 14, Loss: 0.06638364493846893\n",
      "Iteration 465, Batch: 15, Loss: 0.06373852491378784\n",
      "Iteration 465, Batch: 16, Loss: 0.039742015302181244\n",
      "Iteration 465, Batch: 17, Loss: 0.09284264594316483\n",
      "Iteration 465, Batch: 18, Loss: 0.06558646261692047\n",
      "Iteration 465, Batch: 19, Loss: 0.053865060210227966\n",
      "Iteration 465, Batch: 20, Loss: 0.06523288041353226\n",
      "Iteration 465, Batch: 21, Loss: 0.08343019336462021\n",
      "Iteration 465, Batch: 22, Loss: 0.07588127255439758\n",
      "Iteration 465, Batch: 23, Loss: 0.054643262177705765\n",
      "Iteration 465, Batch: 24, Loss: 0.07102514058351517\n",
      "Iteration 465, Batch: 25, Loss: 0.06613331288099289\n",
      "Iteration 465, Batch: 26, Loss: 0.06698663532733917\n",
      "Iteration 465, Batch: 27, Loss: 0.07557861506938934\n",
      "Iteration 465, Batch: 28, Loss: 0.10103438794612885\n",
      "Iteration 465, Batch: 29, Loss: 0.08661854267120361\n",
      "Iteration 465, Batch: 30, Loss: 0.10810773819684982\n",
      "Iteration 465, Batch: 31, Loss: 0.06317245215177536\n",
      "Iteration 465, Batch: 32, Loss: 0.08667142689228058\n",
      "Iteration 465, Batch: 33, Loss: 0.06371993571519852\n",
      "Iteration 465, Batch: 34, Loss: 0.06214435398578644\n",
      "Iteration 465, Batch: 35, Loss: 0.054631829261779785\n",
      "Iteration 465, Batch: 36, Loss: 0.0851188525557518\n",
      "Iteration 465, Batch: 37, Loss: 0.10117314755916595\n",
      "Iteration 465, Batch: 38, Loss: 0.06872084736824036\n",
      "Iteration 465, Batch: 39, Loss: 0.06272652745246887\n",
      "Iteration 465, Batch: 40, Loss: 0.06199393793940544\n",
      "Iteration 465, Batch: 41, Loss: 0.08119931817054749\n",
      "Iteration 465, Batch: 42, Loss: 0.08568026125431061\n",
      "Iteration 465, Batch: 43, Loss: 0.09316108375787735\n",
      "Iteration 465, Batch: 44, Loss: 0.05674907565116882\n",
      "Iteration 465, Batch: 45, Loss: 0.05135766789317131\n",
      "Iteration 465, Batch: 46, Loss: 0.07369213551282883\n",
      "Iteration 465, Batch: 47, Loss: 0.08066008239984512\n",
      "Iteration 465, Batch: 48, Loss: 0.07719539105892181\n",
      "Iteration 465, Batch: 49, Loss: 0.0343320369720459\n",
      "Number of layers: 10\n",
      "Iteration 466, Batch: 0, Loss: 0.07089240849018097\n",
      "Iteration 466, Batch: 1, Loss: 0.0665152296423912\n",
      "Iteration 466, Batch: 2, Loss: 0.09537816047668457\n",
      "Iteration 466, Batch: 3, Loss: 0.12401442974805832\n",
      "Iteration 466, Batch: 4, Loss: 0.12358921766281128\n",
      "Iteration 466, Batch: 5, Loss: 0.08489286154508591\n",
      "Iteration 466, Batch: 6, Loss: 0.10235986113548279\n",
      "Iteration 466, Batch: 7, Loss: 0.0835820883512497\n",
      "Iteration 466, Batch: 8, Loss: 0.04366742819547653\n",
      "Iteration 466, Batch: 9, Loss: 0.0473739318549633\n",
      "Iteration 466, Batch: 10, Loss: 0.0949685275554657\n",
      "Iteration 466, Batch: 11, Loss: 0.09356533735990524\n",
      "Iteration 466, Batch: 12, Loss: 0.12336022406816483\n",
      "Iteration 466, Batch: 13, Loss: 0.09160395711660385\n",
      "Iteration 466, Batch: 14, Loss: 0.057818327099084854\n",
      "Iteration 466, Batch: 15, Loss: 0.04249318689107895\n",
      "Iteration 466, Batch: 16, Loss: 0.07417970150709152\n",
      "Iteration 466, Batch: 17, Loss: 0.062397364526987076\n",
      "Iteration 466, Batch: 18, Loss: 0.06970027089118958\n",
      "Iteration 466, Batch: 19, Loss: 0.07960973680019379\n",
      "Iteration 466, Batch: 20, Loss: 0.0600300170481205\n",
      "Iteration 466, Batch: 21, Loss: 0.06739602982997894\n",
      "Iteration 466, Batch: 22, Loss: 0.06838836520910263\n",
      "Iteration 466, Batch: 23, Loss: 0.09589041769504547\n",
      "Iteration 466, Batch: 24, Loss: 0.07411561906337738\n",
      "Iteration 466, Batch: 25, Loss: 0.05596616491675377\n",
      "Iteration 466, Batch: 26, Loss: 0.06187908351421356\n",
      "Iteration 466, Batch: 27, Loss: 0.05863003060221672\n",
      "Iteration 466, Batch: 28, Loss: 0.04827530309557915\n",
      "Iteration 466, Batch: 29, Loss: 0.0500478558242321\n",
      "Iteration 466, Batch: 30, Loss: 0.044304244220256805\n",
      "Iteration 466, Batch: 31, Loss: 0.05961782857775688\n",
      "Iteration 466, Batch: 32, Loss: 0.03759745880961418\n",
      "Iteration 466, Batch: 33, Loss: 0.06540611386299133\n",
      "Iteration 466, Batch: 34, Loss: 0.08461536467075348\n",
      "Iteration 466, Batch: 35, Loss: 0.06729849427938461\n",
      "Iteration 466, Batch: 36, Loss: 0.10611508786678314\n",
      "Iteration 466, Batch: 37, Loss: 0.08409865945577621\n",
      "Iteration 466, Batch: 38, Loss: 0.11797168105840683\n",
      "Iteration 466, Batch: 39, Loss: 0.09493546932935715\n",
      "Iteration 466, Batch: 40, Loss: 0.08940257877111435\n",
      "Iteration 466, Batch: 41, Loss: 0.07768581062555313\n",
      "Iteration 466, Batch: 42, Loss: 0.15218383073806763\n",
      "Iteration 466, Batch: 43, Loss: 0.10436192899942398\n",
      "Iteration 466, Batch: 44, Loss: 0.07182245701551437\n",
      "Iteration 466, Batch: 45, Loss: 0.07604251056909561\n",
      "Iteration 466, Batch: 46, Loss: 0.03664292395114899\n",
      "Iteration 466, Batch: 47, Loss: 0.08100534230470657\n",
      "Iteration 466, Batch: 48, Loss: 0.08750888705253601\n",
      "Iteration 466, Batch: 49, Loss: 0.07386361807584763\n",
      "Number of layers: 10\n",
      "Iteration 467, Batch: 0, Loss: 0.04602324217557907\n",
      "Iteration 467, Batch: 1, Loss: 0.04074597358703613\n",
      "Iteration 467, Batch: 2, Loss: 0.07549166679382324\n",
      "Iteration 467, Batch: 3, Loss: 0.10072440654039383\n",
      "Iteration 467, Batch: 4, Loss: 0.06016479805111885\n",
      "Iteration 467, Batch: 5, Loss: 0.08072398602962494\n",
      "Iteration 467, Batch: 6, Loss: 0.07632330060005188\n",
      "Iteration 467, Batch: 7, Loss: 0.0944373682141304\n",
      "Iteration 467, Batch: 8, Loss: 0.11507051438093185\n",
      "Iteration 467, Batch: 9, Loss: 0.06577571481466293\n",
      "Iteration 467, Batch: 10, Loss: 0.05230648070573807\n",
      "Iteration 467, Batch: 11, Loss: 0.06145716458559036\n",
      "Iteration 467, Batch: 12, Loss: 0.08185073733329773\n",
      "Iteration 467, Batch: 13, Loss: 0.05962182953953743\n",
      "Iteration 467, Batch: 14, Loss: 0.07957514375448227\n",
      "Iteration 467, Batch: 15, Loss: 0.04585661739110947\n",
      "Iteration 467, Batch: 16, Loss: 0.08728912472724915\n",
      "Iteration 467, Batch: 17, Loss: 0.06257595866918564\n",
      "Iteration 467, Batch: 18, Loss: 0.04322778433561325\n",
      "Iteration 467, Batch: 19, Loss: 0.039336781948804855\n",
      "Iteration 467, Batch: 20, Loss: 0.07101259380578995\n",
      "Iteration 467, Batch: 21, Loss: 0.05078057944774628\n",
      "Iteration 467, Batch: 22, Loss: 0.0697084441781044\n",
      "Iteration 467, Batch: 23, Loss: 0.07677190750837326\n",
      "Iteration 467, Batch: 24, Loss: 0.06330674141645432\n",
      "Iteration 467, Batch: 25, Loss: 0.061177559196949005\n",
      "Iteration 467, Batch: 26, Loss: 0.054425716400146484\n",
      "Iteration 467, Batch: 27, Loss: 0.07298987358808517\n",
      "Iteration 467, Batch: 28, Loss: 0.039053402841091156\n",
      "Iteration 467, Batch: 29, Loss: 0.06851162016391754\n",
      "Iteration 467, Batch: 30, Loss: 0.035512156784534454\n",
      "Iteration 467, Batch: 31, Loss: 0.06693925708532333\n",
      "Iteration 467, Batch: 32, Loss: 0.037183064967393875\n",
      "Iteration 467, Batch: 33, Loss: 0.08550187945365906\n",
      "Iteration 467, Batch: 34, Loss: 0.07253938168287277\n",
      "Iteration 467, Batch: 35, Loss: 0.06501786410808563\n",
      "Iteration 467, Batch: 36, Loss: 0.07217832654714584\n",
      "Iteration 467, Batch: 37, Loss: 0.08826087415218353\n",
      "Iteration 467, Batch: 38, Loss: 0.054166458547115326\n",
      "Iteration 467, Batch: 39, Loss: 0.05314541608095169\n",
      "Iteration 467, Batch: 40, Loss: 0.06378751248121262\n",
      "Iteration 467, Batch: 41, Loss: 0.07289516925811768\n",
      "Iteration 467, Batch: 42, Loss: 0.04773479327559471\n",
      "Iteration 467, Batch: 43, Loss: 0.05801200866699219\n",
      "Iteration 467, Batch: 44, Loss: 0.04376591742038727\n",
      "Iteration 467, Batch: 45, Loss: 0.0630633756518364\n",
      "Iteration 467, Batch: 46, Loss: 0.0693550631403923\n",
      "Iteration 467, Batch: 47, Loss: 0.06407219171524048\n",
      "Iteration 467, Batch: 48, Loss: 0.05031397193670273\n",
      "Iteration 467, Batch: 49, Loss: 0.056859090924263\n",
      "Number of layers: 10\n",
      "Iteration 468, Batch: 0, Loss: 0.05609028786420822\n",
      "Iteration 468, Batch: 1, Loss: 0.10143588483333588\n",
      "Iteration 468, Batch: 2, Loss: 0.07029381394386292\n",
      "Iteration 468, Batch: 3, Loss: 0.07563650608062744\n",
      "Iteration 468, Batch: 4, Loss: 0.044657353311777115\n",
      "Iteration 468, Batch: 5, Loss: 0.05332188308238983\n",
      "Iteration 468, Batch: 6, Loss: 0.03652232885360718\n",
      "Iteration 468, Batch: 7, Loss: 0.07833965867757797\n",
      "Iteration 468, Batch: 8, Loss: 0.06286650151014328\n",
      "Iteration 468, Batch: 9, Loss: 0.05077739432454109\n",
      "Iteration 468, Batch: 10, Loss: 0.025486210361123085\n",
      "Iteration 468, Batch: 11, Loss: 0.0686613917350769\n",
      "Iteration 468, Batch: 12, Loss: 0.06764911860227585\n",
      "Iteration 468, Batch: 13, Loss: 0.04049324989318848\n",
      "Iteration 468, Batch: 14, Loss: 0.06940541416406631\n",
      "Iteration 468, Batch: 15, Loss: 0.04339747503399849\n",
      "Iteration 468, Batch: 16, Loss: 0.07217566668987274\n",
      "Iteration 468, Batch: 17, Loss: 0.043163739144802094\n",
      "Iteration 468, Batch: 18, Loss: 0.057229384779930115\n",
      "Iteration 468, Batch: 19, Loss: 0.07431653141975403\n",
      "Iteration 468, Batch: 20, Loss: 0.07385987788438797\n",
      "Iteration 468, Batch: 21, Loss: 0.0545591339468956\n",
      "Iteration 468, Batch: 22, Loss: 0.05426841229200363\n",
      "Iteration 468, Batch: 23, Loss: 0.06806214898824692\n",
      "Iteration 468, Batch: 24, Loss: 0.04886212572455406\n",
      "Iteration 468, Batch: 25, Loss: 0.0500805526971817\n",
      "Iteration 468, Batch: 26, Loss: 0.0829034149646759\n",
      "Iteration 468, Batch: 27, Loss: 0.033503368496894836\n",
      "Iteration 468, Batch: 28, Loss: 0.06867389380931854\n",
      "Iteration 468, Batch: 29, Loss: 0.08736342191696167\n",
      "Iteration 468, Batch: 30, Loss: 0.05439486354589462\n",
      "Iteration 468, Batch: 31, Loss: 0.06948935240507126\n",
      "Iteration 468, Batch: 32, Loss: 0.08027103543281555\n",
      "Iteration 468, Batch: 33, Loss: 0.05260337144136429\n",
      "Iteration 468, Batch: 34, Loss: 0.03693236783146858\n",
      "Iteration 468, Batch: 35, Loss: 0.07305284589529037\n",
      "Iteration 468, Batch: 36, Loss: 0.05114259570837021\n",
      "Iteration 468, Batch: 37, Loss: 0.057804521173238754\n",
      "Iteration 468, Batch: 38, Loss: 0.07015626132488251\n",
      "Iteration 468, Batch: 39, Loss: 0.08580803871154785\n",
      "Iteration 468, Batch: 40, Loss: 0.052497584372758865\n",
      "Iteration 468, Batch: 41, Loss: 0.053868673741817474\n",
      "Iteration 468, Batch: 42, Loss: 0.06842386722564697\n",
      "Iteration 468, Batch: 43, Loss: 0.05307333171367645\n",
      "Iteration 468, Batch: 44, Loss: 0.05246807634830475\n",
      "Iteration 468, Batch: 45, Loss: 0.03385259956121445\n",
      "Iteration 468, Batch: 46, Loss: 0.07770710438489914\n",
      "Iteration 468, Batch: 47, Loss: 0.0746670812368393\n",
      "Iteration 468, Batch: 48, Loss: 0.0714394822716713\n",
      "Iteration 468, Batch: 49, Loss: 0.08112457394599915\n",
      "Number of layers: 10\n",
      "Iteration 469, Batch: 0, Loss: 0.04800723120570183\n",
      "Iteration 469, Batch: 1, Loss: 0.043474867939949036\n",
      "Iteration 469, Batch: 2, Loss: 0.0730239599943161\n",
      "Iteration 469, Batch: 3, Loss: 0.08246244490146637\n",
      "Iteration 469, Batch: 4, Loss: 0.06021140143275261\n",
      "Iteration 469, Batch: 5, Loss: 0.1778596043586731\n",
      "Iteration 469, Batch: 6, Loss: 0.0555766299366951\n",
      "Iteration 469, Batch: 7, Loss: 0.05665097013115883\n",
      "Iteration 469, Batch: 8, Loss: 0.03869287297129631\n",
      "Iteration 469, Batch: 9, Loss: 0.05818438529968262\n",
      "Iteration 469, Batch: 10, Loss: 0.05894988775253296\n",
      "Iteration 469, Batch: 11, Loss: 0.062175214290618896\n",
      "Iteration 469, Batch: 12, Loss: 0.04044855013489723\n",
      "Iteration 469, Batch: 13, Loss: 0.06448471546173096\n",
      "Iteration 469, Batch: 14, Loss: 0.052621860057115555\n",
      "Iteration 469, Batch: 15, Loss: 0.1469631791114807\n",
      "Iteration 469, Batch: 16, Loss: 0.0707668587565422\n",
      "Iteration 469, Batch: 17, Loss: 0.07797197997570038\n",
      "Iteration 469, Batch: 18, Loss: 0.08041402697563171\n",
      "Iteration 469, Batch: 19, Loss: 0.0902104526758194\n",
      "Iteration 469, Batch: 20, Loss: 0.07695505768060684\n",
      "Iteration 469, Batch: 21, Loss: 0.04797499254345894\n",
      "Iteration 469, Batch: 22, Loss: 0.07956601679325104\n",
      "Iteration 469, Batch: 23, Loss: 0.07051453739404678\n",
      "Iteration 469, Batch: 24, Loss: 0.07244525104761124\n",
      "Iteration 469, Batch: 25, Loss: 0.051082298159599304\n",
      "Iteration 469, Batch: 26, Loss: 0.08379500359296799\n",
      "Iteration 469, Batch: 27, Loss: 0.05229482427239418\n",
      "Iteration 469, Batch: 28, Loss: 0.07213154435157776\n",
      "Iteration 469, Batch: 29, Loss: 0.05112408474087715\n",
      "Iteration 469, Batch: 30, Loss: 0.06983321905136108\n",
      "Iteration 469, Batch: 31, Loss: 0.07807235419750214\n",
      "Iteration 469, Batch: 32, Loss: 0.06467520445585251\n",
      "Iteration 469, Batch: 33, Loss: 0.07420732080936432\n",
      "Iteration 469, Batch: 34, Loss: 0.0623934268951416\n",
      "Iteration 469, Batch: 35, Loss: 0.06584246456623077\n",
      "Iteration 469, Batch: 36, Loss: 0.04747502878308296\n",
      "Iteration 469, Batch: 37, Loss: 0.04466606676578522\n",
      "Iteration 469, Batch: 38, Loss: 0.08098094910383224\n",
      "Iteration 469, Batch: 39, Loss: 0.04221228137612343\n",
      "Iteration 469, Batch: 40, Loss: 0.0650973990559578\n",
      "Iteration 469, Batch: 41, Loss: 0.04880616441369057\n",
      "Iteration 469, Batch: 42, Loss: 0.04227045923471451\n",
      "Iteration 469, Batch: 43, Loss: 0.09370597451925278\n",
      "Iteration 469, Batch: 44, Loss: 0.0779380276799202\n",
      "Iteration 469, Batch: 45, Loss: 0.05653468891978264\n",
      "Iteration 469, Batch: 46, Loss: 0.04982055723667145\n",
      "Iteration 469, Batch: 47, Loss: 0.04317399486899376\n",
      "Iteration 469, Batch: 48, Loss: 0.055224232375621796\n",
      "Iteration 469, Batch: 49, Loss: 0.08070453256368637\n",
      "Number of layers: 10\n",
      "Iteration 470, Batch: 0, Loss: 0.05680585652589798\n",
      "Iteration 470, Batch: 1, Loss: 0.05394669249653816\n",
      "Iteration 470, Batch: 2, Loss: 0.06817951053380966\n",
      "Iteration 470, Batch: 3, Loss: 0.05323152244091034\n",
      "Iteration 470, Batch: 4, Loss: 0.06540169566869736\n",
      "Iteration 470, Batch: 5, Loss: 0.08075558394193649\n",
      "Iteration 470, Batch: 6, Loss: 0.07265802472829819\n",
      "Iteration 470, Batch: 7, Loss: 0.059561263769865036\n",
      "Iteration 470, Batch: 8, Loss: 0.06804975122213364\n",
      "Iteration 470, Batch: 9, Loss: 0.06341931223869324\n",
      "Iteration 470, Batch: 10, Loss: 0.05864166095852852\n",
      "Iteration 470, Batch: 11, Loss: 0.056578926742076874\n",
      "Iteration 470, Batch: 12, Loss: 0.07253780961036682\n",
      "Iteration 470, Batch: 13, Loss: 0.06506770104169846\n",
      "Iteration 470, Batch: 14, Loss: 0.09157031774520874\n",
      "Iteration 470, Batch: 15, Loss: 0.05956483632326126\n",
      "Iteration 470, Batch: 16, Loss: 0.056892599910497665\n",
      "Iteration 470, Batch: 17, Loss: 0.05159281566739082\n",
      "Iteration 470, Batch: 18, Loss: 0.08946357667446136\n",
      "Iteration 470, Batch: 19, Loss: 0.05508683621883392\n",
      "Iteration 470, Batch: 20, Loss: 0.0734637901186943\n",
      "Iteration 470, Batch: 21, Loss: 0.07099076360464096\n",
      "Iteration 470, Batch: 22, Loss: 0.04753468558192253\n",
      "Iteration 470, Batch: 23, Loss: 0.05340816453099251\n",
      "Iteration 470, Batch: 24, Loss: 0.06192127242684364\n",
      "Iteration 470, Batch: 25, Loss: 0.0525071881711483\n",
      "Iteration 470, Batch: 26, Loss: 0.07052262127399445\n",
      "Iteration 470, Batch: 27, Loss: 0.042389124631881714\n",
      "Iteration 470, Batch: 28, Loss: 0.050671838223934174\n",
      "Iteration 470, Batch: 29, Loss: 0.058990418910980225\n",
      "Iteration 470, Batch: 30, Loss: 0.07632763683795929\n",
      "Iteration 470, Batch: 31, Loss: 0.06638109683990479\n",
      "Iteration 470, Batch: 32, Loss: 0.05088752135634422\n",
      "Iteration 470, Batch: 33, Loss: 0.05220726877450943\n",
      "Iteration 470, Batch: 34, Loss: 0.054848287254571915\n",
      "Iteration 470, Batch: 35, Loss: 0.05710095167160034\n",
      "Iteration 470, Batch: 36, Loss: 0.054996784776449203\n",
      "Iteration 470, Batch: 37, Loss: 0.05539906397461891\n",
      "Iteration 470, Batch: 38, Loss: 0.03524574264883995\n",
      "Iteration 470, Batch: 39, Loss: 0.033552899956703186\n",
      "Iteration 470, Batch: 40, Loss: 0.04640001058578491\n",
      "Iteration 470, Batch: 41, Loss: 0.04654981568455696\n",
      "Iteration 470, Batch: 42, Loss: 0.05495445057749748\n",
      "Iteration 470, Batch: 43, Loss: 0.03382665663957596\n",
      "Iteration 470, Batch: 44, Loss: 0.043926775455474854\n",
      "Iteration 470, Batch: 45, Loss: 0.06366617232561111\n",
      "Iteration 470, Batch: 46, Loss: 0.04286368936300278\n",
      "Iteration 470, Batch: 47, Loss: 0.0939633920788765\n",
      "Iteration 470, Batch: 48, Loss: 0.0736999660730362\n",
      "Iteration 470, Batch: 49, Loss: 0.0807633101940155\n",
      "Number of layers: 10\n",
      "Iteration 471, Batch: 0, Loss: 0.07171480357646942\n",
      "Iteration 471, Batch: 1, Loss: 0.07301222532987595\n",
      "Iteration 471, Batch: 2, Loss: 0.06919313967227936\n",
      "Iteration 471, Batch: 3, Loss: 0.056598491966724396\n",
      "Iteration 471, Batch: 4, Loss: 0.07926489412784576\n",
      "Iteration 471, Batch: 5, Loss: 0.06046663597226143\n",
      "Iteration 471, Batch: 6, Loss: 0.03384194150567055\n",
      "Iteration 471, Batch: 7, Loss: 0.06881725043058395\n",
      "Iteration 471, Batch: 8, Loss: 0.059707555919885635\n",
      "Iteration 471, Batch: 9, Loss: 0.08720186352729797\n",
      "Iteration 471, Batch: 10, Loss: 0.06046485900878906\n",
      "Iteration 471, Batch: 11, Loss: 0.04263686016201973\n",
      "Iteration 471, Batch: 12, Loss: 0.06507879495620728\n",
      "Iteration 471, Batch: 13, Loss: 0.061651185154914856\n",
      "Iteration 471, Batch: 14, Loss: 0.06400534510612488\n",
      "Iteration 471, Batch: 15, Loss: 0.04018834978342056\n",
      "Iteration 471, Batch: 16, Loss: 0.03888338804244995\n",
      "Iteration 471, Batch: 17, Loss: 0.037950098514556885\n",
      "Iteration 471, Batch: 18, Loss: 0.06228058785200119\n",
      "Iteration 471, Batch: 19, Loss: 0.05109144002199173\n",
      "Iteration 471, Batch: 20, Loss: 0.06343895941972733\n",
      "Iteration 471, Batch: 21, Loss: 0.052081603556871414\n",
      "Iteration 471, Batch: 22, Loss: 0.0743793398141861\n",
      "Iteration 471, Batch: 23, Loss: 0.05489363893866539\n",
      "Iteration 471, Batch: 24, Loss: 0.07442628592252731\n",
      "Iteration 471, Batch: 25, Loss: 0.06480538845062256\n",
      "Iteration 471, Batch: 26, Loss: 0.06475195288658142\n",
      "Iteration 471, Batch: 27, Loss: 0.04306505247950554\n",
      "Iteration 471, Batch: 28, Loss: 0.05344633385539055\n",
      "Iteration 471, Batch: 29, Loss: 0.062373217195272446\n",
      "Iteration 471, Batch: 30, Loss: 0.07802233844995499\n",
      "Iteration 471, Batch: 31, Loss: 0.03698157146573067\n",
      "Iteration 471, Batch: 32, Loss: 0.056330226361751556\n",
      "Iteration 471, Batch: 33, Loss: 0.06941208988428116\n",
      "Iteration 471, Batch: 34, Loss: 0.08401854336261749\n",
      "Iteration 471, Batch: 35, Loss: 0.10297820717096329\n",
      "Iteration 471, Batch: 36, Loss: 0.05992230772972107\n",
      "Iteration 471, Batch: 37, Loss: 0.06562282145023346\n",
      "Iteration 471, Batch: 38, Loss: 0.08985887467861176\n",
      "Iteration 471, Batch: 39, Loss: 0.0619041807949543\n",
      "Iteration 471, Batch: 40, Loss: 0.08054305613040924\n",
      "Iteration 471, Batch: 41, Loss: 0.0760122686624527\n",
      "Iteration 471, Batch: 42, Loss: 0.06526180356740952\n",
      "Iteration 471, Batch: 43, Loss: 0.05943160876631737\n",
      "Iteration 471, Batch: 44, Loss: 0.10684274137020111\n",
      "Iteration 471, Batch: 45, Loss: 0.049411218613386154\n",
      "Iteration 471, Batch: 46, Loss: 0.05886295065283775\n",
      "Iteration 471, Batch: 47, Loss: 0.06448527425527573\n",
      "Iteration 471, Batch: 48, Loss: 0.044984281063079834\n",
      "Iteration 471, Batch: 49, Loss: 0.0578061006963253\n",
      "Number of layers: 10\n",
      "Iteration 472, Batch: 0, Loss: 0.06044979766011238\n",
      "Iteration 472, Batch: 1, Loss: 0.044135931879282\n",
      "Iteration 472, Batch: 2, Loss: 0.03392377868294716\n",
      "Iteration 472, Batch: 3, Loss: 0.042340926826000214\n",
      "Iteration 472, Batch: 4, Loss: 0.07065796107053757\n",
      "Iteration 472, Batch: 5, Loss: 0.05681668221950531\n",
      "Iteration 472, Batch: 6, Loss: 0.055455997586250305\n",
      "Iteration 472, Batch: 7, Loss: 0.04760032892227173\n",
      "Iteration 472, Batch: 8, Loss: 0.08372712135314941\n",
      "Iteration 472, Batch: 9, Loss: 0.06101583316922188\n",
      "Iteration 472, Batch: 10, Loss: 0.04768257960677147\n",
      "Iteration 472, Batch: 11, Loss: 0.07837755978107452\n",
      "Iteration 472, Batch: 12, Loss: 0.08226443827152252\n",
      "Iteration 472, Batch: 13, Loss: 0.0538942813873291\n",
      "Iteration 472, Batch: 14, Loss: 0.08387532085180283\n",
      "Iteration 472, Batch: 15, Loss: 0.10746718943119049\n",
      "Iteration 472, Batch: 16, Loss: 0.061999499797821045\n",
      "Iteration 472, Batch: 17, Loss: 0.06603818386793137\n",
      "Iteration 472, Batch: 18, Loss: 0.051229141652584076\n",
      "Iteration 472, Batch: 19, Loss: 0.04384007304906845\n",
      "Iteration 472, Batch: 20, Loss: 0.07323875278234482\n",
      "Iteration 472, Batch: 21, Loss: 0.05690036341547966\n",
      "Iteration 472, Batch: 22, Loss: 0.06298665702342987\n",
      "Iteration 472, Batch: 23, Loss: 0.07145464420318604\n",
      "Iteration 472, Batch: 24, Loss: 0.034564774483442307\n",
      "Iteration 472, Batch: 25, Loss: 0.0778455138206482\n",
      "Iteration 472, Batch: 26, Loss: 0.07918082177639008\n",
      "Iteration 472, Batch: 27, Loss: 0.06266890466213226\n",
      "Iteration 472, Batch: 28, Loss: 0.03329053148627281\n",
      "Iteration 472, Batch: 29, Loss: 0.06852521747350693\n",
      "Iteration 472, Batch: 30, Loss: 0.060216087847948074\n",
      "Iteration 472, Batch: 31, Loss: 0.07310039550065994\n",
      "Iteration 472, Batch: 32, Loss: 0.06639432162046432\n",
      "Iteration 472, Batch: 33, Loss: 0.05065465718507767\n",
      "Iteration 472, Batch: 34, Loss: 0.06867775321006775\n",
      "Iteration 472, Batch: 35, Loss: 0.05100841447710991\n",
      "Iteration 472, Batch: 36, Loss: 0.048711955547332764\n",
      "Iteration 472, Batch: 37, Loss: 0.050328418612480164\n",
      "Iteration 472, Batch: 38, Loss: 0.060448892414569855\n",
      "Iteration 472, Batch: 39, Loss: 0.06007710471749306\n",
      "Iteration 472, Batch: 40, Loss: 0.08212394267320633\n",
      "Iteration 472, Batch: 41, Loss: 0.05881146714091301\n",
      "Iteration 472, Batch: 42, Loss: 0.056948766112327576\n",
      "Iteration 472, Batch: 43, Loss: 0.06672851741313934\n",
      "Iteration 472, Batch: 44, Loss: 0.062123458832502365\n",
      "Iteration 472, Batch: 45, Loss: 0.07257085293531418\n",
      "Iteration 472, Batch: 46, Loss: 0.09180948883295059\n",
      "Iteration 472, Batch: 47, Loss: 0.07859347760677338\n",
      "Iteration 472, Batch: 48, Loss: 0.09658841788768768\n",
      "Iteration 472, Batch: 49, Loss: 0.061541847884655\n",
      "Number of layers: 10\n",
      "Iteration 473, Batch: 0, Loss: 0.07124920934438705\n",
      "Iteration 473, Batch: 1, Loss: 0.09144483506679535\n",
      "Iteration 473, Batch: 2, Loss: 0.0598708838224411\n",
      "Iteration 473, Batch: 3, Loss: 0.049505963921546936\n",
      "Iteration 473, Batch: 4, Loss: 0.06272764503955841\n",
      "Iteration 473, Batch: 5, Loss: 0.07952593266963959\n",
      "Iteration 473, Batch: 6, Loss: 0.061177682131528854\n",
      "Iteration 473, Batch: 7, Loss: 0.04824719950556755\n",
      "Iteration 473, Batch: 8, Loss: 0.0632631704211235\n",
      "Iteration 473, Batch: 9, Loss: 0.05311036854982376\n",
      "Iteration 473, Batch: 10, Loss: 0.05518195033073425\n",
      "Iteration 473, Batch: 11, Loss: 0.0852583646774292\n",
      "Iteration 473, Batch: 12, Loss: 0.0597323440015316\n",
      "Iteration 473, Batch: 13, Loss: 0.05804691091179848\n",
      "Iteration 473, Batch: 14, Loss: 0.06779062002897263\n",
      "Iteration 473, Batch: 15, Loss: 0.08548144996166229\n",
      "Iteration 473, Batch: 16, Loss: 0.10018021613359451\n",
      "Iteration 473, Batch: 17, Loss: 0.0542432963848114\n",
      "Iteration 473, Batch: 18, Loss: 0.04135506972670555\n",
      "Iteration 473, Batch: 19, Loss: 0.07092314958572388\n",
      "Iteration 473, Batch: 20, Loss: 0.07439830899238586\n",
      "Iteration 473, Batch: 21, Loss: 0.05822235345840454\n",
      "Iteration 473, Batch: 22, Loss: 0.05814309045672417\n",
      "Iteration 473, Batch: 23, Loss: 0.05813370645046234\n",
      "Iteration 473, Batch: 24, Loss: 0.06445896625518799\n",
      "Iteration 473, Batch: 25, Loss: 0.05801248922944069\n",
      "Iteration 473, Batch: 26, Loss: 0.070931077003479\n",
      "Iteration 473, Batch: 27, Loss: 0.08317487686872482\n",
      "Iteration 473, Batch: 28, Loss: 0.07895912230014801\n",
      "Iteration 473, Batch: 29, Loss: 0.06311256438493729\n",
      "Iteration 473, Batch: 30, Loss: 0.08825962990522385\n",
      "Iteration 473, Batch: 31, Loss: 0.05335555970668793\n",
      "Iteration 473, Batch: 32, Loss: 0.05719352141022682\n",
      "Iteration 473, Batch: 33, Loss: 0.07322406768798828\n",
      "Iteration 473, Batch: 34, Loss: 0.05196330323815346\n",
      "Iteration 473, Batch: 35, Loss: 0.059437692165374756\n",
      "Iteration 473, Batch: 36, Loss: 0.054678887128829956\n",
      "Iteration 473, Batch: 37, Loss: 0.03540142625570297\n",
      "Iteration 473, Batch: 38, Loss: 0.06694348901510239\n",
      "Iteration 473, Batch: 39, Loss: 0.07348132878541946\n",
      "Iteration 473, Batch: 40, Loss: 0.04673413187265396\n",
      "Iteration 473, Batch: 41, Loss: 0.06241818144917488\n",
      "Iteration 473, Batch: 42, Loss: 0.05255355313420296\n",
      "Iteration 473, Batch: 43, Loss: 0.06682027876377106\n",
      "Iteration 473, Batch: 44, Loss: 0.060023076832294464\n",
      "Iteration 473, Batch: 45, Loss: 0.036034129559993744\n",
      "Iteration 473, Batch: 46, Loss: 0.08771978318691254\n",
      "Iteration 473, Batch: 47, Loss: 0.06725085526704788\n",
      "Iteration 473, Batch: 48, Loss: 0.06495270878076553\n",
      "Iteration 473, Batch: 49, Loss: 0.052437108010053635\n",
      "Number of layers: 10\n",
      "Iteration 474, Batch: 0, Loss: 0.06236939877271652\n",
      "Iteration 474, Batch: 1, Loss: 0.10407224297523499\n",
      "Iteration 474, Batch: 2, Loss: 0.06444206833839417\n",
      "Iteration 474, Batch: 3, Loss: 0.08788463473320007\n",
      "Iteration 474, Batch: 4, Loss: 0.05847227945923805\n",
      "Iteration 474, Batch: 5, Loss: 0.07390550523996353\n",
      "Iteration 474, Batch: 6, Loss: 0.057786379009485245\n",
      "Iteration 474, Batch: 7, Loss: 0.04509894922375679\n",
      "Iteration 474, Batch: 8, Loss: 0.05328868702054024\n",
      "Iteration 474, Batch: 9, Loss: 0.06563370674848557\n",
      "Iteration 474, Batch: 10, Loss: 0.03747507929801941\n",
      "Iteration 474, Batch: 11, Loss: 0.046657003462314606\n",
      "Iteration 474, Batch: 12, Loss: 0.055289044976234436\n",
      "Iteration 474, Batch: 13, Loss: 0.04171811044216156\n",
      "Iteration 474, Batch: 14, Loss: 0.0752495527267456\n",
      "Iteration 474, Batch: 15, Loss: 0.07119832187891006\n",
      "Iteration 474, Batch: 16, Loss: 0.04233862832188606\n",
      "Iteration 474, Batch: 17, Loss: 0.07237612456083298\n",
      "Iteration 474, Batch: 18, Loss: 0.07092892378568649\n",
      "Iteration 474, Batch: 19, Loss: 0.06733957678079605\n",
      "Iteration 474, Batch: 20, Loss: 0.05266222730278969\n",
      "Iteration 474, Batch: 21, Loss: 0.061071813106536865\n",
      "Iteration 474, Batch: 22, Loss: 0.0891231819987297\n",
      "Iteration 474, Batch: 23, Loss: 0.053102634847164154\n",
      "Iteration 474, Batch: 24, Loss: 0.04253365471959114\n",
      "Iteration 474, Batch: 25, Loss: 0.05966140702366829\n",
      "Iteration 474, Batch: 26, Loss: 0.06717883795499802\n",
      "Iteration 474, Batch: 27, Loss: 0.04661162197589874\n",
      "Iteration 474, Batch: 28, Loss: 0.07555461674928665\n",
      "Iteration 474, Batch: 29, Loss: 0.06631819903850555\n",
      "Iteration 474, Batch: 30, Loss: 0.04645972326397896\n",
      "Iteration 474, Batch: 31, Loss: 0.06153542920947075\n",
      "Iteration 474, Batch: 32, Loss: 0.054047588258981705\n",
      "Iteration 474, Batch: 33, Loss: 0.06450649350881577\n",
      "Iteration 474, Batch: 34, Loss: 0.046468522399663925\n",
      "Iteration 474, Batch: 35, Loss: 0.03520394116640091\n",
      "Iteration 474, Batch: 36, Loss: 0.03966271132230759\n",
      "Iteration 474, Batch: 37, Loss: 0.05191586911678314\n",
      "Iteration 474, Batch: 38, Loss: 0.0448688305914402\n",
      "Iteration 474, Batch: 39, Loss: 0.045649297535419464\n",
      "Iteration 474, Batch: 40, Loss: 0.08065108954906464\n",
      "Iteration 474, Batch: 41, Loss: 0.0744016170501709\n",
      "Iteration 474, Batch: 42, Loss: 0.05390124022960663\n",
      "Iteration 474, Batch: 43, Loss: 0.055372875183820724\n",
      "Iteration 474, Batch: 44, Loss: 0.06357868760824203\n",
      "Iteration 474, Batch: 45, Loss: 0.07691485434770584\n",
      "Iteration 474, Batch: 46, Loss: 0.04338899627327919\n",
      "Iteration 474, Batch: 47, Loss: 0.06093600019812584\n",
      "Iteration 474, Batch: 48, Loss: 0.0652645081281662\n",
      "Iteration 474, Batch: 49, Loss: 0.06379341334104538\n",
      "Number of layers: 10\n",
      "Iteration 475, Batch: 0, Loss: 0.06268174201250076\n",
      "Iteration 475, Batch: 1, Loss: 0.04869413375854492\n",
      "Iteration 475, Batch: 2, Loss: 0.06057173013687134\n",
      "Iteration 475, Batch: 3, Loss: 0.07230602204799652\n",
      "Iteration 475, Batch: 4, Loss: 0.052077893167734146\n",
      "Iteration 475, Batch: 5, Loss: 0.04600441828370094\n",
      "Iteration 475, Batch: 6, Loss: 0.06399475783109665\n",
      "Iteration 475, Batch: 7, Loss: 0.052173566073179245\n",
      "Iteration 475, Batch: 8, Loss: 0.06277168542146683\n",
      "Iteration 475, Batch: 9, Loss: 0.07364730536937714\n",
      "Iteration 475, Batch: 10, Loss: 0.06634548306465149\n",
      "Iteration 475, Batch: 11, Loss: 0.030797407031059265\n",
      "Iteration 475, Batch: 12, Loss: 0.06257171183824539\n",
      "Iteration 475, Batch: 13, Loss: 0.07187527418136597\n",
      "Iteration 475, Batch: 14, Loss: 0.0666150376200676\n",
      "Iteration 475, Batch: 15, Loss: 0.05445670336484909\n",
      "Iteration 475, Batch: 16, Loss: 0.0631304532289505\n",
      "Iteration 475, Batch: 17, Loss: 0.06394031643867493\n",
      "Iteration 475, Batch: 18, Loss: 0.08349089324474335\n",
      "Iteration 475, Batch: 19, Loss: 0.06893634796142578\n",
      "Iteration 475, Batch: 20, Loss: 0.05541997775435448\n",
      "Iteration 475, Batch: 21, Loss: 0.045188579708337784\n",
      "Iteration 475, Batch: 22, Loss: 0.05271131545305252\n",
      "Iteration 475, Batch: 23, Loss: 0.06083863973617554\n",
      "Iteration 475, Batch: 24, Loss: 0.061341769993305206\n",
      "Iteration 475, Batch: 25, Loss: 0.03409681096673012\n",
      "Iteration 475, Batch: 26, Loss: 0.05149378255009651\n",
      "Iteration 475, Batch: 27, Loss: 0.0347013846039772\n",
      "Iteration 475, Batch: 28, Loss: 0.06447461247444153\n",
      "Iteration 475, Batch: 29, Loss: 0.05520450323820114\n",
      "Iteration 475, Batch: 30, Loss: 0.0761551707983017\n",
      "Iteration 475, Batch: 31, Loss: 0.05467440187931061\n",
      "Iteration 475, Batch: 32, Loss: 0.059022512286901474\n",
      "Iteration 475, Batch: 33, Loss: 0.048614636063575745\n",
      "Iteration 475, Batch: 34, Loss: 0.07186274230480194\n",
      "Iteration 475, Batch: 35, Loss: 0.06247711181640625\n",
      "Iteration 475, Batch: 36, Loss: 0.08024365454912186\n",
      "Iteration 475, Batch: 37, Loss: 0.07017174363136292\n",
      "Iteration 475, Batch: 38, Loss: 0.06511004269123077\n",
      "Iteration 475, Batch: 39, Loss: 0.035033367574214935\n",
      "Iteration 475, Batch: 40, Loss: 0.03801209107041359\n",
      "Iteration 475, Batch: 41, Loss: 0.04399756342172623\n",
      "Iteration 475, Batch: 42, Loss: 0.06874086707830429\n",
      "Iteration 475, Batch: 43, Loss: 0.06556276977062225\n",
      "Iteration 475, Batch: 44, Loss: 0.0604318268597126\n",
      "Iteration 475, Batch: 45, Loss: 0.03469636291265488\n",
      "Iteration 475, Batch: 46, Loss: 0.04335198178887367\n",
      "Iteration 475, Batch: 47, Loss: 0.08310090750455856\n",
      "Iteration 475, Batch: 48, Loss: 0.05911945179104805\n",
      "Iteration 475, Batch: 49, Loss: 0.07680775225162506\n",
      "Number of layers: 10\n",
      "Iteration 476, Batch: 0, Loss: 0.05218369513750076\n",
      "Iteration 476, Batch: 1, Loss: 0.04438243806362152\n",
      "Iteration 476, Batch: 2, Loss: 0.08006048202514648\n",
      "Iteration 476, Batch: 3, Loss: 0.05776067450642586\n",
      "Iteration 476, Batch: 4, Loss: 0.05485469847917557\n",
      "Iteration 476, Batch: 5, Loss: 0.06470461934804916\n",
      "Iteration 476, Batch: 6, Loss: 0.05789948254823685\n",
      "Iteration 476, Batch: 7, Loss: 0.08762642741203308\n",
      "Iteration 476, Batch: 8, Loss: 0.046582382172346115\n",
      "Iteration 476, Batch: 9, Loss: 0.049750104546546936\n",
      "Iteration 476, Batch: 10, Loss: 0.04649234190583229\n",
      "Iteration 476, Batch: 11, Loss: 0.0585457980632782\n",
      "Iteration 476, Batch: 12, Loss: 0.06449666619300842\n",
      "Iteration 476, Batch: 13, Loss: 0.05189584568142891\n",
      "Iteration 476, Batch: 14, Loss: 0.05670810490846634\n",
      "Iteration 476, Batch: 15, Loss: 0.06429295986890793\n",
      "Iteration 476, Batch: 16, Loss: 0.07313036918640137\n",
      "Iteration 476, Batch: 17, Loss: 0.04341219738125801\n",
      "Iteration 476, Batch: 18, Loss: 0.04881654679775238\n",
      "Iteration 476, Batch: 19, Loss: 0.04550359770655632\n",
      "Iteration 476, Batch: 20, Loss: 0.05453062802553177\n",
      "Iteration 476, Batch: 21, Loss: 0.05770747363567352\n",
      "Iteration 476, Batch: 22, Loss: 0.060887716710567474\n",
      "Iteration 476, Batch: 23, Loss: 0.06888864189386368\n",
      "Iteration 476, Batch: 24, Loss: 0.06025256961584091\n",
      "Iteration 476, Batch: 25, Loss: 0.06218678504228592\n",
      "Iteration 476, Batch: 26, Loss: 0.07303901761770248\n",
      "Iteration 476, Batch: 27, Loss: 0.08159902691841125\n",
      "Iteration 476, Batch: 28, Loss: 0.06885877251625061\n",
      "Iteration 476, Batch: 29, Loss: 0.0646614283323288\n",
      "Iteration 476, Batch: 30, Loss: 0.06754647940397263\n",
      "Iteration 476, Batch: 31, Loss: 0.054880622774362564\n",
      "Iteration 476, Batch: 32, Loss: 0.08517900109291077\n",
      "Iteration 476, Batch: 33, Loss: 0.09308700263500214\n",
      "Iteration 476, Batch: 34, Loss: 0.06831223517656326\n",
      "Iteration 476, Batch: 35, Loss: 0.07411439716815948\n",
      "Iteration 476, Batch: 36, Loss: 0.04682775214314461\n",
      "Iteration 476, Batch: 37, Loss: 0.059863049536943436\n",
      "Iteration 476, Batch: 38, Loss: 0.041816581040620804\n",
      "Iteration 476, Batch: 39, Loss: 0.05012183636426926\n",
      "Iteration 476, Batch: 40, Loss: 0.05696309730410576\n",
      "Iteration 476, Batch: 41, Loss: 0.0629744604229927\n",
      "Iteration 476, Batch: 42, Loss: 0.06082179769873619\n",
      "Iteration 476, Batch: 43, Loss: 0.07522895932197571\n",
      "Iteration 476, Batch: 44, Loss: 0.050265248864889145\n",
      "Iteration 476, Batch: 45, Loss: 0.04046144336462021\n",
      "Iteration 476, Batch: 46, Loss: 0.06844672560691833\n",
      "Iteration 476, Batch: 47, Loss: 0.05717291682958603\n",
      "Iteration 476, Batch: 48, Loss: 0.05526461452245712\n",
      "Iteration 476, Batch: 49, Loss: 0.07391201704740524\n",
      "Number of layers: 10\n",
      "Iteration 477, Batch: 0, Loss: 0.04690757766366005\n",
      "Iteration 477, Batch: 1, Loss: 0.0665133148431778\n",
      "Iteration 477, Batch: 2, Loss: 0.07384771853685379\n",
      "Iteration 477, Batch: 3, Loss: 0.06699693202972412\n",
      "Iteration 477, Batch: 4, Loss: 0.05688908323645592\n",
      "Iteration 477, Batch: 5, Loss: 0.09310173988342285\n",
      "Iteration 477, Batch: 6, Loss: 0.10721953958272934\n",
      "Iteration 477, Batch: 7, Loss: 0.05303286388516426\n",
      "Iteration 477, Batch: 8, Loss: 0.07703525573015213\n",
      "Iteration 477, Batch: 9, Loss: 0.03146175667643547\n",
      "Iteration 477, Batch: 10, Loss: 0.07392321527004242\n",
      "Iteration 477, Batch: 11, Loss: 0.04736683890223503\n",
      "Iteration 477, Batch: 12, Loss: 0.043927066028118134\n",
      "Iteration 477, Batch: 13, Loss: 0.05742925405502319\n",
      "Iteration 477, Batch: 14, Loss: 0.060402367264032364\n",
      "Iteration 477, Batch: 15, Loss: 0.039131078869104385\n",
      "Iteration 477, Batch: 16, Loss: 0.0676170065999031\n",
      "Iteration 477, Batch: 17, Loss: 0.05439801141619682\n",
      "Iteration 477, Batch: 18, Loss: 0.06854645907878876\n",
      "Iteration 477, Batch: 19, Loss: 0.029009031131863594\n",
      "Iteration 477, Batch: 20, Loss: 0.08280502259731293\n",
      "Iteration 477, Batch: 21, Loss: 0.05738171935081482\n",
      "Iteration 477, Batch: 22, Loss: 0.07487811148166656\n",
      "Iteration 477, Batch: 23, Loss: 0.06747821718454361\n",
      "Iteration 477, Batch: 24, Loss: 0.061769094318151474\n",
      "Iteration 477, Batch: 25, Loss: 0.06059124693274498\n",
      "Iteration 477, Batch: 26, Loss: 0.04889386147260666\n",
      "Iteration 477, Batch: 27, Loss: 0.045459549874067307\n",
      "Iteration 477, Batch: 28, Loss: 0.0677436888217926\n",
      "Iteration 477, Batch: 29, Loss: 0.07314405590295792\n",
      "Iteration 477, Batch: 30, Loss: 0.06969264149665833\n",
      "Iteration 477, Batch: 31, Loss: 0.04673244431614876\n",
      "Iteration 477, Batch: 32, Loss: 0.0637538731098175\n",
      "Iteration 477, Batch: 33, Loss: 0.05011359229683876\n",
      "Iteration 477, Batch: 34, Loss: 0.053931694477796555\n",
      "Iteration 477, Batch: 35, Loss: 0.07485941797494888\n",
      "Iteration 477, Batch: 36, Loss: 0.042196787893772125\n",
      "Iteration 477, Batch: 37, Loss: 0.07833890616893768\n",
      "Iteration 477, Batch: 38, Loss: 0.07849889248609543\n",
      "Iteration 477, Batch: 39, Loss: 0.07307297736406326\n",
      "Iteration 477, Batch: 40, Loss: 0.05685017630457878\n",
      "Iteration 477, Batch: 41, Loss: 0.07540733367204666\n",
      "Iteration 477, Batch: 42, Loss: 0.0666583925485611\n",
      "Iteration 477, Batch: 43, Loss: 0.056504521518945694\n",
      "Iteration 477, Batch: 44, Loss: 0.06400010734796524\n",
      "Iteration 477, Batch: 45, Loss: 0.05668387934565544\n",
      "Iteration 477, Batch: 46, Loss: 0.04698140174150467\n",
      "Iteration 477, Batch: 47, Loss: 0.05485298112034798\n",
      "Iteration 477, Batch: 48, Loss: 0.075217105448246\n",
      "Iteration 477, Batch: 49, Loss: 0.04067891836166382\n",
      "Number of layers: 10\n",
      "Iteration 478, Batch: 0, Loss: 0.05508095771074295\n",
      "Iteration 478, Batch: 1, Loss: 0.05876060202717781\n",
      "Iteration 478, Batch: 2, Loss: 0.05822483450174332\n",
      "Iteration 478, Batch: 3, Loss: 0.06871447712182999\n",
      "Iteration 478, Batch: 4, Loss: 0.035272300243377686\n",
      "Iteration 478, Batch: 5, Loss: 0.07255926728248596\n",
      "Iteration 478, Batch: 6, Loss: 0.081458680331707\n",
      "Iteration 478, Batch: 7, Loss: 0.031131457537412643\n",
      "Iteration 478, Batch: 8, Loss: 0.03620806336402893\n",
      "Iteration 478, Batch: 9, Loss: 0.037162359803915024\n",
      "Iteration 478, Batch: 10, Loss: 0.06044703349471092\n",
      "Iteration 478, Batch: 11, Loss: 0.04756615683436394\n",
      "Iteration 478, Batch: 12, Loss: 0.04387642815709114\n",
      "Iteration 478, Batch: 13, Loss: 0.048558156937360764\n",
      "Iteration 478, Batch: 14, Loss: 0.06017361581325531\n",
      "Iteration 478, Batch: 15, Loss: 0.08810235559940338\n",
      "Iteration 478, Batch: 16, Loss: 0.0560452826321125\n",
      "Iteration 478, Batch: 17, Loss: 0.04248581454157829\n",
      "Iteration 478, Batch: 18, Loss: 0.05339668318629265\n",
      "Iteration 478, Batch: 19, Loss: 0.06153940036892891\n",
      "Iteration 478, Batch: 20, Loss: 0.07348058372735977\n",
      "Iteration 478, Batch: 21, Loss: 0.048157233744859695\n",
      "Iteration 478, Batch: 22, Loss: 0.037120990455150604\n",
      "Iteration 478, Batch: 23, Loss: 0.032426103949546814\n",
      "Iteration 478, Batch: 24, Loss: 0.05174567550420761\n",
      "Iteration 478, Batch: 25, Loss: 0.046106889843940735\n",
      "Iteration 478, Batch: 26, Loss: 0.04891807585954666\n",
      "Iteration 478, Batch: 27, Loss: 0.037811923772096634\n",
      "Iteration 478, Batch: 28, Loss: 0.052082184702157974\n",
      "Iteration 478, Batch: 29, Loss: 0.03063812106847763\n",
      "Iteration 478, Batch: 30, Loss: 0.049488380551338196\n",
      "Iteration 478, Batch: 31, Loss: 0.06610413640737534\n",
      "Iteration 478, Batch: 32, Loss: 0.04935035482048988\n",
      "Iteration 478, Batch: 33, Loss: 0.05631719529628754\n",
      "Iteration 478, Batch: 34, Loss: 0.0576949305832386\n",
      "Iteration 478, Batch: 35, Loss: 0.05456484854221344\n",
      "Iteration 478, Batch: 36, Loss: 0.05193028971552849\n",
      "Iteration 478, Batch: 37, Loss: 0.06990497559309006\n",
      "Iteration 478, Batch: 38, Loss: 0.058208245784044266\n",
      "Iteration 478, Batch: 39, Loss: 0.03429492935538292\n",
      "Iteration 478, Batch: 40, Loss: 0.0645659789443016\n",
      "Iteration 478, Batch: 41, Loss: 0.05044153705239296\n",
      "Iteration 478, Batch: 42, Loss: 0.0328952819108963\n",
      "Iteration 478, Batch: 43, Loss: 0.07160720974206924\n",
      "Iteration 478, Batch: 44, Loss: 0.07820134609937668\n",
      "Iteration 478, Batch: 45, Loss: 0.09553536027669907\n",
      "Iteration 478, Batch: 46, Loss: 0.091468945145607\n",
      "Iteration 478, Batch: 47, Loss: 0.086435966193676\n",
      "Iteration 478, Batch: 48, Loss: 0.05175982043147087\n",
      "Iteration 478, Batch: 49, Loss: 0.03756777569651604\n",
      "Number of layers: 10\n",
      "Iteration 479, Batch: 0, Loss: 0.06686174124479294\n",
      "Iteration 479, Batch: 1, Loss: 0.07162723690271378\n",
      "Iteration 479, Batch: 2, Loss: 0.034211691468954086\n",
      "Iteration 479, Batch: 3, Loss: 0.06894370913505554\n",
      "Iteration 479, Batch: 4, Loss: 0.02755855582654476\n",
      "Iteration 479, Batch: 5, Loss: 0.07284098118543625\n",
      "Iteration 479, Batch: 6, Loss: 0.06549408286809921\n",
      "Iteration 479, Batch: 7, Loss: 0.0423395037651062\n",
      "Iteration 479, Batch: 8, Loss: 0.05797586217522621\n",
      "Iteration 479, Batch: 9, Loss: 0.07153557986021042\n",
      "Iteration 479, Batch: 10, Loss: 0.032093606889247894\n",
      "Iteration 479, Batch: 11, Loss: 0.045436516404151917\n",
      "Iteration 479, Batch: 12, Loss: 0.03655347228050232\n",
      "Iteration 479, Batch: 13, Loss: 0.06732514500617981\n",
      "Iteration 479, Batch: 14, Loss: 0.04647685959935188\n",
      "Iteration 479, Batch: 15, Loss: 0.049026720225811005\n",
      "Iteration 479, Batch: 16, Loss: 0.05371502786874771\n",
      "Iteration 479, Batch: 17, Loss: 0.0565154142677784\n",
      "Iteration 479, Batch: 18, Loss: 0.053202636539936066\n",
      "Iteration 479, Batch: 19, Loss: 0.06284153461456299\n",
      "Iteration 479, Batch: 20, Loss: 0.06339704245328903\n",
      "Iteration 479, Batch: 21, Loss: 0.06556985527276993\n",
      "Iteration 479, Batch: 22, Loss: 0.045665062963962555\n",
      "Iteration 479, Batch: 23, Loss: 0.05307717248797417\n",
      "Iteration 479, Batch: 24, Loss: 0.05522419884800911\n",
      "Iteration 479, Batch: 25, Loss: 0.06948112696409225\n",
      "Iteration 479, Batch: 26, Loss: 0.05306290090084076\n",
      "Iteration 479, Batch: 27, Loss: 0.060055892914533615\n",
      "Iteration 479, Batch: 28, Loss: 0.04474141448736191\n",
      "Iteration 479, Batch: 29, Loss: 0.0726378932595253\n",
      "Iteration 479, Batch: 30, Loss: 0.06232091784477234\n",
      "Iteration 479, Batch: 31, Loss: 0.044250886887311935\n",
      "Iteration 479, Batch: 32, Loss: 0.07844968885183334\n",
      "Iteration 479, Batch: 33, Loss: 0.06869189441204071\n",
      "Iteration 479, Batch: 34, Loss: 0.07402034848928452\n",
      "Iteration 479, Batch: 35, Loss: 0.04999744892120361\n",
      "Iteration 479, Batch: 36, Loss: 0.05987978354096413\n",
      "Iteration 479, Batch: 37, Loss: 0.05657678470015526\n",
      "Iteration 479, Batch: 38, Loss: 0.054434508085250854\n",
      "Iteration 479, Batch: 39, Loss: 0.04076068475842476\n",
      "Iteration 479, Batch: 40, Loss: 0.062021154910326004\n",
      "Iteration 479, Batch: 41, Loss: 0.04086725041270256\n",
      "Iteration 479, Batch: 42, Loss: 0.05056297034025192\n",
      "Iteration 479, Batch: 43, Loss: 0.05576777458190918\n",
      "Iteration 479, Batch: 44, Loss: 0.06651495397090912\n",
      "Iteration 479, Batch: 45, Loss: 0.05682218819856644\n",
      "Iteration 479, Batch: 46, Loss: 0.07039530575275421\n",
      "Iteration 479, Batch: 47, Loss: 0.06076624244451523\n",
      "Iteration 479, Batch: 48, Loss: 0.05128530412912369\n",
      "Iteration 479, Batch: 49, Loss: 0.034299105405807495\n",
      "Number of layers: 10\n",
      "Iteration 480, Batch: 0, Loss: 0.07867938280105591\n",
      "Iteration 480, Batch: 1, Loss: 0.03948385268449783\n",
      "Iteration 480, Batch: 2, Loss: 0.04448547586798668\n",
      "Iteration 480, Batch: 3, Loss: 0.049872297793626785\n",
      "Iteration 480, Batch: 4, Loss: 0.04554826766252518\n",
      "Iteration 480, Batch: 5, Loss: 0.03207512944936752\n",
      "Iteration 480, Batch: 6, Loss: 0.06245068460702896\n",
      "Iteration 480, Batch: 7, Loss: 0.059610918164253235\n",
      "Iteration 480, Batch: 8, Loss: 0.06478352844715118\n",
      "Iteration 480, Batch: 9, Loss: 0.05677085742354393\n",
      "Iteration 480, Batch: 10, Loss: 0.060284119099378586\n",
      "Iteration 480, Batch: 11, Loss: 0.08091934025287628\n",
      "Iteration 480, Batch: 12, Loss: 0.05106497183442116\n",
      "Iteration 480, Batch: 13, Loss: 0.07420125603675842\n",
      "Iteration 480, Batch: 14, Loss: 0.07134900987148285\n",
      "Iteration 480, Batch: 15, Loss: 0.05296315997838974\n",
      "Iteration 480, Batch: 16, Loss: 0.0633847713470459\n",
      "Iteration 480, Batch: 17, Loss: 0.04972011595964432\n",
      "Iteration 480, Batch: 18, Loss: 0.0476989783346653\n",
      "Iteration 480, Batch: 19, Loss: 0.045677389949560165\n",
      "Iteration 480, Batch: 20, Loss: 0.03970706835389137\n",
      "Iteration 480, Batch: 21, Loss: 0.05487213656306267\n",
      "Iteration 480, Batch: 22, Loss: 0.04957372695207596\n",
      "Iteration 480, Batch: 23, Loss: 0.05562277510762215\n",
      "Iteration 480, Batch: 24, Loss: 0.07044955343008041\n",
      "Iteration 480, Batch: 25, Loss: 0.08044479042291641\n",
      "Iteration 480, Batch: 26, Loss: 0.07739488780498505\n",
      "Iteration 480, Batch: 27, Loss: 0.052381277084350586\n",
      "Iteration 480, Batch: 28, Loss: 0.04973907396197319\n",
      "Iteration 480, Batch: 29, Loss: 0.06336945295333862\n",
      "Iteration 480, Batch: 30, Loss: 0.06770674884319305\n",
      "Iteration 480, Batch: 31, Loss: 0.0734604075551033\n",
      "Iteration 480, Batch: 32, Loss: 0.06344333291053772\n",
      "Iteration 480, Batch: 33, Loss: 0.02748885378241539\n",
      "Iteration 480, Batch: 34, Loss: 0.06014860421419144\n",
      "Iteration 480, Batch: 35, Loss: 0.051506269723176956\n",
      "Iteration 480, Batch: 36, Loss: 0.060773856937885284\n",
      "Iteration 480, Batch: 37, Loss: 0.0564408004283905\n",
      "Iteration 480, Batch: 38, Loss: 0.06101473048329353\n",
      "Iteration 480, Batch: 39, Loss: 0.04968029260635376\n",
      "Iteration 480, Batch: 40, Loss: 0.04120390862226486\n",
      "Iteration 480, Batch: 41, Loss: 0.0946417823433876\n",
      "Iteration 480, Batch: 42, Loss: 0.04733027517795563\n",
      "Iteration 480, Batch: 43, Loss: 0.06080375611782074\n",
      "Iteration 480, Batch: 44, Loss: 0.045645587146282196\n",
      "Iteration 480, Batch: 45, Loss: 0.07551097869873047\n",
      "Iteration 480, Batch: 46, Loss: 0.05131296068429947\n",
      "Iteration 480, Batch: 47, Loss: 0.046899184584617615\n",
      "Iteration 480, Batch: 48, Loss: 0.06958381831645966\n",
      "Iteration 480, Batch: 49, Loss: 0.06237483769655228\n",
      "Number of layers: 10\n",
      "Iteration 481, Batch: 0, Loss: 0.06427346169948578\n",
      "Iteration 481, Batch: 1, Loss: 0.07098443061113358\n",
      "Iteration 481, Batch: 2, Loss: 0.058132562786340714\n",
      "Iteration 481, Batch: 3, Loss: 0.04652011767029762\n",
      "Iteration 481, Batch: 4, Loss: 0.048774488270282745\n",
      "Iteration 481, Batch: 5, Loss: 0.050808899104595184\n",
      "Iteration 481, Batch: 6, Loss: 0.07723559439182281\n",
      "Iteration 481, Batch: 7, Loss: 0.06655056029558182\n",
      "Iteration 481, Batch: 8, Loss: 0.07994748651981354\n",
      "Iteration 481, Batch: 9, Loss: 0.06858084350824356\n",
      "Iteration 481, Batch: 10, Loss: 0.06506089866161346\n",
      "Iteration 481, Batch: 11, Loss: 0.05222128704190254\n",
      "Iteration 481, Batch: 12, Loss: 0.06093255802989006\n",
      "Iteration 481, Batch: 13, Loss: 0.059212576597929\n",
      "Iteration 481, Batch: 14, Loss: 0.03484247624874115\n",
      "Iteration 481, Batch: 15, Loss: 0.05940694361925125\n",
      "Iteration 481, Batch: 16, Loss: 0.06816664338111877\n",
      "Iteration 481, Batch: 17, Loss: 0.05064123123884201\n",
      "Iteration 481, Batch: 18, Loss: 0.04760413244366646\n",
      "Iteration 481, Batch: 19, Loss: 0.06477734446525574\n",
      "Iteration 481, Batch: 20, Loss: 0.07565594464540482\n",
      "Iteration 481, Batch: 21, Loss: 0.06058096885681152\n",
      "Iteration 481, Batch: 22, Loss: 0.05645431578159332\n",
      "Iteration 481, Batch: 23, Loss: 0.07268115133047104\n",
      "Iteration 481, Batch: 24, Loss: 0.07949085533618927\n",
      "Iteration 481, Batch: 25, Loss: 0.050872087478637695\n",
      "Iteration 481, Batch: 26, Loss: 0.056672170758247375\n",
      "Iteration 481, Batch: 27, Loss: 0.047613270580768585\n",
      "Iteration 481, Batch: 28, Loss: 0.05066056549549103\n",
      "Iteration 481, Batch: 29, Loss: 0.05442853644490242\n",
      "Iteration 481, Batch: 30, Loss: 0.04684240370988846\n",
      "Iteration 481, Batch: 31, Loss: 0.05221821367740631\n",
      "Iteration 481, Batch: 32, Loss: 0.07040886580944061\n",
      "Iteration 481, Batch: 33, Loss: 0.06304217875003815\n",
      "Iteration 481, Batch: 34, Loss: 0.0805746465921402\n",
      "Iteration 481, Batch: 35, Loss: 0.06105654686689377\n",
      "Iteration 481, Batch: 36, Loss: 0.07888153195381165\n",
      "Iteration 481, Batch: 37, Loss: 0.0905470997095108\n",
      "Iteration 481, Batch: 38, Loss: 0.09243278205394745\n",
      "Iteration 481, Batch: 39, Loss: 0.12551188468933105\n",
      "Iteration 481, Batch: 40, Loss: 0.11929883807897568\n",
      "Iteration 481, Batch: 41, Loss: 0.046885307878255844\n",
      "Iteration 481, Batch: 42, Loss: 0.061286818236112595\n",
      "Iteration 481, Batch: 43, Loss: 0.07579316943883896\n",
      "Iteration 481, Batch: 44, Loss: 0.05016869306564331\n",
      "Iteration 481, Batch: 45, Loss: 0.06075287610292435\n",
      "Iteration 481, Batch: 46, Loss: 0.05749545991420746\n",
      "Iteration 481, Batch: 47, Loss: 0.06429079174995422\n",
      "Iteration 481, Batch: 48, Loss: 0.07104115188121796\n",
      "Iteration 481, Batch: 49, Loss: 0.052099283784627914\n",
      "Number of layers: 10\n",
      "Iteration 482, Batch: 0, Loss: 0.07056229561567307\n",
      "Iteration 482, Batch: 1, Loss: 0.042410582304000854\n",
      "Iteration 482, Batch: 2, Loss: 0.07657282799482346\n",
      "Iteration 482, Batch: 3, Loss: 0.0775798112154007\n",
      "Iteration 482, Batch: 4, Loss: 0.06799877434968948\n",
      "Iteration 482, Batch: 5, Loss: 0.0694350153207779\n",
      "Iteration 482, Batch: 6, Loss: 0.07987330108880997\n",
      "Iteration 482, Batch: 7, Loss: 0.046708982437849045\n",
      "Iteration 482, Batch: 8, Loss: 0.059476982802152634\n",
      "Iteration 482, Batch: 9, Loss: 0.0696520283818245\n",
      "Iteration 482, Batch: 10, Loss: 0.06871718913316727\n",
      "Iteration 482, Batch: 11, Loss: 0.06632430106401443\n",
      "Iteration 482, Batch: 12, Loss: 0.04679998755455017\n",
      "Iteration 482, Batch: 13, Loss: 0.05489252507686615\n",
      "Iteration 482, Batch: 14, Loss: 0.062354497611522675\n",
      "Iteration 482, Batch: 15, Loss: 0.08879753202199936\n",
      "Iteration 482, Batch: 16, Loss: 0.06691721081733704\n",
      "Iteration 482, Batch: 17, Loss: 0.07026467472314835\n",
      "Iteration 482, Batch: 18, Loss: 0.05142568424344063\n",
      "Iteration 482, Batch: 19, Loss: 0.04711694270372391\n",
      "Iteration 482, Batch: 20, Loss: 0.08155316114425659\n",
      "Iteration 482, Batch: 21, Loss: 0.05168260633945465\n",
      "Iteration 482, Batch: 22, Loss: 0.047009143978357315\n",
      "Iteration 482, Batch: 23, Loss: 0.04395399987697601\n",
      "Iteration 482, Batch: 24, Loss: 0.033567145466804504\n",
      "Iteration 482, Batch: 25, Loss: 0.058886222541332245\n",
      "Iteration 482, Batch: 26, Loss: 0.030650019645690918\n",
      "Iteration 482, Batch: 27, Loss: 0.04394202306866646\n",
      "Iteration 482, Batch: 28, Loss: 0.03450006991624832\n",
      "Iteration 482, Batch: 29, Loss: 0.04803727939724922\n",
      "Iteration 482, Batch: 30, Loss: 0.05241312459111214\n",
      "Iteration 482, Batch: 31, Loss: 0.05035249516367912\n",
      "Iteration 482, Batch: 32, Loss: 0.051317520439624786\n",
      "Iteration 482, Batch: 33, Loss: 0.07171720266342163\n",
      "Iteration 482, Batch: 34, Loss: 0.056669048964977264\n",
      "Iteration 482, Batch: 35, Loss: 0.053342096507549286\n",
      "Iteration 482, Batch: 36, Loss: 0.06778913736343384\n",
      "Iteration 482, Batch: 37, Loss: 0.07342104613780975\n",
      "Iteration 482, Batch: 38, Loss: 0.06782982498407364\n",
      "Iteration 482, Batch: 39, Loss: 0.07538161426782608\n",
      "Iteration 482, Batch: 40, Loss: 0.04785577580332756\n",
      "Iteration 482, Batch: 41, Loss: 0.05782392993569374\n",
      "Iteration 482, Batch: 42, Loss: 0.08563661575317383\n",
      "Iteration 482, Batch: 43, Loss: 0.059426553547382355\n",
      "Iteration 482, Batch: 44, Loss: 0.0763903334736824\n",
      "Iteration 482, Batch: 45, Loss: 0.09068088233470917\n",
      "Iteration 482, Batch: 46, Loss: 0.055315617471933365\n",
      "Iteration 482, Batch: 47, Loss: 0.07410472631454468\n",
      "Iteration 482, Batch: 48, Loss: 0.11143888533115387\n",
      "Iteration 482, Batch: 49, Loss: 0.05143359676003456\n",
      "Number of layers: 10\n",
      "Iteration 483, Batch: 0, Loss: 0.09135375916957855\n",
      "Iteration 483, Batch: 1, Loss: 0.050764456391334534\n",
      "Iteration 483, Batch: 2, Loss: 0.02850332111120224\n",
      "Iteration 483, Batch: 3, Loss: 0.03997768461704254\n",
      "Iteration 483, Batch: 4, Loss: 0.05607740581035614\n",
      "Iteration 483, Batch: 5, Loss: 0.06266526877880096\n",
      "Iteration 483, Batch: 6, Loss: 0.06396219879388809\n",
      "Iteration 483, Batch: 7, Loss: 0.05636255070567131\n",
      "Iteration 483, Batch: 8, Loss: 0.07799758017063141\n",
      "Iteration 483, Batch: 9, Loss: 0.09421680122613907\n",
      "Iteration 483, Batch: 10, Loss: 0.04416743293404579\n",
      "Iteration 483, Batch: 11, Loss: 0.044881466776132584\n",
      "Iteration 483, Batch: 12, Loss: 0.05392882227897644\n",
      "Iteration 483, Batch: 13, Loss: 0.05998671427369118\n",
      "Iteration 483, Batch: 14, Loss: 0.05806366726756096\n",
      "Iteration 483, Batch: 15, Loss: 0.05710712820291519\n",
      "Iteration 483, Batch: 16, Loss: 0.045074548572301865\n",
      "Iteration 483, Batch: 17, Loss: 0.055765312165021896\n",
      "Iteration 483, Batch: 18, Loss: 0.06802655756473541\n",
      "Iteration 483, Batch: 19, Loss: 0.08122215420007706\n",
      "Iteration 483, Batch: 20, Loss: 0.04862447455525398\n",
      "Iteration 483, Batch: 21, Loss: 0.043437790125608444\n",
      "Iteration 483, Batch: 22, Loss: 0.04428509622812271\n",
      "Iteration 483, Batch: 23, Loss: 0.06297282874584198\n",
      "Iteration 483, Batch: 24, Loss: 0.054783567786216736\n",
      "Iteration 483, Batch: 25, Loss: 0.04959157854318619\n",
      "Iteration 483, Batch: 26, Loss: 0.09272500872612\n",
      "Iteration 483, Batch: 27, Loss: 0.03901496157050133\n",
      "Iteration 483, Batch: 28, Loss: 0.050662167370319366\n",
      "Iteration 483, Batch: 29, Loss: 0.0734637901186943\n",
      "Iteration 483, Batch: 30, Loss: 0.035924505442380905\n",
      "Iteration 483, Batch: 31, Loss: 0.06559525430202484\n",
      "Iteration 483, Batch: 32, Loss: 0.06573384255170822\n",
      "Iteration 483, Batch: 33, Loss: 0.07241450250148773\n",
      "Iteration 483, Batch: 34, Loss: 0.07118897885084152\n",
      "Iteration 483, Batch: 35, Loss: 0.06819463521242142\n",
      "Iteration 483, Batch: 36, Loss: 0.046592649072408676\n",
      "Iteration 483, Batch: 37, Loss: 0.06157950311899185\n",
      "Iteration 483, Batch: 38, Loss: 0.04654131829738617\n",
      "Iteration 483, Batch: 39, Loss: 0.06515639275312424\n",
      "Iteration 483, Batch: 40, Loss: 0.07171373814344406\n",
      "Iteration 483, Batch: 41, Loss: 0.06061805039644241\n",
      "Iteration 483, Batch: 42, Loss: 0.038712866604328156\n",
      "Iteration 483, Batch: 43, Loss: 0.05575577914714813\n",
      "Iteration 483, Batch: 44, Loss: 0.0464751236140728\n",
      "Iteration 483, Batch: 45, Loss: 0.038927316665649414\n",
      "Iteration 483, Batch: 46, Loss: 0.06031833589076996\n",
      "Iteration 483, Batch: 47, Loss: 0.041273754090070724\n",
      "Iteration 483, Batch: 48, Loss: 0.05114930123090744\n",
      "Iteration 483, Batch: 49, Loss: 0.027809318155050278\n",
      "Number of layers: 10\n",
      "Iteration 484, Batch: 0, Loss: 0.07282651215791702\n",
      "Iteration 484, Batch: 1, Loss: 0.07975361496210098\n",
      "Iteration 484, Batch: 2, Loss: 0.05415111780166626\n",
      "Iteration 484, Batch: 3, Loss: 0.041933078318834305\n",
      "Iteration 484, Batch: 4, Loss: 0.028720466420054436\n",
      "Iteration 484, Batch: 5, Loss: 0.055360082536935806\n",
      "Iteration 484, Batch: 6, Loss: 0.06670980900526047\n",
      "Iteration 484, Batch: 7, Loss: 0.046235039830207825\n",
      "Iteration 484, Batch: 8, Loss: 0.04235474392771721\n",
      "Iteration 484, Batch: 9, Loss: 0.03265329450368881\n",
      "Iteration 484, Batch: 10, Loss: 0.0562174990773201\n",
      "Iteration 484, Batch: 11, Loss: 0.034346792846918106\n",
      "Iteration 484, Batch: 12, Loss: 0.05826776474714279\n",
      "Iteration 484, Batch: 13, Loss: 0.03715088218450546\n",
      "Iteration 484, Batch: 14, Loss: 0.05849693343043327\n",
      "Iteration 484, Batch: 15, Loss: 0.05769353359937668\n",
      "Iteration 484, Batch: 16, Loss: 0.022355632856488228\n",
      "Iteration 484, Batch: 17, Loss: 0.047839488834142685\n",
      "Iteration 484, Batch: 18, Loss: 0.06398570537567139\n",
      "Iteration 484, Batch: 19, Loss: 0.0390743687748909\n",
      "Iteration 484, Batch: 20, Loss: 0.05344444140791893\n",
      "Iteration 484, Batch: 21, Loss: 0.05776423215866089\n",
      "Iteration 484, Batch: 22, Loss: 0.04293156415224075\n",
      "Iteration 484, Batch: 23, Loss: 0.03760448843240738\n",
      "Iteration 484, Batch: 24, Loss: 0.04409250244498253\n",
      "Iteration 484, Batch: 25, Loss: 0.0750935897231102\n",
      "Iteration 484, Batch: 26, Loss: 0.05039804428815842\n",
      "Iteration 484, Batch: 27, Loss: 0.044379640370607376\n",
      "Iteration 484, Batch: 28, Loss: 0.0450408048927784\n",
      "Iteration 484, Batch: 29, Loss: 0.07196655869483948\n",
      "Iteration 484, Batch: 30, Loss: 0.031942080706357956\n",
      "Iteration 484, Batch: 31, Loss: 0.0789208710193634\n",
      "Iteration 484, Batch: 32, Loss: 0.050943925976753235\n",
      "Iteration 484, Batch: 33, Loss: 0.06035543233156204\n",
      "Iteration 484, Batch: 34, Loss: 0.04591534659266472\n",
      "Iteration 484, Batch: 35, Loss: 0.049210723489522934\n",
      "Iteration 484, Batch: 36, Loss: 0.03852890804409981\n",
      "Iteration 484, Batch: 37, Loss: 0.03576607257127762\n",
      "Iteration 484, Batch: 38, Loss: 0.06673493981361389\n",
      "Iteration 484, Batch: 39, Loss: 0.05593603104352951\n",
      "Iteration 484, Batch: 40, Loss: 0.08924791216850281\n",
      "Iteration 484, Batch: 41, Loss: 0.040821537375450134\n",
      "Iteration 484, Batch: 42, Loss: 0.06025455519556999\n",
      "Iteration 484, Batch: 43, Loss: 0.03623666614294052\n",
      "Iteration 484, Batch: 44, Loss: 0.06625035405158997\n",
      "Iteration 484, Batch: 45, Loss: 0.05658050253987312\n",
      "Iteration 484, Batch: 46, Loss: 0.05595855414867401\n",
      "Iteration 484, Batch: 47, Loss: 0.03538841754198074\n",
      "Iteration 484, Batch: 48, Loss: 0.03968217223882675\n",
      "Iteration 484, Batch: 49, Loss: 0.0406993143260479\n",
      "Number of layers: 10\n",
      "Iteration 485, Batch: 0, Loss: 0.037567105144262314\n",
      "Iteration 485, Batch: 1, Loss: 0.06643076986074448\n",
      "Iteration 485, Batch: 2, Loss: 0.03756177797913551\n",
      "Iteration 485, Batch: 3, Loss: 0.05709930136799812\n",
      "Iteration 485, Batch: 4, Loss: 0.054966628551483154\n",
      "Iteration 485, Batch: 5, Loss: 0.10302647948265076\n",
      "Iteration 485, Batch: 6, Loss: 0.06219867616891861\n",
      "Iteration 485, Batch: 7, Loss: 0.05057857558131218\n",
      "Iteration 485, Batch: 8, Loss: 0.0613238699734211\n",
      "Iteration 485, Batch: 9, Loss: 0.06494821608066559\n",
      "Iteration 485, Batch: 10, Loss: 0.0597991980612278\n",
      "Iteration 485, Batch: 11, Loss: 0.056531790643930435\n",
      "Iteration 485, Batch: 12, Loss: 0.058913540095090866\n",
      "Iteration 485, Batch: 13, Loss: 0.07059721648693085\n",
      "Iteration 485, Batch: 14, Loss: 0.054849643260240555\n",
      "Iteration 485, Batch: 15, Loss: 0.07568913698196411\n",
      "Iteration 485, Batch: 16, Loss: 0.04174472764134407\n",
      "Iteration 485, Batch: 17, Loss: 0.05007459595799446\n",
      "Iteration 485, Batch: 18, Loss: 0.04622255265712738\n",
      "Iteration 485, Batch: 19, Loss: 0.02716190367937088\n",
      "Iteration 485, Batch: 20, Loss: 0.05177256464958191\n",
      "Iteration 485, Batch: 21, Loss: 0.06334776431322098\n",
      "Iteration 485, Batch: 22, Loss: 0.05136902257800102\n",
      "Iteration 485, Batch: 23, Loss: 0.052114762365818024\n",
      "Iteration 485, Batch: 24, Loss: 0.07291888445615768\n",
      "Iteration 485, Batch: 25, Loss: 0.06367197632789612\n",
      "Iteration 485, Batch: 26, Loss: 0.03943971171975136\n",
      "Iteration 485, Batch: 27, Loss: 0.05261385813355446\n",
      "Iteration 485, Batch: 28, Loss: 0.07359464466571808\n",
      "Iteration 485, Batch: 29, Loss: 0.07595870643854141\n",
      "Iteration 485, Batch: 30, Loss: 0.040207020938396454\n",
      "Iteration 485, Batch: 31, Loss: 0.05526247248053551\n",
      "Iteration 485, Batch: 32, Loss: 0.0401264913380146\n",
      "Iteration 485, Batch: 33, Loss: 0.07068639248609543\n",
      "Iteration 485, Batch: 34, Loss: 0.04505312442779541\n",
      "Iteration 485, Batch: 35, Loss: 0.044361475855112076\n",
      "Iteration 485, Batch: 36, Loss: 0.0795682892203331\n",
      "Iteration 485, Batch: 37, Loss: 0.03481675684452057\n",
      "Iteration 485, Batch: 38, Loss: 0.027366885915398598\n",
      "Iteration 485, Batch: 39, Loss: 0.03963408246636391\n",
      "Iteration 485, Batch: 40, Loss: 0.053199250251054764\n",
      "Iteration 485, Batch: 41, Loss: 0.03732035681605339\n",
      "Iteration 485, Batch: 42, Loss: 0.057715222239494324\n",
      "Iteration 485, Batch: 43, Loss: 0.06040845438838005\n",
      "Iteration 485, Batch: 44, Loss: 0.04207474738359451\n",
      "Iteration 485, Batch: 45, Loss: 0.04556024447083473\n",
      "Iteration 485, Batch: 46, Loss: 0.06089331954717636\n",
      "Iteration 485, Batch: 47, Loss: 0.05070706084370613\n",
      "Iteration 485, Batch: 48, Loss: 0.05706709623336792\n",
      "Iteration 485, Batch: 49, Loss: 0.05599622428417206\n",
      "Number of layers: 10\n",
      "Iteration 486, Batch: 0, Loss: 0.057589367032051086\n",
      "Iteration 486, Batch: 1, Loss: 0.06716489046812057\n",
      "Iteration 486, Batch: 2, Loss: 0.0785108283162117\n",
      "Iteration 486, Batch: 3, Loss: 0.07716032862663269\n",
      "Iteration 486, Batch: 4, Loss: 0.0667833760380745\n",
      "Iteration 486, Batch: 5, Loss: 0.06368108838796616\n",
      "Iteration 486, Batch: 6, Loss: 0.03918597474694252\n",
      "Iteration 486, Batch: 7, Loss: 0.061606910079717636\n",
      "Iteration 486, Batch: 8, Loss: 0.07433509826660156\n",
      "Iteration 486, Batch: 9, Loss: 0.09242337942123413\n",
      "Iteration 486, Batch: 10, Loss: 0.04914088547229767\n",
      "Iteration 486, Batch: 11, Loss: 0.0791025161743164\n",
      "Iteration 486, Batch: 12, Loss: 0.05376315116882324\n",
      "Iteration 486, Batch: 13, Loss: 0.03351892903447151\n",
      "Iteration 486, Batch: 14, Loss: 0.06471095979213715\n",
      "Iteration 486, Batch: 15, Loss: 0.05040145292878151\n",
      "Iteration 486, Batch: 16, Loss: 0.0704902932047844\n",
      "Iteration 486, Batch: 17, Loss: 0.0732409805059433\n",
      "Iteration 486, Batch: 18, Loss: 0.04061629995703697\n",
      "Iteration 486, Batch: 19, Loss: 0.0793774276971817\n",
      "Iteration 486, Batch: 20, Loss: 0.05710479989647865\n",
      "Iteration 486, Batch: 21, Loss: 0.035458292812108994\n",
      "Iteration 486, Batch: 22, Loss: 0.05592344328761101\n",
      "Iteration 486, Batch: 23, Loss: 0.0534968264400959\n",
      "Iteration 486, Batch: 24, Loss: 0.051146164536476135\n",
      "Iteration 486, Batch: 25, Loss: 0.03992133587598801\n",
      "Iteration 486, Batch: 26, Loss: 0.05121118575334549\n",
      "Iteration 486, Batch: 27, Loss: 0.07104659080505371\n",
      "Iteration 486, Batch: 28, Loss: 0.0775577500462532\n",
      "Iteration 486, Batch: 29, Loss: 0.07243456691503525\n",
      "Iteration 486, Batch: 30, Loss: 0.05364290997385979\n",
      "Iteration 486, Batch: 31, Loss: 0.03122422844171524\n",
      "Iteration 486, Batch: 32, Loss: 0.07532978057861328\n",
      "Iteration 486, Batch: 33, Loss: 0.06872927397489548\n",
      "Iteration 486, Batch: 34, Loss: 0.06639967113733292\n",
      "Iteration 486, Batch: 35, Loss: 0.03552847355604172\n",
      "Iteration 486, Batch: 36, Loss: 0.06389129161834717\n",
      "Iteration 486, Batch: 37, Loss: 0.0331612229347229\n",
      "Iteration 486, Batch: 38, Loss: 0.06244192644953728\n",
      "Iteration 486, Batch: 39, Loss: 0.04740350693464279\n",
      "Iteration 486, Batch: 40, Loss: 0.035609081387519836\n",
      "Iteration 486, Batch: 41, Loss: 0.04732735455036163\n",
      "Iteration 486, Batch: 42, Loss: 0.04520689323544502\n",
      "Iteration 486, Batch: 43, Loss: 0.06488042324781418\n",
      "Iteration 486, Batch: 44, Loss: 0.07155716419219971\n",
      "Iteration 486, Batch: 45, Loss: 0.04028895124793053\n",
      "Iteration 486, Batch: 46, Loss: 0.054479245096445084\n",
      "Iteration 486, Batch: 47, Loss: 0.07118640840053558\n",
      "Iteration 486, Batch: 48, Loss: 0.05578719452023506\n",
      "Iteration 486, Batch: 49, Loss: 0.06293529272079468\n",
      "Number of layers: 10\n",
      "Iteration 487, Batch: 0, Loss: 0.05416170135140419\n",
      "Iteration 487, Batch: 1, Loss: 0.027222007513046265\n",
      "Iteration 487, Batch: 2, Loss: 0.09522136300802231\n",
      "Iteration 487, Batch: 3, Loss: 0.04761282354593277\n",
      "Iteration 487, Batch: 4, Loss: 0.05895980820059776\n",
      "Iteration 487, Batch: 5, Loss: 0.05560881644487381\n",
      "Iteration 487, Batch: 6, Loss: 0.06231749430298805\n",
      "Iteration 487, Batch: 7, Loss: 0.057225119322538376\n",
      "Iteration 487, Batch: 8, Loss: 0.026074010878801346\n",
      "Iteration 487, Batch: 9, Loss: 0.06798367202281952\n",
      "Iteration 487, Batch: 10, Loss: 0.07400825619697571\n",
      "Iteration 487, Batch: 11, Loss: 0.0643407329916954\n",
      "Iteration 487, Batch: 12, Loss: 0.04710608720779419\n",
      "Iteration 487, Batch: 13, Loss: 0.048867274075746536\n",
      "Iteration 487, Batch: 14, Loss: 0.05518249422311783\n",
      "Iteration 487, Batch: 15, Loss: 0.060742463916540146\n",
      "Iteration 487, Batch: 16, Loss: 0.041074227541685104\n",
      "Iteration 487, Batch: 17, Loss: 0.039897371083498\n",
      "Iteration 487, Batch: 18, Loss: 0.03906979784369469\n",
      "Iteration 487, Batch: 19, Loss: 0.05672994628548622\n",
      "Iteration 487, Batch: 20, Loss: 0.055643681436777115\n",
      "Iteration 487, Batch: 21, Loss: 0.0305645652115345\n",
      "Iteration 487, Batch: 22, Loss: 0.0449538417160511\n",
      "Iteration 487, Batch: 23, Loss: 0.05912009999155998\n",
      "Iteration 487, Batch: 24, Loss: 0.04637444391846657\n",
      "Iteration 487, Batch: 25, Loss: 0.050137415528297424\n",
      "Iteration 487, Batch: 26, Loss: 0.07071021944284439\n",
      "Iteration 487, Batch: 27, Loss: 0.1049412190914154\n",
      "Iteration 487, Batch: 28, Loss: 0.08670617640018463\n",
      "Iteration 487, Batch: 29, Loss: 0.10488929599523544\n",
      "Iteration 487, Batch: 30, Loss: 0.07379407435655594\n",
      "Iteration 487, Batch: 31, Loss: 0.08216358721256256\n",
      "Iteration 487, Batch: 32, Loss: 0.07707048207521439\n",
      "Iteration 487, Batch: 33, Loss: 0.08980496972799301\n",
      "Iteration 487, Batch: 34, Loss: 0.06848350167274475\n",
      "Iteration 487, Batch: 35, Loss: 0.05646146088838577\n",
      "Iteration 487, Batch: 36, Loss: 0.045265525579452515\n",
      "Iteration 487, Batch: 37, Loss: 0.09471510350704193\n",
      "Iteration 487, Batch: 38, Loss: 0.05633949488401413\n",
      "Iteration 487, Batch: 39, Loss: 0.06601201742887497\n",
      "Iteration 487, Batch: 40, Loss: 0.03757724538445473\n",
      "Iteration 487, Batch: 41, Loss: 0.046573132276535034\n",
      "Iteration 487, Batch: 42, Loss: 0.03949182480573654\n",
      "Iteration 487, Batch: 43, Loss: 0.09024644643068314\n",
      "Iteration 487, Batch: 44, Loss: 0.062157828360795975\n",
      "Iteration 487, Batch: 45, Loss: 0.0565810464322567\n",
      "Iteration 487, Batch: 46, Loss: 0.05497404932975769\n",
      "Iteration 487, Batch: 47, Loss: 0.0811537653207779\n",
      "Iteration 487, Batch: 48, Loss: 0.0543658621609211\n",
      "Iteration 487, Batch: 49, Loss: 0.059709738940000534\n",
      "Number of layers: 10\n",
      "Iteration 488, Batch: 0, Loss: 0.06285512447357178\n",
      "Iteration 488, Batch: 1, Loss: 0.05156845599412918\n",
      "Iteration 488, Batch: 2, Loss: 0.06016300991177559\n",
      "Iteration 488, Batch: 3, Loss: 0.02108444646000862\n",
      "Iteration 488, Batch: 4, Loss: 0.055424366146326065\n",
      "Iteration 488, Batch: 5, Loss: 0.037527989596128464\n",
      "Iteration 488, Batch: 6, Loss: 0.07548262178897858\n",
      "Iteration 488, Batch: 7, Loss: 0.032166801393032074\n",
      "Iteration 488, Batch: 8, Loss: 0.05743879824876785\n",
      "Iteration 488, Batch: 9, Loss: 0.04473619535565376\n",
      "Iteration 488, Batch: 10, Loss: 0.03546183556318283\n",
      "Iteration 488, Batch: 11, Loss: 0.043665170669555664\n",
      "Iteration 488, Batch: 12, Loss: 0.04066971689462662\n",
      "Iteration 488, Batch: 13, Loss: 0.03812547028064728\n",
      "Iteration 488, Batch: 14, Loss: 0.0395110584795475\n",
      "Iteration 488, Batch: 15, Loss: 0.03499249741435051\n",
      "Iteration 488, Batch: 16, Loss: 0.06650633364915848\n",
      "Iteration 488, Batch: 17, Loss: 0.04592135176062584\n",
      "Iteration 488, Batch: 18, Loss: 0.031295765191316605\n",
      "Iteration 488, Batch: 19, Loss: 0.05035252124071121\n",
      "Iteration 488, Batch: 20, Loss: 0.039268866181373596\n",
      "Iteration 488, Batch: 21, Loss: 0.07471923530101776\n",
      "Iteration 488, Batch: 22, Loss: 0.07942577451467514\n",
      "Iteration 488, Batch: 23, Loss: 0.051702503114938736\n",
      "Iteration 488, Batch: 24, Loss: 0.08380305767059326\n",
      "Iteration 488, Batch: 25, Loss: 0.054130759090185165\n",
      "Iteration 488, Batch: 26, Loss: 0.03803673014044762\n",
      "Iteration 488, Batch: 27, Loss: 0.039643023163080215\n",
      "Iteration 488, Batch: 28, Loss: 0.046725787222385406\n",
      "Iteration 488, Batch: 29, Loss: 0.04660413786768913\n",
      "Iteration 488, Batch: 30, Loss: 0.05282784253358841\n",
      "Iteration 488, Batch: 31, Loss: 0.04843847081065178\n",
      "Iteration 488, Batch: 32, Loss: 0.05055953934788704\n",
      "Iteration 488, Batch: 33, Loss: 0.05744169279932976\n",
      "Iteration 488, Batch: 34, Loss: 0.05229289084672928\n",
      "Iteration 488, Batch: 35, Loss: 0.05004813149571419\n",
      "Iteration 488, Batch: 36, Loss: 0.030897177755832672\n",
      "Iteration 488, Batch: 37, Loss: 0.06715655326843262\n",
      "Iteration 488, Batch: 38, Loss: 0.07791907340288162\n",
      "Iteration 488, Batch: 39, Loss: 0.05755576491355896\n",
      "Iteration 488, Batch: 40, Loss: 0.03891812637448311\n",
      "Iteration 488, Batch: 41, Loss: 0.07052148878574371\n",
      "Iteration 488, Batch: 42, Loss: 0.048995837569236755\n",
      "Iteration 488, Batch: 43, Loss: 0.05835239589214325\n",
      "Iteration 488, Batch: 44, Loss: 0.06799399107694626\n",
      "Iteration 488, Batch: 45, Loss: 0.08436622470617294\n",
      "Iteration 488, Batch: 46, Loss: 0.11993028223514557\n",
      "Iteration 488, Batch: 47, Loss: 0.05963198468089104\n",
      "Iteration 488, Batch: 48, Loss: 0.08804447203874588\n",
      "Iteration 488, Batch: 49, Loss: 0.10173708945512772\n",
      "Number of layers: 10\n",
      "Iteration 489, Batch: 0, Loss: 0.0713486596941948\n",
      "Iteration 489, Batch: 1, Loss: 0.082930788397789\n",
      "Iteration 489, Batch: 2, Loss: 0.11147037148475647\n",
      "Iteration 489, Batch: 3, Loss: 0.020496148616075516\n",
      "Iteration 489, Batch: 4, Loss: 0.06539342552423477\n",
      "Iteration 489, Batch: 5, Loss: 0.05857367441058159\n",
      "Iteration 489, Batch: 6, Loss: 0.07027255743741989\n",
      "Iteration 489, Batch: 7, Loss: 0.07768861949443817\n",
      "Iteration 489, Batch: 8, Loss: 0.04594302922487259\n",
      "Iteration 489, Batch: 9, Loss: 0.04474872350692749\n",
      "Iteration 489, Batch: 10, Loss: 0.044315170496702194\n",
      "Iteration 489, Batch: 11, Loss: 0.04051493853330612\n",
      "Iteration 489, Batch: 12, Loss: 0.05272163450717926\n",
      "Iteration 489, Batch: 13, Loss: 0.08269306272268295\n",
      "Iteration 489, Batch: 14, Loss: 0.04918627813458443\n",
      "Iteration 489, Batch: 15, Loss: 0.05723050981760025\n",
      "Iteration 489, Batch: 16, Loss: 0.04179418459534645\n",
      "Iteration 489, Batch: 17, Loss: 0.05729859694838524\n",
      "Iteration 489, Batch: 18, Loss: 0.04333357885479927\n",
      "Iteration 489, Batch: 19, Loss: 0.07963995635509491\n",
      "Iteration 489, Batch: 20, Loss: 0.039551008492708206\n",
      "Iteration 489, Batch: 21, Loss: 0.03837062790989876\n",
      "Iteration 489, Batch: 22, Loss: 0.0656820759177208\n",
      "Iteration 489, Batch: 23, Loss: 0.061696894466876984\n",
      "Iteration 489, Batch: 24, Loss: 0.09076503664255142\n",
      "Iteration 489, Batch: 25, Loss: 0.0708816722035408\n",
      "Iteration 489, Batch: 26, Loss: 0.057042159140110016\n",
      "Iteration 489, Batch: 27, Loss: 0.0397530272603035\n",
      "Iteration 489, Batch: 28, Loss: 0.0490749254822731\n",
      "Iteration 489, Batch: 29, Loss: 0.0428345762193203\n",
      "Iteration 489, Batch: 30, Loss: 0.0728011503815651\n",
      "Iteration 489, Batch: 31, Loss: 0.05956459417939186\n",
      "Iteration 489, Batch: 32, Loss: 0.08219533413648605\n",
      "Iteration 489, Batch: 33, Loss: 0.06823278963565826\n",
      "Iteration 489, Batch: 34, Loss: 0.06332080066204071\n",
      "Iteration 489, Batch: 35, Loss: 0.029549842700362206\n",
      "Iteration 489, Batch: 36, Loss: 0.06502921134233475\n",
      "Iteration 489, Batch: 37, Loss: 0.054176051169633865\n",
      "Iteration 489, Batch: 38, Loss: 0.056618183851242065\n",
      "Iteration 489, Batch: 39, Loss: 0.0602417066693306\n",
      "Iteration 489, Batch: 40, Loss: 0.062134310603141785\n",
      "Iteration 489, Batch: 41, Loss: 0.0286907609552145\n",
      "Iteration 489, Batch: 42, Loss: 0.07115446776151657\n",
      "Iteration 489, Batch: 43, Loss: 0.06161903217434883\n",
      "Iteration 489, Batch: 44, Loss: 0.04323645681142807\n",
      "Iteration 489, Batch: 45, Loss: 0.05057405307888985\n",
      "Iteration 489, Batch: 46, Loss: 0.05002611130475998\n",
      "Iteration 489, Batch: 47, Loss: 0.05606498941779137\n",
      "Iteration 489, Batch: 48, Loss: 0.051404137164354324\n",
      "Iteration 489, Batch: 49, Loss: 0.0688566267490387\n",
      "Number of layers: 10\n",
      "Iteration 490, Batch: 0, Loss: 0.04637549817562103\n",
      "Iteration 490, Batch: 1, Loss: 0.037255819886922836\n",
      "Iteration 490, Batch: 2, Loss: 0.05382167547941208\n",
      "Iteration 490, Batch: 3, Loss: 0.046157605946063995\n",
      "Iteration 490, Batch: 4, Loss: 0.07669589668512344\n",
      "Iteration 490, Batch: 5, Loss: 0.06565339863300323\n",
      "Iteration 490, Batch: 6, Loss: 0.04008723050355911\n",
      "Iteration 490, Batch: 7, Loss: 0.06407661736011505\n",
      "Iteration 490, Batch: 8, Loss: 0.068985715508461\n",
      "Iteration 490, Batch: 9, Loss: 0.08688109368085861\n",
      "Iteration 490, Batch: 10, Loss: 0.04155873507261276\n",
      "Iteration 490, Batch: 11, Loss: 0.05756567791104317\n",
      "Iteration 490, Batch: 12, Loss: 0.052537668496370316\n",
      "Iteration 490, Batch: 13, Loss: 0.06436249613761902\n",
      "Iteration 490, Batch: 14, Loss: 0.07142456620931625\n",
      "Iteration 490, Batch: 15, Loss: 0.07247294485569\n",
      "Iteration 490, Batch: 16, Loss: 0.10590721666812897\n",
      "Iteration 490, Batch: 17, Loss: 0.08405370265245438\n",
      "Iteration 490, Batch: 18, Loss: 0.09316196292638779\n",
      "Iteration 490, Batch: 19, Loss: 0.06302624195814133\n",
      "Iteration 490, Batch: 20, Loss: 0.0780501663684845\n",
      "Iteration 490, Batch: 21, Loss: 0.06873132288455963\n",
      "Iteration 490, Batch: 22, Loss: 0.06979526579380035\n",
      "Iteration 490, Batch: 23, Loss: 0.06640242040157318\n",
      "Iteration 490, Batch: 24, Loss: 0.05219366028904915\n",
      "Iteration 490, Batch: 25, Loss: 0.04414999857544899\n",
      "Iteration 490, Batch: 26, Loss: 0.08076049387454987\n",
      "Iteration 490, Batch: 27, Loss: 0.041103143244981766\n",
      "Iteration 490, Batch: 28, Loss: 0.06953486055135727\n",
      "Iteration 490, Batch: 29, Loss: 0.07780926674604416\n",
      "Iteration 490, Batch: 30, Loss: 0.06341200321912766\n",
      "Iteration 490, Batch: 31, Loss: 0.04209772124886513\n",
      "Iteration 490, Batch: 32, Loss: 0.06940989196300507\n",
      "Iteration 490, Batch: 33, Loss: 0.04382883012294769\n",
      "Iteration 490, Batch: 34, Loss: 0.0540187731385231\n",
      "Iteration 490, Batch: 35, Loss: 0.024665450677275658\n",
      "Iteration 490, Batch: 36, Loss: 0.04148491844534874\n",
      "Iteration 490, Batch: 37, Loss: 0.04476603493094444\n",
      "Iteration 490, Batch: 38, Loss: 0.03796923905611038\n",
      "Iteration 490, Batch: 39, Loss: 0.050650160759687424\n",
      "Iteration 490, Batch: 40, Loss: 0.05660419911146164\n",
      "Iteration 490, Batch: 41, Loss: 0.056171998381614685\n",
      "Iteration 490, Batch: 42, Loss: 0.03191016614437103\n",
      "Iteration 490, Batch: 43, Loss: 0.05515594407916069\n",
      "Iteration 490, Batch: 44, Loss: 0.04764090105891228\n",
      "Iteration 490, Batch: 45, Loss: 0.06593340635299683\n",
      "Iteration 490, Batch: 46, Loss: 0.041672006249427795\n",
      "Iteration 490, Batch: 47, Loss: 0.04579012840986252\n",
      "Iteration 490, Batch: 48, Loss: 0.06158595532178879\n",
      "Iteration 490, Batch: 49, Loss: 0.06576747447252274\n",
      "Number of layers: 10\n",
      "Iteration 491, Batch: 0, Loss: 0.044870756566524506\n",
      "Iteration 491, Batch: 1, Loss: 0.057768791913986206\n",
      "Iteration 491, Batch: 2, Loss: 0.05056503042578697\n",
      "Iteration 491, Batch: 3, Loss: 0.05702579766511917\n",
      "Iteration 491, Batch: 4, Loss: 0.06717676669359207\n",
      "Iteration 491, Batch: 5, Loss: 0.06810294091701508\n",
      "Iteration 491, Batch: 6, Loss: 0.05307570844888687\n",
      "Iteration 491, Batch: 7, Loss: 0.037336453795433044\n",
      "Iteration 491, Batch: 8, Loss: 0.039514414966106415\n",
      "Iteration 491, Batch: 9, Loss: 0.07913478463888168\n",
      "Iteration 491, Batch: 10, Loss: 0.04252001643180847\n",
      "Iteration 491, Batch: 11, Loss: 0.06440896540880203\n",
      "Iteration 491, Batch: 12, Loss: 0.06654887646436691\n",
      "Iteration 491, Batch: 13, Loss: 0.06075412780046463\n",
      "Iteration 491, Batch: 14, Loss: 0.051192980259656906\n",
      "Iteration 491, Batch: 15, Loss: 0.05475449562072754\n",
      "Iteration 491, Batch: 16, Loss: 0.06428050994873047\n",
      "Iteration 491, Batch: 17, Loss: 0.05504145100712776\n",
      "Iteration 491, Batch: 18, Loss: 0.07059407979249954\n",
      "Iteration 491, Batch: 19, Loss: 0.06390167772769928\n",
      "Iteration 491, Batch: 20, Loss: 0.06763701885938644\n",
      "Iteration 491, Batch: 21, Loss: 0.07568253576755524\n",
      "Iteration 491, Batch: 22, Loss: 0.051079317927360535\n",
      "Iteration 491, Batch: 23, Loss: 0.038893070071935654\n",
      "Iteration 491, Batch: 24, Loss: 0.03750526160001755\n",
      "Iteration 491, Batch: 25, Loss: 0.040519677102565765\n",
      "Iteration 491, Batch: 26, Loss: 0.04999902471899986\n",
      "Iteration 491, Batch: 27, Loss: 0.052979037165641785\n",
      "Iteration 491, Batch: 28, Loss: 0.04950796067714691\n",
      "Iteration 491, Batch: 29, Loss: 0.05913137272000313\n",
      "Iteration 491, Batch: 30, Loss: 0.057558316737413406\n",
      "Iteration 491, Batch: 31, Loss: 0.05260390043258667\n",
      "Iteration 491, Batch: 32, Loss: 0.0594942569732666\n",
      "Iteration 491, Batch: 33, Loss: 0.06311029940843582\n",
      "Iteration 491, Batch: 34, Loss: 0.07456553727388382\n",
      "Iteration 491, Batch: 35, Loss: 0.07158952206373215\n",
      "Iteration 491, Batch: 36, Loss: 0.07284680008888245\n",
      "Iteration 491, Batch: 37, Loss: 0.03860190510749817\n",
      "Iteration 491, Batch: 38, Loss: 0.06012069433927536\n",
      "Iteration 491, Batch: 39, Loss: 0.08037294447422028\n",
      "Iteration 491, Batch: 40, Loss: 0.04736107587814331\n",
      "Iteration 491, Batch: 41, Loss: 0.07896112650632858\n",
      "Iteration 491, Batch: 42, Loss: 0.07718311250209808\n",
      "Iteration 491, Batch: 43, Loss: 0.04473288357257843\n",
      "Iteration 491, Batch: 44, Loss: 0.06523822993040085\n",
      "Iteration 491, Batch: 45, Loss: 0.04072484001517296\n",
      "Iteration 491, Batch: 46, Loss: 0.03525709733366966\n",
      "Iteration 491, Batch: 47, Loss: 0.04445842653512955\n",
      "Iteration 491, Batch: 48, Loss: 0.04073936864733696\n",
      "Iteration 491, Batch: 49, Loss: 0.030346058309078217\n",
      "Number of layers: 10\n",
      "Iteration 492, Batch: 0, Loss: 0.03672940284013748\n",
      "Iteration 492, Batch: 1, Loss: 0.05484738200902939\n",
      "Iteration 492, Batch: 2, Loss: 0.03925318643450737\n",
      "Iteration 492, Batch: 3, Loss: 0.027166999876499176\n",
      "Iteration 492, Batch: 4, Loss: 0.04297197237610817\n",
      "Iteration 492, Batch: 5, Loss: 0.052019085735082626\n",
      "Iteration 492, Batch: 6, Loss: 0.03126377984881401\n",
      "Iteration 492, Batch: 7, Loss: 0.04667506739497185\n",
      "Iteration 492, Batch: 8, Loss: 0.09123759716749191\n",
      "Iteration 492, Batch: 9, Loss: 0.0274204108864069\n",
      "Iteration 492, Batch: 10, Loss: 0.07065831124782562\n",
      "Iteration 492, Batch: 11, Loss: 0.04412740096449852\n",
      "Iteration 492, Batch: 12, Loss: 0.06488647311925888\n",
      "Iteration 492, Batch: 13, Loss: 0.03803979977965355\n",
      "Iteration 492, Batch: 14, Loss: 0.05800171568989754\n",
      "Iteration 492, Batch: 15, Loss: 0.05253772437572479\n",
      "Iteration 492, Batch: 16, Loss: 0.04151761531829834\n",
      "Iteration 492, Batch: 17, Loss: 0.0644523873925209\n",
      "Iteration 492, Batch: 18, Loss: 0.043834514915943146\n",
      "Iteration 492, Batch: 19, Loss: 0.06882119178771973\n",
      "Iteration 492, Batch: 20, Loss: 0.03020363673567772\n",
      "Iteration 492, Batch: 21, Loss: 0.06683456152677536\n",
      "Iteration 492, Batch: 22, Loss: 0.05369482934474945\n",
      "Iteration 492, Batch: 23, Loss: 0.04923255741596222\n",
      "Iteration 492, Batch: 24, Loss: 0.06574898213148117\n",
      "Iteration 492, Batch: 25, Loss: 0.03522894158959389\n",
      "Iteration 492, Batch: 26, Loss: 0.05933485925197601\n",
      "Iteration 492, Batch: 27, Loss: 0.07422444969415665\n",
      "Iteration 492, Batch: 28, Loss: 0.06417397409677505\n",
      "Iteration 492, Batch: 29, Loss: 0.04726570472121239\n",
      "Iteration 492, Batch: 30, Loss: 0.04940512031316757\n",
      "Iteration 492, Batch: 31, Loss: 0.036238279193639755\n",
      "Iteration 492, Batch: 32, Loss: 0.04271659627556801\n",
      "Iteration 492, Batch: 33, Loss: 0.032500095665454865\n",
      "Iteration 492, Batch: 34, Loss: 0.05043098330497742\n",
      "Iteration 492, Batch: 35, Loss: 0.06007048115134239\n",
      "Iteration 492, Batch: 36, Loss: 0.07731834799051285\n",
      "Iteration 492, Batch: 37, Loss: 0.040979254990816116\n",
      "Iteration 492, Batch: 38, Loss: 0.04910590127110481\n",
      "Iteration 492, Batch: 39, Loss: 0.03918818011879921\n",
      "Iteration 492, Batch: 40, Loss: 0.0639965683221817\n",
      "Iteration 492, Batch: 41, Loss: 0.0945419892668724\n",
      "Iteration 492, Batch: 42, Loss: 0.04368866980075836\n",
      "Iteration 492, Batch: 43, Loss: 0.061260536313056946\n",
      "Iteration 492, Batch: 44, Loss: 0.05338447540998459\n",
      "Iteration 492, Batch: 45, Loss: 0.06065842881798744\n",
      "Iteration 492, Batch: 46, Loss: 0.04705251753330231\n",
      "Iteration 492, Batch: 47, Loss: 0.0415726937353611\n",
      "Iteration 492, Batch: 48, Loss: 0.041496939957141876\n",
      "Iteration 492, Batch: 49, Loss: 0.03704558312892914\n",
      "Number of layers: 10\n",
      "Iteration 493, Batch: 0, Loss: 0.053036879748106\n",
      "Iteration 493, Batch: 1, Loss: 0.06859641522169113\n",
      "Iteration 493, Batch: 2, Loss: 0.04352083057165146\n",
      "Iteration 493, Batch: 3, Loss: 0.04169629514217377\n",
      "Iteration 493, Batch: 4, Loss: 0.04203512892127037\n",
      "Iteration 493, Batch: 5, Loss: 0.05911334976553917\n",
      "Iteration 493, Batch: 6, Loss: 0.05674463137984276\n",
      "Iteration 493, Batch: 7, Loss: 0.04510444030165672\n",
      "Iteration 493, Batch: 8, Loss: 0.04422007501125336\n",
      "Iteration 493, Batch: 9, Loss: 0.055218715220689774\n",
      "Iteration 493, Batch: 10, Loss: 0.05028093606233597\n",
      "Iteration 493, Batch: 11, Loss: 0.0708741545677185\n",
      "Iteration 493, Batch: 12, Loss: 0.053076598793268204\n",
      "Iteration 493, Batch: 13, Loss: 0.0547352060675621\n",
      "Iteration 493, Batch: 14, Loss: 0.031212124973535538\n",
      "Iteration 493, Batch: 15, Loss: 0.06126374006271362\n",
      "Iteration 493, Batch: 16, Loss: 0.020318949595093727\n",
      "Iteration 493, Batch: 17, Loss: 0.04150352254509926\n",
      "Iteration 493, Batch: 18, Loss: 0.04338108003139496\n",
      "Iteration 493, Batch: 19, Loss: 0.07792423665523529\n",
      "Iteration 493, Batch: 20, Loss: 0.043039314448833466\n",
      "Iteration 493, Batch: 21, Loss: 0.07200279086828232\n",
      "Iteration 493, Batch: 22, Loss: 0.05285939574241638\n",
      "Iteration 493, Batch: 23, Loss: 0.049148108810186386\n",
      "Iteration 493, Batch: 24, Loss: 0.052685994654893875\n",
      "Iteration 493, Batch: 25, Loss: 0.041075341403484344\n",
      "Iteration 493, Batch: 26, Loss: 0.0697459951043129\n",
      "Iteration 493, Batch: 27, Loss: 0.028439335525035858\n",
      "Iteration 493, Batch: 28, Loss: 0.05998905003070831\n",
      "Iteration 493, Batch: 29, Loss: 0.0510280504822731\n",
      "Iteration 493, Batch: 30, Loss: 0.04751267284154892\n",
      "Iteration 493, Batch: 31, Loss: 0.07094541937112808\n",
      "Iteration 493, Batch: 32, Loss: 0.024278607219457626\n",
      "Iteration 493, Batch: 33, Loss: 0.0442282110452652\n",
      "Iteration 493, Batch: 34, Loss: 0.027307510375976562\n",
      "Iteration 493, Batch: 35, Loss: 0.06730310618877411\n",
      "Iteration 493, Batch: 36, Loss: 0.02335548587143421\n",
      "Iteration 493, Batch: 37, Loss: 0.03670912981033325\n",
      "Iteration 493, Batch: 38, Loss: 0.031168803572654724\n",
      "Iteration 493, Batch: 39, Loss: 0.04910419508814812\n",
      "Iteration 493, Batch: 40, Loss: 0.03568112105131149\n",
      "Iteration 493, Batch: 41, Loss: 0.043149758130311966\n",
      "Iteration 493, Batch: 42, Loss: 0.06482896953821182\n",
      "Iteration 493, Batch: 43, Loss: 0.0904608964920044\n",
      "Iteration 493, Batch: 44, Loss: 0.03345286846160889\n",
      "Iteration 493, Batch: 45, Loss: 0.04495825245976448\n",
      "Iteration 493, Batch: 46, Loss: 0.038654625415802\n",
      "Iteration 493, Batch: 47, Loss: 0.04713891074061394\n",
      "Iteration 493, Batch: 48, Loss: 0.05157652869820595\n",
      "Iteration 493, Batch: 49, Loss: 0.05820332467556\n",
      "Number of layers: 10\n",
      "Iteration 494, Batch: 0, Loss: 0.051760654896497726\n",
      "Iteration 494, Batch: 1, Loss: 0.03782859817147255\n",
      "Iteration 494, Batch: 2, Loss: 0.058252446353435516\n",
      "Iteration 494, Batch: 3, Loss: 0.08039865642786026\n",
      "Iteration 494, Batch: 4, Loss: 0.045423079282045364\n",
      "Iteration 494, Batch: 5, Loss: 0.05014900118112564\n",
      "Iteration 494, Batch: 6, Loss: 0.05164550989866257\n",
      "Iteration 494, Batch: 7, Loss: 0.03687269240617752\n",
      "Iteration 494, Batch: 8, Loss: 0.058125585317611694\n",
      "Iteration 494, Batch: 9, Loss: 0.0525493323802948\n",
      "Iteration 494, Batch: 10, Loss: 0.05037989094853401\n",
      "Iteration 494, Batch: 11, Loss: 0.03122798353433609\n",
      "Iteration 494, Batch: 12, Loss: 0.09430859982967377\n",
      "Iteration 494, Batch: 13, Loss: 0.0741165280342102\n",
      "Iteration 494, Batch: 14, Loss: 0.03998250141739845\n",
      "Iteration 494, Batch: 15, Loss: 0.061959922313690186\n",
      "Iteration 494, Batch: 16, Loss: 0.0811491534113884\n",
      "Iteration 494, Batch: 17, Loss: 0.03609180450439453\n",
      "Iteration 494, Batch: 18, Loss: 0.03880944103002548\n",
      "Iteration 494, Batch: 19, Loss: 0.044626958668231964\n",
      "Iteration 494, Batch: 20, Loss: 0.03470800444483757\n",
      "Iteration 494, Batch: 21, Loss: 0.037932686507701874\n",
      "Iteration 494, Batch: 22, Loss: 0.04412604868412018\n",
      "Iteration 494, Batch: 23, Loss: 0.03010951541364193\n",
      "Iteration 494, Batch: 24, Loss: 0.041744258254766464\n",
      "Iteration 494, Batch: 25, Loss: 0.050301142036914825\n",
      "Iteration 494, Batch: 26, Loss: 0.0517519935965538\n",
      "Iteration 494, Batch: 27, Loss: 0.03280127793550491\n",
      "Iteration 494, Batch: 28, Loss: 0.03997119888663292\n",
      "Iteration 494, Batch: 29, Loss: 0.07022307068109512\n",
      "Iteration 494, Batch: 30, Loss: 0.04742491990327835\n",
      "Iteration 494, Batch: 31, Loss: 0.041569728404283524\n",
      "Iteration 494, Batch: 32, Loss: 0.07576743513345718\n",
      "Iteration 494, Batch: 33, Loss: 0.05763785541057587\n",
      "Iteration 494, Batch: 34, Loss: 0.0841146856546402\n",
      "Iteration 494, Batch: 35, Loss: 0.07096917927265167\n",
      "Iteration 494, Batch: 36, Loss: 0.09999188035726547\n",
      "Iteration 494, Batch: 37, Loss: 0.08144893497228622\n",
      "Iteration 494, Batch: 38, Loss: 0.10066327452659607\n",
      "Iteration 494, Batch: 39, Loss: 0.056748177856206894\n",
      "Iteration 494, Batch: 40, Loss: 0.05916696414351463\n",
      "Iteration 494, Batch: 41, Loss: 0.05601009353995323\n",
      "Iteration 494, Batch: 42, Loss: 0.06914736330509186\n",
      "Iteration 494, Batch: 43, Loss: 0.08705613762140274\n",
      "Iteration 494, Batch: 44, Loss: 0.08609388768672943\n",
      "Iteration 494, Batch: 45, Loss: 0.07770131528377533\n",
      "Iteration 494, Batch: 46, Loss: 0.0697193592786789\n",
      "Iteration 494, Batch: 47, Loss: 0.06184228137135506\n",
      "Iteration 494, Batch: 48, Loss: 0.045507218688726425\n",
      "Iteration 494, Batch: 49, Loss: 0.05778326466679573\n",
      "Number of layers: 10\n",
      "Iteration 495, Batch: 0, Loss: 0.05426805466413498\n",
      "Iteration 495, Batch: 1, Loss: 0.0394376665353775\n",
      "Iteration 495, Batch: 2, Loss: 0.057356175035238266\n",
      "Iteration 495, Batch: 3, Loss: 0.052590932697057724\n",
      "Iteration 495, Batch: 4, Loss: 0.04323690012097359\n",
      "Iteration 495, Batch: 5, Loss: 0.05276298522949219\n",
      "Iteration 495, Batch: 6, Loss: 0.049550656229257584\n",
      "Iteration 495, Batch: 7, Loss: 0.0574614591896534\n",
      "Iteration 495, Batch: 8, Loss: 0.05795987322926521\n",
      "Iteration 495, Batch: 9, Loss: 0.05153759941458702\n",
      "Iteration 495, Batch: 10, Loss: 0.07665134966373444\n",
      "Iteration 495, Batch: 11, Loss: 0.04158852621912956\n",
      "Iteration 495, Batch: 12, Loss: 0.07019778341054916\n",
      "Iteration 495, Batch: 13, Loss: 0.09652543812990189\n",
      "Iteration 495, Batch: 14, Loss: 0.06178946793079376\n",
      "Iteration 495, Batch: 15, Loss: 0.07379981130361557\n",
      "Iteration 495, Batch: 16, Loss: 0.043610792607069016\n",
      "Iteration 495, Batch: 17, Loss: 0.05437961965799332\n",
      "Iteration 495, Batch: 18, Loss: 0.06669975072145462\n",
      "Iteration 495, Batch: 19, Loss: 0.08820033818483353\n",
      "Iteration 495, Batch: 20, Loss: 0.06879942119121552\n",
      "Iteration 495, Batch: 21, Loss: 0.06023041903972626\n",
      "Iteration 495, Batch: 22, Loss: 0.05052278935909271\n",
      "Iteration 495, Batch: 23, Loss: 0.0498383603990078\n",
      "Iteration 495, Batch: 24, Loss: 0.043448176234960556\n",
      "Iteration 495, Batch: 25, Loss: 0.04092482477426529\n",
      "Iteration 495, Batch: 26, Loss: 0.03536327928304672\n",
      "Iteration 495, Batch: 27, Loss: 0.025601627305150032\n",
      "Iteration 495, Batch: 28, Loss: 0.06568022072315216\n",
      "Iteration 495, Batch: 29, Loss: 0.04149220883846283\n",
      "Iteration 495, Batch: 30, Loss: 0.03674643486738205\n",
      "Iteration 495, Batch: 31, Loss: 0.03288590535521507\n",
      "Iteration 495, Batch: 32, Loss: 0.04660424217581749\n",
      "Iteration 495, Batch: 33, Loss: 0.039159901440143585\n",
      "Iteration 495, Batch: 34, Loss: 0.04671354964375496\n",
      "Iteration 495, Batch: 35, Loss: 0.05143918842077255\n",
      "Iteration 495, Batch: 36, Loss: 0.03360598534345627\n",
      "Iteration 495, Batch: 37, Loss: 0.04386758804321289\n",
      "Iteration 495, Batch: 38, Loss: 0.04162178933620453\n",
      "Iteration 495, Batch: 39, Loss: 0.062039174139499664\n",
      "Iteration 495, Batch: 40, Loss: 0.05221598967909813\n",
      "Iteration 495, Batch: 41, Loss: 0.03962224721908569\n",
      "Iteration 495, Batch: 42, Loss: 0.04550684988498688\n",
      "Iteration 495, Batch: 43, Loss: 0.08313953131437302\n",
      "Iteration 495, Batch: 44, Loss: 0.038635797798633575\n",
      "Iteration 495, Batch: 45, Loss: 0.03453002870082855\n",
      "Iteration 495, Batch: 46, Loss: 0.06918437778949738\n",
      "Iteration 495, Batch: 47, Loss: 0.07850483804941177\n",
      "Iteration 495, Batch: 48, Loss: 0.048340070992708206\n",
      "Iteration 495, Batch: 49, Loss: 0.05692090466618538\n",
      "Number of layers: 10\n",
      "Iteration 496, Batch: 0, Loss: 0.05377822741866112\n",
      "Iteration 496, Batch: 1, Loss: 0.031574130058288574\n",
      "Iteration 496, Batch: 2, Loss: 0.02505502663552761\n",
      "Iteration 496, Batch: 3, Loss: 0.04135625436902046\n",
      "Iteration 496, Batch: 4, Loss: 0.05352608487010002\n",
      "Iteration 496, Batch: 5, Loss: 0.02779616042971611\n",
      "Iteration 496, Batch: 6, Loss: 0.060119569301605225\n",
      "Iteration 496, Batch: 7, Loss: 0.06291822344064713\n",
      "Iteration 496, Batch: 8, Loss: 0.04049902781844139\n",
      "Iteration 496, Batch: 9, Loss: 0.042324166744947433\n",
      "Iteration 496, Batch: 10, Loss: 0.048796892166137695\n",
      "Iteration 496, Batch: 11, Loss: 0.03537588194012642\n",
      "Iteration 496, Batch: 12, Loss: 0.09320702403783798\n",
      "Iteration 496, Batch: 13, Loss: 0.042713236063718796\n",
      "Iteration 496, Batch: 14, Loss: 0.07711449265480042\n",
      "Iteration 496, Batch: 15, Loss: 0.0371248796582222\n",
      "Iteration 496, Batch: 16, Loss: 0.0461256317794323\n",
      "Iteration 496, Batch: 17, Loss: 0.056829407811164856\n",
      "Iteration 496, Batch: 18, Loss: 0.0343683660030365\n",
      "Iteration 496, Batch: 19, Loss: 0.05640403553843498\n",
      "Iteration 496, Batch: 20, Loss: 0.06645714491605759\n",
      "Iteration 496, Batch: 21, Loss: 0.048278119415044785\n",
      "Iteration 496, Batch: 22, Loss: 0.04090321436524391\n",
      "Iteration 496, Batch: 23, Loss: 0.05832574889063835\n",
      "Iteration 496, Batch: 24, Loss: 0.03862542659044266\n",
      "Iteration 496, Batch: 25, Loss: 0.045594196766614914\n",
      "Iteration 496, Batch: 26, Loss: 0.028521699830889702\n",
      "Iteration 496, Batch: 27, Loss: 0.075156070291996\n",
      "Iteration 496, Batch: 28, Loss: 0.051460009068250656\n",
      "Iteration 496, Batch: 29, Loss: 0.0379827618598938\n",
      "Iteration 496, Batch: 30, Loss: 0.04173357039690018\n",
      "Iteration 496, Batch: 31, Loss: 0.05658819526433945\n",
      "Iteration 496, Batch: 32, Loss: 0.08509077876806259\n",
      "Iteration 496, Batch: 33, Loss: 0.04574174806475639\n",
      "Iteration 496, Batch: 34, Loss: 0.039699818938970566\n",
      "Iteration 496, Batch: 35, Loss: 0.05233575776219368\n",
      "Iteration 496, Batch: 36, Loss: 0.05904654413461685\n",
      "Iteration 496, Batch: 37, Loss: 0.035014890134334564\n",
      "Iteration 496, Batch: 38, Loss: 0.0814560130238533\n",
      "Iteration 496, Batch: 39, Loss: 0.09117484837770462\n",
      "Iteration 496, Batch: 40, Loss: 0.07609307020902634\n",
      "Iteration 496, Batch: 41, Loss: 0.07371141761541367\n",
      "Iteration 496, Batch: 42, Loss: 0.06853888183832169\n",
      "Iteration 496, Batch: 43, Loss: 0.05783437564969063\n",
      "Iteration 496, Batch: 44, Loss: 0.07829954475164413\n",
      "Iteration 496, Batch: 45, Loss: 0.05311450734734535\n",
      "Iteration 496, Batch: 46, Loss: 0.03680506348609924\n",
      "Iteration 496, Batch: 47, Loss: 0.0632900521159172\n",
      "Iteration 496, Batch: 48, Loss: 0.05222221091389656\n",
      "Iteration 496, Batch: 49, Loss: 0.051223233342170715\n",
      "Number of layers: 10\n",
      "Iteration 497, Batch: 0, Loss: 0.04345123842358589\n",
      "Iteration 497, Batch: 1, Loss: 0.048431314527988434\n",
      "Iteration 497, Batch: 2, Loss: 0.03831319138407707\n",
      "Iteration 497, Batch: 3, Loss: 0.04374010115861893\n",
      "Iteration 497, Batch: 4, Loss: 0.03747106343507767\n",
      "Iteration 497, Batch: 5, Loss: 0.04267115518450737\n",
      "Iteration 497, Batch: 6, Loss: 0.040971677750349045\n",
      "Iteration 497, Batch: 7, Loss: 0.052683353424072266\n",
      "Iteration 497, Batch: 8, Loss: 0.046928804367780685\n",
      "Iteration 497, Batch: 9, Loss: 0.04092844948172569\n",
      "Iteration 497, Batch: 10, Loss: 0.08157142251729965\n",
      "Iteration 497, Batch: 11, Loss: 0.056024760007858276\n",
      "Iteration 497, Batch: 12, Loss: 0.049957845360040665\n",
      "Iteration 497, Batch: 13, Loss: 0.052015528082847595\n",
      "Iteration 497, Batch: 14, Loss: 0.06744133681058884\n",
      "Iteration 497, Batch: 15, Loss: 0.043603770434856415\n",
      "Iteration 497, Batch: 16, Loss: 0.06359803676605225\n",
      "Iteration 497, Batch: 17, Loss: 0.037238478660583496\n",
      "Iteration 497, Batch: 18, Loss: 0.04369453713297844\n",
      "Iteration 497, Batch: 19, Loss: 0.05138423666357994\n",
      "Iteration 497, Batch: 20, Loss: 0.0636085569858551\n",
      "Iteration 497, Batch: 21, Loss: 0.056876733899116516\n",
      "Iteration 497, Batch: 22, Loss: 0.03971945866942406\n",
      "Iteration 497, Batch: 23, Loss: 0.046613749116659164\n",
      "Iteration 497, Batch: 24, Loss: 0.026523293927311897\n",
      "Iteration 497, Batch: 25, Loss: 0.08721484243869781\n",
      "Iteration 497, Batch: 26, Loss: 0.07516904175281525\n",
      "Iteration 497, Batch: 27, Loss: 0.054144486784935\n",
      "Iteration 497, Batch: 28, Loss: 0.08426322788000107\n",
      "Iteration 497, Batch: 29, Loss: 0.04917043447494507\n",
      "Iteration 497, Batch: 30, Loss: 0.0803891196846962\n",
      "Iteration 497, Batch: 31, Loss: 0.057620570063591\n",
      "Iteration 497, Batch: 32, Loss: 0.05401505529880524\n",
      "Iteration 497, Batch: 33, Loss: 0.07958698272705078\n",
      "Iteration 497, Batch: 34, Loss: 0.03817944973707199\n",
      "Iteration 497, Batch: 35, Loss: 0.05075309798121452\n",
      "Iteration 497, Batch: 36, Loss: 0.0557357594370842\n",
      "Iteration 497, Batch: 37, Loss: 0.08837932348251343\n",
      "Iteration 497, Batch: 38, Loss: 0.043393637984991074\n",
      "Iteration 497, Batch: 39, Loss: 0.051651544868946075\n",
      "Iteration 497, Batch: 40, Loss: 0.05659738555550575\n",
      "Iteration 497, Batch: 41, Loss: 0.07268083095550537\n",
      "Iteration 497, Batch: 42, Loss: 0.07101898640394211\n",
      "Iteration 497, Batch: 43, Loss: 0.037001073360443115\n",
      "Iteration 497, Batch: 44, Loss: 0.06661517173051834\n",
      "Iteration 497, Batch: 45, Loss: 0.03985367715358734\n",
      "Iteration 497, Batch: 46, Loss: 0.03600442782044411\n",
      "Iteration 497, Batch: 47, Loss: 0.055101361125707626\n",
      "Iteration 497, Batch: 48, Loss: 0.02578030526638031\n",
      "Iteration 497, Batch: 49, Loss: 0.04010523110628128\n",
      "Number of layers: 10\n",
      "Iteration 498, Batch: 0, Loss: 0.04132746905088425\n",
      "Iteration 498, Batch: 1, Loss: 0.05138201639056206\n",
      "Iteration 498, Batch: 2, Loss: 0.03462794050574303\n",
      "Iteration 498, Batch: 3, Loss: 0.06282884627580643\n",
      "Iteration 498, Batch: 4, Loss: 0.04447442293167114\n",
      "Iteration 498, Batch: 5, Loss: 0.07610232383012772\n",
      "Iteration 498, Batch: 6, Loss: 0.042973898351192474\n",
      "Iteration 498, Batch: 7, Loss: 0.031161870807409286\n",
      "Iteration 498, Batch: 8, Loss: 0.06441060453653336\n",
      "Iteration 498, Batch: 9, Loss: 0.03330109640955925\n",
      "Iteration 498, Batch: 10, Loss: 0.05203297361731529\n",
      "Iteration 498, Batch: 11, Loss: 0.02816110849380493\n",
      "Iteration 498, Batch: 12, Loss: 0.03181668743491173\n",
      "Iteration 498, Batch: 13, Loss: 0.041248951107263565\n",
      "Iteration 498, Batch: 14, Loss: 0.04839791730046272\n",
      "Iteration 498, Batch: 15, Loss: 0.035538721829652786\n",
      "Iteration 498, Batch: 16, Loss: 0.05531235411763191\n",
      "Iteration 498, Batch: 17, Loss: 0.04959944635629654\n",
      "Iteration 498, Batch: 18, Loss: 0.0747700184583664\n",
      "Iteration 498, Batch: 19, Loss: 0.034781575202941895\n",
      "Iteration 498, Batch: 20, Loss: 0.061319321393966675\n",
      "Iteration 498, Batch: 21, Loss: 0.05041459947824478\n",
      "Iteration 498, Batch: 22, Loss: 0.040721964091062546\n",
      "Iteration 498, Batch: 23, Loss: 0.04915148764848709\n",
      "Iteration 498, Batch: 24, Loss: 0.07599449902772903\n",
      "Iteration 498, Batch: 25, Loss: 0.04058575630187988\n",
      "Iteration 498, Batch: 26, Loss: 0.05141855776309967\n",
      "Iteration 498, Batch: 27, Loss: 0.038177430629730225\n",
      "Iteration 498, Batch: 28, Loss: 0.05638007074594498\n",
      "Iteration 498, Batch: 29, Loss: 0.05920208618044853\n",
      "Iteration 498, Batch: 30, Loss: 0.060789238661527634\n",
      "Iteration 498, Batch: 31, Loss: 0.06305988132953644\n",
      "Iteration 498, Batch: 32, Loss: 0.056539181619882584\n",
      "Iteration 498, Batch: 33, Loss: 0.0552353672683239\n",
      "Iteration 498, Batch: 34, Loss: 0.06371907144784927\n",
      "Iteration 498, Batch: 35, Loss: 0.04894792288541794\n",
      "Iteration 498, Batch: 36, Loss: 0.05327137932181358\n",
      "Iteration 498, Batch: 37, Loss: 0.07800782471895218\n",
      "Iteration 498, Batch: 38, Loss: 0.05532063543796539\n",
      "Iteration 498, Batch: 39, Loss: 0.04117388278245926\n",
      "Iteration 498, Batch: 40, Loss: 0.046203095465898514\n",
      "Iteration 498, Batch: 41, Loss: 0.04475964978337288\n",
      "Iteration 498, Batch: 42, Loss: 0.01756928116083145\n",
      "Iteration 498, Batch: 43, Loss: 0.044314708560705185\n",
      "Iteration 498, Batch: 44, Loss: 0.04175814613699913\n",
      "Iteration 498, Batch: 45, Loss: 0.1125166118144989\n",
      "Iteration 498, Batch: 46, Loss: 0.06478823721408844\n",
      "Iteration 498, Batch: 47, Loss: 0.028748463839292526\n",
      "Iteration 498, Batch: 48, Loss: 0.0364382229745388\n",
      "Iteration 498, Batch: 49, Loss: 0.0656740814447403\n",
      "Number of layers: 10\n",
      "Iteration 499, Batch: 0, Loss: 0.03797085955739021\n",
      "Iteration 499, Batch: 1, Loss: 0.06544526666402817\n",
      "Iteration 499, Batch: 2, Loss: 0.04488343745470047\n",
      "Iteration 499, Batch: 3, Loss: 0.0429384782910347\n",
      "Iteration 499, Batch: 4, Loss: 0.046094853430986404\n",
      "Iteration 499, Batch: 5, Loss: 0.047622326761484146\n",
      "Iteration 499, Batch: 6, Loss: 0.05683155730366707\n",
      "Iteration 499, Batch: 7, Loss: 0.0658581405878067\n",
      "Iteration 499, Batch: 8, Loss: 0.04038095474243164\n",
      "Iteration 499, Batch: 9, Loss: 0.037613630294799805\n",
      "Iteration 499, Batch: 10, Loss: 0.052419424057006836\n",
      "Iteration 499, Batch: 11, Loss: 0.04339003935456276\n",
      "Iteration 499, Batch: 12, Loss: 0.04996528849005699\n",
      "Iteration 499, Batch: 13, Loss: 0.04853959009051323\n",
      "Iteration 499, Batch: 14, Loss: 0.04601659998297691\n",
      "Iteration 499, Batch: 15, Loss: 0.041192080825567245\n",
      "Iteration 499, Batch: 16, Loss: 0.02613067626953125\n",
      "Iteration 499, Batch: 17, Loss: 0.03919447958469391\n",
      "Iteration 499, Batch: 18, Loss: 0.034271325916051865\n",
      "Iteration 499, Batch: 19, Loss: 0.02735419198870659\n",
      "Iteration 499, Batch: 20, Loss: 0.050815608352422714\n",
      "Iteration 499, Batch: 21, Loss: 0.05727938935160637\n",
      "Iteration 499, Batch: 22, Loss: 0.03768998384475708\n",
      "Iteration 499, Batch: 23, Loss: 0.03282994404435158\n",
      "Iteration 499, Batch: 24, Loss: 0.03739359602332115\n",
      "Iteration 499, Batch: 25, Loss: 0.04801825061440468\n",
      "Iteration 499, Batch: 26, Loss: 0.048521604388952255\n",
      "Iteration 499, Batch: 27, Loss: 0.026572486385703087\n",
      "Iteration 499, Batch: 28, Loss: 0.05976318567991257\n",
      "Iteration 499, Batch: 29, Loss: 0.07924267649650574\n",
      "Iteration 499, Batch: 30, Loss: 0.0436958409845829\n",
      "Iteration 499, Batch: 31, Loss: 0.04788713529706001\n",
      "Iteration 499, Batch: 32, Loss: 0.052684105932712555\n",
      "Iteration 499, Batch: 33, Loss: 0.053949985653162\n",
      "Iteration 499, Batch: 34, Loss: 0.07680830359458923\n",
      "Iteration 499, Batch: 35, Loss: 0.050596680492162704\n",
      "Iteration 499, Batch: 36, Loss: 0.06746363639831543\n",
      "Iteration 499, Batch: 37, Loss: 0.06263358145952225\n",
      "Iteration 499, Batch: 38, Loss: 0.062327031046152115\n",
      "Iteration 499, Batch: 39, Loss: 0.06515921652317047\n",
      "Iteration 499, Batch: 40, Loss: 0.06376753002405167\n",
      "Iteration 499, Batch: 41, Loss: 0.05405506491661072\n",
      "Iteration 499, Batch: 42, Loss: 0.058246366679668427\n",
      "Iteration 499, Batch: 43, Loss: 0.0574931800365448\n",
      "Iteration 499, Batch: 44, Loss: 0.05364714190363884\n",
      "Iteration 499, Batch: 45, Loss: 0.05743229389190674\n",
      "Iteration 499, Batch: 46, Loss: 0.0672498419880867\n",
      "Iteration 499, Batch: 47, Loss: 0.06783106923103333\n",
      "Iteration 499, Batch: 48, Loss: 0.042028024792671204\n",
      "Iteration 499, Batch: 49, Loss: 0.04786844179034233\n",
      "Number of layers: 10\n",
      "Iteration 500, Batch: 0, Loss: 0.059455860406160355\n",
      "Iteration 500, Batch: 1, Loss: 0.0609143003821373\n",
      "Iteration 500, Batch: 2, Loss: 0.04214606061577797\n",
      "Iteration 500, Batch: 3, Loss: 0.050763089209795\n",
      "Iteration 500, Batch: 4, Loss: 0.03874286636710167\n",
      "Iteration 500, Batch: 5, Loss: 0.04916911944746971\n",
      "Iteration 500, Batch: 6, Loss: 0.055630363523960114\n",
      "Iteration 500, Batch: 7, Loss: 0.02432491071522236\n",
      "Iteration 500, Batch: 8, Loss: 0.03713644668459892\n",
      "Iteration 500, Batch: 9, Loss: 0.031437281519174576\n",
      "Iteration 500, Batch: 10, Loss: 0.03467223793268204\n",
      "Iteration 500, Batch: 11, Loss: 0.05705084651708603\n",
      "Iteration 500, Batch: 12, Loss: 0.07560339570045471\n",
      "Iteration 500, Batch: 13, Loss: 0.05521559715270996\n",
      "Iteration 500, Batch: 14, Loss: 0.044685594737529755\n",
      "Iteration 500, Batch: 15, Loss: 0.021785425022244453\n",
      "Iteration 500, Batch: 16, Loss: 0.06474606692790985\n",
      "Iteration 500, Batch: 17, Loss: 0.04126594960689545\n",
      "Iteration 500, Batch: 18, Loss: 0.05905668064951897\n",
      "Iteration 500, Batch: 19, Loss: 0.04470718279480934\n",
      "Iteration 500, Batch: 20, Loss: 0.056974299252033234\n",
      "Iteration 500, Batch: 21, Loss: 0.03988809883594513\n",
      "Iteration 500, Batch: 22, Loss: 0.06050505116581917\n",
      "Iteration 500, Batch: 23, Loss: 0.02070632018148899\n",
      "Iteration 500, Batch: 24, Loss: 0.08187097311019897\n",
      "Iteration 500, Batch: 25, Loss: 0.06555487215518951\n",
      "Iteration 500, Batch: 26, Loss: 0.06555533409118652\n",
      "Iteration 500, Batch: 27, Loss: 0.04792294278740883\n",
      "Iteration 500, Batch: 28, Loss: 0.07858385145664215\n",
      "Iteration 500, Batch: 29, Loss: 0.03341082111001015\n",
      "Iteration 500, Batch: 30, Loss: 0.04249726980924606\n",
      "Iteration 500, Batch: 31, Loss: 0.031873252242803574\n",
      "Iteration 500, Batch: 32, Loss: 0.039564017206430435\n",
      "Iteration 500, Batch: 33, Loss: 0.049626026302576065\n",
      "Iteration 500, Batch: 34, Loss: 0.05205361172556877\n",
      "Iteration 500, Batch: 35, Loss: 0.05754774808883667\n",
      "Iteration 500, Batch: 36, Loss: 0.03709409013390541\n",
      "Iteration 500, Batch: 37, Loss: 0.07493598759174347\n",
      "Iteration 500, Batch: 38, Loss: 0.07699273526668549\n",
      "Iteration 500, Batch: 39, Loss: 0.05042922869324684\n",
      "Iteration 500, Batch: 40, Loss: 0.052731383591890335\n",
      "Iteration 500, Batch: 41, Loss: 0.0492342971265316\n",
      "Iteration 500, Batch: 42, Loss: 0.03387727588415146\n",
      "Iteration 500, Batch: 43, Loss: 0.051697857677936554\n",
      "Iteration 500, Batch: 44, Loss: 0.061449144035577774\n",
      "Iteration 500, Batch: 45, Loss: 0.054821450263261795\n",
      "Iteration 500, Batch: 46, Loss: 0.06939315795898438\n",
      "Iteration 500, Batch: 47, Loss: 0.05542657896876335\n",
      "Iteration 500, Batch: 48, Loss: 0.05884641408920288\n",
      "Iteration 500, Batch: 49, Loss: 0.050190772861242294\n",
      "Number of layers: 10\n",
      "Iteration 501, Batch: 0, Loss: 0.06262540072202682\n",
      "Iteration 501, Batch: 1, Loss: 0.06997314095497131\n",
      "Iteration 501, Batch: 2, Loss: 0.041740186512470245\n",
      "Iteration 501, Batch: 3, Loss: 0.052781689912080765\n",
      "Iteration 501, Batch: 4, Loss: 0.05448731407523155\n",
      "Iteration 501, Batch: 5, Loss: 0.054655130952596664\n",
      "Iteration 501, Batch: 6, Loss: 0.08727087080478668\n",
      "Iteration 501, Batch: 7, Loss: 0.048583853989839554\n",
      "Iteration 501, Batch: 8, Loss: 0.041710808873176575\n",
      "Iteration 501, Batch: 9, Loss: 0.0323408804833889\n",
      "Iteration 501, Batch: 10, Loss: 0.03974489867687225\n",
      "Iteration 501, Batch: 11, Loss: 0.05467581748962402\n",
      "Iteration 501, Batch: 12, Loss: 0.04154184088110924\n",
      "Iteration 501, Batch: 13, Loss: 0.05913424491882324\n",
      "Iteration 501, Batch: 14, Loss: 0.044493939727544785\n",
      "Iteration 501, Batch: 15, Loss: 0.045882128179073334\n",
      "Iteration 501, Batch: 16, Loss: 0.07563119381666183\n",
      "Iteration 501, Batch: 17, Loss: 0.05452265590429306\n",
      "Iteration 501, Batch: 18, Loss: 0.050406914204359055\n",
      "Iteration 501, Batch: 19, Loss: 0.042793236672878265\n",
      "Iteration 501, Batch: 20, Loss: 0.029318073764443398\n",
      "Iteration 501, Batch: 21, Loss: 0.050000570714473724\n",
      "Iteration 501, Batch: 22, Loss: 0.05482245236635208\n",
      "Iteration 501, Batch: 23, Loss: 0.03537484630942345\n",
      "Iteration 501, Batch: 24, Loss: 0.05252275988459587\n",
      "Iteration 501, Batch: 25, Loss: 0.05712172016501427\n",
      "Iteration 501, Batch: 26, Loss: 0.03775208443403244\n",
      "Iteration 501, Batch: 27, Loss: 0.05126166343688965\n",
      "Iteration 501, Batch: 28, Loss: 0.05556274205446243\n",
      "Iteration 501, Batch: 29, Loss: 0.04936559498310089\n",
      "Iteration 501, Batch: 30, Loss: 0.054240789264440536\n",
      "Iteration 501, Batch: 31, Loss: 0.04054000601172447\n",
      "Iteration 501, Batch: 32, Loss: 0.01926243305206299\n",
      "Iteration 501, Batch: 33, Loss: 0.04232985898852348\n",
      "Iteration 501, Batch: 34, Loss: 0.047422975301742554\n",
      "Iteration 501, Batch: 35, Loss: 0.03626551106572151\n",
      "Iteration 501, Batch: 36, Loss: 0.05473742261528969\n",
      "Iteration 501, Batch: 37, Loss: 0.0589098297059536\n",
      "Iteration 501, Batch: 38, Loss: 0.06031505763530731\n",
      "Iteration 501, Batch: 39, Loss: 0.046674925833940506\n",
      "Iteration 501, Batch: 40, Loss: 0.0599939227104187\n",
      "Iteration 501, Batch: 41, Loss: 0.048875149339437485\n",
      "Iteration 501, Batch: 42, Loss: 0.06090704724192619\n",
      "Iteration 501, Batch: 43, Loss: 0.04108263924717903\n",
      "Iteration 501, Batch: 44, Loss: 0.049172382801771164\n",
      "Iteration 501, Batch: 45, Loss: 0.07846934348344803\n",
      "Iteration 501, Batch: 46, Loss: 0.06682965904474258\n",
      "Iteration 501, Batch: 47, Loss: 0.046764593571424484\n",
      "Iteration 501, Batch: 48, Loss: 0.08577579259872437\n",
      "Iteration 501, Batch: 49, Loss: 0.039726417511701584\n",
      "Number of layers: 10\n",
      "Iteration 502, Batch: 0, Loss: 0.10028453916311264\n",
      "Iteration 502, Batch: 1, Loss: 0.0822548195719719\n",
      "Iteration 502, Batch: 2, Loss: 0.07329162210226059\n",
      "Iteration 502, Batch: 3, Loss: 0.05187276750802994\n",
      "Iteration 502, Batch: 4, Loss: 0.043993834406137466\n",
      "Iteration 502, Batch: 5, Loss: 0.05414406582713127\n",
      "Iteration 502, Batch: 6, Loss: 0.02346799336373806\n",
      "Iteration 502, Batch: 7, Loss: 0.0954665094614029\n",
      "Iteration 502, Batch: 8, Loss: 0.0580601766705513\n",
      "Iteration 502, Batch: 9, Loss: 0.05959281697869301\n",
      "Iteration 502, Batch: 10, Loss: 0.050943173468112946\n",
      "Iteration 502, Batch: 11, Loss: 0.0457531213760376\n",
      "Iteration 502, Batch: 12, Loss: 0.04942770302295685\n",
      "Iteration 502, Batch: 13, Loss: 0.04651939123868942\n",
      "Iteration 502, Batch: 14, Loss: 0.05281253531575203\n",
      "Iteration 502, Batch: 15, Loss: 0.0501360148191452\n",
      "Iteration 502, Batch: 16, Loss: 0.043358005583286285\n",
      "Iteration 502, Batch: 17, Loss: 0.034370057284832\n",
      "Iteration 502, Batch: 18, Loss: 0.047619499266147614\n",
      "Iteration 502, Batch: 19, Loss: 0.03964769095182419\n",
      "Iteration 502, Batch: 20, Loss: 0.06653670966625214\n",
      "Iteration 502, Batch: 21, Loss: 0.08400116860866547\n",
      "Iteration 502, Batch: 22, Loss: 0.05570234730839729\n",
      "Iteration 502, Batch: 23, Loss: 0.0697133019566536\n",
      "Iteration 502, Batch: 24, Loss: 0.058940205723047256\n",
      "Iteration 502, Batch: 25, Loss: 0.048331309109926224\n",
      "Iteration 502, Batch: 26, Loss: 0.0823703333735466\n",
      "Iteration 502, Batch: 27, Loss: 0.051353324204683304\n",
      "Iteration 502, Batch: 28, Loss: 0.0436013787984848\n",
      "Iteration 502, Batch: 29, Loss: 0.04803061857819557\n",
      "Iteration 502, Batch: 30, Loss: 0.08857814222574234\n",
      "Iteration 502, Batch: 31, Loss: 0.06733662635087967\n",
      "Iteration 502, Batch: 32, Loss: 0.08384094387292862\n",
      "Iteration 502, Batch: 33, Loss: 0.057510606944561005\n",
      "Iteration 502, Batch: 34, Loss: 0.06338157504796982\n",
      "Iteration 502, Batch: 35, Loss: 0.0571659654378891\n",
      "Iteration 502, Batch: 36, Loss: 0.06944090873003006\n",
      "Iteration 502, Batch: 37, Loss: 0.07608948647975922\n",
      "Iteration 502, Batch: 38, Loss: 0.058795735239982605\n",
      "Iteration 502, Batch: 39, Loss: 0.07987449318170547\n",
      "Iteration 502, Batch: 40, Loss: 0.043106526136398315\n",
      "Iteration 502, Batch: 41, Loss: 0.07471324503421783\n",
      "Iteration 502, Batch: 42, Loss: 0.04258974641561508\n",
      "Iteration 502, Batch: 43, Loss: 0.04315650835633278\n",
      "Iteration 502, Batch: 44, Loss: 0.03819385543465614\n",
      "Iteration 502, Batch: 45, Loss: 0.035058844834566116\n",
      "Iteration 502, Batch: 46, Loss: 0.034427203238010406\n",
      "Iteration 502, Batch: 47, Loss: 0.04271405190229416\n",
      "Iteration 502, Batch: 48, Loss: 0.04802395775914192\n",
      "Iteration 502, Batch: 49, Loss: 0.06021745875477791\n",
      "Number of layers: 10\n",
      "Iteration 503, Batch: 0, Loss: 0.045286472886800766\n",
      "Iteration 503, Batch: 1, Loss: 0.043137434870004654\n",
      "Iteration 503, Batch: 2, Loss: 0.05148271843791008\n",
      "Iteration 503, Batch: 3, Loss: 0.05774793401360512\n",
      "Iteration 503, Batch: 4, Loss: 0.057697903364896774\n",
      "Iteration 503, Batch: 5, Loss: 0.056049659848213196\n",
      "Iteration 503, Batch: 6, Loss: 0.056444719433784485\n",
      "Iteration 503, Batch: 7, Loss: 0.039671313017606735\n",
      "Iteration 503, Batch: 8, Loss: 0.04739292338490486\n",
      "Iteration 503, Batch: 9, Loss: 0.04867246374487877\n",
      "Iteration 503, Batch: 10, Loss: 0.0421954020857811\n",
      "Iteration 503, Batch: 11, Loss: 0.031557224690914154\n",
      "Iteration 503, Batch: 12, Loss: 0.03815564140677452\n",
      "Iteration 503, Batch: 13, Loss: 0.07755405455827713\n",
      "Iteration 503, Batch: 14, Loss: 0.04373237490653992\n",
      "Iteration 503, Batch: 15, Loss: 0.0596829317510128\n",
      "Iteration 503, Batch: 16, Loss: 0.05167866498231888\n",
      "Iteration 503, Batch: 17, Loss: 0.059890251606702805\n",
      "Iteration 503, Batch: 18, Loss: 0.03857235610485077\n",
      "Iteration 503, Batch: 19, Loss: 0.06479625403881073\n",
      "Iteration 503, Batch: 20, Loss: 0.056957654654979706\n",
      "Iteration 503, Batch: 21, Loss: 0.05807391554117203\n",
      "Iteration 503, Batch: 22, Loss: 0.05188470706343651\n",
      "Iteration 503, Batch: 23, Loss: 0.037604521960020065\n",
      "Iteration 503, Batch: 24, Loss: 0.06050723418593407\n",
      "Iteration 503, Batch: 25, Loss: 0.037195418030023575\n",
      "Iteration 503, Batch: 26, Loss: 0.05770661309361458\n",
      "Iteration 503, Batch: 27, Loss: 0.04569466412067413\n",
      "Iteration 503, Batch: 28, Loss: 0.061347708106040955\n",
      "Iteration 503, Batch: 29, Loss: 0.04101981967687607\n",
      "Iteration 503, Batch: 30, Loss: 0.060887522995471954\n",
      "Iteration 503, Batch: 31, Loss: 0.0580701045691967\n",
      "Iteration 503, Batch: 32, Loss: 0.027251800522208214\n",
      "Iteration 503, Batch: 33, Loss: 0.05263502150774002\n",
      "Iteration 503, Batch: 34, Loss: 0.06332799047231674\n",
      "Iteration 503, Batch: 35, Loss: 0.05261753126978874\n",
      "Iteration 503, Batch: 36, Loss: 0.040803201496601105\n",
      "Iteration 503, Batch: 37, Loss: 0.051819391548633575\n",
      "Iteration 503, Batch: 38, Loss: 0.06495744735002518\n",
      "Iteration 503, Batch: 39, Loss: 0.03753232955932617\n",
      "Iteration 503, Batch: 40, Loss: 0.05574166774749756\n",
      "Iteration 503, Batch: 41, Loss: 0.0675632655620575\n",
      "Iteration 503, Batch: 42, Loss: 0.08258914947509766\n",
      "Iteration 503, Batch: 43, Loss: 0.07388082146644592\n",
      "Iteration 503, Batch: 44, Loss: 0.07083851844072342\n",
      "Iteration 503, Batch: 45, Loss: 0.03385622799396515\n",
      "Iteration 503, Batch: 46, Loss: 0.05799272283911705\n",
      "Iteration 503, Batch: 47, Loss: 0.04234566166996956\n",
      "Iteration 503, Batch: 48, Loss: 0.025044912472367287\n",
      "Iteration 503, Batch: 49, Loss: 0.04669489711523056\n",
      "Number of layers: 10\n",
      "Iteration 504, Batch: 0, Loss: 0.041493795812129974\n",
      "Iteration 504, Batch: 1, Loss: 0.04559562727808952\n",
      "Iteration 504, Batch: 2, Loss: 0.04588105529546738\n",
      "Iteration 504, Batch: 3, Loss: 0.02249308116734028\n",
      "Iteration 504, Batch: 4, Loss: 0.07317052781581879\n",
      "Iteration 504, Batch: 5, Loss: 0.02436203882098198\n",
      "Iteration 504, Batch: 6, Loss: 0.04265270009636879\n",
      "Iteration 504, Batch: 7, Loss: 0.06373154371976852\n",
      "Iteration 504, Batch: 8, Loss: 0.049525145441293716\n",
      "Iteration 504, Batch: 9, Loss: 0.08489315211772919\n",
      "Iteration 504, Batch: 10, Loss: 0.04036519303917885\n",
      "Iteration 504, Batch: 11, Loss: 0.04010751470923424\n",
      "Iteration 504, Batch: 12, Loss: 0.04545562341809273\n",
      "Iteration 504, Batch: 13, Loss: 0.039673127233982086\n",
      "Iteration 504, Batch: 14, Loss: 0.029223041608929634\n",
      "Iteration 504, Batch: 15, Loss: 0.06693527102470398\n",
      "Iteration 504, Batch: 16, Loss: 0.03304203972220421\n",
      "Iteration 504, Batch: 17, Loss: 0.04944334551692009\n",
      "Iteration 504, Batch: 18, Loss: 0.03787641227245331\n",
      "Iteration 504, Batch: 19, Loss: 0.04271545633673668\n",
      "Iteration 504, Batch: 20, Loss: 0.05735889822244644\n",
      "Iteration 504, Batch: 21, Loss: 0.02729877457022667\n",
      "Iteration 504, Batch: 22, Loss: 0.07218974828720093\n",
      "Iteration 504, Batch: 23, Loss: 0.061530884355306625\n",
      "Iteration 504, Batch: 24, Loss: 0.08877990394830704\n",
      "Iteration 504, Batch: 25, Loss: 0.07880585640668869\n",
      "Iteration 504, Batch: 26, Loss: 0.0882679745554924\n",
      "Iteration 504, Batch: 27, Loss: 0.05678411200642586\n",
      "Iteration 504, Batch: 28, Loss: 0.06713780015707016\n",
      "Iteration 504, Batch: 29, Loss: 0.05428973212838173\n",
      "Iteration 504, Batch: 30, Loss: 0.033423591405153275\n",
      "Iteration 504, Batch: 31, Loss: 0.05422692000865936\n",
      "Iteration 504, Batch: 32, Loss: 0.030218230560421944\n",
      "Iteration 504, Batch: 33, Loss: 0.05205714702606201\n",
      "Iteration 504, Batch: 34, Loss: 0.06386788934469223\n",
      "Iteration 504, Batch: 35, Loss: 0.07206988334655762\n",
      "Iteration 504, Batch: 36, Loss: 0.06819848716259003\n",
      "Iteration 504, Batch: 37, Loss: 0.02782473899424076\n",
      "Iteration 504, Batch: 38, Loss: 0.029268840327858925\n",
      "Iteration 504, Batch: 39, Loss: 0.07030458003282547\n",
      "Iteration 504, Batch: 40, Loss: 0.04497195780277252\n",
      "Iteration 504, Batch: 41, Loss: 0.041666049510240555\n",
      "Iteration 504, Batch: 42, Loss: 0.04674915224313736\n",
      "Iteration 504, Batch: 43, Loss: 0.0357624776661396\n",
      "Iteration 504, Batch: 44, Loss: 0.029982272535562515\n",
      "Iteration 504, Batch: 45, Loss: 0.061307813972234726\n",
      "Iteration 504, Batch: 46, Loss: 0.04620930179953575\n",
      "Iteration 504, Batch: 47, Loss: 0.04774661362171173\n",
      "Iteration 504, Batch: 48, Loss: 0.07988645136356354\n",
      "Iteration 504, Batch: 49, Loss: 0.04079609736800194\n",
      "Number of layers: 10\n",
      "Iteration 505, Batch: 0, Loss: 0.056004032492637634\n",
      "Iteration 505, Batch: 1, Loss: 0.04108878970146179\n",
      "Iteration 505, Batch: 2, Loss: 0.04178127646446228\n",
      "Iteration 505, Batch: 3, Loss: 0.028818663209676743\n",
      "Iteration 505, Batch: 4, Loss: 0.043850142508745193\n",
      "Iteration 505, Batch: 5, Loss: 0.047076184302568436\n",
      "Iteration 505, Batch: 6, Loss: 0.05072484910488129\n",
      "Iteration 505, Batch: 7, Loss: 0.049348313361406326\n",
      "Iteration 505, Batch: 8, Loss: 0.03401618078351021\n",
      "Iteration 505, Batch: 9, Loss: 0.05223989859223366\n",
      "Iteration 505, Batch: 10, Loss: 0.026524050161242485\n",
      "Iteration 505, Batch: 11, Loss: 0.05697379633784294\n",
      "Iteration 505, Batch: 12, Loss: 0.03502778336405754\n",
      "Iteration 505, Batch: 13, Loss: 0.03841602802276611\n",
      "Iteration 505, Batch: 14, Loss: 0.02348877675831318\n",
      "Iteration 505, Batch: 15, Loss: 0.05875255540013313\n",
      "Iteration 505, Batch: 16, Loss: 0.030403558164834976\n",
      "Iteration 505, Batch: 17, Loss: 0.0615694522857666\n",
      "Iteration 505, Batch: 18, Loss: 0.043750423938035965\n",
      "Iteration 505, Batch: 19, Loss: 0.05118921026587486\n",
      "Iteration 505, Batch: 20, Loss: 0.06399917602539062\n",
      "Iteration 505, Batch: 21, Loss: 0.05357930064201355\n",
      "Iteration 505, Batch: 22, Loss: 0.06475830078125\n",
      "Iteration 505, Batch: 23, Loss: 0.07626977562904358\n",
      "Iteration 505, Batch: 24, Loss: 0.05559111014008522\n",
      "Iteration 505, Batch: 25, Loss: 0.0931539312005043\n",
      "Iteration 505, Batch: 26, Loss: 0.07191897928714752\n",
      "Iteration 505, Batch: 27, Loss: 0.06389452517032623\n",
      "Iteration 505, Batch: 28, Loss: 0.038229383528232574\n",
      "Iteration 505, Batch: 29, Loss: 0.06838536262512207\n",
      "Iteration 505, Batch: 30, Loss: 0.09129119664430618\n",
      "Iteration 505, Batch: 31, Loss: 0.05319105088710785\n",
      "Iteration 505, Batch: 32, Loss: 0.04290640726685524\n",
      "Iteration 505, Batch: 33, Loss: 0.0546136274933815\n",
      "Iteration 505, Batch: 34, Loss: 0.06155608221888542\n",
      "Iteration 505, Batch: 35, Loss: 0.0679125115275383\n",
      "Iteration 505, Batch: 36, Loss: 0.041504520922899246\n",
      "Iteration 505, Batch: 37, Loss: 0.0728624016046524\n",
      "Iteration 505, Batch: 38, Loss: 0.029792584478855133\n",
      "Iteration 505, Batch: 39, Loss: 0.06760721653699875\n",
      "Iteration 505, Batch: 40, Loss: 0.026143746450543404\n",
      "Iteration 505, Batch: 41, Loss: 0.04574804753065109\n",
      "Iteration 505, Batch: 42, Loss: 0.04717665910720825\n",
      "Iteration 505, Batch: 43, Loss: 0.049298495054244995\n",
      "Iteration 505, Batch: 44, Loss: 0.032230280339717865\n",
      "Iteration 505, Batch: 45, Loss: 0.05202753096818924\n",
      "Iteration 505, Batch: 46, Loss: 0.04840270057320595\n",
      "Iteration 505, Batch: 47, Loss: 0.04283660650253296\n",
      "Iteration 505, Batch: 48, Loss: 0.05702522024512291\n",
      "Iteration 505, Batch: 49, Loss: 0.033175114542245865\n",
      "Number of layers: 10\n",
      "Iteration 506, Batch: 0, Loss: 0.08054696768522263\n",
      "Iteration 506, Batch: 1, Loss: 0.0743718221783638\n",
      "Iteration 506, Batch: 2, Loss: 0.11493469029664993\n",
      "Iteration 506, Batch: 3, Loss: 0.09485872834920883\n",
      "Iteration 506, Batch: 4, Loss: 0.06101100146770477\n",
      "Iteration 506, Batch: 5, Loss: 0.07706908881664276\n",
      "Iteration 506, Batch: 6, Loss: 0.06077088043093681\n",
      "Iteration 506, Batch: 7, Loss: 0.0615810863673687\n",
      "Iteration 506, Batch: 8, Loss: 0.05913813039660454\n",
      "Iteration 506, Batch: 9, Loss: 0.04204927384853363\n",
      "Iteration 506, Batch: 10, Loss: 0.04369717463850975\n",
      "Iteration 506, Batch: 11, Loss: 0.052907560020685196\n",
      "Iteration 506, Batch: 12, Loss: 0.04419448599219322\n",
      "Iteration 506, Batch: 13, Loss: 0.057064302265644073\n",
      "Iteration 506, Batch: 14, Loss: 0.02494112029671669\n",
      "Iteration 506, Batch: 15, Loss: 0.06889046728610992\n",
      "Iteration 506, Batch: 16, Loss: 0.06714235991239548\n",
      "Iteration 506, Batch: 17, Loss: 0.05581190809607506\n",
      "Iteration 506, Batch: 18, Loss: 0.03583206981420517\n",
      "Iteration 506, Batch: 19, Loss: 0.053373679518699646\n",
      "Iteration 506, Batch: 20, Loss: 0.043418142944574356\n",
      "Iteration 506, Batch: 21, Loss: 0.02748376876115799\n",
      "Iteration 506, Batch: 22, Loss: 0.07605411112308502\n",
      "Iteration 506, Batch: 23, Loss: 0.03899609297513962\n",
      "Iteration 506, Batch: 24, Loss: 0.042948804795742035\n",
      "Iteration 506, Batch: 25, Loss: 0.05492470785975456\n",
      "Iteration 506, Batch: 26, Loss: 0.08517146855592728\n",
      "Iteration 506, Batch: 27, Loss: 0.07487557083368301\n",
      "Iteration 506, Batch: 28, Loss: 0.08504383265972137\n",
      "Iteration 506, Batch: 29, Loss: 0.09220939129590988\n",
      "Iteration 506, Batch: 30, Loss: 0.11271855980157852\n",
      "Iteration 506, Batch: 31, Loss: 0.12983892858028412\n",
      "Iteration 506, Batch: 32, Loss: 0.08739768713712692\n",
      "Iteration 506, Batch: 33, Loss: 0.06704460829496384\n",
      "Iteration 506, Batch: 34, Loss: 0.10907415300607681\n",
      "Iteration 506, Batch: 35, Loss: 0.08928052335977554\n",
      "Iteration 506, Batch: 36, Loss: 0.09272394329309464\n",
      "Iteration 506, Batch: 37, Loss: 0.11775097250938416\n",
      "Iteration 506, Batch: 38, Loss: 0.0942302867770195\n",
      "Iteration 506, Batch: 39, Loss: 0.07508005946874619\n",
      "Iteration 506, Batch: 40, Loss: 0.06135571375489235\n",
      "Iteration 506, Batch: 41, Loss: 0.04868021979928017\n",
      "Iteration 506, Batch: 42, Loss: 0.053866222500801086\n",
      "Iteration 506, Batch: 43, Loss: 0.06472166627645493\n",
      "Iteration 506, Batch: 44, Loss: 0.05460479110479355\n",
      "Iteration 506, Batch: 45, Loss: 0.06371109932661057\n",
      "Iteration 506, Batch: 46, Loss: 0.05272148177027702\n",
      "Iteration 506, Batch: 47, Loss: 0.07042650878429413\n",
      "Iteration 506, Batch: 48, Loss: 0.05708921328186989\n",
      "Iteration 506, Batch: 49, Loss: 0.04269448295235634\n",
      "Number of layers: 10\n",
      "Iteration 507, Batch: 0, Loss: 0.055004190653562546\n",
      "Iteration 507, Batch: 1, Loss: 0.05390583723783493\n",
      "Iteration 507, Batch: 2, Loss: 0.05486373230814934\n",
      "Iteration 507, Batch: 3, Loss: 0.059962887316942215\n",
      "Iteration 507, Batch: 4, Loss: 0.044045742601156235\n",
      "Iteration 507, Batch: 5, Loss: 0.08943622559309006\n",
      "Iteration 507, Batch: 6, Loss: 0.04865600913763046\n",
      "Iteration 507, Batch: 7, Loss: 0.056399088352918625\n",
      "Iteration 507, Batch: 8, Loss: 0.07360681891441345\n",
      "Iteration 507, Batch: 9, Loss: 0.08401615917682648\n",
      "Iteration 507, Batch: 10, Loss: 0.05522677302360535\n",
      "Iteration 507, Batch: 11, Loss: 0.060742929577827454\n",
      "Iteration 507, Batch: 12, Loss: 0.02397572435438633\n",
      "Iteration 507, Batch: 13, Loss: 0.06165874004364014\n",
      "Iteration 507, Batch: 14, Loss: 0.049096621572971344\n",
      "Iteration 507, Batch: 15, Loss: 0.03735997900366783\n",
      "Iteration 507, Batch: 16, Loss: 0.05693068355321884\n",
      "Iteration 507, Batch: 17, Loss: 0.08419384062290192\n",
      "Iteration 507, Batch: 18, Loss: 0.06563776731491089\n",
      "Iteration 507, Batch: 19, Loss: 0.04688351973891258\n",
      "Iteration 507, Batch: 20, Loss: 0.11672668159008026\n",
      "Iteration 507, Batch: 21, Loss: 0.08797864615917206\n",
      "Iteration 507, Batch: 22, Loss: 0.04996601864695549\n",
      "Iteration 507, Batch: 23, Loss: 0.054742392152547836\n",
      "Iteration 507, Batch: 24, Loss: 0.05705014616250992\n",
      "Iteration 507, Batch: 25, Loss: 0.04591555520892143\n",
      "Iteration 507, Batch: 26, Loss: 0.06905381381511688\n",
      "Iteration 507, Batch: 27, Loss: 0.06570742279291153\n",
      "Iteration 507, Batch: 28, Loss: 0.07324422895908356\n",
      "Iteration 507, Batch: 29, Loss: 0.043055687099695206\n",
      "Iteration 507, Batch: 30, Loss: 0.052061405032873154\n",
      "Iteration 507, Batch: 31, Loss: 0.060842592269182205\n",
      "Iteration 507, Batch: 32, Loss: 0.04899946600198746\n",
      "Iteration 507, Batch: 33, Loss: 0.0413697250187397\n",
      "Iteration 507, Batch: 34, Loss: 0.047365374863147736\n",
      "Iteration 507, Batch: 35, Loss: 0.0507892444729805\n",
      "Iteration 507, Batch: 36, Loss: 0.05563036724925041\n",
      "Iteration 507, Batch: 37, Loss: 0.04241805896162987\n",
      "Iteration 507, Batch: 38, Loss: 0.05908513069152832\n",
      "Iteration 507, Batch: 39, Loss: 0.035198938101530075\n",
      "Iteration 507, Batch: 40, Loss: 0.034698180854320526\n",
      "Iteration 507, Batch: 41, Loss: 0.03717086464166641\n",
      "Iteration 507, Batch: 42, Loss: 0.06806273013353348\n",
      "Iteration 507, Batch: 43, Loss: 0.059178538620471954\n",
      "Iteration 507, Batch: 44, Loss: 0.05013219639658928\n",
      "Iteration 507, Batch: 45, Loss: 0.04292665421962738\n",
      "Iteration 507, Batch: 46, Loss: 0.040915392339229584\n",
      "Iteration 507, Batch: 47, Loss: 0.05016490817070007\n",
      "Iteration 507, Batch: 48, Loss: 0.05210406705737114\n",
      "Iteration 507, Batch: 49, Loss: 0.033169396221637726\n",
      "Number of layers: 10\n",
      "Iteration 508, Batch: 0, Loss: 0.04938467964529991\n",
      "Iteration 508, Batch: 1, Loss: 0.05511883273720741\n",
      "Iteration 508, Batch: 2, Loss: 0.0542696937918663\n",
      "Iteration 508, Batch: 3, Loss: 0.06747865676879883\n",
      "Iteration 508, Batch: 4, Loss: 0.05470453202724457\n",
      "Iteration 508, Batch: 5, Loss: 0.054255254566669464\n",
      "Iteration 508, Batch: 6, Loss: 0.037630535662174225\n",
      "Iteration 508, Batch: 7, Loss: 0.05524351820349693\n",
      "Iteration 508, Batch: 8, Loss: 0.06729605793952942\n",
      "Iteration 508, Batch: 9, Loss: 0.046430885791778564\n",
      "Iteration 508, Batch: 10, Loss: 0.039169810712337494\n",
      "Iteration 508, Batch: 11, Loss: 0.04273839294910431\n",
      "Iteration 508, Batch: 12, Loss: 0.07743260264396667\n",
      "Iteration 508, Batch: 13, Loss: 0.04216819629073143\n",
      "Iteration 508, Batch: 14, Loss: 0.05270213633775711\n",
      "Iteration 508, Batch: 15, Loss: 0.04284863546490669\n",
      "Iteration 508, Batch: 16, Loss: 0.03308674693107605\n",
      "Iteration 508, Batch: 17, Loss: 0.031859491020441055\n",
      "Iteration 508, Batch: 18, Loss: 0.03907538205385208\n",
      "Iteration 508, Batch: 19, Loss: 0.044466283172369\n",
      "Iteration 508, Batch: 20, Loss: 0.03569496423006058\n",
      "Iteration 508, Batch: 21, Loss: 0.04692605882883072\n",
      "Iteration 508, Batch: 22, Loss: 0.06201363727450371\n",
      "Iteration 508, Batch: 23, Loss: 0.04619117081165314\n",
      "Iteration 508, Batch: 24, Loss: 0.06253563612699509\n",
      "Iteration 508, Batch: 25, Loss: 0.03548208996653557\n",
      "Iteration 508, Batch: 26, Loss: 0.041710853576660156\n",
      "Iteration 508, Batch: 27, Loss: 0.05578940361738205\n",
      "Iteration 508, Batch: 28, Loss: 0.06240050122141838\n",
      "Iteration 508, Batch: 29, Loss: 0.042645521461963654\n",
      "Iteration 508, Batch: 30, Loss: 0.07413051277399063\n",
      "Iteration 508, Batch: 31, Loss: 0.046094611287117004\n",
      "Iteration 508, Batch: 32, Loss: 0.07229401171207428\n",
      "Iteration 508, Batch: 33, Loss: 0.06689609587192535\n",
      "Iteration 508, Batch: 34, Loss: 0.04373832419514656\n",
      "Iteration 508, Batch: 35, Loss: 0.05224745720624924\n",
      "Iteration 508, Batch: 36, Loss: 0.06603319942951202\n",
      "Iteration 508, Batch: 37, Loss: 0.06639585644006729\n",
      "Iteration 508, Batch: 38, Loss: 0.05697856843471527\n",
      "Iteration 508, Batch: 39, Loss: 0.04025736823678017\n",
      "Iteration 508, Batch: 40, Loss: 0.04109909012913704\n",
      "Iteration 508, Batch: 41, Loss: 0.056516122072935104\n",
      "Iteration 508, Batch: 42, Loss: 0.06360120326280594\n",
      "Iteration 508, Batch: 43, Loss: 0.03805073723196983\n",
      "Iteration 508, Batch: 44, Loss: 0.05104167386889458\n",
      "Iteration 508, Batch: 45, Loss: 0.04581792280077934\n",
      "Iteration 508, Batch: 46, Loss: 0.04067782312631607\n",
      "Iteration 508, Batch: 47, Loss: 0.03559674695134163\n",
      "Iteration 508, Batch: 48, Loss: 0.035065386444330215\n",
      "Iteration 508, Batch: 49, Loss: 0.0838225707411766\n",
      "Number of layers: 10\n",
      "Iteration 509, Batch: 0, Loss: 0.04148649051785469\n",
      "Iteration 509, Batch: 1, Loss: 0.03155744448304176\n",
      "Iteration 509, Batch: 2, Loss: 0.03771029785275459\n",
      "Iteration 509, Batch: 3, Loss: 0.06828951835632324\n",
      "Iteration 509, Batch: 4, Loss: 0.050478629767894745\n",
      "Iteration 509, Batch: 5, Loss: 0.03898587450385094\n",
      "Iteration 509, Batch: 6, Loss: 0.06930317729711533\n",
      "Iteration 509, Batch: 7, Loss: 0.05151328817009926\n",
      "Iteration 509, Batch: 8, Loss: 0.03779783472418785\n",
      "Iteration 509, Batch: 9, Loss: 0.03759392723441124\n",
      "Iteration 509, Batch: 10, Loss: 0.04001232981681824\n",
      "Iteration 509, Batch: 11, Loss: 0.027469569817185402\n",
      "Iteration 509, Batch: 12, Loss: 0.05554613098502159\n",
      "Iteration 509, Batch: 13, Loss: 0.05869985744357109\n",
      "Iteration 509, Batch: 14, Loss: 0.025241278111934662\n",
      "Iteration 509, Batch: 15, Loss: 0.045032091438770294\n",
      "Iteration 509, Batch: 16, Loss: 0.05345692113041878\n",
      "Iteration 509, Batch: 17, Loss: 0.05564951151609421\n",
      "Iteration 509, Batch: 18, Loss: 0.05539008975028992\n",
      "Iteration 509, Batch: 19, Loss: 0.0463104248046875\n",
      "Iteration 509, Batch: 20, Loss: 0.038551896810531616\n",
      "Iteration 509, Batch: 21, Loss: 0.06849727779626846\n",
      "Iteration 509, Batch: 22, Loss: 0.058520421385765076\n",
      "Iteration 509, Batch: 23, Loss: 0.03294997289776802\n",
      "Iteration 509, Batch: 24, Loss: 0.06466366350650787\n",
      "Iteration 509, Batch: 25, Loss: 0.040823765099048615\n",
      "Iteration 509, Batch: 26, Loss: 0.04280516505241394\n",
      "Iteration 509, Batch: 27, Loss: 0.021535582840442657\n",
      "Iteration 509, Batch: 28, Loss: 0.032255884259939194\n",
      "Iteration 509, Batch: 29, Loss: 0.03537124767899513\n",
      "Iteration 509, Batch: 30, Loss: 0.04845568537712097\n",
      "Iteration 509, Batch: 31, Loss: 0.03199676051735878\n",
      "Iteration 509, Batch: 32, Loss: 0.04175794869661331\n",
      "Iteration 509, Batch: 33, Loss: 0.06519703567028046\n",
      "Iteration 509, Batch: 34, Loss: 0.046572282910346985\n",
      "Iteration 509, Batch: 35, Loss: 0.0845259577035904\n",
      "Iteration 509, Batch: 36, Loss: 0.04814649373292923\n",
      "Iteration 509, Batch: 37, Loss: 0.07948081195354462\n",
      "Iteration 509, Batch: 38, Loss: 0.08620376139879227\n",
      "Iteration 509, Batch: 39, Loss: 0.05294195935130119\n",
      "Iteration 509, Batch: 40, Loss: 0.06675044447183609\n",
      "Iteration 509, Batch: 41, Loss: 0.04128748178482056\n",
      "Iteration 509, Batch: 42, Loss: 0.055637624114751816\n",
      "Iteration 509, Batch: 43, Loss: 0.041736211627721786\n",
      "Iteration 509, Batch: 44, Loss: 0.06141720339655876\n",
      "Iteration 509, Batch: 45, Loss: 0.03150131553411484\n",
      "Iteration 509, Batch: 46, Loss: 0.04564676061272621\n",
      "Iteration 509, Batch: 47, Loss: 0.048092182725667953\n",
      "Iteration 509, Batch: 48, Loss: 0.02858162298798561\n",
      "Iteration 509, Batch: 49, Loss: 0.05801995098590851\n",
      "Number of layers: 10\n",
      "Iteration 510, Batch: 0, Loss: 0.02903316356241703\n",
      "Iteration 510, Batch: 1, Loss: 0.03825816884636879\n",
      "Iteration 510, Batch: 2, Loss: 0.04170773923397064\n",
      "Iteration 510, Batch: 3, Loss: 0.05685708299279213\n",
      "Iteration 510, Batch: 4, Loss: 0.06343470513820648\n",
      "Iteration 510, Batch: 5, Loss: 0.05472632125020027\n",
      "Iteration 510, Batch: 6, Loss: 0.037818651646375656\n",
      "Iteration 510, Batch: 7, Loss: 0.0399455688893795\n",
      "Iteration 510, Batch: 8, Loss: 0.054349251091480255\n",
      "Iteration 510, Batch: 9, Loss: 0.06896750628948212\n",
      "Iteration 510, Batch: 10, Loss: 0.06482290476560593\n",
      "Iteration 510, Batch: 11, Loss: 0.030923299491405487\n",
      "Iteration 510, Batch: 12, Loss: 0.0569099523127079\n",
      "Iteration 510, Batch: 13, Loss: 0.03933599218726158\n",
      "Iteration 510, Batch: 14, Loss: 0.05811932310461998\n",
      "Iteration 510, Batch: 15, Loss: 0.07834971696138382\n",
      "Iteration 510, Batch: 16, Loss: 0.046841200441122055\n",
      "Iteration 510, Batch: 17, Loss: 0.04567507281899452\n",
      "Iteration 510, Batch: 18, Loss: 0.08414453268051147\n",
      "Iteration 510, Batch: 19, Loss: 0.041960377246141434\n",
      "Iteration 510, Batch: 20, Loss: 0.043885231018066406\n",
      "Iteration 510, Batch: 21, Loss: 0.06826543807983398\n",
      "Iteration 510, Batch: 22, Loss: 0.052202191203832626\n",
      "Iteration 510, Batch: 23, Loss: 0.044097986072301865\n",
      "Iteration 510, Batch: 24, Loss: 0.04965881630778313\n",
      "Iteration 510, Batch: 25, Loss: 0.039851948618888855\n",
      "Iteration 510, Batch: 26, Loss: 0.050236448645591736\n",
      "Iteration 510, Batch: 27, Loss: 0.0558217316865921\n",
      "Iteration 510, Batch: 28, Loss: 0.02258330211043358\n",
      "Iteration 510, Batch: 29, Loss: 0.04058636724948883\n",
      "Iteration 510, Batch: 30, Loss: 0.036761023104190826\n",
      "Iteration 510, Batch: 31, Loss: 0.04585661739110947\n",
      "Iteration 510, Batch: 32, Loss: 0.03871205076575279\n",
      "Iteration 510, Batch: 33, Loss: 0.05199846625328064\n",
      "Iteration 510, Batch: 34, Loss: 0.06209912523627281\n",
      "Iteration 510, Batch: 35, Loss: 0.030831502750515938\n",
      "Iteration 510, Batch: 36, Loss: 0.0399010106921196\n",
      "Iteration 510, Batch: 37, Loss: 0.06896849721670151\n",
      "Iteration 510, Batch: 38, Loss: 0.060482367873191833\n",
      "Iteration 510, Batch: 39, Loss: 0.0362941212952137\n",
      "Iteration 510, Batch: 40, Loss: 0.042290009558200836\n",
      "Iteration 510, Batch: 41, Loss: 0.0357801653444767\n",
      "Iteration 510, Batch: 42, Loss: 0.049033861607313156\n",
      "Iteration 510, Batch: 43, Loss: 0.043339259922504425\n",
      "Iteration 510, Batch: 44, Loss: 0.05424584820866585\n",
      "Iteration 510, Batch: 45, Loss: 0.045773591846227646\n",
      "Iteration 510, Batch: 46, Loss: 0.0420558862388134\n",
      "Iteration 510, Batch: 47, Loss: 0.04681290686130524\n",
      "Iteration 510, Batch: 48, Loss: 0.06425561755895615\n",
      "Iteration 510, Batch: 49, Loss: 0.07324368506669998\n",
      "Number of layers: 10\n",
      "Iteration 511, Batch: 0, Loss: 0.10360091924667358\n",
      "Iteration 511, Batch: 1, Loss: 0.07432377338409424\n",
      "Iteration 511, Batch: 2, Loss: 0.07207352668046951\n",
      "Iteration 511, Batch: 3, Loss: 0.10036282241344452\n",
      "Iteration 511, Batch: 4, Loss: 0.0824485495686531\n",
      "Iteration 511, Batch: 5, Loss: 0.04955096170306206\n",
      "Iteration 511, Batch: 6, Loss: 0.05915333703160286\n",
      "Iteration 511, Batch: 7, Loss: 0.030671309679746628\n",
      "Iteration 511, Batch: 8, Loss: 0.06838290393352509\n",
      "Iteration 511, Batch: 9, Loss: 0.0614103265106678\n",
      "Iteration 511, Batch: 10, Loss: 0.05568411573767662\n",
      "Iteration 511, Batch: 11, Loss: 0.024028122425079346\n",
      "Iteration 511, Batch: 12, Loss: 0.033045656979084015\n",
      "Iteration 511, Batch: 13, Loss: 0.027149049565196037\n",
      "Iteration 511, Batch: 14, Loss: 0.0616987869143486\n",
      "Iteration 511, Batch: 15, Loss: 0.04286830872297287\n",
      "Iteration 511, Batch: 16, Loss: 0.07136823236942291\n",
      "Iteration 511, Batch: 17, Loss: 0.0705191045999527\n",
      "Iteration 511, Batch: 18, Loss: 0.04316645488142967\n",
      "Iteration 511, Batch: 19, Loss: 0.056777339428663254\n",
      "Iteration 511, Batch: 20, Loss: 0.05878729745745659\n",
      "Iteration 511, Batch: 21, Loss: 0.06354717910289764\n",
      "Iteration 511, Batch: 22, Loss: 0.029523415490984917\n",
      "Iteration 511, Batch: 23, Loss: 0.07550348341464996\n",
      "Iteration 511, Batch: 24, Loss: 0.060124289244413376\n",
      "Iteration 511, Batch: 25, Loss: 0.06090546399354935\n",
      "Iteration 511, Batch: 26, Loss: 0.053845036774873734\n",
      "Iteration 511, Batch: 27, Loss: 0.05595509707927704\n",
      "Iteration 511, Batch: 28, Loss: 0.04919739440083504\n",
      "Iteration 511, Batch: 29, Loss: 0.07735332101583481\n",
      "Iteration 511, Batch: 30, Loss: 0.04763515293598175\n",
      "Iteration 511, Batch: 31, Loss: 0.07787888497114182\n",
      "Iteration 511, Batch: 32, Loss: 0.08574972301721573\n",
      "Iteration 511, Batch: 33, Loss: 0.05725198611617088\n",
      "Iteration 511, Batch: 34, Loss: 0.04676109552383423\n",
      "Iteration 511, Batch: 35, Loss: 0.053734492510557175\n",
      "Iteration 511, Batch: 36, Loss: 0.06500232219696045\n",
      "Iteration 511, Batch: 37, Loss: 0.06058116629719734\n",
      "Iteration 511, Batch: 38, Loss: 0.03171072527766228\n",
      "Iteration 511, Batch: 39, Loss: 0.03964716196060181\n",
      "Iteration 511, Batch: 40, Loss: 0.08179305493831635\n",
      "Iteration 511, Batch: 41, Loss: 0.0664912685751915\n",
      "Iteration 511, Batch: 42, Loss: 0.045771144330501556\n",
      "Iteration 511, Batch: 43, Loss: 0.05628935620188713\n",
      "Iteration 511, Batch: 44, Loss: 0.02558969147503376\n",
      "Iteration 511, Batch: 45, Loss: 0.029834050685167313\n",
      "Iteration 511, Batch: 46, Loss: 0.06431039422750473\n",
      "Iteration 511, Batch: 47, Loss: 0.03107321634888649\n",
      "Iteration 511, Batch: 48, Loss: 0.0374501571059227\n",
      "Iteration 511, Batch: 49, Loss: 0.07241637259721756\n",
      "Number of layers: 10\n",
      "Iteration 512, Batch: 0, Loss: 0.042225949466228485\n",
      "Iteration 512, Batch: 1, Loss: 0.048339203000068665\n",
      "Iteration 512, Batch: 2, Loss: 0.03927642107009888\n",
      "Iteration 512, Batch: 3, Loss: 0.053019847720861435\n",
      "Iteration 512, Batch: 4, Loss: 0.035611819475889206\n",
      "Iteration 512, Batch: 5, Loss: 0.047151364386081696\n",
      "Iteration 512, Batch: 6, Loss: 0.05360109731554985\n",
      "Iteration 512, Batch: 7, Loss: 0.04807448759675026\n",
      "Iteration 512, Batch: 8, Loss: 0.07452783733606339\n",
      "Iteration 512, Batch: 9, Loss: 0.0209715124219656\n",
      "Iteration 512, Batch: 10, Loss: 0.05208569020032883\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 56\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Backpropagate\u001b[39;00m\n\u001b[1;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Batch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(it, counter, loss\u001b[38;5;241m.\u001b[39mitem()))\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# Update counter\u001b[39;00m\n\u001b[1;32m     59\u001b[0m counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Specify parameters\n",
    "N = 10000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "train_data = sample_data(N, n, k, device=device)\n",
    "train_dataset = SubmatrixDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "# Specify parameters\n",
    "num_epochs = 550\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "\n",
    "# Training routine\n",
    "model = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"test_2_intermediate_1.pth\", weights_only=False))\n",
    "\n",
    "# Store optimal model\n",
    "model_opt = deepcopy(model)\n",
    "min_loss = float(\"inf\")\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "\n",
    "for it in range(451, num_epochs):\n",
    "    counter = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    # If iteration is small, train with 9 layers\n",
    "    if it < 250:\n",
    "        model.num_layers = 7\n",
    "    elif it < 400:\n",
    "        model.num_layers = 9\n",
    "    else:\n",
    "        model.num_layers = 10\n",
    "\n",
    "    print(\"Number of layers: {}\".format(model.num_layers))\n",
    "    # Iterate through batches\n",
    "    for batch in train_dataloader:\n",
    "        # Get loss from model\n",
    "        loss = mc_loss_batch_simul(model, batch, time_array, n, k, time_threshold=400, p_bad=0.05, device=device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "        # Backpropagate\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Iteration {}, Batch: {}, Loss: {}\".format(it, counter, loss.item()))\n",
    "\n",
    "        # Update counter\n",
    "        counter += 1\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    # Update best model\n",
    "    with torch.no_grad():\n",
    "        if loss_total < min_loss:\n",
    "            min_loss = loss_total\n",
    "            model_opt = deepcopy(model)\n",
    "\n",
    "# Save optimal model to save time for generation\n",
    "torch.save(model_opt.state_dict(), \"test_2_intermediate_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c62fe169-d437-446a-b7e7-8686603c7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimal model to save time for generation\n",
    "torch.save(model_opt.state_dict(), \"test_2_intermediate_1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "845ad70b-d50e-4e0f-a027-39e50b316e58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Batch: 0, Loss: 0.330413818359375\n",
      "Iteration 0, Batch: 1, Loss: 0.3485041856765747\n",
      "Iteration 0, Batch: 2, Loss: 0.34244078397750854\n",
      "Iteration 0, Batch: 3, Loss: 0.3060445189476013\n",
      "Iteration 0, Batch: 4, Loss: 0.3012356758117676\n",
      "Iteration 0, Batch: 5, Loss: 0.3082054555416107\n",
      "Iteration 0, Batch: 6, Loss: 0.2291203886270523\n",
      "Iteration 0, Batch: 7, Loss: 0.28342196345329285\n",
      "Iteration 0, Batch: 8, Loss: 0.2412627935409546\n",
      "Iteration 0, Batch: 9, Loss: 0.2583494782447815\n",
      "Iteration 0, Batch: 10, Loss: 0.19799531996250153\n",
      "Iteration 0, Batch: 11, Loss: 0.220412939786911\n",
      "Iteration 0, Batch: 12, Loss: 0.2063935399055481\n",
      "Iteration 0, Batch: 13, Loss: 0.16330884397029877\n",
      "Iteration 0, Batch: 14, Loss: 0.19248099625110626\n",
      "Iteration 0, Batch: 15, Loss: 0.15064303576946259\n",
      "Iteration 0, Batch: 16, Loss: 0.15794652700424194\n",
      "Iteration 0, Batch: 17, Loss: 0.16061237454414368\n",
      "Iteration 0, Batch: 18, Loss: 0.17020124197006226\n",
      "Iteration 0, Batch: 19, Loss: 0.1301945447921753\n",
      "Iteration 0, Batch: 20, Loss: 0.13885439932346344\n",
      "Iteration 0, Batch: 21, Loss: 0.12116210907697678\n",
      "Iteration 0, Batch: 22, Loss: 0.11854945868253708\n",
      "Iteration 0, Batch: 23, Loss: 0.14556047320365906\n",
      "Iteration 0, Batch: 24, Loss: 0.09678470343351364\n",
      "Iteration 0, Batch: 25, Loss: 0.1011444628238678\n",
      "Iteration 0, Batch: 26, Loss: 0.09202182292938232\n",
      "Iteration 0, Batch: 27, Loss: 0.07487122714519501\n",
      "Iteration 0, Batch: 28, Loss: 0.09460773319005966\n",
      "Iteration 0, Batch: 29, Loss: 0.10007364302873611\n",
      "Iteration 0, Batch: 30, Loss: 0.12164348363876343\n",
      "Iteration 0, Batch: 31, Loss: 0.10084863752126694\n",
      "Iteration 0, Batch: 32, Loss: 0.1404690146446228\n",
      "Iteration 0, Batch: 33, Loss: 0.16826722025871277\n",
      "Iteration 0, Batch: 34, Loss: 0.0850333496928215\n",
      "Iteration 0, Batch: 35, Loss: 0.12820321321487427\n",
      "Iteration 0, Batch: 36, Loss: 0.1090025082230568\n",
      "Iteration 0, Batch: 37, Loss: 0.08216516673564911\n",
      "Iteration 0, Batch: 38, Loss: 0.12420070171356201\n",
      "Iteration 0, Batch: 39, Loss: 0.11260855197906494\n",
      "Iteration 0, Batch: 40, Loss: 0.09960772097110748\n",
      "Iteration 0, Batch: 41, Loss: 0.0863448977470398\n",
      "Iteration 0, Batch: 42, Loss: 0.08348847925662994\n",
      "Iteration 0, Batch: 43, Loss: 0.08657262474298477\n",
      "Iteration 0, Batch: 44, Loss: 0.10757274180650711\n",
      "Iteration 0, Batch: 45, Loss: 0.09635257720947266\n",
      "Iteration 0, Batch: 46, Loss: 0.11195121705532074\n",
      "Iteration 0, Batch: 47, Loss: 0.0985378846526146\n",
      "Iteration 0, Batch: 48, Loss: 0.07638666778802872\n",
      "Iteration 0, Batch: 49, Loss: 0.09480956941843033\n",
      "Iteration 1, Batch: 0, Loss: 0.09088888764381409\n",
      "Iteration 1, Batch: 1, Loss: 0.08629142493009567\n",
      "Iteration 1, Batch: 2, Loss: 0.05512089654803276\n",
      "Iteration 1, Batch: 3, Loss: 0.12972432374954224\n",
      "Iteration 1, Batch: 4, Loss: 0.09634721279144287\n",
      "Iteration 1, Batch: 5, Loss: 0.05644480511546135\n",
      "Iteration 1, Batch: 6, Loss: 0.07558589428663254\n",
      "Iteration 1, Batch: 7, Loss: 0.10276023298501968\n",
      "Iteration 1, Batch: 8, Loss: 0.09846293926239014\n",
      "Iteration 1, Batch: 9, Loss: 0.12581540644168854\n",
      "Iteration 1, Batch: 10, Loss: 0.10883162170648575\n",
      "Iteration 1, Batch: 11, Loss: 0.0797400251030922\n",
      "Iteration 1, Batch: 12, Loss: 0.08257942646741867\n",
      "Iteration 1, Batch: 13, Loss: 0.07869761437177658\n",
      "Iteration 1, Batch: 14, Loss: 0.07295288145542145\n",
      "Iteration 1, Batch: 15, Loss: 0.04694660007953644\n",
      "Iteration 1, Batch: 16, Loss: 0.09717118740081787\n",
      "Iteration 1, Batch: 17, Loss: 0.09979596734046936\n",
      "Iteration 1, Batch: 18, Loss: 0.09402269870042801\n",
      "Iteration 1, Batch: 19, Loss: 0.08599244803190231\n",
      "Iteration 1, Batch: 20, Loss: 0.1310238391160965\n",
      "Iteration 1, Batch: 21, Loss: 0.0856427550315857\n",
      "Iteration 1, Batch: 22, Loss: 0.05883753299713135\n",
      "Iteration 1, Batch: 23, Loss: 0.07813321053981781\n",
      "Iteration 1, Batch: 24, Loss: 0.08538062870502472\n",
      "Iteration 1, Batch: 25, Loss: 0.056499991565942764\n",
      "Iteration 1, Batch: 26, Loss: 0.06999386101961136\n",
      "Iteration 1, Batch: 27, Loss: 0.07828308641910553\n",
      "Iteration 1, Batch: 28, Loss: 0.08136115968227386\n",
      "Iteration 1, Batch: 29, Loss: 0.08637576550245285\n",
      "Iteration 1, Batch: 30, Loss: 0.066823311150074\n",
      "Iteration 1, Batch: 31, Loss: 0.0874868631362915\n",
      "Iteration 1, Batch: 32, Loss: 0.09165835380554199\n",
      "Iteration 1, Batch: 33, Loss: 0.10612687468528748\n",
      "Iteration 1, Batch: 34, Loss: 0.08178054541349411\n",
      "Iteration 1, Batch: 35, Loss: 0.08214858174324036\n",
      "Iteration 1, Batch: 36, Loss: 0.09322545677423477\n",
      "Iteration 1, Batch: 37, Loss: 0.08501625061035156\n",
      "Iteration 1, Batch: 38, Loss: 0.04559021070599556\n",
      "Iteration 1, Batch: 39, Loss: 0.08850666880607605\n",
      "Iteration 1, Batch: 40, Loss: 0.05511188507080078\n",
      "Iteration 1, Batch: 41, Loss: 0.10445993393659592\n",
      "Iteration 1, Batch: 42, Loss: 0.08339183777570724\n",
      "Iteration 1, Batch: 43, Loss: 0.10443209111690521\n",
      "Iteration 1, Batch: 44, Loss: 0.02794070728123188\n",
      "Iteration 1, Batch: 45, Loss: 0.08366880565881729\n",
      "Iteration 1, Batch: 46, Loss: 0.07091788947582245\n",
      "Iteration 1, Batch: 47, Loss: 0.07067122310400009\n",
      "Iteration 1, Batch: 48, Loss: 0.07830113917589188\n",
      "Iteration 1, Batch: 49, Loss: 0.08558952063322067\n",
      "Iteration 2, Batch: 0, Loss: 0.051007870584726334\n",
      "Iteration 2, Batch: 1, Loss: 0.08238134533166885\n",
      "Iteration 2, Batch: 2, Loss: 0.08664438873529434\n",
      "Iteration 2, Batch: 3, Loss: 0.07638492435216904\n",
      "Iteration 2, Batch: 4, Loss: 0.07004699856042862\n",
      "Iteration 2, Batch: 5, Loss: 0.05999916046857834\n",
      "Iteration 2, Batch: 6, Loss: 0.06622196733951569\n",
      "Iteration 2, Batch: 7, Loss: 0.08392190933227539\n",
      "Iteration 2, Batch: 8, Loss: 0.08022652566432953\n",
      "Iteration 2, Batch: 9, Loss: 0.06530696898698807\n",
      "Iteration 2, Batch: 10, Loss: 0.10566330701112747\n",
      "Iteration 2, Batch: 11, Loss: 0.06266102194786072\n",
      "Iteration 2, Batch: 12, Loss: 0.06816526502370834\n",
      "Iteration 2, Batch: 13, Loss: 0.08289219439029694\n",
      "Iteration 2, Batch: 14, Loss: 0.0877072885632515\n",
      "Iteration 2, Batch: 15, Loss: 0.06954698264598846\n",
      "Iteration 2, Batch: 16, Loss: 0.06247054040431976\n",
      "Iteration 2, Batch: 17, Loss: 0.08525140583515167\n",
      "Iteration 2, Batch: 18, Loss: 0.10340426862239838\n",
      "Iteration 2, Batch: 19, Loss: 0.07237888872623444\n",
      "Iteration 2, Batch: 20, Loss: 0.08909180760383606\n",
      "Iteration 2, Batch: 21, Loss: 0.09369727969169617\n",
      "Iteration 2, Batch: 22, Loss: 0.08755992352962494\n",
      "Iteration 2, Batch: 23, Loss: 0.0943484827876091\n",
      "Iteration 2, Batch: 24, Loss: 0.059247054159641266\n",
      "Iteration 2, Batch: 25, Loss: 0.04547444358468056\n",
      "Iteration 2, Batch: 26, Loss: 0.07704458385705948\n",
      "Iteration 2, Batch: 27, Loss: 0.09104842692613602\n",
      "Iteration 2, Batch: 28, Loss: 0.06419289857149124\n",
      "Iteration 2, Batch: 29, Loss: 0.07432553917169571\n",
      "Iteration 2, Batch: 30, Loss: 0.06790086627006531\n",
      "Iteration 2, Batch: 31, Loss: 0.04874106124043465\n",
      "Iteration 2, Batch: 32, Loss: 0.058479875326156616\n",
      "Iteration 2, Batch: 33, Loss: 0.056876782327890396\n",
      "Iteration 2, Batch: 34, Loss: 0.06738411635160446\n",
      "Iteration 2, Batch: 35, Loss: 0.045178644359111786\n",
      "Iteration 2, Batch: 36, Loss: 0.08049983531236649\n",
      "Iteration 2, Batch: 37, Loss: 0.08974292874336243\n",
      "Iteration 2, Batch: 38, Loss: 0.08706813305616379\n",
      "Iteration 2, Batch: 39, Loss: 0.07915790379047394\n",
      "Iteration 2, Batch: 40, Loss: 0.10297039896249771\n",
      "Iteration 2, Batch: 41, Loss: 0.04925072565674782\n",
      "Iteration 2, Batch: 42, Loss: 0.06344310939311981\n",
      "Iteration 2, Batch: 43, Loss: 0.08365081250667572\n",
      "Iteration 2, Batch: 44, Loss: 0.0629688948392868\n",
      "Iteration 2, Batch: 45, Loss: 0.0796746015548706\n",
      "Iteration 2, Batch: 46, Loss: 0.07896251231431961\n",
      "Iteration 2, Batch: 47, Loss: 0.04749757796525955\n",
      "Iteration 2, Batch: 48, Loss: 0.0678325816988945\n",
      "Iteration 2, Batch: 49, Loss: 0.04669582098722458\n",
      "Iteration 3, Batch: 0, Loss: 0.07343145459890366\n",
      "Iteration 3, Batch: 1, Loss: 0.05091796815395355\n",
      "Iteration 3, Batch: 2, Loss: 0.0789923369884491\n",
      "Iteration 3, Batch: 3, Loss: 0.07072611898183823\n",
      "Iteration 3, Batch: 4, Loss: 0.10147389769554138\n",
      "Iteration 3, Batch: 5, Loss: 0.08851108700037003\n",
      "Iteration 3, Batch: 6, Loss: 0.05955900996923447\n",
      "Iteration 3, Batch: 7, Loss: 0.09039775282144547\n",
      "Iteration 3, Batch: 8, Loss: 0.07624809443950653\n",
      "Iteration 3, Batch: 9, Loss: 0.049782875925302505\n",
      "Iteration 3, Batch: 10, Loss: 0.09942577034235\n",
      "Iteration 3, Batch: 11, Loss: 0.09310764074325562\n",
      "Iteration 3, Batch: 12, Loss: 0.07400409132242203\n",
      "Iteration 3, Batch: 13, Loss: 0.05613141506910324\n",
      "Iteration 3, Batch: 14, Loss: 0.04323117062449455\n",
      "Iteration 3, Batch: 15, Loss: 0.04311408847570419\n",
      "Iteration 3, Batch: 16, Loss: 0.06185624375939369\n",
      "Iteration 3, Batch: 17, Loss: 0.0684126764535904\n",
      "Iteration 3, Batch: 18, Loss: 0.05465404689311981\n",
      "Iteration 3, Batch: 19, Loss: 0.0734364315867424\n",
      "Iteration 3, Batch: 20, Loss: 0.05774463340640068\n",
      "Iteration 3, Batch: 21, Loss: 0.08353804051876068\n",
      "Iteration 3, Batch: 22, Loss: 0.08378169685602188\n",
      "Iteration 3, Batch: 23, Loss: 0.07674045115709305\n",
      "Iteration 3, Batch: 24, Loss: 0.07385870814323425\n",
      "Iteration 3, Batch: 25, Loss: 0.08455964922904968\n",
      "Iteration 3, Batch: 26, Loss: 0.07517712563276291\n",
      "Iteration 3, Batch: 27, Loss: 0.07923129200935364\n",
      "Iteration 3, Batch: 28, Loss: 0.07546424865722656\n",
      "Iteration 3, Batch: 29, Loss: 0.05824198201298714\n",
      "Iteration 3, Batch: 30, Loss: 0.04822788015007973\n",
      "Iteration 3, Batch: 31, Loss: 0.09703274816274643\n",
      "Iteration 3, Batch: 32, Loss: 0.07499369233846664\n",
      "Iteration 3, Batch: 33, Loss: 0.043339475989341736\n",
      "Iteration 3, Batch: 34, Loss: 0.06869552284479141\n",
      "Iteration 3, Batch: 35, Loss: 0.07932379096746445\n",
      "Iteration 3, Batch: 36, Loss: 0.07595363259315491\n",
      "Iteration 3, Batch: 37, Loss: 0.05333457514643669\n",
      "Iteration 3, Batch: 38, Loss: 0.05256801098585129\n",
      "Iteration 3, Batch: 39, Loss: 0.04730811342597008\n",
      "Iteration 3, Batch: 40, Loss: 0.06411541998386383\n",
      "Iteration 3, Batch: 41, Loss: 0.053680792450904846\n",
      "Iteration 3, Batch: 42, Loss: 0.05019078031182289\n",
      "Iteration 3, Batch: 43, Loss: 0.06003771349787712\n",
      "Iteration 3, Batch: 44, Loss: 0.072425477206707\n",
      "Iteration 3, Batch: 45, Loss: 0.0649658590555191\n",
      "Iteration 3, Batch: 46, Loss: 0.05687618628144264\n",
      "Iteration 3, Batch: 47, Loss: 0.08574438095092773\n",
      "Iteration 3, Batch: 48, Loss: 0.08662824332714081\n",
      "Iteration 3, Batch: 49, Loss: 0.07321037352085114\n",
      "Iteration 4, Batch: 0, Loss: 0.09378356486558914\n",
      "Iteration 4, Batch: 1, Loss: 0.05265180394053459\n",
      "Iteration 4, Batch: 2, Loss: 0.04518651217222214\n",
      "Iteration 4, Batch: 3, Loss: 0.05784638226032257\n",
      "Iteration 4, Batch: 4, Loss: 0.08712822943925858\n",
      "Iteration 4, Batch: 5, Loss: 0.07936934381723404\n",
      "Iteration 4, Batch: 6, Loss: 0.07325758039951324\n",
      "Iteration 4, Batch: 7, Loss: 0.06875062733888626\n",
      "Iteration 4, Batch: 8, Loss: 0.07482191920280457\n",
      "Iteration 4, Batch: 9, Loss: 0.07489722967147827\n",
      "Iteration 4, Batch: 10, Loss: 0.09662635624408722\n",
      "Iteration 4, Batch: 11, Loss: 0.07186778634786606\n",
      "Iteration 4, Batch: 12, Loss: 0.05628371238708496\n",
      "Iteration 4, Batch: 13, Loss: 0.0805361270904541\n",
      "Iteration 4, Batch: 14, Loss: 0.05103315785527229\n",
      "Iteration 4, Batch: 15, Loss: 0.1021750271320343\n",
      "Iteration 4, Batch: 16, Loss: 0.09352592378854752\n",
      "Iteration 4, Batch: 17, Loss: 0.04584940895438194\n",
      "Iteration 4, Batch: 18, Loss: 0.06548211723566055\n",
      "Iteration 4, Batch: 19, Loss: 0.10032068938016891\n",
      "Iteration 4, Batch: 20, Loss: 0.05881405621767044\n",
      "Iteration 4, Batch: 21, Loss: 0.08590016514062881\n",
      "Iteration 4, Batch: 22, Loss: 0.04770367592573166\n",
      "Iteration 4, Batch: 23, Loss: 0.07056864351034164\n",
      "Iteration 4, Batch: 24, Loss: 0.07509491592645645\n",
      "Iteration 4, Batch: 25, Loss: 0.09235917776823044\n",
      "Iteration 4, Batch: 26, Loss: 0.06014784052968025\n",
      "Iteration 4, Batch: 27, Loss: 0.08244246989488602\n",
      "Iteration 4, Batch: 28, Loss: 0.08743379265069962\n",
      "Iteration 4, Batch: 29, Loss: 0.07845278084278107\n",
      "Iteration 4, Batch: 30, Loss: 0.08786903321743011\n",
      "Iteration 4, Batch: 31, Loss: 0.08370248973369598\n",
      "Iteration 4, Batch: 32, Loss: 0.08315704017877579\n",
      "Iteration 4, Batch: 33, Loss: 0.0777851939201355\n",
      "Iteration 4, Batch: 34, Loss: 0.054048050194978714\n",
      "Iteration 4, Batch: 35, Loss: 0.039441462606191635\n",
      "Iteration 4, Batch: 36, Loss: 0.06814085692167282\n",
      "Iteration 4, Batch: 37, Loss: 0.10433423519134521\n",
      "Iteration 4, Batch: 38, Loss: 0.07235989719629288\n",
      "Iteration 4, Batch: 39, Loss: 0.05668218806385994\n",
      "Iteration 4, Batch: 40, Loss: 0.05719660222530365\n",
      "Iteration 4, Batch: 41, Loss: 0.08354112505912781\n",
      "Iteration 4, Batch: 42, Loss: 0.05946965888142586\n",
      "Iteration 4, Batch: 43, Loss: 0.07375816255807877\n",
      "Iteration 4, Batch: 44, Loss: 0.0754520520567894\n",
      "Iteration 4, Batch: 45, Loss: 0.0554838702082634\n",
      "Iteration 4, Batch: 46, Loss: 0.0665532648563385\n",
      "Iteration 4, Batch: 47, Loss: 0.08998887985944748\n",
      "Iteration 4, Batch: 48, Loss: 0.0935492143034935\n",
      "Iteration 4, Batch: 49, Loss: 0.07109895348548889\n",
      "Iteration 5, Batch: 0, Loss: 0.069117970764637\n",
      "Iteration 5, Batch: 1, Loss: 0.11386756598949432\n",
      "Iteration 5, Batch: 2, Loss: 0.06362394243478775\n",
      "Iteration 5, Batch: 3, Loss: 0.07166239619255066\n",
      "Iteration 5, Batch: 4, Loss: 0.077382393181324\n",
      "Iteration 5, Batch: 5, Loss: 0.06346872448921204\n",
      "Iteration 5, Batch: 6, Loss: 0.07595162093639374\n",
      "Iteration 5, Batch: 7, Loss: 0.07408457249403\n",
      "Iteration 5, Batch: 8, Loss: 0.06686919182538986\n",
      "Iteration 5, Batch: 9, Loss: 0.06307821720838547\n",
      "Iteration 5, Batch: 10, Loss: 0.09236976504325867\n",
      "Iteration 5, Batch: 11, Loss: 0.07695158571004868\n",
      "Iteration 5, Batch: 12, Loss: 0.05259586125612259\n",
      "Iteration 5, Batch: 13, Loss: 0.05304655805230141\n",
      "Iteration 5, Batch: 14, Loss: 0.0903886929154396\n",
      "Iteration 5, Batch: 15, Loss: 0.055745791643857956\n",
      "Iteration 5, Batch: 16, Loss: 0.07949994504451752\n",
      "Iteration 5, Batch: 17, Loss: 0.09014195203781128\n",
      "Iteration 5, Batch: 18, Loss: 0.0755050927400589\n",
      "Iteration 5, Batch: 19, Loss: 0.0764651969075203\n",
      "Iteration 5, Batch: 20, Loss: 0.06271223723888397\n",
      "Iteration 5, Batch: 21, Loss: 0.06316667050123215\n",
      "Iteration 5, Batch: 22, Loss: 0.052772317081689835\n",
      "Iteration 5, Batch: 23, Loss: 0.07937436550855637\n",
      "Iteration 5, Batch: 24, Loss: 0.07626409083604813\n",
      "Iteration 5, Batch: 25, Loss: 0.08632782101631165\n",
      "Iteration 5, Batch: 26, Loss: 0.0581233948469162\n",
      "Iteration 5, Batch: 27, Loss: 0.05602118745446205\n",
      "Iteration 5, Batch: 28, Loss: 0.06315622478723526\n",
      "Iteration 5, Batch: 29, Loss: 0.059394944459199905\n",
      "Iteration 5, Batch: 30, Loss: 0.09170771390199661\n",
      "Iteration 5, Batch: 31, Loss: 0.07687469571828842\n",
      "Iteration 5, Batch: 32, Loss: 0.058904193341732025\n",
      "Iteration 5, Batch: 33, Loss: 0.05255862697958946\n",
      "Iteration 5, Batch: 34, Loss: 0.09352607280015945\n",
      "Iteration 5, Batch: 35, Loss: 0.061676181852817535\n",
      "Iteration 5, Batch: 36, Loss: 0.07145684212446213\n",
      "Iteration 5, Batch: 37, Loss: 0.08793500065803528\n",
      "Iteration 5, Batch: 38, Loss: 0.06730403751134872\n",
      "Iteration 5, Batch: 39, Loss: 0.05299929529428482\n",
      "Iteration 5, Batch: 40, Loss: 0.08344987034797668\n",
      "Iteration 5, Batch: 41, Loss: 0.093116395175457\n",
      "Iteration 5, Batch: 42, Loss: 0.08376765996217728\n",
      "Iteration 5, Batch: 43, Loss: 0.05478532239794731\n",
      "Iteration 5, Batch: 44, Loss: 0.09853223711252213\n",
      "Iteration 5, Batch: 45, Loss: 0.060964565724134445\n",
      "Iteration 5, Batch: 46, Loss: 0.05449414998292923\n",
      "Iteration 5, Batch: 47, Loss: 0.07454180717468262\n",
      "Iteration 5, Batch: 48, Loss: 0.07502290606498718\n",
      "Iteration 5, Batch: 49, Loss: 0.07033572345972061\n",
      "Iteration 6, Batch: 0, Loss: 0.0640922337770462\n",
      "Iteration 6, Batch: 1, Loss: 0.02319171465933323\n",
      "Iteration 6, Batch: 2, Loss: 0.05687688663601875\n",
      "Iteration 6, Batch: 3, Loss: 0.06849547475576401\n",
      "Iteration 6, Batch: 4, Loss: 0.05863097682595253\n",
      "Iteration 6, Batch: 5, Loss: 0.04556431621313095\n",
      "Iteration 6, Batch: 6, Loss: 0.036577604711055756\n",
      "Iteration 6, Batch: 7, Loss: 0.07784166932106018\n",
      "Iteration 6, Batch: 8, Loss: 0.09797954559326172\n",
      "Iteration 6, Batch: 9, Loss: 0.038423314690589905\n",
      "Iteration 6, Batch: 10, Loss: 0.06532008200883865\n",
      "Iteration 6, Batch: 11, Loss: 0.05760979652404785\n",
      "Iteration 6, Batch: 12, Loss: 0.0719766616821289\n",
      "Iteration 6, Batch: 13, Loss: 0.04878706857562065\n",
      "Iteration 6, Batch: 14, Loss: 0.06471604853868484\n",
      "Iteration 6, Batch: 15, Loss: 0.051907505840063095\n",
      "Iteration 6, Batch: 16, Loss: 0.08860541880130768\n",
      "Iteration 6, Batch: 17, Loss: 0.06766809523105621\n",
      "Iteration 6, Batch: 18, Loss: 0.06622274965047836\n",
      "Iteration 6, Batch: 19, Loss: 0.04753606766462326\n",
      "Iteration 6, Batch: 20, Loss: 0.06222544610500336\n",
      "Iteration 6, Batch: 21, Loss: 0.06771630048751831\n",
      "Iteration 6, Batch: 22, Loss: 0.056837499141693115\n",
      "Iteration 6, Batch: 23, Loss: 0.07286771386861801\n",
      "Iteration 6, Batch: 24, Loss: 0.06283369660377502\n",
      "Iteration 6, Batch: 25, Loss: 0.05317598208785057\n",
      "Iteration 6, Batch: 26, Loss: 0.0506468191742897\n",
      "Iteration 6, Batch: 27, Loss: 0.09824712574481964\n",
      "Iteration 6, Batch: 28, Loss: 0.06426706165075302\n",
      "Iteration 6, Batch: 29, Loss: 0.041527412831783295\n",
      "Iteration 6, Batch: 30, Loss: 0.08300098031759262\n",
      "Iteration 6, Batch: 31, Loss: 0.05599989742040634\n",
      "Iteration 6, Batch: 32, Loss: 0.07809937745332718\n",
      "Iteration 6, Batch: 33, Loss: 0.05669071897864342\n",
      "Iteration 6, Batch: 34, Loss: 0.06588329374790192\n",
      "Iteration 6, Batch: 35, Loss: 0.05267913639545441\n",
      "Iteration 6, Batch: 36, Loss: 0.09557203203439713\n",
      "Iteration 6, Batch: 37, Loss: 0.06267175823450089\n",
      "Iteration 6, Batch: 38, Loss: 0.06549038738012314\n",
      "Iteration 6, Batch: 39, Loss: 0.0560956746339798\n",
      "Iteration 6, Batch: 40, Loss: 0.07951141148805618\n",
      "Iteration 6, Batch: 41, Loss: 0.06595101952552795\n",
      "Iteration 6, Batch: 42, Loss: 0.08220954984426498\n",
      "Iteration 6, Batch: 43, Loss: 0.055087413638830185\n",
      "Iteration 6, Batch: 44, Loss: 0.07432758808135986\n",
      "Iteration 6, Batch: 45, Loss: 0.0721021369099617\n",
      "Iteration 6, Batch: 46, Loss: 0.06255723536014557\n",
      "Iteration 6, Batch: 47, Loss: 0.05536770820617676\n",
      "Iteration 6, Batch: 48, Loss: 0.06568815559148788\n",
      "Iteration 6, Batch: 49, Loss: 0.08074440062046051\n",
      "Iteration 7, Batch: 0, Loss: 0.08198251575231552\n",
      "Iteration 7, Batch: 1, Loss: 0.06430046260356903\n",
      "Iteration 7, Batch: 2, Loss: 0.08080170303583145\n",
      "Iteration 7, Batch: 3, Loss: 0.05583750456571579\n",
      "Iteration 7, Batch: 4, Loss: 0.07890116423368454\n",
      "Iteration 7, Batch: 5, Loss: 0.07531650364398956\n",
      "Iteration 7, Batch: 6, Loss: 0.05931510776281357\n",
      "Iteration 7, Batch: 7, Loss: 0.08568842709064484\n",
      "Iteration 7, Batch: 8, Loss: 0.05972615256905556\n",
      "Iteration 7, Batch: 9, Loss: 0.06464199721813202\n",
      "Iteration 7, Batch: 10, Loss: 0.06917791068553925\n",
      "Iteration 7, Batch: 11, Loss: 0.054133981466293335\n",
      "Iteration 7, Batch: 12, Loss: 0.047548212110996246\n",
      "Iteration 7, Batch: 13, Loss: 0.044898971915245056\n",
      "Iteration 7, Batch: 14, Loss: 0.06920075416564941\n",
      "Iteration 7, Batch: 15, Loss: 0.06262366473674774\n",
      "Iteration 7, Batch: 16, Loss: 0.08986835926771164\n",
      "Iteration 7, Batch: 17, Loss: 0.07757402211427689\n",
      "Iteration 7, Batch: 18, Loss: 0.05812619999051094\n",
      "Iteration 7, Batch: 19, Loss: 0.07799125462770462\n",
      "Iteration 7, Batch: 20, Loss: 0.1056690663099289\n",
      "Iteration 7, Batch: 21, Loss: 0.0682624876499176\n",
      "Iteration 7, Batch: 22, Loss: 0.06999444961547852\n",
      "Iteration 7, Batch: 23, Loss: 0.08902586996555328\n",
      "Iteration 7, Batch: 24, Loss: 0.06839385628700256\n",
      "Iteration 7, Batch: 25, Loss: 0.10347682982683182\n",
      "Iteration 7, Batch: 26, Loss: 0.0666879191994667\n",
      "Iteration 7, Batch: 27, Loss: 0.03900584578514099\n",
      "Iteration 7, Batch: 28, Loss: 0.05855075642466545\n",
      "Iteration 7, Batch: 29, Loss: 0.0520828515291214\n",
      "Iteration 7, Batch: 30, Loss: 0.0739162266254425\n",
      "Iteration 7, Batch: 31, Loss: 0.06372480094432831\n",
      "Iteration 7, Batch: 32, Loss: 0.07023648917675018\n",
      "Iteration 7, Batch: 33, Loss: 0.06503844261169434\n",
      "Iteration 7, Batch: 34, Loss: 0.07473445683717728\n",
      "Iteration 7, Batch: 35, Loss: 0.07019969820976257\n",
      "Iteration 7, Batch: 36, Loss: 0.06439448148012161\n",
      "Iteration 7, Batch: 37, Loss: 0.050656676292419434\n",
      "Iteration 7, Batch: 38, Loss: 0.05610961839556694\n",
      "Iteration 7, Batch: 39, Loss: 0.057554882019758224\n",
      "Iteration 7, Batch: 40, Loss: 0.09305556863546371\n",
      "Iteration 7, Batch: 41, Loss: 0.08617675304412842\n",
      "Iteration 7, Batch: 42, Loss: 0.06164100021123886\n",
      "Iteration 7, Batch: 43, Loss: 0.06010999158024788\n",
      "Iteration 7, Batch: 44, Loss: 0.049410391598939896\n",
      "Iteration 7, Batch: 45, Loss: 0.09131761640310287\n",
      "Iteration 7, Batch: 46, Loss: 0.05804169178009033\n",
      "Iteration 7, Batch: 47, Loss: 0.06844216585159302\n",
      "Iteration 7, Batch: 48, Loss: 0.06801550835371017\n",
      "Iteration 7, Batch: 49, Loss: 0.07295899093151093\n",
      "Iteration 8, Batch: 0, Loss: 0.07866225391626358\n",
      "Iteration 8, Batch: 1, Loss: 0.09677992016077042\n",
      "Iteration 8, Batch: 2, Loss: 0.06624691933393478\n",
      "Iteration 8, Batch: 3, Loss: 0.0635339692234993\n",
      "Iteration 8, Batch: 4, Loss: 0.07402065396308899\n",
      "Iteration 8, Batch: 5, Loss: 0.09431171417236328\n",
      "Iteration 8, Batch: 6, Loss: 0.09534069150686264\n",
      "Iteration 8, Batch: 7, Loss: 0.061667755246162415\n",
      "Iteration 8, Batch: 8, Loss: 0.07560265064239502\n",
      "Iteration 8, Batch: 9, Loss: 0.04762934148311615\n",
      "Iteration 8, Batch: 10, Loss: 0.07680981606245041\n",
      "Iteration 8, Batch: 11, Loss: 0.04324810951948166\n",
      "Iteration 8, Batch: 12, Loss: 0.07175923138856888\n",
      "Iteration 8, Batch: 13, Loss: 0.05048345401883125\n",
      "Iteration 8, Batch: 14, Loss: 0.05859099328517914\n",
      "Iteration 8, Batch: 15, Loss: 0.060499150305986404\n",
      "Iteration 8, Batch: 16, Loss: 0.07428260892629623\n",
      "Iteration 8, Batch: 17, Loss: 0.06750087440013885\n",
      "Iteration 8, Batch: 18, Loss: 0.06979210674762726\n",
      "Iteration 8, Batch: 19, Loss: 0.05256465822458267\n",
      "Iteration 8, Batch: 20, Loss: 0.05541757494211197\n",
      "Iteration 8, Batch: 21, Loss: 0.06601063907146454\n",
      "Iteration 8, Batch: 22, Loss: 0.08425579965114594\n",
      "Iteration 8, Batch: 23, Loss: 0.0948294997215271\n",
      "Iteration 8, Batch: 24, Loss: 0.07050621509552002\n",
      "Iteration 8, Batch: 25, Loss: 0.08702009916305542\n",
      "Iteration 8, Batch: 26, Loss: 0.0743328109383583\n",
      "Iteration 8, Batch: 27, Loss: 0.04686059057712555\n",
      "Iteration 8, Batch: 28, Loss: 0.05287088453769684\n",
      "Iteration 8, Batch: 29, Loss: 0.09059274196624756\n",
      "Iteration 8, Batch: 30, Loss: 0.06097245216369629\n",
      "Iteration 8, Batch: 31, Loss: 0.08027694374322891\n",
      "Iteration 8, Batch: 32, Loss: 0.07417955249547958\n",
      "Iteration 8, Batch: 33, Loss: 0.06132812052965164\n",
      "Iteration 8, Batch: 34, Loss: 0.06616290658712387\n",
      "Iteration 8, Batch: 35, Loss: 0.06473876535892487\n",
      "Iteration 8, Batch: 36, Loss: 0.10162464529275894\n",
      "Iteration 8, Batch: 37, Loss: 0.054866962134838104\n",
      "Iteration 8, Batch: 38, Loss: 0.06711538136005402\n",
      "Iteration 8, Batch: 39, Loss: 0.05929933488368988\n",
      "Iteration 8, Batch: 40, Loss: 0.08927937597036362\n",
      "Iteration 8, Batch: 41, Loss: 0.07469547539949417\n",
      "Iteration 8, Batch: 42, Loss: 0.08149529248476028\n",
      "Iteration 8, Batch: 43, Loss: 0.0629730150103569\n",
      "Iteration 8, Batch: 44, Loss: 0.08768880367279053\n",
      "Iteration 8, Batch: 45, Loss: 0.06251435726881027\n",
      "Iteration 8, Batch: 46, Loss: 0.051687199622392654\n",
      "Iteration 8, Batch: 47, Loss: 0.07544782757759094\n",
      "Iteration 8, Batch: 48, Loss: 0.05425123870372772\n",
      "Iteration 8, Batch: 49, Loss: 0.08438050746917725\n",
      "Iteration 9, Batch: 0, Loss: 0.05474338307976723\n",
      "Iteration 9, Batch: 1, Loss: 0.06537333130836487\n",
      "Iteration 9, Batch: 2, Loss: 0.08701975643634796\n",
      "Iteration 9, Batch: 3, Loss: 0.09247791022062302\n",
      "Iteration 9, Batch: 4, Loss: 0.07614664733409882\n",
      "Iteration 9, Batch: 5, Loss: 0.06493013352155685\n",
      "Iteration 9, Batch: 6, Loss: 0.08784288913011551\n",
      "Iteration 9, Batch: 7, Loss: 0.0812404602766037\n",
      "Iteration 9, Batch: 8, Loss: 0.07084433734416962\n",
      "Iteration 9, Batch: 9, Loss: 0.05508888140320778\n",
      "Iteration 9, Batch: 10, Loss: 0.04772362485527992\n",
      "Iteration 9, Batch: 11, Loss: 0.06304454803466797\n",
      "Iteration 9, Batch: 12, Loss: 0.0805053785443306\n",
      "Iteration 9, Batch: 13, Loss: 0.05140816420316696\n",
      "Iteration 9, Batch: 14, Loss: 0.053884752094745636\n",
      "Iteration 9, Batch: 15, Loss: 0.05183223634958267\n",
      "Iteration 9, Batch: 16, Loss: 0.053834814578294754\n",
      "Iteration 9, Batch: 17, Loss: 0.06342333555221558\n",
      "Iteration 9, Batch: 18, Loss: 0.07470002770423889\n",
      "Iteration 9, Batch: 19, Loss: 0.07811266928911209\n",
      "Iteration 9, Batch: 20, Loss: 0.07984912395477295\n",
      "Iteration 9, Batch: 21, Loss: 0.04882579296827316\n",
      "Iteration 9, Batch: 22, Loss: 0.06541929394006729\n",
      "Iteration 9, Batch: 23, Loss: 0.033722516149282455\n",
      "Iteration 9, Batch: 24, Loss: 0.06693179160356522\n",
      "Iteration 9, Batch: 25, Loss: 0.06355193257331848\n",
      "Iteration 9, Batch: 26, Loss: 0.09057727456092834\n",
      "Iteration 9, Batch: 27, Loss: 0.06598623096942902\n",
      "Iteration 9, Batch: 28, Loss: 0.07049128413200378\n",
      "Iteration 9, Batch: 29, Loss: 0.08123014122247696\n",
      "Iteration 9, Batch: 30, Loss: 0.07808160781860352\n",
      "Iteration 9, Batch: 31, Loss: 0.06527721881866455\n",
      "Iteration 9, Batch: 32, Loss: 0.08831597119569778\n",
      "Iteration 9, Batch: 33, Loss: 0.08315090090036392\n",
      "Iteration 9, Batch: 34, Loss: 0.054228439927101135\n",
      "Iteration 9, Batch: 35, Loss: 0.07819723337888718\n",
      "Iteration 9, Batch: 36, Loss: 0.06467200815677643\n",
      "Iteration 9, Batch: 37, Loss: 0.07994700968265533\n",
      "Iteration 9, Batch: 38, Loss: 0.06185096502304077\n",
      "Iteration 9, Batch: 39, Loss: 0.04712292551994324\n",
      "Iteration 9, Batch: 40, Loss: 0.07527424395084381\n",
      "Iteration 9, Batch: 41, Loss: 0.09313678741455078\n",
      "Iteration 9, Batch: 42, Loss: 0.060899518430233\n",
      "Iteration 9, Batch: 43, Loss: 0.0500822588801384\n",
      "Iteration 9, Batch: 44, Loss: 0.059832241386175156\n",
      "Iteration 9, Batch: 45, Loss: 0.05355646088719368\n",
      "Iteration 9, Batch: 46, Loss: 0.033734988421201706\n",
      "Iteration 9, Batch: 47, Loss: 0.07153636962175369\n",
      "Iteration 9, Batch: 48, Loss: 0.07198719680309296\n",
      "Iteration 9, Batch: 49, Loss: 0.07165172696113586\n",
      "Iteration 10, Batch: 0, Loss: 0.08570647984743118\n",
      "Iteration 10, Batch: 1, Loss: 0.06299641728401184\n",
      "Iteration 10, Batch: 2, Loss: 0.05355043336749077\n",
      "Iteration 10, Batch: 3, Loss: 0.07398097962141037\n",
      "Iteration 10, Batch: 4, Loss: 0.0998372808098793\n",
      "Iteration 10, Batch: 5, Loss: 0.059220943599939346\n",
      "Iteration 10, Batch: 6, Loss: 0.06049853190779686\n",
      "Iteration 10, Batch: 7, Loss: 0.06091852858662605\n",
      "Iteration 10, Batch: 8, Loss: 0.053505633026361465\n",
      "Iteration 10, Batch: 9, Loss: 0.084395632147789\n",
      "Iteration 10, Batch: 10, Loss: 0.06963862478733063\n",
      "Iteration 10, Batch: 11, Loss: 0.053864434361457825\n",
      "Iteration 10, Batch: 12, Loss: 0.044352635741233826\n",
      "Iteration 10, Batch: 13, Loss: 0.07458867877721786\n",
      "Iteration 10, Batch: 14, Loss: 0.07630373537540436\n",
      "Iteration 10, Batch: 15, Loss: 0.09452377259731293\n",
      "Iteration 10, Batch: 16, Loss: 0.07142933458089828\n",
      "Iteration 10, Batch: 17, Loss: 0.08903290331363678\n",
      "Iteration 10, Batch: 18, Loss: 0.07345916330814362\n",
      "Iteration 10, Batch: 19, Loss: 0.07938876748085022\n",
      "Iteration 10, Batch: 20, Loss: 0.0674397200345993\n",
      "Iteration 10, Batch: 21, Loss: 0.05506501719355583\n",
      "Iteration 10, Batch: 22, Loss: 0.057705219835042953\n",
      "Iteration 10, Batch: 23, Loss: 0.05783144757151604\n",
      "Iteration 10, Batch: 24, Loss: 0.05689223110675812\n",
      "Iteration 10, Batch: 25, Loss: 0.1079014390707016\n",
      "Iteration 10, Batch: 26, Loss: 0.056154511868953705\n",
      "Iteration 10, Batch: 27, Loss: 0.07177487015724182\n",
      "Iteration 10, Batch: 28, Loss: 0.042855262756347656\n",
      "Iteration 10, Batch: 29, Loss: 0.05840327963232994\n",
      "Iteration 10, Batch: 30, Loss: 0.048709988594055176\n",
      "Iteration 10, Batch: 31, Loss: 0.07417218387126923\n",
      "Iteration 10, Batch: 32, Loss: 0.05753219872713089\n",
      "Iteration 10, Batch: 33, Loss: 0.040756240487098694\n",
      "Iteration 10, Batch: 34, Loss: 0.03726564347743988\n",
      "Iteration 10, Batch: 35, Loss: 0.06310416758060455\n",
      "Iteration 10, Batch: 36, Loss: 0.054308898746967316\n",
      "Iteration 10, Batch: 37, Loss: 0.05555915832519531\n",
      "Iteration 10, Batch: 38, Loss: 0.05270763114094734\n",
      "Iteration 10, Batch: 39, Loss: 0.05942370370030403\n",
      "Iteration 10, Batch: 40, Loss: 0.09072259068489075\n",
      "Iteration 10, Batch: 41, Loss: 0.09368202090263367\n",
      "Iteration 10, Batch: 42, Loss: 0.05347350984811783\n",
      "Iteration 10, Batch: 43, Loss: 0.09606648981571198\n",
      "Iteration 10, Batch: 44, Loss: 0.0689125806093216\n",
      "Iteration 10, Batch: 45, Loss: 0.05259958282113075\n",
      "Iteration 10, Batch: 46, Loss: 0.057616543024778366\n",
      "Iteration 10, Batch: 47, Loss: 0.08790083974599838\n",
      "Iteration 10, Batch: 48, Loss: 0.0644741877913475\n",
      "Iteration 10, Batch: 49, Loss: 0.06150088086724281\n",
      "Iteration 11, Batch: 0, Loss: 0.05891072005033493\n",
      "Iteration 11, Batch: 1, Loss: 0.07495956867933273\n",
      "Iteration 11, Batch: 2, Loss: 0.05081307888031006\n",
      "Iteration 11, Batch: 3, Loss: 0.06994011253118515\n",
      "Iteration 11, Batch: 4, Loss: 0.08148691058158875\n",
      "Iteration 11, Batch: 5, Loss: 0.05560310184955597\n",
      "Iteration 11, Batch: 6, Loss: 0.08314167708158493\n",
      "Iteration 11, Batch: 7, Loss: 0.06801072508096695\n",
      "Iteration 11, Batch: 8, Loss: 0.06788825243711472\n",
      "Iteration 11, Batch: 9, Loss: 0.08354999125003815\n",
      "Iteration 11, Batch: 10, Loss: 0.04747237637639046\n",
      "Iteration 11, Batch: 11, Loss: 0.10974090546369553\n",
      "Iteration 11, Batch: 12, Loss: 0.08369777351617813\n",
      "Iteration 11, Batch: 13, Loss: 0.054528482258319855\n",
      "Iteration 11, Batch: 14, Loss: 0.0707923024892807\n",
      "Iteration 11, Batch: 15, Loss: 0.04365960881114006\n",
      "Iteration 11, Batch: 16, Loss: 0.08107087761163712\n",
      "Iteration 11, Batch: 17, Loss: 0.07816574722528458\n",
      "Iteration 11, Batch: 18, Loss: 0.0666453093290329\n",
      "Iteration 11, Batch: 19, Loss: 0.0519912987947464\n",
      "Iteration 11, Batch: 20, Loss: 0.0747070461511612\n",
      "Iteration 11, Batch: 21, Loss: 0.07542996853590012\n",
      "Iteration 11, Batch: 22, Loss: 0.06071573123335838\n",
      "Iteration 11, Batch: 23, Loss: 0.0476091168820858\n",
      "Iteration 11, Batch: 24, Loss: 0.06388188898563385\n",
      "Iteration 11, Batch: 25, Loss: 0.07328794151544571\n",
      "Iteration 11, Batch: 26, Loss: 0.06772545725107193\n",
      "Iteration 11, Batch: 27, Loss: 0.08849737048149109\n",
      "Iteration 11, Batch: 28, Loss: 0.06422653794288635\n",
      "Iteration 11, Batch: 29, Loss: 0.07053158432245255\n",
      "Iteration 11, Batch: 30, Loss: 0.07916690409183502\n",
      "Iteration 11, Batch: 31, Loss: 0.07510092854499817\n",
      "Iteration 11, Batch: 32, Loss: 0.06245950609445572\n",
      "Iteration 11, Batch: 33, Loss: 0.07418794929981232\n",
      "Iteration 11, Batch: 34, Loss: 0.07731804251670837\n",
      "Iteration 11, Batch: 35, Loss: 0.0743066743016243\n",
      "Iteration 11, Batch: 36, Loss: 0.04634469375014305\n",
      "Iteration 11, Batch: 37, Loss: 0.05294216424226761\n",
      "Iteration 11, Batch: 38, Loss: 0.0520763136446476\n",
      "Iteration 11, Batch: 39, Loss: 0.06509357690811157\n",
      "Iteration 11, Batch: 40, Loss: 0.08569599688053131\n",
      "Iteration 11, Batch: 41, Loss: 0.0771893709897995\n",
      "Iteration 11, Batch: 42, Loss: 0.04461328312754631\n",
      "Iteration 11, Batch: 43, Loss: 0.061253175139427185\n",
      "Iteration 11, Batch: 44, Loss: 0.06163649260997772\n",
      "Iteration 11, Batch: 45, Loss: 0.051662787795066833\n",
      "Iteration 11, Batch: 46, Loss: 0.061019524931907654\n",
      "Iteration 11, Batch: 47, Loss: 0.047300443053245544\n",
      "Iteration 11, Batch: 48, Loss: 0.07388146966695786\n",
      "Iteration 11, Batch: 49, Loss: 0.0711289718747139\n",
      "Iteration 12, Batch: 0, Loss: 0.04222067445516586\n",
      "Iteration 12, Batch: 1, Loss: 0.0633176863193512\n",
      "Iteration 12, Batch: 2, Loss: 0.06480497121810913\n",
      "Iteration 12, Batch: 3, Loss: 0.07215998321771622\n",
      "Iteration 12, Batch: 4, Loss: 0.04292555898427963\n",
      "Iteration 12, Batch: 5, Loss: 0.05922791361808777\n",
      "Iteration 12, Batch: 6, Loss: 0.06642607599496841\n",
      "Iteration 12, Batch: 7, Loss: 0.049035899341106415\n",
      "Iteration 12, Batch: 8, Loss: 0.07363956421613693\n",
      "Iteration 12, Batch: 9, Loss: 0.04328353703022003\n",
      "Iteration 12, Batch: 10, Loss: 0.06697037816047668\n",
      "Iteration 12, Batch: 11, Loss: 0.07750256359577179\n",
      "Iteration 12, Batch: 12, Loss: 0.06204669922590256\n",
      "Iteration 12, Batch: 13, Loss: 0.04447882995009422\n",
      "Iteration 12, Batch: 14, Loss: 0.07532266527414322\n",
      "Iteration 12, Batch: 15, Loss: 0.055989932268857956\n",
      "Iteration 12, Batch: 16, Loss: 0.08383819460868835\n",
      "Iteration 12, Batch: 17, Loss: 0.07583820819854736\n",
      "Iteration 12, Batch: 18, Loss: 0.06868986040353775\n",
      "Iteration 12, Batch: 19, Loss: 0.06540975719690323\n",
      "Iteration 12, Batch: 20, Loss: 0.06600223481655121\n",
      "Iteration 12, Batch: 21, Loss: 0.05894162133336067\n",
      "Iteration 12, Batch: 22, Loss: 0.05653897672891617\n",
      "Iteration 12, Batch: 23, Loss: 0.07890001684427261\n",
      "Iteration 12, Batch: 24, Loss: 0.05161513760685921\n",
      "Iteration 12, Batch: 25, Loss: 0.08482491970062256\n",
      "Iteration 12, Batch: 26, Loss: 0.048453811556100845\n",
      "Iteration 12, Batch: 27, Loss: 0.06022949889302254\n",
      "Iteration 12, Batch: 28, Loss: 0.06726259738206863\n",
      "Iteration 12, Batch: 29, Loss: 0.0759912058711052\n",
      "Iteration 12, Batch: 30, Loss: 0.07073500752449036\n",
      "Iteration 12, Batch: 31, Loss: 0.04593705013394356\n",
      "Iteration 12, Batch: 32, Loss: 0.07926053553819656\n",
      "Iteration 12, Batch: 33, Loss: 0.03962058946490288\n",
      "Iteration 12, Batch: 34, Loss: 0.04056734964251518\n",
      "Iteration 12, Batch: 35, Loss: 0.0605003722012043\n",
      "Iteration 12, Batch: 36, Loss: 0.03613157570362091\n",
      "Iteration 12, Batch: 37, Loss: 0.049694184213876724\n",
      "Iteration 12, Batch: 38, Loss: 0.05628946051001549\n",
      "Iteration 12, Batch: 39, Loss: 0.05440131202340126\n",
      "Iteration 12, Batch: 40, Loss: 0.08017421513795853\n",
      "Iteration 12, Batch: 41, Loss: 0.061518553644418716\n",
      "Iteration 12, Batch: 42, Loss: 0.05915017053484917\n",
      "Iteration 12, Batch: 43, Loss: 0.06789392977952957\n",
      "Iteration 12, Batch: 44, Loss: 0.06455787271261215\n",
      "Iteration 12, Batch: 45, Loss: 0.08899978548288345\n",
      "Iteration 12, Batch: 46, Loss: 0.0725642591714859\n",
      "Iteration 12, Batch: 47, Loss: 0.04787564277648926\n",
      "Iteration 12, Batch: 48, Loss: 0.052851736545562744\n",
      "Iteration 12, Batch: 49, Loss: 0.07548706978559494\n",
      "Iteration 13, Batch: 0, Loss: 0.09202665090560913\n",
      "Iteration 13, Batch: 1, Loss: 0.05203651264309883\n",
      "Iteration 13, Batch: 2, Loss: 0.07432914525270462\n",
      "Iteration 13, Batch: 3, Loss: 0.047360409051179886\n",
      "Iteration 13, Batch: 4, Loss: 0.036000147461891174\n",
      "Iteration 13, Batch: 5, Loss: 0.07487693428993225\n",
      "Iteration 13, Batch: 6, Loss: 0.03418775275349617\n",
      "Iteration 13, Batch: 7, Loss: 0.061891742050647736\n",
      "Iteration 13, Batch: 8, Loss: 0.08420626819133759\n",
      "Iteration 13, Batch: 9, Loss: 0.04713763669133186\n",
      "Iteration 13, Batch: 10, Loss: 0.07194522023200989\n",
      "Iteration 13, Batch: 11, Loss: 0.056729815900325775\n",
      "Iteration 13, Batch: 12, Loss: 0.068039670586586\n",
      "Iteration 13, Batch: 13, Loss: 0.04971010610461235\n",
      "Iteration 13, Batch: 14, Loss: 0.059960830956697464\n",
      "Iteration 13, Batch: 15, Loss: 0.07685434073209763\n",
      "Iteration 13, Batch: 16, Loss: 0.04933546483516693\n",
      "Iteration 13, Batch: 17, Loss: 0.06450189650058746\n",
      "Iteration 13, Batch: 18, Loss: 0.0694977268576622\n",
      "Iteration 13, Batch: 19, Loss: 0.050175175070762634\n",
      "Iteration 13, Batch: 20, Loss: 0.06665384769439697\n",
      "Iteration 13, Batch: 21, Loss: 0.07918323576450348\n",
      "Iteration 13, Batch: 22, Loss: 0.06467711925506592\n",
      "Iteration 13, Batch: 23, Loss: 0.07234912365674973\n",
      "Iteration 13, Batch: 24, Loss: 0.05704858526587486\n",
      "Iteration 13, Batch: 25, Loss: 0.05537348613142967\n",
      "Iteration 13, Batch: 26, Loss: 0.05844411253929138\n",
      "Iteration 13, Batch: 27, Loss: 0.06672254949808121\n",
      "Iteration 13, Batch: 28, Loss: 0.0486280582845211\n",
      "Iteration 13, Batch: 29, Loss: 0.0764550194144249\n",
      "Iteration 13, Batch: 30, Loss: 0.08005405217409134\n",
      "Iteration 13, Batch: 31, Loss: 0.050647225230932236\n",
      "Iteration 13, Batch: 32, Loss: 0.08139550685882568\n",
      "Iteration 13, Batch: 33, Loss: 0.08202886581420898\n",
      "Iteration 13, Batch: 34, Loss: 0.06872185319662094\n",
      "Iteration 13, Batch: 35, Loss: 0.08670522272586823\n",
      "Iteration 13, Batch: 36, Loss: 0.05924241989850998\n",
      "Iteration 13, Batch: 37, Loss: 0.07220672816038132\n",
      "Iteration 13, Batch: 38, Loss: 0.08504390716552734\n",
      "Iteration 13, Batch: 39, Loss: 0.07222885638475418\n",
      "Iteration 13, Batch: 40, Loss: 0.06896807998418808\n",
      "Iteration 13, Batch: 41, Loss: 0.06186038255691528\n",
      "Iteration 13, Batch: 42, Loss: 0.08091864734888077\n",
      "Iteration 13, Batch: 43, Loss: 0.04969296231865883\n",
      "Iteration 13, Batch: 44, Loss: 0.051680173724889755\n",
      "Iteration 13, Batch: 45, Loss: 0.07618045061826706\n",
      "Iteration 13, Batch: 46, Loss: 0.07872260361909866\n",
      "Iteration 13, Batch: 47, Loss: 0.07687430828809738\n",
      "Iteration 13, Batch: 48, Loss: 0.06320828944444656\n",
      "Iteration 13, Batch: 49, Loss: 0.07057256251573563\n",
      "Iteration 14, Batch: 0, Loss: 0.051540255546569824\n",
      "Iteration 14, Batch: 1, Loss: 0.05988191440701485\n",
      "Iteration 14, Batch: 2, Loss: 0.06717412918806076\n",
      "Iteration 14, Batch: 3, Loss: 0.05721467360854149\n",
      "Iteration 14, Batch: 4, Loss: 0.05645732581615448\n",
      "Iteration 14, Batch: 5, Loss: 0.04966061934828758\n",
      "Iteration 14, Batch: 6, Loss: 0.062324099242687225\n",
      "Iteration 14, Batch: 7, Loss: 0.05673128366470337\n",
      "Iteration 14, Batch: 8, Loss: 0.057935088872909546\n",
      "Iteration 14, Batch: 9, Loss: 0.09364308416843414\n",
      "Iteration 14, Batch: 10, Loss: 0.06657103449106216\n",
      "Iteration 14, Batch: 11, Loss: 0.05967192351818085\n",
      "Iteration 14, Batch: 12, Loss: 0.08587639033794403\n",
      "Iteration 14, Batch: 13, Loss: 0.06703060865402222\n",
      "Iteration 14, Batch: 14, Loss: 0.07917768508195877\n",
      "Iteration 14, Batch: 15, Loss: 0.048455461859703064\n",
      "Iteration 14, Batch: 16, Loss: 0.06588032841682434\n",
      "Iteration 14, Batch: 17, Loss: 0.06714660674333572\n",
      "Iteration 14, Batch: 18, Loss: 0.05678573623299599\n",
      "Iteration 14, Batch: 19, Loss: 0.05748783051967621\n",
      "Iteration 14, Batch: 20, Loss: 0.046552665531635284\n",
      "Iteration 14, Batch: 21, Loss: 0.06702610850334167\n",
      "Iteration 14, Batch: 22, Loss: 0.0726449266076088\n",
      "Iteration 14, Batch: 23, Loss: 0.07524440437555313\n",
      "Iteration 14, Batch: 24, Loss: 0.0767790824174881\n",
      "Iteration 14, Batch: 25, Loss: 0.09360403567552567\n",
      "Iteration 14, Batch: 26, Loss: 0.0840626060962677\n",
      "Iteration 14, Batch: 27, Loss: 0.08762582391500473\n",
      "Iteration 14, Batch: 28, Loss: 0.03690486028790474\n",
      "Iteration 14, Batch: 29, Loss: 0.08207929134368896\n",
      "Iteration 14, Batch: 30, Loss: 0.07713832706212997\n",
      "Iteration 14, Batch: 31, Loss: 0.07920113950967789\n",
      "Iteration 14, Batch: 32, Loss: 0.03853730857372284\n",
      "Iteration 14, Batch: 33, Loss: 0.06597242504358292\n",
      "Iteration 14, Batch: 34, Loss: 0.07990822941064835\n",
      "Iteration 14, Batch: 35, Loss: 0.059008363634347916\n",
      "Iteration 14, Batch: 36, Loss: 0.05674133077263832\n",
      "Iteration 14, Batch: 37, Loss: 0.051610592752695084\n",
      "Iteration 14, Batch: 38, Loss: 0.06731009483337402\n",
      "Iteration 14, Batch: 39, Loss: 0.05493882670998573\n",
      "Iteration 14, Batch: 40, Loss: 0.0762692466378212\n",
      "Iteration 14, Batch: 41, Loss: 0.07206068187952042\n",
      "Iteration 14, Batch: 42, Loss: 0.05136451497673988\n",
      "Iteration 14, Batch: 43, Loss: 0.09378797560930252\n",
      "Iteration 14, Batch: 44, Loss: 0.08399192988872528\n",
      "Iteration 14, Batch: 45, Loss: 0.05004694685339928\n",
      "Iteration 14, Batch: 46, Loss: 0.0785815492272377\n",
      "Iteration 14, Batch: 47, Loss: 0.0667438879609108\n",
      "Iteration 14, Batch: 48, Loss: 0.05447537451982498\n",
      "Iteration 14, Batch: 49, Loss: 0.06524815410375595\n",
      "Iteration 15, Batch: 0, Loss: 0.06384973227977753\n",
      "Iteration 15, Batch: 1, Loss: 0.07120726257562637\n",
      "Iteration 15, Batch: 2, Loss: 0.07708849012851715\n",
      "Iteration 15, Batch: 3, Loss: 0.04579605907201767\n",
      "Iteration 15, Batch: 4, Loss: 0.07863611727952957\n",
      "Iteration 15, Batch: 5, Loss: 0.0609716959297657\n",
      "Iteration 15, Batch: 6, Loss: 0.06156647205352783\n",
      "Iteration 15, Batch: 7, Loss: 0.06600823998451233\n",
      "Iteration 15, Batch: 8, Loss: 0.08305127918720245\n",
      "Iteration 15, Batch: 9, Loss: 0.05751131474971771\n",
      "Iteration 15, Batch: 10, Loss: 0.06968513876199722\n",
      "Iteration 15, Batch: 11, Loss: 0.06729218363761902\n",
      "Iteration 15, Batch: 12, Loss: 0.08356056362390518\n",
      "Iteration 15, Batch: 13, Loss: 0.061401933431625366\n",
      "Iteration 15, Batch: 14, Loss: 0.055864475667476654\n",
      "Iteration 15, Batch: 15, Loss: 0.06661943346261978\n",
      "Iteration 15, Batch: 16, Loss: 0.06804480403661728\n",
      "Iteration 15, Batch: 17, Loss: 0.06243696063756943\n",
      "Iteration 15, Batch: 18, Loss: 0.07427072525024414\n",
      "Iteration 15, Batch: 19, Loss: 0.08069173991680145\n",
      "Iteration 15, Batch: 20, Loss: 0.047006797045469284\n",
      "Iteration 15, Batch: 21, Loss: 0.061024636030197144\n",
      "Iteration 15, Batch: 22, Loss: 0.07584307342767715\n",
      "Iteration 15, Batch: 23, Loss: 0.06095227226614952\n",
      "Iteration 15, Batch: 24, Loss: 0.05823713168501854\n",
      "Iteration 15, Batch: 25, Loss: 0.08217305690050125\n",
      "Iteration 15, Batch: 26, Loss: 0.055479247123003006\n",
      "Iteration 15, Batch: 27, Loss: 0.06549244374036789\n",
      "Iteration 15, Batch: 28, Loss: 0.0813831090927124\n",
      "Iteration 15, Batch: 29, Loss: 0.06067214533686638\n",
      "Iteration 15, Batch: 30, Loss: 0.07704949378967285\n",
      "Iteration 15, Batch: 31, Loss: 0.06153332442045212\n",
      "Iteration 15, Batch: 32, Loss: 0.05536922439932823\n",
      "Iteration 15, Batch: 33, Loss: 0.06659507751464844\n",
      "Iteration 15, Batch: 34, Loss: 0.09618091583251953\n",
      "Iteration 15, Batch: 35, Loss: 0.08150302618741989\n",
      "Iteration 15, Batch: 36, Loss: 0.07536526769399643\n",
      "Iteration 15, Batch: 37, Loss: 0.08782095462083817\n",
      "Iteration 15, Batch: 38, Loss: 0.06692564487457275\n",
      "Iteration 15, Batch: 39, Loss: 0.08770610392093658\n",
      "Iteration 15, Batch: 40, Loss: 0.07942522317171097\n",
      "Iteration 15, Batch: 41, Loss: 0.056378915905952454\n",
      "Iteration 15, Batch: 42, Loss: 0.06593804806470871\n",
      "Iteration 15, Batch: 43, Loss: 0.04494469240307808\n",
      "Iteration 15, Batch: 44, Loss: 0.08658935129642487\n",
      "Iteration 15, Batch: 45, Loss: 0.06042652949690819\n",
      "Iteration 15, Batch: 46, Loss: 0.08376625180244446\n",
      "Iteration 15, Batch: 47, Loss: 0.05745870620012283\n",
      "Iteration 15, Batch: 48, Loss: 0.07792775332927704\n",
      "Iteration 15, Batch: 49, Loss: 0.07780931144952774\n",
      "Iteration 16, Batch: 0, Loss: 0.05664903670549393\n",
      "Iteration 16, Batch: 1, Loss: 0.09300580620765686\n",
      "Iteration 16, Batch: 2, Loss: 0.07576274871826172\n",
      "Iteration 16, Batch: 3, Loss: 0.06001835688948631\n",
      "Iteration 16, Batch: 4, Loss: 0.06143016740679741\n",
      "Iteration 16, Batch: 5, Loss: 0.057201169431209564\n",
      "Iteration 16, Batch: 6, Loss: 0.08218231052160263\n",
      "Iteration 16, Batch: 7, Loss: 0.0756649449467659\n",
      "Iteration 16, Batch: 8, Loss: 0.05352446436882019\n",
      "Iteration 16, Batch: 9, Loss: 0.05814924091100693\n",
      "Iteration 16, Batch: 10, Loss: 0.07071499526500702\n",
      "Iteration 16, Batch: 11, Loss: 0.05634395033121109\n",
      "Iteration 16, Batch: 12, Loss: 0.07883863151073456\n",
      "Iteration 16, Batch: 13, Loss: 0.03685455769300461\n",
      "Iteration 16, Batch: 14, Loss: 0.052024662494659424\n",
      "Iteration 16, Batch: 15, Loss: 0.0550202801823616\n",
      "Iteration 16, Batch: 16, Loss: 0.06259331852197647\n",
      "Iteration 16, Batch: 17, Loss: 0.052302341908216476\n",
      "Iteration 16, Batch: 18, Loss: 0.05828575789928436\n",
      "Iteration 16, Batch: 19, Loss: 0.07165863364934921\n",
      "Iteration 16, Batch: 20, Loss: 0.07858386635780334\n",
      "Iteration 16, Batch: 21, Loss: 0.039753179997205734\n",
      "Iteration 16, Batch: 22, Loss: 0.0408676378428936\n",
      "Iteration 16, Batch: 23, Loss: 0.054532669484615326\n",
      "Iteration 16, Batch: 24, Loss: 0.07656025141477585\n",
      "Iteration 16, Batch: 25, Loss: 0.06039993464946747\n",
      "Iteration 16, Batch: 26, Loss: 0.04063030332326889\n",
      "Iteration 16, Batch: 27, Loss: 0.05199528485536575\n",
      "Iteration 16, Batch: 28, Loss: 0.08251125365495682\n",
      "Iteration 16, Batch: 29, Loss: 0.052826326340436935\n",
      "Iteration 16, Batch: 30, Loss: 0.03013676032423973\n",
      "Iteration 16, Batch: 31, Loss: 0.069877028465271\n",
      "Iteration 16, Batch: 32, Loss: 0.041323885321617126\n",
      "Iteration 16, Batch: 33, Loss: 0.049718327820301056\n",
      "Iteration 16, Batch: 34, Loss: 0.08300483226776123\n",
      "Iteration 16, Batch: 35, Loss: 0.046481989324092865\n",
      "Iteration 16, Batch: 36, Loss: 0.08180072903633118\n",
      "Iteration 16, Batch: 37, Loss: 0.03811091184616089\n",
      "Iteration 16, Batch: 38, Loss: 0.07753247767686844\n",
      "Iteration 16, Batch: 39, Loss: 0.06845991313457489\n",
      "Iteration 16, Batch: 40, Loss: 0.09787178039550781\n",
      "Iteration 16, Batch: 41, Loss: 0.09135738015174866\n",
      "Iteration 16, Batch: 42, Loss: 0.07220186293125153\n",
      "Iteration 16, Batch: 43, Loss: 0.04271213337779045\n",
      "Iteration 16, Batch: 44, Loss: 0.08892836421728134\n",
      "Iteration 16, Batch: 45, Loss: 0.07117471843957901\n",
      "Iteration 16, Batch: 46, Loss: 0.0714348554611206\n",
      "Iteration 16, Batch: 47, Loss: 0.034266989678144455\n",
      "Iteration 16, Batch: 48, Loss: 0.08366067707538605\n",
      "Iteration 16, Batch: 49, Loss: 0.049675531685352325\n",
      "Iteration 17, Batch: 0, Loss: 0.058620333671569824\n",
      "Iteration 17, Batch: 1, Loss: 0.07503625005483627\n",
      "Iteration 17, Batch: 2, Loss: 0.08661206066608429\n",
      "Iteration 17, Batch: 3, Loss: 0.05160095542669296\n",
      "Iteration 17, Batch: 4, Loss: 0.08339881896972656\n",
      "Iteration 17, Batch: 5, Loss: 0.05327747389674187\n",
      "Iteration 17, Batch: 6, Loss: 0.07867227494716644\n",
      "Iteration 17, Batch: 7, Loss: 0.06276112794876099\n",
      "Iteration 17, Batch: 8, Loss: 0.0652761235833168\n",
      "Iteration 17, Batch: 9, Loss: 0.054843176156282425\n",
      "Iteration 17, Batch: 10, Loss: 0.042571019381284714\n",
      "Iteration 17, Batch: 11, Loss: 0.0528627373278141\n",
      "Iteration 17, Batch: 12, Loss: 0.054609593003988266\n",
      "Iteration 17, Batch: 13, Loss: 0.039104875177145004\n",
      "Iteration 17, Batch: 14, Loss: 0.06181750074028969\n",
      "Iteration 17, Batch: 15, Loss: 0.05937142297625542\n",
      "Iteration 17, Batch: 16, Loss: 0.06256116181612015\n",
      "Iteration 17, Batch: 17, Loss: 0.04764135181903839\n",
      "Iteration 17, Batch: 18, Loss: 0.06269598752260208\n",
      "Iteration 17, Batch: 19, Loss: 0.053335368633270264\n",
      "Iteration 17, Batch: 20, Loss: 0.045812368392944336\n",
      "Iteration 17, Batch: 21, Loss: 0.047728829085826874\n",
      "Iteration 17, Batch: 22, Loss: 0.05700063705444336\n",
      "Iteration 17, Batch: 23, Loss: 0.06307119131088257\n",
      "Iteration 17, Batch: 24, Loss: 0.05440271645784378\n",
      "Iteration 17, Batch: 25, Loss: 0.0819336473941803\n",
      "Iteration 17, Batch: 26, Loss: 0.05168350040912628\n",
      "Iteration 17, Batch: 27, Loss: 0.03452625870704651\n",
      "Iteration 17, Batch: 28, Loss: 0.08446856588125229\n",
      "Iteration 17, Batch: 29, Loss: 0.07697965949773788\n",
      "Iteration 17, Batch: 30, Loss: 0.042289238423109055\n",
      "Iteration 17, Batch: 31, Loss: 0.07100021094083786\n",
      "Iteration 17, Batch: 32, Loss: 0.06244483217597008\n",
      "Iteration 17, Batch: 33, Loss: 0.04827507212758064\n",
      "Iteration 17, Batch: 34, Loss: 0.07276685535907745\n",
      "Iteration 17, Batch: 35, Loss: 0.04726243019104004\n",
      "Iteration 17, Batch: 36, Loss: 0.08152726292610168\n",
      "Iteration 17, Batch: 37, Loss: 0.06350021809339523\n",
      "Iteration 17, Batch: 38, Loss: 0.057907719165086746\n",
      "Iteration 17, Batch: 39, Loss: 0.07240544259548187\n",
      "Iteration 17, Batch: 40, Loss: 0.048161666840314865\n",
      "Iteration 17, Batch: 41, Loss: 0.061028383672237396\n",
      "Iteration 17, Batch: 42, Loss: 0.0763297826051712\n",
      "Iteration 17, Batch: 43, Loss: 0.06543620675802231\n",
      "Iteration 17, Batch: 44, Loss: 0.07551401853561401\n",
      "Iteration 17, Batch: 45, Loss: 0.07719789445400238\n",
      "Iteration 17, Batch: 46, Loss: 0.062261056154966354\n",
      "Iteration 17, Batch: 47, Loss: 0.07812442630529404\n",
      "Iteration 17, Batch: 48, Loss: 0.0948101058602333\n",
      "Iteration 17, Batch: 49, Loss: 0.06297336518764496\n",
      "Iteration 18, Batch: 0, Loss: 0.06798189133405685\n",
      "Iteration 18, Batch: 1, Loss: 0.09978828579187393\n",
      "Iteration 18, Batch: 2, Loss: 0.08636724948883057\n",
      "Iteration 18, Batch: 3, Loss: 0.08202918618917465\n",
      "Iteration 18, Batch: 4, Loss: 0.059998445212841034\n",
      "Iteration 18, Batch: 5, Loss: 0.08619125187397003\n",
      "Iteration 18, Batch: 6, Loss: 0.07205884158611298\n",
      "Iteration 18, Batch: 7, Loss: 0.08071479946374893\n",
      "Iteration 18, Batch: 8, Loss: 0.07243278622627258\n",
      "Iteration 18, Batch: 9, Loss: 0.06078627333045006\n",
      "Iteration 18, Batch: 10, Loss: 0.05481633171439171\n",
      "Iteration 18, Batch: 11, Loss: 0.10716170072555542\n",
      "Iteration 18, Batch: 12, Loss: 0.09501809626817703\n",
      "Iteration 18, Batch: 13, Loss: 0.07415644824504852\n",
      "Iteration 18, Batch: 14, Loss: 0.06872253119945526\n",
      "Iteration 18, Batch: 15, Loss: 0.10439717769622803\n",
      "Iteration 18, Batch: 16, Loss: 0.065825916826725\n",
      "Iteration 18, Batch: 17, Loss: 0.06950990855693817\n",
      "Iteration 18, Batch: 18, Loss: 0.07711999863386154\n",
      "Iteration 18, Batch: 19, Loss: 0.06596007198095322\n",
      "Iteration 18, Batch: 20, Loss: 0.06596425175666809\n",
      "Iteration 18, Batch: 21, Loss: 0.09305828809738159\n",
      "Iteration 18, Batch: 22, Loss: 0.07169206440448761\n",
      "Iteration 18, Batch: 23, Loss: 0.04316355660557747\n",
      "Iteration 18, Batch: 24, Loss: 0.05528366565704346\n",
      "Iteration 18, Batch: 25, Loss: 0.05669290944933891\n",
      "Iteration 18, Batch: 26, Loss: 0.06713690608739853\n",
      "Iteration 18, Batch: 27, Loss: 0.07039885222911835\n",
      "Iteration 18, Batch: 28, Loss: 0.06887967884540558\n",
      "Iteration 18, Batch: 29, Loss: 0.07304839789867401\n",
      "Iteration 18, Batch: 30, Loss: 0.10182462632656097\n",
      "Iteration 18, Batch: 31, Loss: 0.058133337646722794\n",
      "Iteration 18, Batch: 32, Loss: 0.03955322876572609\n",
      "Iteration 18, Batch: 33, Loss: 0.058004073798656464\n",
      "Iteration 18, Batch: 34, Loss: 0.054516494274139404\n",
      "Iteration 18, Batch: 35, Loss: 0.07171989977359772\n",
      "Iteration 18, Batch: 36, Loss: 0.06692484021186829\n",
      "Iteration 18, Batch: 37, Loss: 0.05162275210022926\n",
      "Iteration 18, Batch: 38, Loss: 0.07367680221796036\n",
      "Iteration 18, Batch: 39, Loss: 0.06637648493051529\n",
      "Iteration 18, Batch: 40, Loss: 0.06593725085258484\n",
      "Iteration 18, Batch: 41, Loss: 0.06516771018505096\n",
      "Iteration 18, Batch: 42, Loss: 0.045088060200214386\n",
      "Iteration 18, Batch: 43, Loss: 0.07892824709415436\n",
      "Iteration 18, Batch: 44, Loss: 0.02830471470952034\n",
      "Iteration 18, Batch: 45, Loss: 0.08025125414133072\n",
      "Iteration 18, Batch: 46, Loss: 0.05809076502919197\n",
      "Iteration 18, Batch: 47, Loss: 0.06494505703449249\n",
      "Iteration 18, Batch: 48, Loss: 0.05042727291584015\n",
      "Iteration 18, Batch: 49, Loss: 0.05555514991283417\n",
      "Iteration 19, Batch: 0, Loss: 0.07268009334802628\n",
      "Iteration 19, Batch: 1, Loss: 0.09868680685758591\n",
      "Iteration 19, Batch: 2, Loss: 0.0790228620171547\n",
      "Iteration 19, Batch: 3, Loss: 0.06745804846286774\n",
      "Iteration 19, Batch: 4, Loss: 0.07016754150390625\n",
      "Iteration 19, Batch: 5, Loss: 0.04775722697377205\n",
      "Iteration 19, Batch: 6, Loss: 0.04719789698719978\n",
      "Iteration 19, Batch: 7, Loss: 0.058696139603853226\n",
      "Iteration 19, Batch: 8, Loss: 0.03973133862018585\n",
      "Iteration 19, Batch: 9, Loss: 0.052055973559617996\n",
      "Iteration 19, Batch: 10, Loss: 0.04325856640934944\n",
      "Iteration 19, Batch: 11, Loss: 0.05409372225403786\n",
      "Iteration 19, Batch: 12, Loss: 0.041708119213581085\n",
      "Iteration 19, Batch: 13, Loss: 0.04790513962507248\n",
      "Iteration 19, Batch: 14, Loss: 0.0733456015586853\n",
      "Iteration 19, Batch: 15, Loss: 0.038934674113988876\n",
      "Iteration 19, Batch: 16, Loss: 0.0684901773929596\n",
      "Iteration 19, Batch: 17, Loss: 0.07009503990411758\n",
      "Iteration 19, Batch: 18, Loss: 0.059248074889183044\n",
      "Iteration 19, Batch: 19, Loss: 0.049151163548231125\n",
      "Iteration 19, Batch: 20, Loss: 0.06306860595941544\n",
      "Iteration 19, Batch: 21, Loss: 0.07804088294506073\n",
      "Iteration 19, Batch: 22, Loss: 0.05450371652841568\n",
      "Iteration 19, Batch: 23, Loss: 0.040974684059619904\n",
      "Iteration 19, Batch: 24, Loss: 0.06188833713531494\n",
      "Iteration 19, Batch: 25, Loss: 0.08233512938022614\n",
      "Iteration 19, Batch: 26, Loss: 0.0643332228064537\n",
      "Iteration 19, Batch: 27, Loss: 0.07849583029747009\n",
      "Iteration 19, Batch: 28, Loss: 0.0522339902818203\n",
      "Iteration 19, Batch: 29, Loss: 0.07954742759466171\n",
      "Iteration 19, Batch: 30, Loss: 0.07068386673927307\n",
      "Iteration 19, Batch: 31, Loss: 0.054506901651620865\n",
      "Iteration 19, Batch: 32, Loss: 0.06942994147539139\n",
      "Iteration 19, Batch: 33, Loss: 0.08441085368394852\n",
      "Iteration 19, Batch: 34, Loss: 0.07608528435230255\n",
      "Iteration 19, Batch: 35, Loss: 0.07089398056268692\n",
      "Iteration 19, Batch: 36, Loss: 0.09354625642299652\n",
      "Iteration 19, Batch: 37, Loss: 0.07974869012832642\n",
      "Iteration 19, Batch: 38, Loss: 0.07890363782644272\n",
      "Iteration 19, Batch: 39, Loss: 0.03242465853691101\n",
      "Iteration 19, Batch: 40, Loss: 0.0833769217133522\n",
      "Iteration 19, Batch: 41, Loss: 0.042447157204151154\n",
      "Iteration 19, Batch: 42, Loss: 0.04964478313922882\n",
      "Iteration 19, Batch: 43, Loss: 0.05763188377022743\n",
      "Iteration 19, Batch: 44, Loss: 0.07024240493774414\n",
      "Iteration 19, Batch: 45, Loss: 0.07465529441833496\n",
      "Iteration 19, Batch: 46, Loss: 0.06717267632484436\n",
      "Iteration 19, Batch: 47, Loss: 0.05660153925418854\n",
      "Iteration 19, Batch: 48, Loss: 0.10808862745761871\n",
      "Iteration 19, Batch: 49, Loss: 0.0795397236943245\n",
      "Iteration 20, Batch: 0, Loss: 0.08264081925153732\n",
      "Iteration 20, Batch: 1, Loss: 0.043031416833400726\n",
      "Iteration 20, Batch: 2, Loss: 0.0759114995598793\n",
      "Iteration 20, Batch: 3, Loss: 0.08963851630687714\n",
      "Iteration 20, Batch: 4, Loss: 0.06411826610565186\n",
      "Iteration 20, Batch: 5, Loss: 0.07417687028646469\n",
      "Iteration 20, Batch: 6, Loss: 0.07836264371871948\n",
      "Iteration 20, Batch: 7, Loss: 0.05003902688622475\n",
      "Iteration 20, Batch: 8, Loss: 0.08703383058309555\n",
      "Iteration 20, Batch: 9, Loss: 0.0713113322854042\n",
      "Iteration 20, Batch: 10, Loss: 0.06095680966973305\n",
      "Iteration 20, Batch: 11, Loss: 0.0813211053609848\n",
      "Iteration 20, Batch: 12, Loss: 0.0666177049279213\n",
      "Iteration 20, Batch: 13, Loss: 0.06615009158849716\n",
      "Iteration 20, Batch: 14, Loss: 0.038125328719615936\n",
      "Iteration 20, Batch: 15, Loss: 0.08579009771347046\n",
      "Iteration 20, Batch: 16, Loss: 0.0780322328209877\n",
      "Iteration 20, Batch: 17, Loss: 0.08558092266321182\n",
      "Iteration 20, Batch: 18, Loss: 0.07298903167247772\n",
      "Iteration 20, Batch: 19, Loss: 0.06466072797775269\n",
      "Iteration 20, Batch: 20, Loss: 0.0691557228565216\n",
      "Iteration 20, Batch: 21, Loss: 0.10706791281700134\n",
      "Iteration 20, Batch: 22, Loss: 0.04216189682483673\n",
      "Iteration 20, Batch: 23, Loss: 0.09878343343734741\n",
      "Iteration 20, Batch: 24, Loss: 0.05077381804585457\n",
      "Iteration 20, Batch: 25, Loss: 0.08368441462516785\n",
      "Iteration 20, Batch: 26, Loss: 0.07125937938690186\n",
      "Iteration 20, Batch: 27, Loss: 0.033307503908872604\n",
      "Iteration 20, Batch: 28, Loss: 0.040758006274700165\n",
      "Iteration 20, Batch: 29, Loss: 0.055168699473142624\n",
      "Iteration 20, Batch: 30, Loss: 0.02635459415614605\n",
      "Iteration 20, Batch: 31, Loss: 0.04627808555960655\n",
      "Iteration 20, Batch: 32, Loss: 0.06530504673719406\n",
      "Iteration 20, Batch: 33, Loss: 0.0659850612282753\n",
      "Iteration 20, Batch: 34, Loss: 0.07419569045305252\n",
      "Iteration 20, Batch: 35, Loss: 0.06447189301252365\n",
      "Iteration 20, Batch: 36, Loss: 0.06581047177314758\n",
      "Iteration 20, Batch: 37, Loss: 0.07786799967288971\n",
      "Iteration 20, Batch: 38, Loss: 0.05804688483476639\n",
      "Iteration 20, Batch: 39, Loss: 0.05968803912401199\n",
      "Iteration 20, Batch: 40, Loss: 0.10699134320020676\n",
      "Iteration 20, Batch: 41, Loss: 0.05825294181704521\n",
      "Iteration 20, Batch: 42, Loss: 0.04601525142788887\n",
      "Iteration 20, Batch: 43, Loss: 0.06816509366035461\n",
      "Iteration 20, Batch: 44, Loss: 0.07064215838909149\n",
      "Iteration 20, Batch: 45, Loss: 0.07917104661464691\n",
      "Iteration 20, Batch: 46, Loss: 0.06812109798192978\n",
      "Iteration 20, Batch: 47, Loss: 0.09237676113843918\n",
      "Iteration 20, Batch: 48, Loss: 0.06777472048997879\n",
      "Iteration 20, Batch: 49, Loss: 0.05426320433616638\n",
      "Iteration 21, Batch: 0, Loss: 0.08460080623626709\n",
      "Iteration 21, Batch: 1, Loss: 0.07032375037670135\n",
      "Iteration 21, Batch: 2, Loss: 0.06213695555925369\n",
      "Iteration 21, Batch: 3, Loss: 0.0691392794251442\n",
      "Iteration 21, Batch: 4, Loss: 0.07364436239004135\n",
      "Iteration 21, Batch: 5, Loss: 0.08501340448856354\n",
      "Iteration 21, Batch: 6, Loss: 0.0648968517780304\n",
      "Iteration 21, Batch: 7, Loss: 0.07938344031572342\n",
      "Iteration 21, Batch: 8, Loss: 0.06391378492116928\n",
      "Iteration 21, Batch: 9, Loss: 0.07174520194530487\n",
      "Iteration 21, Batch: 10, Loss: 0.07269172370433807\n",
      "Iteration 21, Batch: 11, Loss: 0.0986439511179924\n",
      "Iteration 21, Batch: 12, Loss: 0.061735596507787704\n",
      "Iteration 21, Batch: 13, Loss: 0.05006092041730881\n",
      "Iteration 21, Batch: 14, Loss: 0.04309942200779915\n",
      "Iteration 21, Batch: 15, Loss: 0.06711974740028381\n",
      "Iteration 21, Batch: 16, Loss: 0.06476563960313797\n",
      "Iteration 21, Batch: 17, Loss: 0.04926751181483269\n",
      "Iteration 21, Batch: 18, Loss: 0.07703619450330734\n",
      "Iteration 21, Batch: 19, Loss: 0.054678723216056824\n",
      "Iteration 21, Batch: 20, Loss: 0.06197679042816162\n",
      "Iteration 21, Batch: 21, Loss: 0.08849538862705231\n",
      "Iteration 21, Batch: 22, Loss: 0.0705799087882042\n",
      "Iteration 21, Batch: 23, Loss: 0.07844681292772293\n",
      "Iteration 21, Batch: 24, Loss: 0.0699361190199852\n",
      "Iteration 21, Batch: 25, Loss: 0.08152234554290771\n",
      "Iteration 21, Batch: 26, Loss: 0.06125497817993164\n",
      "Iteration 21, Batch: 27, Loss: 0.044145144522190094\n",
      "Iteration 21, Batch: 28, Loss: 0.090985007584095\n",
      "Iteration 21, Batch: 29, Loss: 0.07553264498710632\n",
      "Iteration 21, Batch: 30, Loss: 0.0542779415845871\n",
      "Iteration 21, Batch: 31, Loss: 0.06979816406965256\n",
      "Iteration 21, Batch: 32, Loss: 0.0893578827381134\n",
      "Iteration 21, Batch: 33, Loss: 0.06113578751683235\n",
      "Iteration 21, Batch: 34, Loss: 0.0412006638944149\n",
      "Iteration 21, Batch: 35, Loss: 0.06992997229099274\n",
      "Iteration 21, Batch: 36, Loss: 0.07271350920200348\n",
      "Iteration 21, Batch: 37, Loss: 0.0601813830435276\n",
      "Iteration 21, Batch: 38, Loss: 0.06088850274682045\n",
      "Iteration 21, Batch: 39, Loss: 0.05679105967283249\n",
      "Iteration 21, Batch: 40, Loss: 0.06265859305858612\n",
      "Iteration 21, Batch: 41, Loss: 0.05795721337199211\n",
      "Iteration 21, Batch: 42, Loss: 0.04375458508729935\n",
      "Iteration 21, Batch: 43, Loss: 0.06744246929883957\n",
      "Iteration 21, Batch: 44, Loss: 0.044415783137083054\n",
      "Iteration 21, Batch: 45, Loss: 0.06981304287910461\n",
      "Iteration 21, Batch: 46, Loss: 0.062352463603019714\n",
      "Iteration 21, Batch: 47, Loss: 0.0809163749217987\n",
      "Iteration 21, Batch: 48, Loss: 0.06545255333185196\n",
      "Iteration 21, Batch: 49, Loss: 0.05932966619729996\n",
      "Iteration 22, Batch: 0, Loss: 0.08637969195842743\n",
      "Iteration 22, Batch: 1, Loss: 0.06389152258634567\n",
      "Iteration 22, Batch: 2, Loss: 0.07646505534648895\n",
      "Iteration 22, Batch: 3, Loss: 0.08778423070907593\n",
      "Iteration 22, Batch: 4, Loss: 0.05139530077576637\n",
      "Iteration 22, Batch: 5, Loss: 0.0787828266620636\n",
      "Iteration 22, Batch: 6, Loss: 0.07641691714525223\n",
      "Iteration 22, Batch: 7, Loss: 0.05094947665929794\n",
      "Iteration 22, Batch: 8, Loss: 0.08244621753692627\n",
      "Iteration 22, Batch: 9, Loss: 0.05555330216884613\n",
      "Iteration 22, Batch: 10, Loss: 0.049926623702049255\n",
      "Iteration 22, Batch: 11, Loss: 0.06947724521160126\n",
      "Iteration 22, Batch: 12, Loss: 0.054757338017225266\n",
      "Iteration 22, Batch: 13, Loss: 0.07005200535058975\n",
      "Iteration 22, Batch: 14, Loss: 0.049424804747104645\n",
      "Iteration 22, Batch: 15, Loss: 0.046130742877721786\n",
      "Iteration 22, Batch: 16, Loss: 0.06986105442047119\n",
      "Iteration 22, Batch: 17, Loss: 0.05446484312415123\n",
      "Iteration 22, Batch: 18, Loss: 0.05353615805506706\n",
      "Iteration 22, Batch: 19, Loss: 0.07977398484945297\n",
      "Iteration 22, Batch: 20, Loss: 0.05204366520047188\n",
      "Iteration 22, Batch: 21, Loss: 0.07459480315446854\n",
      "Iteration 22, Batch: 22, Loss: 0.05015358328819275\n",
      "Iteration 22, Batch: 23, Loss: 0.057866837829351425\n",
      "Iteration 22, Batch: 24, Loss: 0.05289522930979729\n",
      "Iteration 22, Batch: 25, Loss: 0.09343978017568588\n",
      "Iteration 22, Batch: 26, Loss: 0.06133774667978287\n",
      "Iteration 22, Batch: 27, Loss: 0.08283089101314545\n",
      "Iteration 22, Batch: 28, Loss: 0.0685608759522438\n",
      "Iteration 22, Batch: 29, Loss: 0.069925956428051\n",
      "Iteration 22, Batch: 30, Loss: 0.08571778982877731\n",
      "Iteration 22, Batch: 31, Loss: 0.05490430071949959\n",
      "Iteration 22, Batch: 32, Loss: 0.07018618285655975\n",
      "Iteration 22, Batch: 33, Loss: 0.07416723668575287\n",
      "Iteration 22, Batch: 34, Loss: 0.04892241209745407\n",
      "Iteration 22, Batch: 35, Loss: 0.07306016236543655\n",
      "Iteration 22, Batch: 36, Loss: 0.08240126073360443\n",
      "Iteration 22, Batch: 37, Loss: 0.06273838877677917\n",
      "Iteration 22, Batch: 38, Loss: 0.08092725276947021\n",
      "Iteration 22, Batch: 39, Loss: 0.07431725412607193\n",
      "Iteration 22, Batch: 40, Loss: 0.07960347086191177\n",
      "Iteration 22, Batch: 41, Loss: 0.06151765584945679\n",
      "Iteration 22, Batch: 42, Loss: 0.05702841654419899\n",
      "Iteration 22, Batch: 43, Loss: 0.07208631932735443\n",
      "Iteration 22, Batch: 44, Loss: 0.057150743901729584\n",
      "Iteration 22, Batch: 45, Loss: 0.06521418690681458\n",
      "Iteration 22, Batch: 46, Loss: 0.060610294342041016\n",
      "Iteration 22, Batch: 47, Loss: 0.06611749529838562\n",
      "Iteration 22, Batch: 48, Loss: 0.062299251556396484\n",
      "Iteration 22, Batch: 49, Loss: 0.06906504184007645\n",
      "Iteration 23, Batch: 0, Loss: 0.09075827151536942\n",
      "Iteration 23, Batch: 1, Loss: 0.06496834009885788\n",
      "Iteration 23, Batch: 2, Loss: 0.07927987724542618\n",
      "Iteration 23, Batch: 3, Loss: 0.0610760897397995\n",
      "Iteration 23, Batch: 4, Loss: 0.051389120519161224\n",
      "Iteration 23, Batch: 5, Loss: 0.098769411444664\n",
      "Iteration 23, Batch: 6, Loss: 0.054878443479537964\n",
      "Iteration 23, Batch: 7, Loss: 0.06232360750436783\n",
      "Iteration 23, Batch: 8, Loss: 0.08047887682914734\n",
      "Iteration 23, Batch: 9, Loss: 0.09485620260238647\n",
      "Iteration 23, Batch: 10, Loss: 0.062384411692619324\n",
      "Iteration 23, Batch: 11, Loss: 0.0614454485476017\n",
      "Iteration 23, Batch: 12, Loss: 0.0391772985458374\n",
      "Iteration 23, Batch: 13, Loss: 0.09357158094644547\n",
      "Iteration 23, Batch: 14, Loss: 0.057957760989665985\n",
      "Iteration 23, Batch: 15, Loss: 0.04147014394402504\n",
      "Iteration 23, Batch: 16, Loss: 0.08148164302110672\n",
      "Iteration 23, Batch: 17, Loss: 0.08444052189588547\n",
      "Iteration 23, Batch: 18, Loss: 0.07164805382490158\n",
      "Iteration 23, Batch: 19, Loss: 0.06925754249095917\n",
      "Iteration 23, Batch: 20, Loss: 0.08696719259023666\n",
      "Iteration 23, Batch: 21, Loss: 0.062115807086229324\n",
      "Iteration 23, Batch: 22, Loss: 0.05838016793131828\n",
      "Iteration 23, Batch: 23, Loss: 0.051898326724767685\n",
      "Iteration 23, Batch: 24, Loss: 0.05298641696572304\n",
      "Iteration 23, Batch: 25, Loss: 0.06311172991991043\n",
      "Iteration 23, Batch: 26, Loss: 0.050665512681007385\n",
      "Iteration 23, Batch: 27, Loss: 0.07744485884904861\n",
      "Iteration 23, Batch: 28, Loss: 0.036448363214731216\n",
      "Iteration 23, Batch: 29, Loss: 0.06509900838136673\n",
      "Iteration 23, Batch: 30, Loss: 0.049075864255428314\n",
      "Iteration 23, Batch: 31, Loss: 0.07531429082155228\n",
      "Iteration 23, Batch: 32, Loss: 0.04637133330106735\n",
      "Iteration 23, Batch: 33, Loss: 0.07393353432416916\n",
      "Iteration 23, Batch: 34, Loss: 0.06163422763347626\n",
      "Iteration 23, Batch: 35, Loss: 0.06336832791566849\n",
      "Iteration 23, Batch: 36, Loss: 0.05503857880830765\n",
      "Iteration 23, Batch: 37, Loss: 0.07978002727031708\n",
      "Iteration 23, Batch: 38, Loss: 0.08411568403244019\n",
      "Iteration 23, Batch: 39, Loss: 0.04250910505652428\n",
      "Iteration 23, Batch: 40, Loss: 0.08101953566074371\n",
      "Iteration 23, Batch: 41, Loss: 0.07245184481143951\n",
      "Iteration 23, Batch: 42, Loss: 0.08199005573987961\n",
      "Iteration 23, Batch: 43, Loss: 0.06731757521629333\n",
      "Iteration 23, Batch: 44, Loss: 0.061934493482112885\n",
      "Iteration 23, Batch: 45, Loss: 0.052709516137838364\n",
      "Iteration 23, Batch: 46, Loss: 0.07709740102291107\n",
      "Iteration 23, Batch: 47, Loss: 0.08173052966594696\n",
      "Iteration 23, Batch: 48, Loss: 0.04761764779686928\n",
      "Iteration 23, Batch: 49, Loss: 0.046570148319005966\n",
      "Iteration 24, Batch: 0, Loss: 0.08457988500595093\n",
      "Iteration 24, Batch: 1, Loss: 0.060200318694114685\n",
      "Iteration 24, Batch: 2, Loss: 0.07462390512228012\n",
      "Iteration 24, Batch: 3, Loss: 0.060706984251737595\n",
      "Iteration 24, Batch: 4, Loss: 0.08303546905517578\n",
      "Iteration 24, Batch: 5, Loss: 0.0646204873919487\n",
      "Iteration 24, Batch: 6, Loss: 0.10049226880073547\n",
      "Iteration 24, Batch: 7, Loss: 0.08589480817317963\n",
      "Iteration 24, Batch: 8, Loss: 0.09668026119470596\n",
      "Iteration 24, Batch: 9, Loss: 0.08874672651290894\n",
      "Iteration 24, Batch: 10, Loss: 0.07931514084339142\n",
      "Iteration 24, Batch: 11, Loss: 0.06980359554290771\n",
      "Iteration 24, Batch: 12, Loss: 0.0670699030160904\n",
      "Iteration 24, Batch: 13, Loss: 0.053129423409700394\n",
      "Iteration 24, Batch: 14, Loss: 0.04172112047672272\n",
      "Iteration 24, Batch: 15, Loss: 0.07023423165082932\n",
      "Iteration 24, Batch: 16, Loss: 0.08547858893871307\n",
      "Iteration 24, Batch: 17, Loss: 0.061394572257995605\n",
      "Iteration 24, Batch: 18, Loss: 0.05426279827952385\n",
      "Iteration 24, Batch: 19, Loss: 0.07366310805082321\n",
      "Iteration 24, Batch: 20, Loss: 0.06521306186914444\n",
      "Iteration 24, Batch: 21, Loss: 0.07200409471988678\n",
      "Iteration 24, Batch: 22, Loss: 0.049334362149238586\n",
      "Iteration 24, Batch: 23, Loss: 0.07053238153457642\n",
      "Iteration 24, Batch: 24, Loss: 0.09303533285856247\n",
      "Iteration 24, Batch: 25, Loss: 0.07951574772596359\n",
      "Iteration 24, Batch: 26, Loss: 0.06752818822860718\n",
      "Iteration 24, Batch: 27, Loss: 0.09180281311273575\n",
      "Iteration 24, Batch: 28, Loss: 0.08082275092601776\n",
      "Iteration 24, Batch: 29, Loss: 0.06457524001598358\n",
      "Iteration 24, Batch: 30, Loss: 0.06272415816783905\n",
      "Iteration 24, Batch: 31, Loss: 0.07586942613124847\n",
      "Iteration 24, Batch: 32, Loss: 0.06557218730449677\n",
      "Iteration 24, Batch: 33, Loss: 0.061072420328855515\n",
      "Iteration 24, Batch: 34, Loss: 0.07120272517204285\n",
      "Iteration 24, Batch: 35, Loss: 0.07228492945432663\n",
      "Iteration 24, Batch: 36, Loss: 0.09764255583286285\n",
      "Iteration 24, Batch: 37, Loss: 0.05750374123454094\n",
      "Iteration 24, Batch: 38, Loss: 0.0881582647562027\n",
      "Iteration 24, Batch: 39, Loss: 0.07284751534461975\n",
      "Iteration 24, Batch: 40, Loss: 0.07388930022716522\n",
      "Iteration 24, Batch: 41, Loss: 0.058425433933734894\n",
      "Iteration 24, Batch: 42, Loss: 0.07679364830255508\n",
      "Iteration 24, Batch: 43, Loss: 0.06051189452409744\n",
      "Iteration 24, Batch: 44, Loss: 0.05790049955248833\n",
      "Iteration 24, Batch: 45, Loss: 0.09063257277011871\n",
      "Iteration 24, Batch: 46, Loss: 0.07187856733798981\n",
      "Iteration 24, Batch: 47, Loss: 0.07034704089164734\n",
      "Iteration 24, Batch: 48, Loss: 0.06366000324487686\n",
      "Iteration 24, Batch: 49, Loss: 0.06929294764995575\n",
      "Iteration 25, Batch: 0, Loss: 0.06348849833011627\n",
      "Iteration 25, Batch: 1, Loss: 0.09315475076436996\n",
      "Iteration 25, Batch: 2, Loss: 0.042921848595142365\n",
      "Iteration 25, Batch: 3, Loss: 0.0910009816288948\n",
      "Iteration 25, Batch: 4, Loss: 0.08604943752288818\n",
      "Iteration 25, Batch: 5, Loss: 0.06625249981880188\n",
      "Iteration 25, Batch: 6, Loss: 0.04881627857685089\n",
      "Iteration 25, Batch: 7, Loss: 0.07846781611442566\n",
      "Iteration 25, Batch: 8, Loss: 0.08768199384212494\n",
      "Iteration 25, Batch: 9, Loss: 0.060793742537498474\n",
      "Iteration 25, Batch: 10, Loss: 0.11715679615736008\n",
      "Iteration 25, Batch: 11, Loss: 0.0733262300491333\n",
      "Iteration 25, Batch: 12, Loss: 0.05677647516131401\n",
      "Iteration 25, Batch: 13, Loss: 0.0776718407869339\n",
      "Iteration 25, Batch: 14, Loss: 0.08501451462507248\n",
      "Iteration 25, Batch: 15, Loss: 0.10819044709205627\n",
      "Iteration 25, Batch: 16, Loss: 0.10311079025268555\n",
      "Iteration 25, Batch: 17, Loss: 0.12391319125890732\n",
      "Iteration 25, Batch: 18, Loss: 0.09739530086517334\n",
      "Iteration 25, Batch: 19, Loss: 0.08388403058052063\n",
      "Iteration 25, Batch: 20, Loss: 0.054646700620651245\n",
      "Iteration 25, Batch: 21, Loss: 0.08125670999288559\n",
      "Iteration 25, Batch: 22, Loss: 0.0764729380607605\n",
      "Iteration 25, Batch: 23, Loss: 0.07926182448863983\n",
      "Iteration 25, Batch: 24, Loss: 0.08722878992557526\n",
      "Iteration 25, Batch: 25, Loss: 0.05825652927160263\n",
      "Iteration 25, Batch: 26, Loss: 0.11368335783481598\n",
      "Iteration 25, Batch: 27, Loss: 0.10396366566419601\n",
      "Iteration 25, Batch: 28, Loss: 0.06671923398971558\n",
      "Iteration 25, Batch: 29, Loss: 0.11179760098457336\n",
      "Iteration 25, Batch: 30, Loss: 0.06971591711044312\n",
      "Iteration 25, Batch: 31, Loss: 0.07054237276315689\n",
      "Iteration 25, Batch: 32, Loss: 0.055504705756902695\n",
      "Iteration 25, Batch: 33, Loss: 0.08658841997385025\n",
      "Iteration 25, Batch: 34, Loss: 0.07181727141141891\n",
      "Iteration 25, Batch: 35, Loss: 0.05105719342827797\n",
      "Iteration 25, Batch: 36, Loss: 0.09097474068403244\n",
      "Iteration 25, Batch: 37, Loss: 0.0823659598827362\n",
      "Iteration 25, Batch: 38, Loss: 0.09166499972343445\n",
      "Iteration 25, Batch: 39, Loss: 0.04653976485133171\n",
      "Iteration 25, Batch: 40, Loss: 0.06295841932296753\n",
      "Iteration 25, Batch: 41, Loss: 0.07753560692071915\n",
      "Iteration 25, Batch: 42, Loss: 0.07519253343343735\n",
      "Iteration 25, Batch: 43, Loss: 0.053050994873046875\n",
      "Iteration 25, Batch: 44, Loss: 0.0794256180524826\n",
      "Iteration 25, Batch: 45, Loss: 0.04905585199594498\n",
      "Iteration 25, Batch: 46, Loss: 0.08132970333099365\n",
      "Iteration 25, Batch: 47, Loss: 0.07949357479810715\n",
      "Iteration 25, Batch: 48, Loss: 0.09887117147445679\n",
      "Iteration 25, Batch: 49, Loss: 0.08215192705392838\n",
      "Iteration 26, Batch: 0, Loss: 0.06301318854093552\n",
      "Iteration 26, Batch: 1, Loss: 0.07881210744380951\n",
      "Iteration 26, Batch: 2, Loss: 0.07772474735975266\n",
      "Iteration 26, Batch: 3, Loss: 0.0792839527130127\n",
      "Iteration 26, Batch: 4, Loss: 0.07637299597263336\n",
      "Iteration 26, Batch: 5, Loss: 0.063252754509449\n",
      "Iteration 26, Batch: 6, Loss: 0.09649103879928589\n",
      "Iteration 26, Batch: 7, Loss: 0.05009043589234352\n",
      "Iteration 26, Batch: 8, Loss: 0.08602035045623779\n",
      "Iteration 26, Batch: 9, Loss: 0.06154637783765793\n",
      "Iteration 26, Batch: 10, Loss: 0.08112472295761108\n",
      "Iteration 26, Batch: 11, Loss: 0.09249980747699738\n",
      "Iteration 26, Batch: 12, Loss: 0.044451210647821426\n",
      "Iteration 26, Batch: 13, Loss: 0.10500465333461761\n",
      "Iteration 26, Batch: 14, Loss: 0.06152055040001869\n",
      "Iteration 26, Batch: 15, Loss: 0.07242368906736374\n",
      "Iteration 26, Batch: 16, Loss: 0.06303277611732483\n",
      "Iteration 26, Batch: 17, Loss: 0.06232369691133499\n",
      "Iteration 26, Batch: 18, Loss: 0.043560855090618134\n",
      "Iteration 26, Batch: 19, Loss: 0.07420060783624649\n",
      "Iteration 26, Batch: 20, Loss: 0.07292643934488297\n",
      "Iteration 26, Batch: 21, Loss: 0.1022060364484787\n",
      "Iteration 26, Batch: 22, Loss: 0.0915917381644249\n",
      "Iteration 26, Batch: 23, Loss: 0.06885910034179688\n",
      "Iteration 26, Batch: 24, Loss: 0.08331657946109772\n",
      "Iteration 26, Batch: 25, Loss: 0.0936080738902092\n",
      "Iteration 26, Batch: 26, Loss: 0.08125820010900497\n",
      "Iteration 26, Batch: 27, Loss: 0.05051421374082565\n",
      "Iteration 26, Batch: 28, Loss: 0.061371974647045135\n",
      "Iteration 26, Batch: 29, Loss: 0.0711737647652626\n",
      "Iteration 26, Batch: 30, Loss: 0.06152573600411415\n",
      "Iteration 26, Batch: 31, Loss: 0.04885168746113777\n",
      "Iteration 26, Batch: 32, Loss: 0.0720575600862503\n",
      "Iteration 26, Batch: 33, Loss: 0.0425109937787056\n",
      "Iteration 26, Batch: 34, Loss: 0.07090773433446884\n",
      "Iteration 26, Batch: 35, Loss: 0.1067654937505722\n",
      "Iteration 26, Batch: 36, Loss: 0.058874957263469696\n",
      "Iteration 26, Batch: 37, Loss: 0.06366294622421265\n",
      "Iteration 26, Batch: 38, Loss: 0.08152390271425247\n",
      "Iteration 26, Batch: 39, Loss: 0.07135133445262909\n",
      "Iteration 26, Batch: 40, Loss: 0.08025340735912323\n",
      "Iteration 26, Batch: 41, Loss: 0.0487547405064106\n",
      "Iteration 26, Batch: 42, Loss: 0.08549470454454422\n",
      "Iteration 26, Batch: 43, Loss: 0.09156302362680435\n",
      "Iteration 26, Batch: 44, Loss: 0.08950486034154892\n",
      "Iteration 26, Batch: 45, Loss: 0.05634049326181412\n",
      "Iteration 26, Batch: 46, Loss: 0.05151236429810524\n",
      "Iteration 26, Batch: 47, Loss: 0.07794596254825592\n",
      "Iteration 26, Batch: 48, Loss: 0.07948295772075653\n",
      "Iteration 26, Batch: 49, Loss: 0.0535208135843277\n",
      "Iteration 27, Batch: 0, Loss: 0.06114601343870163\n",
      "Iteration 27, Batch: 1, Loss: 0.07065583020448685\n",
      "Iteration 27, Batch: 2, Loss: 0.06699428707361221\n",
      "Iteration 27, Batch: 3, Loss: 0.07127807289361954\n",
      "Iteration 27, Batch: 4, Loss: 0.0909915342926979\n",
      "Iteration 27, Batch: 5, Loss: 0.07081247866153717\n",
      "Iteration 27, Batch: 6, Loss: 0.08216224610805511\n",
      "Iteration 27, Batch: 7, Loss: 0.06947483867406845\n",
      "Iteration 27, Batch: 8, Loss: 0.06443149596452713\n",
      "Iteration 27, Batch: 9, Loss: 0.10849666595458984\n",
      "Iteration 27, Batch: 10, Loss: 0.05656496062874794\n",
      "Iteration 27, Batch: 11, Loss: 0.06029827147722244\n",
      "Iteration 27, Batch: 12, Loss: 0.0791555792093277\n",
      "Iteration 27, Batch: 13, Loss: 0.0656074807047844\n",
      "Iteration 27, Batch: 14, Loss: 0.06829988211393356\n",
      "Iteration 27, Batch: 15, Loss: 0.050592679530382156\n",
      "Iteration 27, Batch: 16, Loss: 0.05574159324169159\n",
      "Iteration 27, Batch: 17, Loss: 0.10108062624931335\n",
      "Iteration 27, Batch: 18, Loss: 0.04384017735719681\n",
      "Iteration 27, Batch: 19, Loss: 0.0864003375172615\n",
      "Iteration 27, Batch: 20, Loss: 0.10781776160001755\n",
      "Iteration 27, Batch: 21, Loss: 0.043857771903276443\n",
      "Iteration 27, Batch: 22, Loss: 0.09678459912538528\n",
      "Iteration 27, Batch: 23, Loss: 0.05002657696604729\n",
      "Iteration 27, Batch: 24, Loss: 0.03996016085147858\n",
      "Iteration 27, Batch: 25, Loss: 0.12295804917812347\n",
      "Iteration 27, Batch: 26, Loss: 0.06521647423505783\n",
      "Iteration 27, Batch: 27, Loss: 0.0946042612195015\n",
      "Iteration 27, Batch: 28, Loss: 0.06371617317199707\n",
      "Iteration 27, Batch: 29, Loss: 0.07180893421173096\n",
      "Iteration 27, Batch: 30, Loss: 0.04920347034931183\n",
      "Iteration 27, Batch: 31, Loss: 0.054635465145111084\n",
      "Iteration 27, Batch: 32, Loss: 0.11427365243434906\n",
      "Iteration 27, Batch: 33, Loss: 0.06870635598897934\n",
      "Iteration 27, Batch: 34, Loss: 0.08716937899589539\n",
      "Iteration 27, Batch: 35, Loss: 0.06985940784215927\n",
      "Iteration 27, Batch: 36, Loss: 0.08562759310007095\n",
      "Iteration 27, Batch: 37, Loss: 0.08122379332780838\n",
      "Iteration 27, Batch: 38, Loss: 0.10335741937160492\n",
      "Iteration 27, Batch: 39, Loss: 0.07582555711269379\n",
      "Iteration 27, Batch: 40, Loss: 0.07482390105724335\n",
      "Iteration 27, Batch: 41, Loss: 0.06737230718135834\n",
      "Iteration 27, Batch: 42, Loss: 0.09657659381628036\n",
      "Iteration 27, Batch: 43, Loss: 0.08861163258552551\n",
      "Iteration 27, Batch: 44, Loss: 0.0942923054099083\n",
      "Iteration 27, Batch: 45, Loss: 0.06429136544466019\n",
      "Iteration 27, Batch: 46, Loss: 0.08852482587099075\n",
      "Iteration 27, Batch: 47, Loss: 0.06993573158979416\n",
      "Iteration 27, Batch: 48, Loss: 0.06537755578756332\n",
      "Iteration 27, Batch: 49, Loss: 0.10566438734531403\n",
      "Iteration 28, Batch: 0, Loss: 0.07328307628631592\n",
      "Iteration 28, Batch: 1, Loss: 0.08004413545131683\n",
      "Iteration 28, Batch: 2, Loss: 0.09591937810182571\n",
      "Iteration 28, Batch: 3, Loss: 0.07129864394664764\n",
      "Iteration 28, Batch: 4, Loss: 0.06190888211131096\n",
      "Iteration 28, Batch: 5, Loss: 0.044178374111652374\n",
      "Iteration 28, Batch: 6, Loss: 0.06245868653059006\n",
      "Iteration 28, Batch: 7, Loss: 0.09047903120517731\n",
      "Iteration 28, Batch: 8, Loss: 0.07589247077703476\n",
      "Iteration 28, Batch: 9, Loss: 0.09128425270318985\n",
      "Iteration 28, Batch: 10, Loss: 0.07354848831892014\n",
      "Iteration 28, Batch: 11, Loss: 0.0720595046877861\n",
      "Iteration 28, Batch: 12, Loss: 0.04187082499265671\n",
      "Iteration 28, Batch: 13, Loss: 0.095504030585289\n",
      "Iteration 28, Batch: 14, Loss: 0.05821732059121132\n",
      "Iteration 28, Batch: 15, Loss: 0.0730450451374054\n",
      "Iteration 28, Batch: 16, Loss: 0.1007266417145729\n",
      "Iteration 28, Batch: 17, Loss: 0.08361754566431046\n",
      "Iteration 28, Batch: 18, Loss: 0.08838266134262085\n",
      "Iteration 28, Batch: 19, Loss: 0.07516986131668091\n",
      "Iteration 28, Batch: 20, Loss: 0.08473941683769226\n",
      "Iteration 28, Batch: 21, Loss: 0.051025159657001495\n",
      "Iteration 28, Batch: 22, Loss: 0.11053245514631271\n",
      "Iteration 28, Batch: 23, Loss: 0.05550398677587509\n",
      "Iteration 28, Batch: 24, Loss: 0.06845596432685852\n",
      "Iteration 28, Batch: 25, Loss: 0.0725371316075325\n",
      "Iteration 28, Batch: 26, Loss: 0.09004083275794983\n",
      "Iteration 28, Batch: 27, Loss: 0.0754745826125145\n",
      "Iteration 28, Batch: 28, Loss: 0.06576041877269745\n",
      "Iteration 28, Batch: 29, Loss: 0.06376590579748154\n",
      "Iteration 28, Batch: 30, Loss: 0.0872342437505722\n",
      "Iteration 28, Batch: 31, Loss: 0.07043518871068954\n",
      "Iteration 28, Batch: 32, Loss: 0.07488997280597687\n",
      "Iteration 28, Batch: 33, Loss: 0.09232953190803528\n",
      "Iteration 28, Batch: 34, Loss: 0.09412886202335358\n",
      "Iteration 28, Batch: 35, Loss: 0.06696426123380661\n",
      "Iteration 28, Batch: 36, Loss: 0.054736409336328506\n",
      "Iteration 28, Batch: 37, Loss: 0.10171312838792801\n",
      "Iteration 28, Batch: 38, Loss: 0.07645469158887863\n",
      "Iteration 28, Batch: 39, Loss: 0.07953085005283356\n",
      "Iteration 28, Batch: 40, Loss: 0.0782114788889885\n",
      "Iteration 28, Batch: 41, Loss: 0.0886145606637001\n",
      "Iteration 28, Batch: 42, Loss: 0.08148206770420074\n",
      "Iteration 28, Batch: 43, Loss: 0.08013767004013062\n",
      "Iteration 28, Batch: 44, Loss: 0.08784808963537216\n",
      "Iteration 28, Batch: 45, Loss: 0.06690257042646408\n",
      "Iteration 28, Batch: 46, Loss: 0.0739225298166275\n",
      "Iteration 28, Batch: 47, Loss: 0.09787055850028992\n",
      "Iteration 28, Batch: 48, Loss: 0.08034425228834152\n",
      "Iteration 28, Batch: 49, Loss: 0.09498731046915054\n",
      "Iteration 29, Batch: 0, Loss: 0.09522312134504318\n",
      "Iteration 29, Batch: 1, Loss: 0.08365850150585175\n",
      "Iteration 29, Batch: 2, Loss: 0.08622102439403534\n",
      "Iteration 29, Batch: 3, Loss: 0.07555928081274033\n",
      "Iteration 29, Batch: 4, Loss: 0.04629954323172569\n",
      "Iteration 29, Batch: 5, Loss: 0.05986178293824196\n",
      "Iteration 29, Batch: 6, Loss: 0.07667650282382965\n",
      "Iteration 29, Batch: 7, Loss: 0.08038181066513062\n",
      "Iteration 29, Batch: 8, Loss: 0.056671466678380966\n",
      "Iteration 29, Batch: 9, Loss: 0.0641651600599289\n",
      "Iteration 29, Batch: 10, Loss: 0.09462272375822067\n",
      "Iteration 29, Batch: 11, Loss: 0.05090557038784027\n",
      "Iteration 29, Batch: 12, Loss: 0.06109367683529854\n",
      "Iteration 29, Batch: 13, Loss: 0.09874797612428665\n",
      "Iteration 29, Batch: 14, Loss: 0.0681687593460083\n",
      "Iteration 29, Batch: 15, Loss: 0.06769644469022751\n",
      "Iteration 29, Batch: 16, Loss: 0.08560454100370407\n",
      "Iteration 29, Batch: 17, Loss: 0.05876771733164787\n",
      "Iteration 29, Batch: 18, Loss: 0.08154208958148956\n",
      "Iteration 29, Batch: 19, Loss: 0.06410668045282364\n",
      "Iteration 29, Batch: 20, Loss: 0.07517829537391663\n",
      "Iteration 29, Batch: 21, Loss: 0.045410092920064926\n",
      "Iteration 29, Batch: 22, Loss: 0.08782842755317688\n",
      "Iteration 29, Batch: 23, Loss: 0.054215915501117706\n",
      "Iteration 29, Batch: 24, Loss: 0.052227914333343506\n",
      "Iteration 29, Batch: 25, Loss: 0.06619686633348465\n",
      "Iteration 29, Batch: 26, Loss: 0.07219018042087555\n",
      "Iteration 29, Batch: 27, Loss: 0.09280166774988174\n",
      "Iteration 29, Batch: 28, Loss: 0.0921034961938858\n",
      "Iteration 29, Batch: 29, Loss: 0.06672988086938858\n",
      "Iteration 29, Batch: 30, Loss: 0.061895303428173065\n",
      "Iteration 29, Batch: 31, Loss: 0.07769852131605148\n",
      "Iteration 29, Batch: 32, Loss: 0.06596647948026657\n",
      "Iteration 29, Batch: 33, Loss: 0.0544268898665905\n",
      "Iteration 29, Batch: 34, Loss: 0.10667350888252258\n",
      "Iteration 29, Batch: 35, Loss: 0.0797118991613388\n",
      "Iteration 29, Batch: 36, Loss: 0.06701663136482239\n",
      "Iteration 29, Batch: 37, Loss: 0.04758550599217415\n",
      "Iteration 29, Batch: 38, Loss: 0.08379476517438889\n",
      "Iteration 29, Batch: 39, Loss: 0.04726327955722809\n",
      "Iteration 29, Batch: 40, Loss: 0.08468686789274216\n",
      "Iteration 29, Batch: 41, Loss: 0.07076340168714523\n",
      "Iteration 29, Batch: 42, Loss: 0.05951599031686783\n",
      "Iteration 29, Batch: 43, Loss: 0.051962126046419144\n",
      "Iteration 29, Batch: 44, Loss: 0.07677845656871796\n",
      "Iteration 29, Batch: 45, Loss: 0.08826572448015213\n",
      "Iteration 29, Batch: 46, Loss: 0.07461697608232498\n",
      "Iteration 29, Batch: 47, Loss: 0.0814163014292717\n",
      "Iteration 29, Batch: 48, Loss: 0.06596898287534714\n",
      "Iteration 29, Batch: 49, Loss: 0.07059681415557861\n",
      "Iteration 30, Batch: 0, Loss: 0.05797090008854866\n",
      "Iteration 30, Batch: 1, Loss: 0.0742986872792244\n",
      "Iteration 30, Batch: 2, Loss: 0.07632724940776825\n",
      "Iteration 30, Batch: 3, Loss: 0.07804648578166962\n",
      "Iteration 30, Batch: 4, Loss: 0.10822385549545288\n",
      "Iteration 30, Batch: 5, Loss: 0.07384967058897018\n",
      "Iteration 30, Batch: 6, Loss: 0.0691601112484932\n",
      "Iteration 30, Batch: 7, Loss: 0.06413903832435608\n",
      "Iteration 30, Batch: 8, Loss: 0.06520360708236694\n",
      "Iteration 30, Batch: 9, Loss: 0.056798916310071945\n",
      "Iteration 30, Batch: 10, Loss: 0.09435654431581497\n",
      "Iteration 30, Batch: 11, Loss: 0.06448888778686523\n",
      "Iteration 30, Batch: 12, Loss: 0.06880605965852737\n",
      "Iteration 30, Batch: 13, Loss: 0.05226859822869301\n",
      "Iteration 30, Batch: 14, Loss: 0.1093774363398552\n",
      "Iteration 30, Batch: 15, Loss: 0.06172187626361847\n",
      "Iteration 30, Batch: 16, Loss: 0.08343727141618729\n",
      "Iteration 30, Batch: 17, Loss: 0.07414114475250244\n",
      "Iteration 30, Batch: 18, Loss: 0.08028852194547653\n",
      "Iteration 30, Batch: 19, Loss: 0.09186932444572449\n",
      "Iteration 30, Batch: 20, Loss: 0.09129718691110611\n",
      "Iteration 30, Batch: 21, Loss: 0.05541069433093071\n",
      "Iteration 30, Batch: 22, Loss: 0.05566834658384323\n",
      "Iteration 30, Batch: 23, Loss: 0.05990388244390488\n",
      "Iteration 30, Batch: 24, Loss: 0.050443392246961594\n",
      "Iteration 30, Batch: 25, Loss: 0.06388191133737564\n",
      "Iteration 30, Batch: 26, Loss: 0.054449692368507385\n",
      "Iteration 30, Batch: 27, Loss: 0.06598569452762604\n",
      "Iteration 30, Batch: 28, Loss: 0.1105804294347763\n",
      "Iteration 30, Batch: 29, Loss: 0.07884815335273743\n",
      "Iteration 30, Batch: 30, Loss: 0.09350412338972092\n",
      "Iteration 30, Batch: 31, Loss: 0.06255827844142914\n",
      "Iteration 30, Batch: 32, Loss: 0.057687945663928986\n",
      "Iteration 30, Batch: 33, Loss: 0.0725310817360878\n",
      "Iteration 30, Batch: 34, Loss: 0.062215641140937805\n",
      "Iteration 30, Batch: 35, Loss: 0.08925504237413406\n",
      "Iteration 30, Batch: 36, Loss: 0.058270566165447235\n",
      "Iteration 30, Batch: 37, Loss: 0.08723078668117523\n",
      "Iteration 30, Batch: 38, Loss: 0.07192742079496384\n",
      "Iteration 30, Batch: 39, Loss: 0.07974020391702652\n",
      "Iteration 30, Batch: 40, Loss: 0.08333805203437805\n",
      "Iteration 30, Batch: 41, Loss: 0.06856440752744675\n",
      "Iteration 30, Batch: 42, Loss: 0.055342309176921844\n",
      "Iteration 30, Batch: 43, Loss: 0.09055092930793762\n",
      "Iteration 30, Batch: 44, Loss: 0.10528499633073807\n",
      "Iteration 30, Batch: 45, Loss: 0.06487809866666794\n",
      "Iteration 30, Batch: 46, Loss: 0.041502051055431366\n",
      "Iteration 30, Batch: 47, Loss: 0.05573255568742752\n",
      "Iteration 30, Batch: 48, Loss: 0.07500474154949188\n",
      "Iteration 30, Batch: 49, Loss: 0.08284953236579895\n",
      "Iteration 31, Batch: 0, Loss: 0.06356208026409149\n",
      "Iteration 31, Batch: 1, Loss: 0.07039027661085129\n",
      "Iteration 31, Batch: 2, Loss: 0.059326570481061935\n",
      "Iteration 31, Batch: 3, Loss: 0.07649393379688263\n",
      "Iteration 31, Batch: 4, Loss: 0.06197668984532356\n",
      "Iteration 31, Batch: 5, Loss: 0.055109865963459015\n",
      "Iteration 31, Batch: 6, Loss: 0.049792975187301636\n",
      "Iteration 31, Batch: 7, Loss: 0.057685162872076035\n",
      "Iteration 31, Batch: 8, Loss: 0.07804366946220398\n",
      "Iteration 31, Batch: 9, Loss: 0.0734001025557518\n",
      "Iteration 31, Batch: 10, Loss: 0.07310079783201218\n",
      "Iteration 31, Batch: 11, Loss: 0.09633796662092209\n",
      "Iteration 31, Batch: 12, Loss: 0.08086147159337997\n",
      "Iteration 31, Batch: 13, Loss: 0.07158112525939941\n",
      "Iteration 31, Batch: 14, Loss: 0.08900739252567291\n",
      "Iteration 31, Batch: 15, Loss: 0.05599325895309448\n",
      "Iteration 31, Batch: 16, Loss: 0.0805424302816391\n",
      "Iteration 31, Batch: 17, Loss: 0.07080650329589844\n",
      "Iteration 31, Batch: 18, Loss: 0.08500669151544571\n",
      "Iteration 31, Batch: 19, Loss: 0.09146770089864731\n",
      "Iteration 31, Batch: 20, Loss: 0.05359640344977379\n",
      "Iteration 31, Batch: 21, Loss: 0.09703538566827774\n",
      "Iteration 31, Batch: 22, Loss: 0.06351741403341293\n",
      "Iteration 31, Batch: 23, Loss: 0.13569697737693787\n",
      "Iteration 31, Batch: 24, Loss: 0.08229885250329971\n",
      "Iteration 31, Batch: 25, Loss: 0.07631523907184601\n",
      "Iteration 31, Batch: 26, Loss: 0.08138729631900787\n",
      "Iteration 31, Batch: 27, Loss: 0.05860196426510811\n",
      "Iteration 31, Batch: 28, Loss: 0.06999985873699188\n",
      "Iteration 31, Batch: 29, Loss: 0.08866167813539505\n",
      "Iteration 31, Batch: 30, Loss: 0.079677514731884\n",
      "Iteration 31, Batch: 31, Loss: 0.08404999226331711\n",
      "Iteration 31, Batch: 32, Loss: 0.08479949831962585\n",
      "Iteration 31, Batch: 33, Loss: 0.09998277574777603\n",
      "Iteration 31, Batch: 34, Loss: 0.06493058055639267\n",
      "Iteration 31, Batch: 35, Loss: 0.07330114394426346\n",
      "Iteration 31, Batch: 36, Loss: 0.08131430298089981\n",
      "Iteration 31, Batch: 37, Loss: 0.07613932341337204\n",
      "Iteration 31, Batch: 38, Loss: 0.09444578737020493\n",
      "Iteration 31, Batch: 39, Loss: 0.09955662488937378\n",
      "Iteration 31, Batch: 40, Loss: 0.07542873173952103\n",
      "Iteration 31, Batch: 41, Loss: 0.12210200726985931\n",
      "Iteration 31, Batch: 42, Loss: 0.11380602419376373\n",
      "Iteration 31, Batch: 43, Loss: 0.1107378751039505\n",
      "Iteration 31, Batch: 44, Loss: 0.08976749330759048\n",
      "Iteration 31, Batch: 45, Loss: 0.07917644083499908\n",
      "Iteration 31, Batch: 46, Loss: 0.15771815180778503\n",
      "Iteration 31, Batch: 47, Loss: 0.11859922856092453\n",
      "Iteration 31, Batch: 48, Loss: 0.13795968890190125\n",
      "Iteration 31, Batch: 49, Loss: 0.15030555427074432\n",
      "Iteration 32, Batch: 0, Loss: 0.14037124812602997\n",
      "Iteration 32, Batch: 1, Loss: 0.116666279733181\n",
      "Iteration 32, Batch: 2, Loss: 0.1452096551656723\n",
      "Iteration 32, Batch: 3, Loss: 0.10729913413524628\n",
      "Iteration 32, Batch: 4, Loss: 0.1104128435254097\n",
      "Iteration 32, Batch: 5, Loss: 0.12891152501106262\n",
      "Iteration 32, Batch: 6, Loss: 0.10860120505094528\n",
      "Iteration 32, Batch: 7, Loss: 0.13098673522472382\n",
      "Iteration 32, Batch: 8, Loss: 0.11133245378732681\n",
      "Iteration 32, Batch: 9, Loss: 0.07116426527500153\n",
      "Iteration 32, Batch: 10, Loss: 0.1322523057460785\n",
      "Iteration 32, Batch: 11, Loss: 0.08248326182365417\n",
      "Iteration 32, Batch: 12, Loss: 0.09340667724609375\n",
      "Iteration 32, Batch: 13, Loss: 0.12691238522529602\n",
      "Iteration 32, Batch: 14, Loss: 0.10768257826566696\n",
      "Iteration 32, Batch: 15, Loss: 0.10797225683927536\n",
      "Iteration 32, Batch: 16, Loss: 0.11657273769378662\n",
      "Iteration 32, Batch: 17, Loss: 0.11105531454086304\n",
      "Iteration 32, Batch: 18, Loss: 0.10560521483421326\n",
      "Iteration 32, Batch: 19, Loss: 0.17123770713806152\n",
      "Iteration 32, Batch: 20, Loss: 0.1470753401517868\n",
      "Iteration 32, Batch: 21, Loss: 0.15268762409687042\n",
      "Iteration 32, Batch: 22, Loss: 0.13563622534275055\n",
      "Iteration 32, Batch: 23, Loss: 0.18101763725280762\n",
      "Iteration 32, Batch: 24, Loss: 0.11575011163949966\n",
      "Iteration 32, Batch: 25, Loss: 0.11805605888366699\n",
      "Iteration 32, Batch: 26, Loss: 0.15717414021492004\n",
      "Iteration 32, Batch: 27, Loss: 0.12749163806438446\n",
      "Iteration 32, Batch: 28, Loss: 0.14274227619171143\n",
      "Iteration 32, Batch: 29, Loss: 0.15506389737129211\n",
      "Iteration 32, Batch: 30, Loss: 0.1186753436923027\n",
      "Iteration 32, Batch: 31, Loss: 0.1371423304080963\n",
      "Iteration 32, Batch: 32, Loss: 0.10848382860422134\n",
      "Iteration 32, Batch: 33, Loss: 0.12280453741550446\n",
      "Iteration 32, Batch: 34, Loss: 0.12527482211589813\n",
      "Iteration 32, Batch: 35, Loss: 0.08325657993555069\n",
      "Iteration 32, Batch: 36, Loss: 0.13483621180057526\n",
      "Iteration 32, Batch: 37, Loss: 0.1218431368470192\n",
      "Iteration 32, Batch: 38, Loss: 0.1103987991809845\n",
      "Iteration 32, Batch: 39, Loss: 0.10454937070608139\n",
      "Iteration 32, Batch: 40, Loss: 0.11647044867277145\n",
      "Iteration 32, Batch: 41, Loss: 0.11588907241821289\n",
      "Iteration 32, Batch: 42, Loss: 0.16035544872283936\n",
      "Iteration 32, Batch: 43, Loss: 0.14476510882377625\n",
      "Iteration 32, Batch: 44, Loss: 0.1455746442079544\n",
      "Iteration 32, Batch: 45, Loss: 0.13495413959026337\n",
      "Iteration 32, Batch: 46, Loss: 0.11935146898031235\n",
      "Iteration 32, Batch: 47, Loss: 0.07394732534885406\n",
      "Iteration 32, Batch: 48, Loss: 0.10771887749433517\n",
      "Iteration 32, Batch: 49, Loss: 0.1480739414691925\n",
      "Iteration 33, Batch: 0, Loss: 0.13769426941871643\n",
      "Iteration 33, Batch: 1, Loss: 0.16146428883075714\n",
      "Iteration 33, Batch: 2, Loss: 0.17008048295974731\n",
      "Iteration 33, Batch: 3, Loss: 0.14986753463745117\n",
      "Iteration 33, Batch: 4, Loss: 0.1255940943956375\n",
      "Iteration 33, Batch: 5, Loss: 0.10484162718057632\n",
      "Iteration 33, Batch: 6, Loss: 0.11482527107000351\n",
      "Iteration 33, Batch: 7, Loss: 0.12568391859531403\n",
      "Iteration 33, Batch: 8, Loss: 0.13257810473442078\n",
      "Iteration 33, Batch: 9, Loss: 0.12151076644659042\n",
      "Iteration 33, Batch: 10, Loss: 0.10568931698799133\n",
      "Iteration 33, Batch: 11, Loss: 0.13909859955310822\n",
      "Iteration 33, Batch: 12, Loss: 0.15706075727939606\n",
      "Iteration 33, Batch: 13, Loss: 0.155641108751297\n",
      "Iteration 33, Batch: 14, Loss: 0.11206750571727753\n",
      "Iteration 33, Batch: 15, Loss: 0.11838944256305695\n",
      "Iteration 33, Batch: 16, Loss: 0.12099236249923706\n",
      "Iteration 33, Batch: 17, Loss: 0.15907345712184906\n",
      "Iteration 33, Batch: 18, Loss: 0.1325499266386032\n",
      "Iteration 33, Batch: 19, Loss: 0.15907618403434753\n",
      "Iteration 33, Batch: 20, Loss: 0.13713903725147247\n",
      "Iteration 33, Batch: 21, Loss: 0.09913154691457748\n",
      "Iteration 33, Batch: 22, Loss: 0.097154900431633\n",
      "Iteration 33, Batch: 23, Loss: 0.11394426226615906\n",
      "Iteration 33, Batch: 24, Loss: 0.1466313898563385\n",
      "Iteration 33, Batch: 25, Loss: 0.114238440990448\n",
      "Iteration 33, Batch: 26, Loss: 0.10487321764230728\n",
      "Iteration 33, Batch: 27, Loss: 0.12513703107833862\n",
      "Iteration 33, Batch: 28, Loss: 0.11490285396575928\n",
      "Iteration 33, Batch: 29, Loss: 0.09347769618034363\n",
      "Iteration 33, Batch: 30, Loss: 0.1316044181585312\n",
      "Iteration 33, Batch: 31, Loss: 0.10114060342311859\n",
      "Iteration 33, Batch: 32, Loss: 0.10060832649469376\n",
      "Iteration 33, Batch: 33, Loss: 0.1526098996400833\n",
      "Iteration 33, Batch: 34, Loss: 0.12632133066654205\n",
      "Iteration 33, Batch: 35, Loss: 0.09948209673166275\n",
      "Iteration 33, Batch: 36, Loss: 0.1298355609178543\n",
      "Iteration 33, Batch: 37, Loss: 0.14105050265789032\n",
      "Iteration 33, Batch: 38, Loss: 0.10836444795131683\n",
      "Iteration 33, Batch: 39, Loss: 0.11319846659898758\n",
      "Iteration 33, Batch: 40, Loss: 0.09459879994392395\n",
      "Iteration 33, Batch: 41, Loss: 0.11368497461080551\n",
      "Iteration 33, Batch: 42, Loss: 0.15404248237609863\n",
      "Iteration 33, Batch: 43, Loss: 0.10612253099679947\n",
      "Iteration 33, Batch: 44, Loss: 0.12567150592803955\n",
      "Iteration 33, Batch: 45, Loss: 0.11905154585838318\n",
      "Iteration 33, Batch: 46, Loss: 0.08422568440437317\n",
      "Iteration 33, Batch: 47, Loss: 0.13257117569446564\n",
      "Iteration 33, Batch: 48, Loss: 0.10759187489748001\n",
      "Iteration 33, Batch: 49, Loss: 0.10755127668380737\n",
      "Iteration 34, Batch: 0, Loss: 0.0855536088347435\n",
      "Iteration 34, Batch: 1, Loss: 0.0910487025976181\n",
      "Iteration 34, Batch: 2, Loss: 0.08145931363105774\n",
      "Iteration 34, Batch: 3, Loss: 0.10934918373823166\n",
      "Iteration 34, Batch: 4, Loss: 0.08274784684181213\n",
      "Iteration 34, Batch: 5, Loss: 0.09369087219238281\n",
      "Iteration 34, Batch: 6, Loss: 0.09638700634241104\n",
      "Iteration 34, Batch: 7, Loss: 0.10440932214260101\n",
      "Iteration 34, Batch: 8, Loss: 0.10811091959476471\n",
      "Iteration 34, Batch: 9, Loss: 0.08182400465011597\n",
      "Iteration 34, Batch: 10, Loss: 0.11213242262601852\n",
      "Iteration 34, Batch: 11, Loss: 0.12547846138477325\n",
      "Iteration 34, Batch: 12, Loss: 0.11678962409496307\n",
      "Iteration 34, Batch: 13, Loss: 0.13521772623062134\n",
      "Iteration 34, Batch: 14, Loss: 0.10028111934661865\n",
      "Iteration 34, Batch: 15, Loss: 0.1014447882771492\n",
      "Iteration 34, Batch: 16, Loss: 0.11441829055547714\n",
      "Iteration 34, Batch: 17, Loss: 0.12553152441978455\n",
      "Iteration 34, Batch: 18, Loss: 0.11808179318904877\n",
      "Iteration 34, Batch: 19, Loss: 0.16237792372703552\n",
      "Iteration 34, Batch: 20, Loss: 0.13039202988147736\n",
      "Iteration 34, Batch: 21, Loss: 0.12959589064121246\n",
      "Iteration 34, Batch: 22, Loss: 0.10024894773960114\n",
      "Iteration 34, Batch: 23, Loss: 0.11031278222799301\n",
      "Iteration 34, Batch: 24, Loss: 0.12415944784879684\n",
      "Iteration 34, Batch: 25, Loss: 0.15035253763198853\n",
      "Iteration 34, Batch: 26, Loss: 0.13592158257961273\n",
      "Iteration 34, Batch: 27, Loss: 0.1608697921037674\n",
      "Iteration 34, Batch: 28, Loss: 0.1511262059211731\n",
      "Iteration 34, Batch: 29, Loss: 0.12953481078147888\n",
      "Iteration 34, Batch: 30, Loss: 0.11691922694444656\n",
      "Iteration 34, Batch: 31, Loss: 0.13666383922100067\n",
      "Iteration 34, Batch: 32, Loss: 0.11552802473306656\n",
      "Iteration 34, Batch: 33, Loss: 0.11334000527858734\n",
      "Iteration 34, Batch: 34, Loss: 0.12494384497404099\n",
      "Iteration 34, Batch: 35, Loss: 0.08280767500400543\n",
      "Iteration 34, Batch: 36, Loss: 0.0785558894276619\n",
      "Iteration 34, Batch: 37, Loss: 0.1284119337797165\n",
      "Iteration 34, Batch: 38, Loss: 0.10613541305065155\n",
      "Iteration 34, Batch: 39, Loss: 0.13534633815288544\n",
      "Iteration 34, Batch: 40, Loss: 0.11452020704746246\n",
      "Iteration 34, Batch: 41, Loss: 0.12489654123783112\n",
      "Iteration 34, Batch: 42, Loss: 0.11341563612222672\n",
      "Iteration 34, Batch: 43, Loss: 0.14872804284095764\n",
      "Iteration 34, Batch: 44, Loss: 0.1609351634979248\n",
      "Iteration 34, Batch: 45, Loss: 0.1465577632188797\n",
      "Iteration 34, Batch: 46, Loss: 0.10960730910301208\n",
      "Iteration 34, Batch: 47, Loss: 0.1135842502117157\n",
      "Iteration 34, Batch: 48, Loss: 0.09018795937299728\n",
      "Iteration 34, Batch: 49, Loss: 0.0917101725935936\n",
      "Iteration 35, Batch: 0, Loss: 0.1349797546863556\n",
      "Iteration 35, Batch: 1, Loss: 0.14146099984645844\n",
      "Iteration 35, Batch: 2, Loss: 0.13002052903175354\n",
      "Iteration 35, Batch: 3, Loss: 0.12417075783014297\n",
      "Iteration 35, Batch: 4, Loss: 0.10159734636545181\n",
      "Iteration 35, Batch: 5, Loss: 0.10591132938861847\n",
      "Iteration 35, Batch: 6, Loss: 0.12110146880149841\n",
      "Iteration 35, Batch: 7, Loss: 0.13478098809719086\n",
      "Iteration 35, Batch: 8, Loss: 0.1221005991101265\n",
      "Iteration 35, Batch: 9, Loss: 0.15244023501873016\n",
      "Iteration 35, Batch: 10, Loss: 0.12732039391994476\n",
      "Iteration 35, Batch: 11, Loss: 0.15164180099964142\n",
      "Iteration 35, Batch: 12, Loss: 0.09660197049379349\n",
      "Iteration 35, Batch: 13, Loss: 0.10125264525413513\n",
      "Iteration 35, Batch: 14, Loss: 0.10106169432401657\n",
      "Iteration 35, Batch: 15, Loss: 0.08187400549650192\n",
      "Iteration 35, Batch: 16, Loss: 0.12566477060317993\n",
      "Iteration 35, Batch: 17, Loss: 0.09681504964828491\n",
      "Iteration 35, Batch: 18, Loss: 0.10429023206233978\n",
      "Iteration 35, Batch: 19, Loss: 0.10636426508426666\n",
      "Iteration 35, Batch: 20, Loss: 0.1266651451587677\n",
      "Iteration 35, Batch: 21, Loss: 0.12435748428106308\n",
      "Iteration 35, Batch: 22, Loss: 0.10203143954277039\n",
      "Iteration 35, Batch: 23, Loss: 0.09656871855258942\n",
      "Iteration 35, Batch: 24, Loss: 0.11257659643888474\n",
      "Iteration 35, Batch: 25, Loss: 0.07434500753879547\n",
      "Iteration 35, Batch: 26, Loss: 0.10048822313547134\n",
      "Iteration 35, Batch: 27, Loss: 0.11666519194841385\n",
      "Iteration 35, Batch: 28, Loss: 0.11997488886117935\n",
      "Iteration 35, Batch: 29, Loss: 0.12508657574653625\n",
      "Iteration 35, Batch: 30, Loss: 0.10824212431907654\n",
      "Iteration 35, Batch: 31, Loss: 0.07849226891994476\n",
      "Iteration 35, Batch: 32, Loss: 0.10850225389003754\n",
      "Iteration 35, Batch: 33, Loss: 0.0937500149011612\n",
      "Iteration 35, Batch: 34, Loss: 0.10317064076662064\n",
      "Iteration 35, Batch: 35, Loss: 0.12220806628465652\n",
      "Iteration 35, Batch: 36, Loss: 0.09627935290336609\n",
      "Iteration 35, Batch: 37, Loss: 0.1259315311908722\n",
      "Iteration 35, Batch: 38, Loss: 0.10350896418094635\n",
      "Iteration 35, Batch: 39, Loss: 0.10457195341587067\n",
      "Iteration 35, Batch: 40, Loss: 0.07515877485275269\n",
      "Iteration 35, Batch: 41, Loss: 0.10307054221630096\n",
      "Iteration 35, Batch: 42, Loss: 0.10864812880754471\n",
      "Iteration 35, Batch: 43, Loss: 0.11943763494491577\n",
      "Iteration 35, Batch: 44, Loss: 0.0838998481631279\n",
      "Iteration 35, Batch: 45, Loss: 0.08951597660779953\n",
      "Iteration 35, Batch: 46, Loss: 0.0759846642613411\n",
      "Iteration 35, Batch: 47, Loss: 0.11329177767038345\n",
      "Iteration 35, Batch: 48, Loss: 0.12518048286437988\n",
      "Iteration 35, Batch: 49, Loss: 0.09952293336391449\n",
      "Iteration 36, Batch: 0, Loss: 0.11047201603651047\n",
      "Iteration 36, Batch: 1, Loss: 0.11003526300191879\n",
      "Iteration 36, Batch: 2, Loss: 0.1125374585390091\n",
      "Iteration 36, Batch: 3, Loss: 0.10978516936302185\n",
      "Iteration 36, Batch: 4, Loss: 0.10862746834754944\n",
      "Iteration 36, Batch: 5, Loss: 0.08698461204767227\n",
      "Iteration 36, Batch: 6, Loss: 0.06625864654779434\n",
      "Iteration 36, Batch: 7, Loss: 0.08781806379556656\n",
      "Iteration 36, Batch: 8, Loss: 0.09360723197460175\n",
      "Iteration 36, Batch: 9, Loss: 0.12252184003591537\n",
      "Iteration 36, Batch: 10, Loss: 0.13727430999279022\n",
      "Iteration 36, Batch: 11, Loss: 0.09086740016937256\n",
      "Iteration 36, Batch: 12, Loss: 0.12300758063793182\n",
      "Iteration 36, Batch: 13, Loss: 0.08570102602243423\n",
      "Iteration 36, Batch: 14, Loss: 0.09309571236371994\n",
      "Iteration 36, Batch: 15, Loss: 0.087666355073452\n",
      "Iteration 36, Batch: 16, Loss: 0.09061609208583832\n",
      "Iteration 36, Batch: 17, Loss: 0.08119383454322815\n",
      "Iteration 36, Batch: 18, Loss: 0.1011783555150032\n",
      "Iteration 36, Batch: 19, Loss: 0.0674399584531784\n",
      "Iteration 36, Batch: 20, Loss: 0.08579555153846741\n",
      "Iteration 36, Batch: 21, Loss: 0.11813852190971375\n",
      "Iteration 36, Batch: 22, Loss: 0.12725023925304413\n",
      "Iteration 36, Batch: 23, Loss: 0.08388948440551758\n",
      "Iteration 36, Batch: 24, Loss: 0.07461395114660263\n",
      "Iteration 36, Batch: 25, Loss: 0.08457593619823456\n",
      "Iteration 36, Batch: 26, Loss: 0.1020507961511612\n",
      "Iteration 36, Batch: 27, Loss: 0.09976531565189362\n",
      "Iteration 36, Batch: 28, Loss: 0.07212412357330322\n",
      "Iteration 36, Batch: 29, Loss: 0.11481552571058273\n",
      "Iteration 36, Batch: 30, Loss: 0.1042325496673584\n",
      "Iteration 36, Batch: 31, Loss: 0.09920082241296768\n",
      "Iteration 36, Batch: 32, Loss: 0.1303471028804779\n",
      "Iteration 36, Batch: 33, Loss: 0.10026925057172775\n",
      "Iteration 36, Batch: 34, Loss: 0.08418908715248108\n",
      "Iteration 36, Batch: 35, Loss: 0.08464603126049042\n",
      "Iteration 36, Batch: 36, Loss: 0.09580545127391815\n",
      "Iteration 36, Batch: 37, Loss: 0.10543841123580933\n",
      "Iteration 36, Batch: 38, Loss: 0.11132561415433884\n",
      "Iteration 36, Batch: 39, Loss: 0.13040629029273987\n",
      "Iteration 36, Batch: 40, Loss: 0.09305889904499054\n",
      "Iteration 36, Batch: 41, Loss: 0.093328095972538\n",
      "Iteration 36, Batch: 42, Loss: 0.08813472837209702\n",
      "Iteration 36, Batch: 43, Loss: 0.12989675998687744\n",
      "Iteration 36, Batch: 44, Loss: 0.10311292111873627\n",
      "Iteration 36, Batch: 45, Loss: 0.10468167811632156\n",
      "Iteration 36, Batch: 46, Loss: 0.09445156157016754\n",
      "Iteration 36, Batch: 47, Loss: 0.07141607254743576\n",
      "Iteration 36, Batch: 48, Loss: 0.12135061621665955\n",
      "Iteration 36, Batch: 49, Loss: 0.08639205992221832\n",
      "Iteration 37, Batch: 0, Loss: 0.11614760756492615\n",
      "Iteration 37, Batch: 1, Loss: 0.09477268159389496\n",
      "Iteration 37, Batch: 2, Loss: 0.10132743418216705\n",
      "Iteration 37, Batch: 3, Loss: 0.05941816791892052\n",
      "Iteration 37, Batch: 4, Loss: 0.11839395016431808\n",
      "Iteration 37, Batch: 5, Loss: 0.10632427036762238\n",
      "Iteration 37, Batch: 6, Loss: 0.08303714543581009\n",
      "Iteration 37, Batch: 7, Loss: 0.11516010761260986\n",
      "Iteration 37, Batch: 8, Loss: 0.0903974249958992\n",
      "Iteration 37, Batch: 9, Loss: 0.10232973098754883\n",
      "Iteration 37, Batch: 10, Loss: 0.07085545361042023\n",
      "Iteration 37, Batch: 11, Loss: 0.12171556055545807\n",
      "Iteration 37, Batch: 12, Loss: 0.12350934743881226\n",
      "Iteration 37, Batch: 13, Loss: 0.10173945128917694\n",
      "Iteration 37, Batch: 14, Loss: 0.08483953028917313\n",
      "Iteration 37, Batch: 15, Loss: 0.10832023620605469\n",
      "Iteration 37, Batch: 16, Loss: 0.06951259821653366\n",
      "Iteration 37, Batch: 17, Loss: 0.1566520631313324\n",
      "Iteration 37, Batch: 18, Loss: 0.12379785627126694\n",
      "Iteration 37, Batch: 19, Loss: 0.08934164047241211\n",
      "Iteration 37, Batch: 20, Loss: 0.09451346844434738\n",
      "Iteration 37, Batch: 21, Loss: 0.1036052331328392\n",
      "Iteration 37, Batch: 22, Loss: 0.10586664080619812\n",
      "Iteration 37, Batch: 23, Loss: 0.12029919028282166\n",
      "Iteration 37, Batch: 24, Loss: 0.09063445031642914\n",
      "Iteration 37, Batch: 25, Loss: 0.10783164948225021\n",
      "Iteration 37, Batch: 26, Loss: 0.0742628425359726\n",
      "Iteration 37, Batch: 27, Loss: 0.11734411120414734\n",
      "Iteration 37, Batch: 28, Loss: 0.10891607403755188\n",
      "Iteration 37, Batch: 29, Loss: 0.10611562430858612\n",
      "Iteration 37, Batch: 30, Loss: 0.11399178206920624\n",
      "Iteration 37, Batch: 31, Loss: 0.1079462319612503\n",
      "Iteration 37, Batch: 32, Loss: 0.12479568272829056\n",
      "Iteration 37, Batch: 33, Loss: 0.10788251459598541\n",
      "Iteration 37, Batch: 34, Loss: 0.11211425811052322\n",
      "Iteration 37, Batch: 35, Loss: 0.07447677850723267\n",
      "Iteration 37, Batch: 36, Loss: 0.12077087163925171\n",
      "Iteration 37, Batch: 37, Loss: 0.11141471564769745\n",
      "Iteration 37, Batch: 38, Loss: 0.10191619396209717\n",
      "Iteration 37, Batch: 39, Loss: 0.09655299037694931\n",
      "Iteration 37, Batch: 40, Loss: 0.0903543159365654\n",
      "Iteration 37, Batch: 41, Loss: 0.09439048916101456\n",
      "Iteration 37, Batch: 42, Loss: 0.1120796725153923\n",
      "Iteration 37, Batch: 43, Loss: 0.10232467204332352\n",
      "Iteration 37, Batch: 44, Loss: 0.11965257674455643\n",
      "Iteration 37, Batch: 45, Loss: 0.09656307101249695\n",
      "Iteration 37, Batch: 46, Loss: 0.13069632649421692\n",
      "Iteration 37, Batch: 47, Loss: 0.13084200024604797\n",
      "Iteration 37, Batch: 48, Loss: 0.14997588098049164\n",
      "Iteration 37, Batch: 49, Loss: 0.13022126257419586\n",
      "Iteration 38, Batch: 0, Loss: 0.13617999851703644\n",
      "Iteration 38, Batch: 1, Loss: 0.13621214032173157\n",
      "Iteration 38, Batch: 2, Loss: 0.12034787982702255\n",
      "Iteration 38, Batch: 3, Loss: 0.09049099683761597\n",
      "Iteration 38, Batch: 4, Loss: 0.1203468069434166\n",
      "Iteration 38, Batch: 5, Loss: 0.1085156798362732\n",
      "Iteration 38, Batch: 6, Loss: 0.10379358381032944\n",
      "Iteration 38, Batch: 7, Loss: 0.11956141144037247\n",
      "Iteration 38, Batch: 8, Loss: 0.12739476561546326\n",
      "Iteration 38, Batch: 9, Loss: 0.11355012655258179\n",
      "Iteration 38, Batch: 10, Loss: 0.13052143156528473\n",
      "Iteration 38, Batch: 11, Loss: 0.121514230966568\n",
      "Iteration 38, Batch: 12, Loss: 0.11377527564764023\n",
      "Iteration 38, Batch: 13, Loss: 0.10268671065568924\n",
      "Iteration 38, Batch: 14, Loss: 0.08513370156288147\n",
      "Iteration 38, Batch: 15, Loss: 0.08768197894096375\n",
      "Iteration 38, Batch: 16, Loss: 0.08190526068210602\n",
      "Iteration 38, Batch: 17, Loss: 0.0941123366355896\n",
      "Iteration 38, Batch: 18, Loss: 0.07086428254842758\n",
      "Iteration 38, Batch: 19, Loss: 0.13036207854747772\n",
      "Iteration 38, Batch: 20, Loss: 0.15121221542358398\n",
      "Iteration 38, Batch: 21, Loss: 0.12824487686157227\n",
      "Iteration 38, Batch: 22, Loss: 0.09637875109910965\n",
      "Iteration 38, Batch: 23, Loss: 0.10383991152048111\n",
      "Iteration 38, Batch: 24, Loss: 0.10075517743825912\n",
      "Iteration 38, Batch: 25, Loss: 0.10855981707572937\n",
      "Iteration 38, Batch: 26, Loss: 0.09960075467824936\n",
      "Iteration 38, Batch: 27, Loss: 0.10875560343265533\n",
      "Iteration 38, Batch: 28, Loss: 0.13412953913211823\n",
      "Iteration 38, Batch: 29, Loss: 0.11970639228820801\n",
      "Iteration 38, Batch: 30, Loss: 0.13154631853103638\n",
      "Iteration 38, Batch: 31, Loss: 0.09760323166847229\n",
      "Iteration 38, Batch: 32, Loss: 0.1242557242512703\n",
      "Iteration 38, Batch: 33, Loss: 0.1341964453458786\n",
      "Iteration 38, Batch: 34, Loss: 0.14991630613803864\n",
      "Iteration 38, Batch: 35, Loss: 0.09955465793609619\n",
      "Iteration 38, Batch: 36, Loss: 0.12510600686073303\n",
      "Iteration 38, Batch: 37, Loss: 0.12125290930271149\n",
      "Iteration 38, Batch: 38, Loss: 0.07311117649078369\n",
      "Iteration 38, Batch: 39, Loss: 0.10663636773824692\n",
      "Iteration 38, Batch: 40, Loss: 0.0822347030043602\n",
      "Iteration 38, Batch: 41, Loss: 0.11520010977983475\n",
      "Iteration 38, Batch: 42, Loss: 0.1105412095785141\n",
      "Iteration 38, Batch: 43, Loss: 0.10248715430498123\n",
      "Iteration 38, Batch: 44, Loss: 0.08183202892541885\n",
      "Iteration 38, Batch: 45, Loss: 0.12183230370283127\n",
      "Iteration 38, Batch: 46, Loss: 0.07176864892244339\n",
      "Iteration 38, Batch: 47, Loss: 0.10821843892335892\n",
      "Iteration 38, Batch: 48, Loss: 0.126847505569458\n",
      "Iteration 38, Batch: 49, Loss: 0.1103094071149826\n",
      "Iteration 39, Batch: 0, Loss: 0.08101426810026169\n",
      "Iteration 39, Batch: 1, Loss: 0.08536484837532043\n",
      "Iteration 39, Batch: 2, Loss: 0.0852426141500473\n",
      "Iteration 39, Batch: 3, Loss: 0.09658227860927582\n",
      "Iteration 39, Batch: 4, Loss: 0.08360175788402557\n",
      "Iteration 39, Batch: 5, Loss: 0.13796837627887726\n",
      "Iteration 39, Batch: 6, Loss: 0.12686030566692352\n",
      "Iteration 39, Batch: 7, Loss: 0.0997539684176445\n",
      "Iteration 39, Batch: 8, Loss: 0.0861307680606842\n",
      "Iteration 39, Batch: 9, Loss: 0.10004046559333801\n",
      "Iteration 39, Batch: 10, Loss: 0.09645625948905945\n",
      "Iteration 39, Batch: 11, Loss: 0.0989328920841217\n",
      "Iteration 39, Batch: 12, Loss: 0.09662221372127533\n",
      "Iteration 39, Batch: 13, Loss: 0.09303058683872223\n",
      "Iteration 39, Batch: 14, Loss: 0.08694492280483246\n",
      "Iteration 39, Batch: 15, Loss: 0.10606208443641663\n",
      "Iteration 39, Batch: 16, Loss: 0.1048116683959961\n",
      "Iteration 39, Batch: 17, Loss: 0.08793673664331436\n",
      "Iteration 39, Batch: 18, Loss: 0.08153518289327621\n",
      "Iteration 39, Batch: 19, Loss: 0.1022239550948143\n",
      "Iteration 39, Batch: 20, Loss: 0.09791305661201477\n",
      "Iteration 39, Batch: 21, Loss: 0.10387512296438217\n",
      "Iteration 39, Batch: 22, Loss: 0.11923147737979889\n",
      "Iteration 39, Batch: 23, Loss: 0.08668363094329834\n",
      "Iteration 39, Batch: 24, Loss: 0.06437987089157104\n",
      "Iteration 39, Batch: 25, Loss: 0.08637313544750214\n",
      "Iteration 39, Batch: 26, Loss: 0.0950978472828865\n",
      "Iteration 39, Batch: 27, Loss: 0.1004493236541748\n",
      "Iteration 39, Batch: 28, Loss: 0.12737692892551422\n",
      "Iteration 39, Batch: 29, Loss: 0.10896220803260803\n",
      "Iteration 39, Batch: 30, Loss: 0.10375729948282242\n",
      "Iteration 39, Batch: 31, Loss: 0.11645635217428207\n",
      "Iteration 39, Batch: 32, Loss: 0.1281827986240387\n",
      "Iteration 39, Batch: 33, Loss: 0.0682019293308258\n",
      "Iteration 39, Batch: 34, Loss: 0.09199929982423782\n",
      "Iteration 39, Batch: 35, Loss: 0.0796581357717514\n",
      "Iteration 39, Batch: 36, Loss: 0.12454219907522202\n",
      "Iteration 39, Batch: 37, Loss: 0.09310927242040634\n",
      "Iteration 39, Batch: 38, Loss: 0.06033125892281532\n",
      "Iteration 39, Batch: 39, Loss: 0.11236139386892319\n",
      "Iteration 39, Batch: 40, Loss: 0.0872437059879303\n",
      "Iteration 39, Batch: 41, Loss: 0.0653529092669487\n",
      "Iteration 39, Batch: 42, Loss: 0.08325643092393875\n",
      "Iteration 39, Batch: 43, Loss: 0.08679302781820297\n",
      "Iteration 39, Batch: 44, Loss: 0.08007945120334625\n",
      "Iteration 39, Batch: 45, Loss: 0.10453163087368011\n",
      "Iteration 39, Batch: 46, Loss: 0.06554192304611206\n",
      "Iteration 39, Batch: 47, Loss: 0.09219112247228622\n",
      "Iteration 39, Batch: 48, Loss: 0.09133759140968323\n",
      "Iteration 39, Batch: 49, Loss: 0.11920716613531113\n",
      "Iteration 40, Batch: 0, Loss: 0.07474131137132645\n",
      "Iteration 40, Batch: 1, Loss: 0.09759679436683655\n",
      "Iteration 40, Batch: 2, Loss: 0.08996398001909256\n",
      "Iteration 40, Batch: 3, Loss: 0.07301031798124313\n",
      "Iteration 40, Batch: 4, Loss: 0.0871569812297821\n",
      "Iteration 40, Batch: 5, Loss: 0.09134845435619354\n",
      "Iteration 40, Batch: 6, Loss: 0.1082637757062912\n",
      "Iteration 40, Batch: 7, Loss: 0.10390608757734299\n",
      "Iteration 40, Batch: 8, Loss: 0.11919999867677689\n",
      "Iteration 40, Batch: 9, Loss: 0.15030409395694733\n",
      "Iteration 40, Batch: 10, Loss: 0.09975216537714005\n",
      "Iteration 40, Batch: 11, Loss: 0.12131103873252869\n",
      "Iteration 40, Batch: 12, Loss: 0.11819599568843842\n",
      "Iteration 40, Batch: 13, Loss: 0.1306571513414383\n",
      "Iteration 40, Batch: 14, Loss: 0.13851594924926758\n",
      "Iteration 40, Batch: 15, Loss: 0.1379394382238388\n",
      "Iteration 40, Batch: 16, Loss: 0.11007367819547653\n",
      "Iteration 40, Batch: 17, Loss: 0.13943961262702942\n",
      "Iteration 40, Batch: 18, Loss: 0.14150328934192657\n",
      "Iteration 40, Batch: 19, Loss: 0.11630231887102127\n",
      "Iteration 40, Batch: 20, Loss: 0.09203942865133286\n",
      "Iteration 40, Batch: 21, Loss: 0.12844230234622955\n",
      "Iteration 40, Batch: 22, Loss: 0.07568459212779999\n",
      "Iteration 40, Batch: 23, Loss: 0.10265962034463882\n",
      "Iteration 40, Batch: 24, Loss: 0.09013068675994873\n",
      "Iteration 40, Batch: 25, Loss: 0.10539281368255615\n",
      "Iteration 40, Batch: 26, Loss: 0.11615655571222305\n",
      "Iteration 40, Batch: 27, Loss: 0.05631212145090103\n",
      "Iteration 40, Batch: 28, Loss: 0.11029227823019028\n",
      "Iteration 40, Batch: 29, Loss: 0.1313597410917282\n",
      "Iteration 40, Batch: 30, Loss: 0.09355645626783371\n",
      "Iteration 40, Batch: 31, Loss: 0.08834298700094223\n",
      "Iteration 40, Batch: 32, Loss: 0.08515994250774384\n",
      "Iteration 40, Batch: 33, Loss: 0.092875175178051\n",
      "Iteration 40, Batch: 34, Loss: 0.11322816461324692\n",
      "Iteration 40, Batch: 35, Loss: 0.1188657358288765\n",
      "Iteration 40, Batch: 36, Loss: 0.09512847661972046\n",
      "Iteration 40, Batch: 37, Loss: 0.10559171438217163\n",
      "Iteration 40, Batch: 38, Loss: 0.10590360313653946\n",
      "Iteration 40, Batch: 39, Loss: 0.07732091099023819\n",
      "Iteration 40, Batch: 40, Loss: 0.08897515386343002\n",
      "Iteration 40, Batch: 41, Loss: 0.12673068046569824\n",
      "Iteration 40, Batch: 42, Loss: 0.11020180583000183\n",
      "Iteration 40, Batch: 43, Loss: 0.10441375523805618\n",
      "Iteration 40, Batch: 44, Loss: 0.09347252547740936\n",
      "Iteration 40, Batch: 45, Loss: 0.12378081679344177\n",
      "Iteration 40, Batch: 46, Loss: 0.09075465053319931\n",
      "Iteration 40, Batch: 47, Loss: 0.10338178277015686\n",
      "Iteration 40, Batch: 48, Loss: 0.07323416322469711\n",
      "Iteration 40, Batch: 49, Loss: 0.06526625156402588\n",
      "Iteration 41, Batch: 0, Loss: 0.06658420711755753\n",
      "Iteration 41, Batch: 1, Loss: 0.07948736101388931\n",
      "Iteration 41, Batch: 2, Loss: 0.08237842470407486\n",
      "Iteration 41, Batch: 3, Loss: 0.08846855163574219\n",
      "Iteration 41, Batch: 4, Loss: 0.04149387404322624\n",
      "Iteration 41, Batch: 5, Loss: 0.07444482296705246\n",
      "Iteration 41, Batch: 6, Loss: 0.07593362033367157\n",
      "Iteration 41, Batch: 7, Loss: 0.09697218984365463\n",
      "Iteration 41, Batch: 8, Loss: 0.0903511717915535\n",
      "Iteration 41, Batch: 9, Loss: 0.11112061142921448\n",
      "Iteration 41, Batch: 10, Loss: 0.09540826827287674\n",
      "Iteration 41, Batch: 11, Loss: 0.09686800092458725\n",
      "Iteration 41, Batch: 12, Loss: 0.08743242174386978\n",
      "Iteration 41, Batch: 13, Loss: 0.08505306392908096\n",
      "Iteration 41, Batch: 14, Loss: 0.07087882608175278\n",
      "Iteration 41, Batch: 15, Loss: 0.08586277067661285\n",
      "Iteration 41, Batch: 16, Loss: 0.06512808054685593\n",
      "Iteration 41, Batch: 17, Loss: 0.06936708092689514\n",
      "Iteration 41, Batch: 18, Loss: 0.0651780366897583\n",
      "Iteration 41, Batch: 19, Loss: 0.10172692686319351\n",
      "Iteration 41, Batch: 20, Loss: 0.08057646453380585\n",
      "Iteration 41, Batch: 21, Loss: 0.0964089184999466\n",
      "Iteration 41, Batch: 22, Loss: 0.10237037390470505\n",
      "Iteration 41, Batch: 23, Loss: 0.08268146216869354\n",
      "Iteration 41, Batch: 24, Loss: 0.10094945877790451\n",
      "Iteration 41, Batch: 25, Loss: 0.09760133177042007\n",
      "Iteration 41, Batch: 26, Loss: 0.08117670565843582\n",
      "Iteration 41, Batch: 27, Loss: 0.04711839556694031\n",
      "Iteration 41, Batch: 28, Loss: 0.09814029186964035\n",
      "Iteration 41, Batch: 29, Loss: 0.1133950874209404\n",
      "Iteration 41, Batch: 30, Loss: 0.10532943159341812\n",
      "Iteration 41, Batch: 31, Loss: 0.09400495141744614\n",
      "Iteration 41, Batch: 32, Loss: 0.07683958113193512\n",
      "Iteration 41, Batch: 33, Loss: 0.08410815894603729\n",
      "Iteration 41, Batch: 34, Loss: 0.15199433267116547\n",
      "Iteration 41, Batch: 35, Loss: 0.10254234075546265\n",
      "Iteration 41, Batch: 36, Loss: 0.12562775611877441\n",
      "Iteration 41, Batch: 37, Loss: 0.08861083537340164\n",
      "Iteration 41, Batch: 38, Loss: 0.10976794362068176\n",
      "Iteration 41, Batch: 39, Loss: 0.07423678040504456\n",
      "Iteration 41, Batch: 40, Loss: 0.10630297660827637\n",
      "Iteration 41, Batch: 41, Loss: 0.10824363678693771\n",
      "Iteration 41, Batch: 42, Loss: 0.08483939617872238\n",
      "Iteration 41, Batch: 43, Loss: 0.11513011902570724\n",
      "Iteration 41, Batch: 44, Loss: 0.1805969625711441\n",
      "Iteration 41, Batch: 45, Loss: 0.14542575180530548\n",
      "Iteration 41, Batch: 46, Loss: 0.17586082220077515\n",
      "Iteration 41, Batch: 47, Loss: 0.08485008031129837\n",
      "Iteration 41, Batch: 48, Loss: 0.1054001972079277\n",
      "Iteration 41, Batch: 49, Loss: 0.0924718901515007\n",
      "Iteration 42, Batch: 0, Loss: 0.08830292522907257\n",
      "Iteration 42, Batch: 1, Loss: 0.0993502214550972\n",
      "Iteration 42, Batch: 2, Loss: 0.08354853093624115\n",
      "Iteration 42, Batch: 3, Loss: 0.11440164595842361\n",
      "Iteration 42, Batch: 4, Loss: 0.1046813353896141\n",
      "Iteration 42, Batch: 5, Loss: 0.09397993981838226\n",
      "Iteration 42, Batch: 6, Loss: 0.10381003469228745\n",
      "Iteration 42, Batch: 7, Loss: 0.06172839179635048\n",
      "Iteration 42, Batch: 8, Loss: 0.09357235580682755\n",
      "Iteration 42, Batch: 9, Loss: 0.0780138298869133\n",
      "Iteration 42, Batch: 10, Loss: 0.09516934305429459\n",
      "Iteration 42, Batch: 11, Loss: 0.10679225623607635\n",
      "Iteration 42, Batch: 12, Loss: 0.1285676509141922\n",
      "Iteration 42, Batch: 13, Loss: 0.08028020709753036\n",
      "Iteration 42, Batch: 14, Loss: 0.0868845134973526\n",
      "Iteration 42, Batch: 15, Loss: 0.10404690355062485\n",
      "Iteration 42, Batch: 16, Loss: 0.06607726961374283\n",
      "Iteration 42, Batch: 17, Loss: 0.06732521206140518\n",
      "Iteration 42, Batch: 18, Loss: 0.09940997511148453\n",
      "Iteration 42, Batch: 19, Loss: 0.10052713751792908\n",
      "Iteration 42, Batch: 20, Loss: 0.08193173259496689\n",
      "Iteration 42, Batch: 21, Loss: 0.1080152690410614\n",
      "Iteration 42, Batch: 22, Loss: 0.09054490923881531\n",
      "Iteration 42, Batch: 23, Loss: 0.07842005789279938\n",
      "Iteration 42, Batch: 24, Loss: 0.09210250526666641\n",
      "Iteration 42, Batch: 25, Loss: 0.09905277192592621\n",
      "Iteration 42, Batch: 26, Loss: 0.13522668182849884\n",
      "Iteration 42, Batch: 27, Loss: 0.10709746927022934\n",
      "Iteration 42, Batch: 28, Loss: 0.08546726405620575\n",
      "Iteration 42, Batch: 29, Loss: 0.126959890127182\n",
      "Iteration 42, Batch: 30, Loss: 0.10328511893749237\n",
      "Iteration 42, Batch: 31, Loss: 0.10817493498325348\n",
      "Iteration 42, Batch: 32, Loss: 0.12196031212806702\n",
      "Iteration 42, Batch: 33, Loss: 0.08586210012435913\n",
      "Iteration 42, Batch: 34, Loss: 0.05959916487336159\n",
      "Iteration 42, Batch: 35, Loss: 0.1132085993885994\n",
      "Iteration 42, Batch: 36, Loss: 0.09659608453512192\n",
      "Iteration 42, Batch: 37, Loss: 0.062035392969846725\n",
      "Iteration 42, Batch: 38, Loss: 0.10168012231588364\n",
      "Iteration 42, Batch: 39, Loss: 0.06233508139848709\n",
      "Iteration 42, Batch: 40, Loss: 0.08518338948488235\n",
      "Iteration 42, Batch: 41, Loss: 0.12079107016324997\n",
      "Iteration 42, Batch: 42, Loss: 0.09121429175138474\n",
      "Iteration 42, Batch: 43, Loss: 0.0993252545595169\n",
      "Iteration 42, Batch: 44, Loss: 0.08687055110931396\n",
      "Iteration 42, Batch: 45, Loss: 0.08060847967863083\n",
      "Iteration 42, Batch: 46, Loss: 0.15658041834831238\n",
      "Iteration 42, Batch: 47, Loss: 0.09656760096549988\n",
      "Iteration 42, Batch: 48, Loss: 0.07736710458993912\n",
      "Iteration 42, Batch: 49, Loss: 0.10921367257833481\n",
      "Iteration 43, Batch: 0, Loss: 0.07922010123729706\n",
      "Iteration 43, Batch: 1, Loss: 0.0798865407705307\n",
      "Iteration 43, Batch: 2, Loss: 0.09794841706752777\n",
      "Iteration 43, Batch: 3, Loss: 0.11025582998991013\n",
      "Iteration 43, Batch: 4, Loss: 0.13178594410419464\n",
      "Iteration 43, Batch: 5, Loss: 0.175252303481102\n",
      "Iteration 43, Batch: 6, Loss: 0.17413738369941711\n",
      "Iteration 43, Batch: 7, Loss: 0.15985126793384552\n",
      "Iteration 43, Batch: 8, Loss: 0.1310141384601593\n",
      "Iteration 43, Batch: 9, Loss: 0.10264087468385696\n",
      "Iteration 43, Batch: 10, Loss: 0.11809774488210678\n",
      "Iteration 43, Batch: 11, Loss: 0.07595863938331604\n",
      "Iteration 43, Batch: 12, Loss: 0.10234642773866653\n",
      "Iteration 43, Batch: 13, Loss: 0.07223956286907196\n",
      "Iteration 43, Batch: 14, Loss: 0.12607066333293915\n",
      "Iteration 43, Batch: 15, Loss: 0.15382054448127747\n",
      "Iteration 43, Batch: 16, Loss: 0.13477112352848053\n",
      "Iteration 43, Batch: 17, Loss: 0.16715984046459198\n",
      "Iteration 43, Batch: 18, Loss: 0.16280700266361237\n",
      "Iteration 43, Batch: 19, Loss: 0.16164465248584747\n",
      "Iteration 43, Batch: 20, Loss: 0.08524370193481445\n",
      "Iteration 43, Batch: 21, Loss: 0.10498353838920593\n",
      "Iteration 43, Batch: 22, Loss: 0.09713691473007202\n",
      "Iteration 43, Batch: 23, Loss: 0.13312654197216034\n",
      "Iteration 43, Batch: 24, Loss: 0.13336236774921417\n",
      "Iteration 43, Batch: 25, Loss: 0.11433093994855881\n",
      "Iteration 43, Batch: 26, Loss: 0.07281739264726639\n",
      "Iteration 43, Batch: 27, Loss: 0.09049399942159653\n",
      "Iteration 43, Batch: 28, Loss: 0.09382947534322739\n",
      "Iteration 43, Batch: 29, Loss: 0.095457524061203\n",
      "Iteration 43, Batch: 30, Loss: 0.10877784341573715\n",
      "Iteration 43, Batch: 31, Loss: 0.11464010179042816\n",
      "Iteration 43, Batch: 32, Loss: 0.09800198674201965\n",
      "Iteration 43, Batch: 33, Loss: 0.11627613753080368\n",
      "Iteration 43, Batch: 34, Loss: 0.07468017935752869\n",
      "Iteration 43, Batch: 35, Loss: 0.09994333237409592\n",
      "Iteration 43, Batch: 36, Loss: 0.07695768028497696\n",
      "Iteration 43, Batch: 37, Loss: 0.08959519118070602\n",
      "Iteration 43, Batch: 38, Loss: 0.06271229684352875\n",
      "Iteration 43, Batch: 39, Loss: 0.09770011901855469\n",
      "Iteration 43, Batch: 40, Loss: 0.10617876797914505\n",
      "Iteration 43, Batch: 41, Loss: 0.11594963073730469\n",
      "Iteration 43, Batch: 42, Loss: 0.11468987911939621\n",
      "Iteration 43, Batch: 43, Loss: 0.0909501239657402\n",
      "Iteration 43, Batch: 44, Loss: 0.09563854336738586\n",
      "Iteration 43, Batch: 45, Loss: 0.099122054874897\n",
      "Iteration 43, Batch: 46, Loss: 0.08708230406045914\n",
      "Iteration 43, Batch: 47, Loss: 0.08026120811700821\n",
      "Iteration 43, Batch: 48, Loss: 0.10745399445295334\n",
      "Iteration 43, Batch: 49, Loss: 0.06445348262786865\n",
      "Iteration 44, Batch: 0, Loss: 0.07772635668516159\n",
      "Iteration 44, Batch: 1, Loss: 0.1031913161277771\n",
      "Iteration 44, Batch: 2, Loss: 0.07255634665489197\n",
      "Iteration 44, Batch: 3, Loss: 0.10771217197179794\n",
      "Iteration 44, Batch: 4, Loss: 0.11083482205867767\n",
      "Iteration 44, Batch: 5, Loss: 0.056069906800985336\n",
      "Iteration 44, Batch: 6, Loss: 0.08689044415950775\n",
      "Iteration 44, Batch: 7, Loss: 0.07318439334630966\n",
      "Iteration 44, Batch: 8, Loss: 0.06878704577684402\n",
      "Iteration 44, Batch: 9, Loss: 0.06544065475463867\n",
      "Iteration 44, Batch: 10, Loss: 0.09590913355350494\n",
      "Iteration 44, Batch: 11, Loss: 0.08830010890960693\n",
      "Iteration 44, Batch: 12, Loss: 0.06658974289894104\n",
      "Iteration 44, Batch: 13, Loss: 0.08706776052713394\n",
      "Iteration 44, Batch: 14, Loss: 0.0814075842499733\n",
      "Iteration 44, Batch: 15, Loss: 0.0833715870976448\n",
      "Iteration 44, Batch: 16, Loss: 0.08780696988105774\n",
      "Iteration 44, Batch: 17, Loss: 0.07975877821445465\n",
      "Iteration 44, Batch: 18, Loss: 0.08349743485450745\n",
      "Iteration 44, Batch: 19, Loss: 0.09024251997470856\n",
      "Iteration 44, Batch: 20, Loss: 0.08286039531230927\n",
      "Iteration 44, Batch: 21, Loss: 0.09134658426046371\n",
      "Iteration 44, Batch: 22, Loss: 0.11597733199596405\n",
      "Iteration 44, Batch: 23, Loss: 0.10426601767539978\n",
      "Iteration 44, Batch: 24, Loss: 0.11371079832315445\n",
      "Iteration 44, Batch: 25, Loss: 0.07433360069990158\n",
      "Iteration 44, Batch: 26, Loss: 0.10333797335624695\n",
      "Iteration 44, Batch: 27, Loss: 0.08206251263618469\n",
      "Iteration 44, Batch: 28, Loss: 0.08828584104776382\n",
      "Iteration 44, Batch: 29, Loss: 0.09188026189804077\n",
      "Iteration 44, Batch: 30, Loss: 0.0947512611746788\n",
      "Iteration 44, Batch: 31, Loss: 0.06318903714418411\n",
      "Iteration 44, Batch: 32, Loss: 0.08136095106601715\n",
      "Iteration 44, Batch: 33, Loss: 0.09169691056013107\n",
      "Iteration 44, Batch: 34, Loss: 0.08980485796928406\n",
      "Iteration 44, Batch: 35, Loss: 0.08438058942556381\n",
      "Iteration 44, Batch: 36, Loss: 0.09498303383588791\n",
      "Iteration 44, Batch: 37, Loss: 0.06236109510064125\n",
      "Iteration 44, Batch: 38, Loss: 0.09497136622667313\n",
      "Iteration 44, Batch: 39, Loss: 0.076200470328331\n",
      "Iteration 44, Batch: 40, Loss: 0.1008114442229271\n",
      "Iteration 44, Batch: 41, Loss: 0.09527841955423355\n",
      "Iteration 44, Batch: 42, Loss: 0.09492708742618561\n",
      "Iteration 44, Batch: 43, Loss: 0.09151912480592728\n",
      "Iteration 44, Batch: 44, Loss: 0.1155969575047493\n",
      "Iteration 44, Batch: 45, Loss: 0.09488778561353683\n",
      "Iteration 44, Batch: 46, Loss: 0.10177049040794373\n",
      "Iteration 44, Batch: 47, Loss: 0.0767049714922905\n",
      "Iteration 44, Batch: 48, Loss: 0.060479361563920975\n",
      "Iteration 44, Batch: 49, Loss: 0.06544259190559387\n",
      "Iteration 45, Batch: 0, Loss: 0.06288505345582962\n",
      "Iteration 45, Batch: 1, Loss: 0.06737722456455231\n",
      "Iteration 45, Batch: 2, Loss: 0.09261231869459152\n",
      "Iteration 45, Batch: 3, Loss: 0.12285546213388443\n",
      "Iteration 45, Batch: 4, Loss: 0.11236556619405746\n",
      "Iteration 45, Batch: 5, Loss: 0.08644919842481613\n",
      "Iteration 45, Batch: 6, Loss: 0.09308067709207535\n",
      "Iteration 45, Batch: 7, Loss: 0.10709334909915924\n",
      "Iteration 45, Batch: 8, Loss: 0.10518993437290192\n",
      "Iteration 45, Batch: 9, Loss: 0.11453843861818314\n",
      "Iteration 45, Batch: 10, Loss: 0.05981958284974098\n",
      "Iteration 45, Batch: 11, Loss: 0.09995920211076736\n",
      "Iteration 45, Batch: 12, Loss: 0.058549802750349045\n",
      "Iteration 45, Batch: 13, Loss: 0.06319810450077057\n",
      "Iteration 45, Batch: 14, Loss: 0.12148094177246094\n",
      "Iteration 45, Batch: 15, Loss: 0.11047562956809998\n",
      "Iteration 45, Batch: 16, Loss: 0.08904923498630524\n",
      "Iteration 45, Batch: 17, Loss: 0.08401785045862198\n",
      "Iteration 45, Batch: 18, Loss: 0.1453860104084015\n",
      "Iteration 45, Batch: 19, Loss: 0.11644882708787918\n",
      "Iteration 45, Batch: 20, Loss: 0.10639151930809021\n",
      "Iteration 45, Batch: 21, Loss: 0.09565909951925278\n",
      "Iteration 45, Batch: 22, Loss: 0.07518837600946426\n",
      "Iteration 45, Batch: 23, Loss: 0.09312252700328827\n",
      "Iteration 45, Batch: 24, Loss: 0.09501877427101135\n",
      "Iteration 45, Batch: 25, Loss: 0.07739881426095963\n",
      "Iteration 45, Batch: 26, Loss: 0.09045165032148361\n",
      "Iteration 45, Batch: 27, Loss: 0.09356510639190674\n",
      "Iteration 45, Batch: 28, Loss: 0.10903051495552063\n",
      "Iteration 45, Batch: 29, Loss: 0.09720390290021896\n",
      "Iteration 45, Batch: 30, Loss: 0.10201986134052277\n",
      "Iteration 45, Batch: 31, Loss: 0.07685289531946182\n",
      "Iteration 45, Batch: 32, Loss: 0.09698890149593353\n",
      "Iteration 45, Batch: 33, Loss: 0.08397302776575089\n",
      "Iteration 45, Batch: 34, Loss: 0.07355229556560516\n",
      "Iteration 45, Batch: 35, Loss: 0.062039658427238464\n",
      "Iteration 45, Batch: 36, Loss: 0.10118148475885391\n",
      "Iteration 45, Batch: 37, Loss: 0.07630311697721481\n",
      "Iteration 45, Batch: 38, Loss: 0.080774687230587\n",
      "Iteration 45, Batch: 39, Loss: 0.07497097551822662\n",
      "Iteration 45, Batch: 40, Loss: 0.11179224401712418\n",
      "Iteration 45, Batch: 41, Loss: 0.10724709182977676\n",
      "Iteration 45, Batch: 42, Loss: 0.08717080950737\n",
      "Iteration 45, Batch: 43, Loss: 0.08188633620738983\n",
      "Iteration 45, Batch: 44, Loss: 0.09589096158742905\n",
      "Iteration 45, Batch: 45, Loss: 0.09228639304637909\n",
      "Iteration 45, Batch: 46, Loss: 0.06964752823114395\n",
      "Iteration 45, Batch: 47, Loss: 0.07994180917739868\n",
      "Iteration 45, Batch: 48, Loss: 0.08330285549163818\n",
      "Iteration 45, Batch: 49, Loss: 0.07499410212039948\n",
      "Iteration 46, Batch: 0, Loss: 0.08935507386922836\n",
      "Iteration 46, Batch: 1, Loss: 0.08213170617818832\n",
      "Iteration 46, Batch: 2, Loss: 0.09038150310516357\n",
      "Iteration 46, Batch: 3, Loss: 0.0758693665266037\n",
      "Iteration 46, Batch: 4, Loss: 0.09102024137973785\n",
      "Iteration 46, Batch: 5, Loss: 0.0810934528708458\n",
      "Iteration 46, Batch: 6, Loss: 0.093215711414814\n",
      "Iteration 46, Batch: 7, Loss: 0.11028918623924255\n",
      "Iteration 46, Batch: 8, Loss: 0.07601277530193329\n",
      "Iteration 46, Batch: 9, Loss: 0.07303212583065033\n",
      "Iteration 46, Batch: 10, Loss: 0.05504419282078743\n",
      "Iteration 46, Batch: 11, Loss: 0.08143800497055054\n",
      "Iteration 46, Batch: 12, Loss: 0.05996260419487953\n",
      "Iteration 46, Batch: 13, Loss: 0.09498783200979233\n",
      "Iteration 46, Batch: 14, Loss: 0.07504305243492126\n",
      "Iteration 46, Batch: 15, Loss: 0.06536770612001419\n",
      "Iteration 46, Batch: 16, Loss: 0.09023458510637283\n",
      "Iteration 46, Batch: 17, Loss: 0.08962982892990112\n",
      "Iteration 46, Batch: 18, Loss: 0.07982965558767319\n",
      "Iteration 46, Batch: 19, Loss: 0.06769825518131256\n",
      "Iteration 46, Batch: 20, Loss: 0.061956990510225296\n",
      "Iteration 46, Batch: 21, Loss: 0.07392176240682602\n",
      "Iteration 46, Batch: 22, Loss: 0.09622655063867569\n",
      "Iteration 46, Batch: 23, Loss: 0.058224473148584366\n",
      "Iteration 46, Batch: 24, Loss: 0.07490295171737671\n",
      "Iteration 46, Batch: 25, Loss: 0.07145514339208603\n",
      "Iteration 46, Batch: 26, Loss: 0.08148139715194702\n",
      "Iteration 46, Batch: 27, Loss: 0.05692514777183533\n",
      "Iteration 46, Batch: 28, Loss: 0.05026080086827278\n",
      "Iteration 46, Batch: 29, Loss: 0.0798659399151802\n",
      "Iteration 46, Batch: 30, Loss: 0.0726698562502861\n",
      "Iteration 46, Batch: 31, Loss: 0.055180661380290985\n",
      "Iteration 46, Batch: 32, Loss: 0.040771789848804474\n",
      "Iteration 46, Batch: 33, Loss: 0.0625775009393692\n",
      "Iteration 46, Batch: 34, Loss: 0.06212569400668144\n",
      "Iteration 46, Batch: 35, Loss: 0.058612626045942307\n",
      "Iteration 46, Batch: 36, Loss: 0.06512486934661865\n",
      "Iteration 46, Batch: 37, Loss: 0.09273966401815414\n",
      "Iteration 46, Batch: 38, Loss: 0.07042297720909119\n",
      "Iteration 46, Batch: 39, Loss: 0.07226541638374329\n",
      "Iteration 46, Batch: 40, Loss: 0.08597762882709503\n",
      "Iteration 46, Batch: 41, Loss: 0.07835087180137634\n",
      "Iteration 46, Batch: 42, Loss: 0.06594976782798767\n",
      "Iteration 46, Batch: 43, Loss: 0.10056377202272415\n",
      "Iteration 46, Batch: 44, Loss: 0.06395260989665985\n",
      "Iteration 46, Batch: 45, Loss: 0.09779185801744461\n",
      "Iteration 46, Batch: 46, Loss: 0.06887868791818619\n",
      "Iteration 46, Batch: 47, Loss: 0.043256357312202454\n",
      "Iteration 46, Batch: 48, Loss: 0.0778699740767479\n",
      "Iteration 46, Batch: 49, Loss: 0.08067138493061066\n",
      "Iteration 47, Batch: 0, Loss: 0.12776216864585876\n",
      "Iteration 47, Batch: 1, Loss: 0.09522967785596848\n",
      "Iteration 47, Batch: 2, Loss: 0.07619522511959076\n",
      "Iteration 47, Batch: 3, Loss: 0.047209128737449646\n",
      "Iteration 47, Batch: 4, Loss: 0.08704584091901779\n",
      "Iteration 47, Batch: 5, Loss: 0.10516620427370071\n",
      "Iteration 47, Batch: 6, Loss: 0.07387709617614746\n",
      "Iteration 47, Batch: 7, Loss: 0.07898019254207611\n",
      "Iteration 47, Batch: 8, Loss: 0.11101888120174408\n",
      "Iteration 47, Batch: 9, Loss: 0.09836629778146744\n",
      "Iteration 47, Batch: 10, Loss: 0.09908340126276016\n",
      "Iteration 47, Batch: 11, Loss: 0.07874169945716858\n",
      "Iteration 47, Batch: 12, Loss: 0.083846315741539\n",
      "Iteration 47, Batch: 13, Loss: 0.07462414354085922\n",
      "Iteration 47, Batch: 14, Loss: 0.063519187271595\n",
      "Iteration 47, Batch: 15, Loss: 0.09129432588815689\n",
      "Iteration 47, Batch: 16, Loss: 0.08540763705968857\n",
      "Iteration 47, Batch: 17, Loss: 0.07700634002685547\n",
      "Iteration 47, Batch: 18, Loss: 0.0960359200835228\n",
      "Iteration 47, Batch: 19, Loss: 0.10303916782140732\n",
      "Iteration 47, Batch: 20, Loss: 0.1042388305068016\n",
      "Iteration 47, Batch: 21, Loss: 0.08804874122142792\n",
      "Iteration 47, Batch: 22, Loss: 0.09450200200080872\n",
      "Iteration 47, Batch: 23, Loss: 0.10178252309560776\n",
      "Iteration 47, Batch: 24, Loss: 0.08155786246061325\n",
      "Iteration 47, Batch: 25, Loss: 0.08479718118906021\n",
      "Iteration 47, Batch: 26, Loss: 0.08683742582798004\n",
      "Iteration 47, Batch: 27, Loss: 0.10143905133008957\n",
      "Iteration 47, Batch: 28, Loss: 0.093572698533535\n",
      "Iteration 47, Batch: 29, Loss: 0.06592275202274323\n",
      "Iteration 47, Batch: 30, Loss: 0.06400144845247269\n",
      "Iteration 47, Batch: 31, Loss: 0.1201382577419281\n",
      "Iteration 47, Batch: 32, Loss: 0.07492198795080185\n",
      "Iteration 47, Batch: 33, Loss: 0.08229562640190125\n",
      "Iteration 47, Batch: 34, Loss: 0.07243552058935165\n",
      "Iteration 47, Batch: 35, Loss: 0.08850939571857452\n",
      "Iteration 47, Batch: 36, Loss: 0.10021214932203293\n",
      "Iteration 47, Batch: 37, Loss: 0.09351751953363419\n",
      "Iteration 47, Batch: 38, Loss: 0.07263319194316864\n",
      "Iteration 47, Batch: 39, Loss: 0.09327798336744308\n",
      "Iteration 47, Batch: 40, Loss: 0.11609670519828796\n",
      "Iteration 47, Batch: 41, Loss: 0.08779283612966537\n",
      "Iteration 47, Batch: 42, Loss: 0.10336645692586899\n",
      "Iteration 47, Batch: 43, Loss: 0.05525398626923561\n",
      "Iteration 47, Batch: 44, Loss: 0.09113644063472748\n",
      "Iteration 47, Batch: 45, Loss: 0.055098023265600204\n",
      "Iteration 47, Batch: 46, Loss: 0.09273090213537216\n",
      "Iteration 47, Batch: 47, Loss: 0.06381434202194214\n",
      "Iteration 47, Batch: 48, Loss: 0.06575159728527069\n",
      "Iteration 47, Batch: 49, Loss: 0.11268065124750137\n",
      "Iteration 48, Batch: 0, Loss: 0.10205455869436264\n",
      "Iteration 48, Batch: 1, Loss: 0.09085623174905777\n",
      "Iteration 48, Batch: 2, Loss: 0.08746121823787689\n",
      "Iteration 48, Batch: 3, Loss: 0.10723159462213516\n",
      "Iteration 48, Batch: 4, Loss: 0.08724550157785416\n",
      "Iteration 48, Batch: 5, Loss: 0.07024882733821869\n",
      "Iteration 48, Batch: 6, Loss: 0.06545975804328918\n",
      "Iteration 48, Batch: 7, Loss: 0.08419288694858551\n",
      "Iteration 48, Batch: 8, Loss: 0.08165771514177322\n",
      "Iteration 48, Batch: 9, Loss: 0.09117469936609268\n",
      "Iteration 48, Batch: 10, Loss: 0.0914081484079361\n",
      "Iteration 48, Batch: 11, Loss: 0.11500956863164902\n",
      "Iteration 48, Batch: 12, Loss: 0.07039594650268555\n",
      "Iteration 48, Batch: 13, Loss: 0.07859952002763748\n",
      "Iteration 48, Batch: 14, Loss: 0.054511137306690216\n",
      "Iteration 48, Batch: 15, Loss: 0.08512988686561584\n",
      "Iteration 48, Batch: 16, Loss: 0.07725860178470612\n",
      "Iteration 48, Batch: 17, Loss: 0.054554861038923264\n",
      "Iteration 48, Batch: 18, Loss: 0.058637846261262894\n",
      "Iteration 48, Batch: 19, Loss: 0.07170489430427551\n",
      "Iteration 48, Batch: 20, Loss: 0.07729065418243408\n",
      "Iteration 48, Batch: 21, Loss: 0.0630628690123558\n",
      "Iteration 48, Batch: 22, Loss: 0.0772961750626564\n",
      "Iteration 48, Batch: 23, Loss: 0.0698651671409607\n",
      "Iteration 48, Batch: 24, Loss: 0.054708946496248245\n",
      "Iteration 48, Batch: 25, Loss: 0.07879457622766495\n",
      "Iteration 48, Batch: 26, Loss: 0.07303310185670853\n",
      "Iteration 48, Batch: 27, Loss: 0.09848019480705261\n",
      "Iteration 48, Batch: 28, Loss: 0.09535098820924759\n",
      "Iteration 48, Batch: 29, Loss: 0.0763026773929596\n",
      "Iteration 48, Batch: 30, Loss: 0.07375627011060715\n",
      "Iteration 48, Batch: 31, Loss: 0.077367402613163\n",
      "Iteration 48, Batch: 32, Loss: 0.05533745139837265\n",
      "Iteration 48, Batch: 33, Loss: 0.029897894710302353\n",
      "Iteration 48, Batch: 34, Loss: 0.07616116106510162\n",
      "Iteration 48, Batch: 35, Loss: 0.08188482373952866\n",
      "Iteration 48, Batch: 36, Loss: 0.05097704753279686\n",
      "Iteration 48, Batch: 37, Loss: 0.07868657261133194\n",
      "Iteration 48, Batch: 38, Loss: 0.055139798671007156\n",
      "Iteration 48, Batch: 39, Loss: 0.05897663161158562\n",
      "Iteration 48, Batch: 40, Loss: 0.07464444637298584\n",
      "Iteration 48, Batch: 41, Loss: 0.06428466737270355\n",
      "Iteration 48, Batch: 42, Loss: 0.08887898176908493\n",
      "Iteration 48, Batch: 43, Loss: 0.04599982127547264\n",
      "Iteration 48, Batch: 44, Loss: 0.06693726032972336\n",
      "Iteration 48, Batch: 45, Loss: 0.056867312639951706\n",
      "Iteration 48, Batch: 46, Loss: 0.08067101985216141\n",
      "Iteration 48, Batch: 47, Loss: 0.1029234305024147\n",
      "Iteration 48, Batch: 48, Loss: 0.06848233938217163\n",
      "Iteration 48, Batch: 49, Loss: 0.07035014033317566\n",
      "Iteration 49, Batch: 0, Loss: 0.07960810512304306\n",
      "Iteration 49, Batch: 1, Loss: 0.09305757284164429\n",
      "Iteration 49, Batch: 2, Loss: 0.09206071496009827\n",
      "Iteration 49, Batch: 3, Loss: 0.07489728927612305\n",
      "Iteration 49, Batch: 4, Loss: 0.10595247894525528\n",
      "Iteration 49, Batch: 5, Loss: 0.08156920969486237\n",
      "Iteration 49, Batch: 6, Loss: 0.07050178945064545\n",
      "Iteration 49, Batch: 7, Loss: 0.07868944108486176\n",
      "Iteration 49, Batch: 8, Loss: 0.08627111464738846\n",
      "Iteration 49, Batch: 9, Loss: 0.08716907352209091\n",
      "Iteration 49, Batch: 10, Loss: 0.05587136745452881\n",
      "Iteration 49, Batch: 11, Loss: 0.06625742465257645\n",
      "Iteration 49, Batch: 12, Loss: 0.06845779716968536\n",
      "Iteration 49, Batch: 13, Loss: 0.08058006316423416\n",
      "Iteration 49, Batch: 14, Loss: 0.07240466773509979\n",
      "Iteration 49, Batch: 15, Loss: 0.08308723568916321\n",
      "Iteration 49, Batch: 16, Loss: 0.09109774976968765\n",
      "Iteration 49, Batch: 17, Loss: 0.07876219600439072\n",
      "Iteration 49, Batch: 18, Loss: 0.0575682632625103\n",
      "Iteration 49, Batch: 19, Loss: 0.05528198555111885\n",
      "Iteration 49, Batch: 20, Loss: 0.07525888085365295\n",
      "Iteration 49, Batch: 21, Loss: 0.07590800523757935\n",
      "Iteration 49, Batch: 22, Loss: 0.07282797992229462\n",
      "Iteration 49, Batch: 23, Loss: 0.08504869043827057\n",
      "Iteration 49, Batch: 24, Loss: 0.05805647373199463\n",
      "Iteration 49, Batch: 25, Loss: 0.05484457686543465\n",
      "Iteration 49, Batch: 26, Loss: 0.034122005105018616\n",
      "Iteration 49, Batch: 27, Loss: 0.06969110667705536\n",
      "Iteration 49, Batch: 28, Loss: 0.08063900470733643\n",
      "Iteration 49, Batch: 29, Loss: 0.048430077731609344\n",
      "Iteration 49, Batch: 30, Loss: 0.06953373551368713\n",
      "Iteration 49, Batch: 31, Loss: 0.0693107396364212\n",
      "Iteration 49, Batch: 32, Loss: 0.04287659376859665\n",
      "Iteration 49, Batch: 33, Loss: 0.10730049014091492\n",
      "Iteration 49, Batch: 34, Loss: 0.08566278219223022\n",
      "Iteration 49, Batch: 35, Loss: 0.06706400960683823\n",
      "Iteration 49, Batch: 36, Loss: 0.0745074450969696\n",
      "Iteration 49, Batch: 37, Loss: 0.07419948279857635\n",
      "Iteration 49, Batch: 38, Loss: 0.06607434153556824\n",
      "Iteration 49, Batch: 39, Loss: 0.10056684166193008\n",
      "Iteration 49, Batch: 40, Loss: 0.07030551135540009\n",
      "Iteration 49, Batch: 41, Loss: 0.06793488562107086\n",
      "Iteration 49, Batch: 42, Loss: 0.05661509558558464\n",
      "Iteration 49, Batch: 43, Loss: 0.07852254807949066\n",
      "Iteration 49, Batch: 44, Loss: 0.05937271937727928\n",
      "Iteration 49, Batch: 45, Loss: 0.09424595534801483\n",
      "Iteration 49, Batch: 46, Loss: 0.10064204782247543\n",
      "Iteration 49, Batch: 47, Loss: 0.07866690307855606\n",
      "Iteration 49, Batch: 48, Loss: 0.06333594769239426\n",
      "Iteration 49, Batch: 49, Loss: 0.057120781391859055\n",
      "Iteration 50, Batch: 0, Loss: 0.04880617931485176\n",
      "Iteration 50, Batch: 1, Loss: 0.04657384380698204\n",
      "Iteration 50, Batch: 2, Loss: 0.052285049110651016\n",
      "Iteration 50, Batch: 3, Loss: 0.05062197893857956\n",
      "Iteration 50, Batch: 4, Loss: 0.040130190551280975\n",
      "Iteration 50, Batch: 5, Loss: 0.051260631531476974\n",
      "Iteration 50, Batch: 6, Loss: 0.06236690282821655\n",
      "Iteration 50, Batch: 7, Loss: 0.060310039669275284\n",
      "Iteration 50, Batch: 8, Loss: 0.05426012724637985\n",
      "Iteration 50, Batch: 9, Loss: 0.07407014071941376\n",
      "Iteration 50, Batch: 10, Loss: 0.05422709509730339\n",
      "Iteration 50, Batch: 11, Loss: 0.06344445049762726\n",
      "Iteration 50, Batch: 12, Loss: 0.07334350049495697\n",
      "Iteration 50, Batch: 13, Loss: 0.06603467464447021\n",
      "Iteration 50, Batch: 14, Loss: 0.06250461935997009\n",
      "Iteration 50, Batch: 15, Loss: 0.06004064902663231\n",
      "Iteration 50, Batch: 16, Loss: 0.04251239821314812\n",
      "Iteration 50, Batch: 17, Loss: 0.04752349853515625\n",
      "Iteration 50, Batch: 18, Loss: 0.06824710965156555\n",
      "Iteration 50, Batch: 19, Loss: 0.07584378123283386\n",
      "Iteration 50, Batch: 20, Loss: 0.047886140644550323\n",
      "Iteration 50, Batch: 21, Loss: 0.052846334874629974\n",
      "Iteration 50, Batch: 22, Loss: 0.08615041524171829\n",
      "Iteration 50, Batch: 23, Loss: 0.048632800579071045\n",
      "Iteration 50, Batch: 24, Loss: 0.06601997464895248\n",
      "Iteration 50, Batch: 25, Loss: 0.05105733871459961\n",
      "Iteration 50, Batch: 26, Loss: 0.0547703355550766\n",
      "Iteration 50, Batch: 27, Loss: 0.0815260037779808\n",
      "Iteration 50, Batch: 28, Loss: 0.060725800693035126\n",
      "Iteration 50, Batch: 29, Loss: 0.06450318545103073\n",
      "Iteration 50, Batch: 30, Loss: 0.08374572545289993\n",
      "Iteration 50, Batch: 31, Loss: 0.08149563521146774\n",
      "Iteration 50, Batch: 32, Loss: 0.042856328189373016\n",
      "Iteration 50, Batch: 33, Loss: 0.06634850800037384\n",
      "Iteration 50, Batch: 34, Loss: 0.060064639896154404\n",
      "Iteration 50, Batch: 35, Loss: 0.061665210872888565\n",
      "Iteration 50, Batch: 36, Loss: 0.06018981337547302\n",
      "Iteration 50, Batch: 37, Loss: 0.0712171345949173\n",
      "Iteration 50, Batch: 38, Loss: 0.05406918004155159\n",
      "Iteration 50, Batch: 39, Loss: 0.09635570645332336\n",
      "Iteration 50, Batch: 40, Loss: 0.06035846471786499\n",
      "Iteration 50, Batch: 41, Loss: 0.0576188750565052\n",
      "Iteration 50, Batch: 42, Loss: 0.07044743001461029\n",
      "Iteration 50, Batch: 43, Loss: 0.0675787702202797\n",
      "Iteration 50, Batch: 44, Loss: 0.09924954921007156\n",
      "Iteration 50, Batch: 45, Loss: 0.08942403644323349\n",
      "Iteration 50, Batch: 46, Loss: 0.07627709209918976\n",
      "Iteration 50, Batch: 47, Loss: 0.05142907053232193\n",
      "Iteration 50, Batch: 48, Loss: 0.05588078498840332\n",
      "Iteration 50, Batch: 49, Loss: 0.06106747314333916\n",
      "Iteration 51, Batch: 0, Loss: 0.05636882036924362\n",
      "Iteration 51, Batch: 1, Loss: 0.052710626274347305\n",
      "Iteration 51, Batch: 2, Loss: 0.07017064094543457\n",
      "Iteration 51, Batch: 3, Loss: 0.07858913391828537\n",
      "Iteration 51, Batch: 4, Loss: 0.07724462449550629\n",
      "Iteration 51, Batch: 5, Loss: 0.058990783989429474\n",
      "Iteration 51, Batch: 6, Loss: 0.06710073351860046\n",
      "Iteration 51, Batch: 7, Loss: 0.09605169296264648\n",
      "Iteration 51, Batch: 8, Loss: 0.06720738112926483\n",
      "Iteration 51, Batch: 9, Loss: 0.062282416969537735\n",
      "Iteration 51, Batch: 10, Loss: 0.052553690969944\n",
      "Iteration 51, Batch: 11, Loss: 0.0921640396118164\n",
      "Iteration 51, Batch: 12, Loss: 0.060983169823884964\n",
      "Iteration 51, Batch: 13, Loss: 0.08010564744472504\n",
      "Iteration 51, Batch: 14, Loss: 0.05062701925635338\n",
      "Iteration 51, Batch: 15, Loss: 0.059737272560596466\n",
      "Iteration 51, Batch: 16, Loss: 0.037789154797792435\n",
      "Iteration 51, Batch: 17, Loss: 0.05328090488910675\n",
      "Iteration 51, Batch: 18, Loss: 0.06793048977851868\n",
      "Iteration 51, Batch: 19, Loss: 0.1061289831995964\n",
      "Iteration 51, Batch: 20, Loss: 0.08057983964681625\n",
      "Iteration 51, Batch: 21, Loss: 0.049519747495651245\n",
      "Iteration 51, Batch: 22, Loss: 0.0624455101788044\n",
      "Iteration 51, Batch: 23, Loss: 0.07488584518432617\n",
      "Iteration 51, Batch: 24, Loss: 0.0636429563164711\n",
      "Iteration 51, Batch: 25, Loss: 0.05893921107053757\n",
      "Iteration 51, Batch: 26, Loss: 0.0692455843091011\n",
      "Iteration 51, Batch: 27, Loss: 0.05922427028417587\n",
      "Iteration 51, Batch: 28, Loss: 0.06943385303020477\n",
      "Iteration 51, Batch: 29, Loss: 0.08709033578634262\n",
      "Iteration 51, Batch: 30, Loss: 0.056313760578632355\n",
      "Iteration 51, Batch: 31, Loss: 0.07863600552082062\n",
      "Iteration 51, Batch: 32, Loss: 0.055913057178258896\n",
      "Iteration 51, Batch: 33, Loss: 0.055460281670093536\n",
      "Iteration 51, Batch: 34, Loss: 0.060911767184734344\n",
      "Iteration 51, Batch: 35, Loss: 0.06698071211576462\n",
      "Iteration 51, Batch: 36, Loss: 0.03805375099182129\n",
      "Iteration 51, Batch: 37, Loss: 0.06811162084341049\n",
      "Iteration 51, Batch: 38, Loss: 0.06192499026656151\n",
      "Iteration 51, Batch: 39, Loss: 0.1009819507598877\n",
      "Iteration 51, Batch: 40, Loss: 0.09082647413015366\n",
      "Iteration 51, Batch: 41, Loss: 0.06183391064405441\n",
      "Iteration 51, Batch: 42, Loss: 0.06436310708522797\n",
      "Iteration 51, Batch: 43, Loss: 0.06508186459541321\n",
      "Iteration 51, Batch: 44, Loss: 0.07510258257389069\n",
      "Iteration 51, Batch: 45, Loss: 0.06737503409385681\n",
      "Iteration 51, Batch: 46, Loss: 0.07747585326433182\n",
      "Iteration 51, Batch: 47, Loss: 0.04934075102210045\n",
      "Iteration 51, Batch: 48, Loss: 0.059924058616161346\n",
      "Iteration 51, Batch: 49, Loss: 0.05171909183263779\n",
      "Iteration 52, Batch: 0, Loss: 0.04727794975042343\n",
      "Iteration 52, Batch: 1, Loss: 0.049805983901023865\n",
      "Iteration 52, Batch: 2, Loss: 0.08390651643276215\n",
      "Iteration 52, Batch: 3, Loss: 0.047342557460069656\n",
      "Iteration 52, Batch: 4, Loss: 0.051888350397348404\n",
      "Iteration 52, Batch: 5, Loss: 0.05370171368122101\n",
      "Iteration 52, Batch: 6, Loss: 0.01742541231215\n",
      "Iteration 52, Batch: 7, Loss: 0.04551270231604576\n",
      "Iteration 52, Batch: 8, Loss: 0.0773915946483612\n",
      "Iteration 52, Batch: 9, Loss: 0.07229668647050858\n",
      "Iteration 52, Batch: 10, Loss: 0.056998226791620255\n",
      "Iteration 52, Batch: 11, Loss: 0.06744310259819031\n",
      "Iteration 52, Batch: 12, Loss: 0.05582877993583679\n",
      "Iteration 52, Batch: 13, Loss: 0.07052691280841827\n",
      "Iteration 52, Batch: 14, Loss: 0.060378849506378174\n",
      "Iteration 52, Batch: 15, Loss: 0.07816009223461151\n",
      "Iteration 52, Batch: 16, Loss: 0.05521202087402344\n",
      "Iteration 52, Batch: 17, Loss: 0.055342718958854675\n",
      "Iteration 52, Batch: 18, Loss: 0.05899088829755783\n",
      "Iteration 52, Batch: 19, Loss: 0.055259380489587784\n",
      "Iteration 52, Batch: 20, Loss: 0.040988482534885406\n",
      "Iteration 52, Batch: 21, Loss: 0.05810224264860153\n",
      "Iteration 52, Batch: 22, Loss: 0.04342818632721901\n",
      "Iteration 52, Batch: 23, Loss: 0.04057195410132408\n",
      "Iteration 52, Batch: 24, Loss: 0.05468687042593956\n",
      "Iteration 52, Batch: 25, Loss: 0.06660792976617813\n",
      "Iteration 52, Batch: 26, Loss: 0.06978011876344681\n",
      "Iteration 52, Batch: 27, Loss: 0.040056779980659485\n",
      "Iteration 52, Batch: 28, Loss: 0.06173832342028618\n",
      "Iteration 52, Batch: 29, Loss: 0.05469678342342377\n",
      "Iteration 52, Batch: 30, Loss: 0.0380067378282547\n",
      "Iteration 52, Batch: 31, Loss: 0.07088004052639008\n",
      "Iteration 52, Batch: 32, Loss: 0.05088112875819206\n",
      "Iteration 52, Batch: 33, Loss: 0.0935516357421875\n",
      "Iteration 52, Batch: 34, Loss: 0.07611459493637085\n",
      "Iteration 52, Batch: 35, Loss: 0.06218673661351204\n",
      "Iteration 52, Batch: 36, Loss: 0.05715654417872429\n",
      "Iteration 52, Batch: 37, Loss: 0.03842176869511604\n",
      "Iteration 52, Batch: 38, Loss: 0.07248885184526443\n",
      "Iteration 52, Batch: 39, Loss: 0.044073399156332016\n",
      "Iteration 52, Batch: 40, Loss: 0.03581935912370682\n",
      "Iteration 52, Batch: 41, Loss: 0.03915341943502426\n",
      "Iteration 52, Batch: 42, Loss: 0.059901293367147446\n",
      "Iteration 52, Batch: 43, Loss: 0.05390336737036705\n",
      "Iteration 52, Batch: 44, Loss: 0.038115233182907104\n",
      "Iteration 52, Batch: 45, Loss: 0.061280470341444016\n",
      "Iteration 52, Batch: 46, Loss: 0.06022763252258301\n",
      "Iteration 52, Batch: 47, Loss: 0.060122422873973846\n",
      "Iteration 52, Batch: 48, Loss: 0.07419786602258682\n",
      "Iteration 52, Batch: 49, Loss: 0.06046062707901001\n",
      "Iteration 53, Batch: 0, Loss: 0.044669635593891144\n",
      "Iteration 53, Batch: 1, Loss: 0.03645556420087814\n",
      "Iteration 53, Batch: 2, Loss: 0.06066778674721718\n",
      "Iteration 53, Batch: 3, Loss: 0.07155740261077881\n",
      "Iteration 53, Batch: 4, Loss: 0.03341418132185936\n",
      "Iteration 53, Batch: 5, Loss: 0.052751462906599045\n",
      "Iteration 53, Batch: 6, Loss: 0.05555456131696701\n",
      "Iteration 53, Batch: 7, Loss: 0.07873644679784775\n",
      "Iteration 53, Batch: 8, Loss: 0.07181967794895172\n",
      "Iteration 53, Batch: 9, Loss: 0.06050634756684303\n",
      "Iteration 53, Batch: 10, Loss: 0.03261954337358475\n",
      "Iteration 53, Batch: 11, Loss: 0.0754835233092308\n",
      "Iteration 53, Batch: 12, Loss: 0.06421760469675064\n",
      "Iteration 53, Batch: 13, Loss: 0.06948026269674301\n",
      "Iteration 53, Batch: 14, Loss: 0.044857341796159744\n",
      "Iteration 53, Batch: 15, Loss: 0.06186515837907791\n",
      "Iteration 53, Batch: 16, Loss: 0.06518971174955368\n",
      "Iteration 53, Batch: 17, Loss: 0.05724282190203667\n",
      "Iteration 53, Batch: 18, Loss: 0.02893197536468506\n",
      "Iteration 53, Batch: 19, Loss: 0.043473269790410995\n",
      "Iteration 53, Batch: 20, Loss: 0.08740997314453125\n",
      "Iteration 53, Batch: 21, Loss: 0.06427265703678131\n",
      "Iteration 53, Batch: 22, Loss: 0.06720905750989914\n",
      "Iteration 53, Batch: 23, Loss: 0.04962164908647537\n",
      "Iteration 53, Batch: 24, Loss: 0.04740801826119423\n",
      "Iteration 53, Batch: 25, Loss: 0.057677291333675385\n",
      "Iteration 53, Batch: 26, Loss: 0.04387076571583748\n",
      "Iteration 53, Batch: 27, Loss: 0.040009547024965286\n",
      "Iteration 53, Batch: 28, Loss: 0.03830837458372116\n",
      "Iteration 53, Batch: 29, Loss: 0.07287129014730453\n",
      "Iteration 53, Batch: 30, Loss: 0.05623123049736023\n",
      "Iteration 53, Batch: 31, Loss: 0.047271717339754105\n",
      "Iteration 53, Batch: 32, Loss: 0.0988268256187439\n",
      "Iteration 53, Batch: 33, Loss: 0.06982523202896118\n",
      "Iteration 53, Batch: 34, Loss: 0.04667208716273308\n",
      "Iteration 53, Batch: 35, Loss: 0.04876791685819626\n",
      "Iteration 53, Batch: 36, Loss: 0.05225751921534538\n",
      "Iteration 53, Batch: 37, Loss: 0.05705965682864189\n",
      "Iteration 53, Batch: 38, Loss: 0.04438534751534462\n",
      "Iteration 53, Batch: 39, Loss: 0.051652126014232635\n",
      "Iteration 53, Batch: 40, Loss: 0.07741400599479675\n",
      "Iteration 53, Batch: 41, Loss: 0.05486126244068146\n",
      "Iteration 53, Batch: 42, Loss: 0.07194805145263672\n",
      "Iteration 53, Batch: 43, Loss: 0.05028275400400162\n",
      "Iteration 53, Batch: 44, Loss: 0.0578019805252552\n",
      "Iteration 53, Batch: 45, Loss: 0.08706329017877579\n",
      "Iteration 53, Batch: 46, Loss: 0.03835050389170647\n",
      "Iteration 53, Batch: 47, Loss: 0.05340467393398285\n",
      "Iteration 53, Batch: 48, Loss: 0.05508871003985405\n",
      "Iteration 53, Batch: 49, Loss: 0.052735336124897\n",
      "Iteration 54, Batch: 0, Loss: 0.06688425689935684\n",
      "Iteration 54, Batch: 1, Loss: 0.05180027708411217\n",
      "Iteration 54, Batch: 2, Loss: 0.06667720526456833\n",
      "Iteration 54, Batch: 3, Loss: 0.0604357048869133\n",
      "Iteration 54, Batch: 4, Loss: 0.06365553289651871\n",
      "Iteration 54, Batch: 5, Loss: 0.06538712978363037\n",
      "Iteration 54, Batch: 6, Loss: 0.040882136672735214\n",
      "Iteration 54, Batch: 7, Loss: 0.0613568089902401\n",
      "Iteration 54, Batch: 8, Loss: 0.05440047010779381\n",
      "Iteration 54, Batch: 9, Loss: 0.06363166868686676\n",
      "Iteration 54, Batch: 10, Loss: 0.048086028546094894\n",
      "Iteration 54, Batch: 11, Loss: 0.07044388353824615\n",
      "Iteration 54, Batch: 12, Loss: 0.03193521127104759\n",
      "Iteration 54, Batch: 13, Loss: 0.06431759893894196\n",
      "Iteration 54, Batch: 14, Loss: 0.0460302010178566\n",
      "Iteration 54, Batch: 15, Loss: 0.04985285550355911\n",
      "Iteration 54, Batch: 16, Loss: 0.06994131207466125\n",
      "Iteration 54, Batch: 17, Loss: 0.044128529727458954\n",
      "Iteration 54, Batch: 18, Loss: 0.065849170088768\n",
      "Iteration 54, Batch: 19, Loss: 0.04903306066989899\n",
      "Iteration 54, Batch: 20, Loss: 0.07609574496746063\n",
      "Iteration 54, Batch: 21, Loss: 0.07016318291425705\n",
      "Iteration 54, Batch: 22, Loss: 0.06445378065109253\n",
      "Iteration 54, Batch: 23, Loss: 0.07882991433143616\n",
      "Iteration 54, Batch: 24, Loss: 0.08305898308753967\n",
      "Iteration 54, Batch: 25, Loss: 0.04389699175953865\n",
      "Iteration 54, Batch: 26, Loss: 0.04857560992240906\n",
      "Iteration 54, Batch: 27, Loss: 0.04935450851917267\n",
      "Iteration 54, Batch: 28, Loss: 0.05572345480322838\n",
      "Iteration 54, Batch: 29, Loss: 0.07258223742246628\n",
      "Iteration 54, Batch: 30, Loss: 0.062506765127182\n",
      "Iteration 54, Batch: 31, Loss: 0.08736716210842133\n",
      "Iteration 54, Batch: 32, Loss: 0.07625313848257065\n",
      "Iteration 54, Batch: 33, Loss: 0.056100063025951385\n",
      "Iteration 54, Batch: 34, Loss: 0.05989978834986687\n",
      "Iteration 54, Batch: 35, Loss: 0.07757224142551422\n",
      "Iteration 54, Batch: 36, Loss: 0.07949748635292053\n",
      "Iteration 54, Batch: 37, Loss: 0.057421062141656876\n",
      "Iteration 54, Batch: 38, Loss: 0.07366309314966202\n",
      "Iteration 54, Batch: 39, Loss: 0.08381267637014389\n",
      "Iteration 54, Batch: 40, Loss: 0.09359067678451538\n",
      "Iteration 54, Batch: 41, Loss: 0.045540500432252884\n",
      "Iteration 54, Batch: 42, Loss: 0.0929962620139122\n",
      "Iteration 54, Batch: 43, Loss: 0.06593840569257736\n",
      "Iteration 54, Batch: 44, Loss: 0.05765508487820625\n",
      "Iteration 54, Batch: 45, Loss: 0.07158356159925461\n",
      "Iteration 54, Batch: 46, Loss: 0.05650968477129936\n",
      "Iteration 54, Batch: 47, Loss: 0.050639860332012177\n",
      "Iteration 54, Batch: 48, Loss: 0.06606379896402359\n",
      "Iteration 54, Batch: 49, Loss: 0.06927948445081711\n",
      "Iteration 55, Batch: 0, Loss: 0.06034766137599945\n",
      "Iteration 55, Batch: 1, Loss: 0.05259612575173378\n",
      "Iteration 55, Batch: 2, Loss: 0.0666598528623581\n",
      "Iteration 55, Batch: 3, Loss: 0.05900682881474495\n",
      "Iteration 55, Batch: 4, Loss: 0.08108215779066086\n",
      "Iteration 55, Batch: 5, Loss: 0.04311586171388626\n",
      "Iteration 55, Batch: 6, Loss: 0.03744156286120415\n",
      "Iteration 55, Batch: 7, Loss: 0.04395689442753792\n",
      "Iteration 55, Batch: 8, Loss: 0.0795116201043129\n",
      "Iteration 55, Batch: 9, Loss: 0.05306067317724228\n",
      "Iteration 55, Batch: 10, Loss: 0.06180660426616669\n",
      "Iteration 55, Batch: 11, Loss: 0.06595711410045624\n",
      "Iteration 55, Batch: 12, Loss: 0.05305065959692001\n",
      "Iteration 55, Batch: 13, Loss: 0.08855544030666351\n",
      "Iteration 55, Batch: 14, Loss: 0.08710739761590958\n",
      "Iteration 55, Batch: 15, Loss: 0.06392558664083481\n",
      "Iteration 55, Batch: 16, Loss: 0.07133802771568298\n",
      "Iteration 55, Batch: 17, Loss: 0.07030793279409409\n",
      "Iteration 55, Batch: 18, Loss: 0.032774630934000015\n",
      "Iteration 55, Batch: 19, Loss: 0.0741739571094513\n",
      "Iteration 55, Batch: 20, Loss: 0.059623222798109055\n",
      "Iteration 55, Batch: 21, Loss: 0.04788612201809883\n",
      "Iteration 55, Batch: 22, Loss: 0.06742019206285477\n",
      "Iteration 55, Batch: 23, Loss: 0.040402282029390335\n",
      "Iteration 55, Batch: 24, Loss: 0.041341185569763184\n",
      "Iteration 55, Batch: 25, Loss: 0.05281122401356697\n",
      "Iteration 55, Batch: 26, Loss: 0.07146607339382172\n",
      "Iteration 55, Batch: 27, Loss: 0.0555008165538311\n",
      "Iteration 55, Batch: 28, Loss: 0.06750097870826721\n",
      "Iteration 55, Batch: 29, Loss: 0.04254111647605896\n",
      "Iteration 55, Batch: 30, Loss: 0.08569654077291489\n",
      "Iteration 55, Batch: 31, Loss: 0.06987251341342926\n",
      "Iteration 55, Batch: 32, Loss: 0.06251458078622818\n",
      "Iteration 55, Batch: 33, Loss: 0.06383409351110458\n",
      "Iteration 55, Batch: 34, Loss: 0.053338345140218735\n",
      "Iteration 55, Batch: 35, Loss: 0.060588959604501724\n",
      "Iteration 55, Batch: 36, Loss: 0.044453807175159454\n",
      "Iteration 55, Batch: 37, Loss: 0.08874553442001343\n",
      "Iteration 55, Batch: 38, Loss: 0.07056193053722382\n",
      "Iteration 55, Batch: 39, Loss: 0.08639553189277649\n",
      "Iteration 55, Batch: 40, Loss: 0.05393143370747566\n",
      "Iteration 55, Batch: 41, Loss: 0.044130127876996994\n",
      "Iteration 55, Batch: 42, Loss: 0.05110243707895279\n",
      "Iteration 55, Batch: 43, Loss: 0.04204149171710014\n",
      "Iteration 55, Batch: 44, Loss: 0.04022900387644768\n",
      "Iteration 55, Batch: 45, Loss: 0.07176505774259567\n",
      "Iteration 55, Batch: 46, Loss: 0.04671002924442291\n",
      "Iteration 55, Batch: 47, Loss: 0.07117818295955658\n",
      "Iteration 55, Batch: 48, Loss: 0.0640173926949501\n",
      "Iteration 55, Batch: 49, Loss: 0.05351409688591957\n",
      "Iteration 56, Batch: 0, Loss: 0.058617815375328064\n",
      "Iteration 56, Batch: 1, Loss: 0.07550560683012009\n",
      "Iteration 56, Batch: 2, Loss: 0.08039260655641556\n",
      "Iteration 56, Batch: 3, Loss: 0.07255201041698456\n",
      "Iteration 56, Batch: 4, Loss: 0.041082244366407394\n",
      "Iteration 56, Batch: 5, Loss: 0.05179895833134651\n",
      "Iteration 56, Batch: 6, Loss: 0.06099556013941765\n",
      "Iteration 56, Batch: 7, Loss: 0.048197053372859955\n",
      "Iteration 56, Batch: 8, Loss: 0.07014533877372742\n",
      "Iteration 56, Batch: 9, Loss: 0.052130214869976044\n",
      "Iteration 56, Batch: 10, Loss: 0.043717801570892334\n",
      "Iteration 56, Batch: 11, Loss: 0.07628782093524933\n",
      "Iteration 56, Batch: 12, Loss: 0.07414242625236511\n",
      "Iteration 56, Batch: 13, Loss: 0.05821515992283821\n",
      "Iteration 56, Batch: 14, Loss: 0.0811024084687233\n",
      "Iteration 56, Batch: 15, Loss: 0.06861918419599533\n",
      "Iteration 56, Batch: 16, Loss: 0.053361356258392334\n",
      "Iteration 56, Batch: 17, Loss: 0.03860342130064964\n",
      "Iteration 56, Batch: 18, Loss: 0.057513780891895294\n",
      "Iteration 56, Batch: 19, Loss: 0.04644671827554703\n",
      "Iteration 56, Batch: 20, Loss: 0.047127749770879745\n",
      "Iteration 56, Batch: 21, Loss: 0.049293287098407745\n",
      "Iteration 56, Batch: 22, Loss: 0.08900351077318192\n",
      "Iteration 56, Batch: 23, Loss: 0.08005640655755997\n",
      "Iteration 56, Batch: 24, Loss: 0.04295741766691208\n",
      "Iteration 56, Batch: 25, Loss: 0.03744605928659439\n",
      "Iteration 56, Batch: 26, Loss: 0.05049927532672882\n",
      "Iteration 56, Batch: 27, Loss: 0.06464960426092148\n",
      "Iteration 56, Batch: 28, Loss: 0.03741065785288811\n",
      "Iteration 56, Batch: 29, Loss: 0.05570420250296593\n",
      "Iteration 56, Batch: 30, Loss: 0.06216397136449814\n",
      "Iteration 56, Batch: 31, Loss: 0.05801847204566002\n",
      "Iteration 56, Batch: 32, Loss: 0.05810564011335373\n",
      "Iteration 56, Batch: 33, Loss: 0.04154535382986069\n",
      "Iteration 56, Batch: 34, Loss: 0.0504995733499527\n",
      "Iteration 56, Batch: 35, Loss: 0.03889596834778786\n",
      "Iteration 56, Batch: 36, Loss: 0.04763723909854889\n",
      "Iteration 56, Batch: 37, Loss: 0.07007654756307602\n",
      "Iteration 56, Batch: 38, Loss: 0.05004020407795906\n",
      "Iteration 56, Batch: 39, Loss: 0.03727015107870102\n",
      "Iteration 56, Batch: 40, Loss: 0.06498789042234421\n",
      "Iteration 56, Batch: 41, Loss: 0.0865425392985344\n",
      "Iteration 56, Batch: 42, Loss: 0.05628209933638573\n",
      "Iteration 56, Batch: 43, Loss: 0.03964092954993248\n",
      "Iteration 56, Batch: 44, Loss: 0.057440489530563354\n",
      "Iteration 56, Batch: 45, Loss: 0.08611318469047546\n",
      "Iteration 56, Batch: 46, Loss: 0.05519739165902138\n",
      "Iteration 56, Batch: 47, Loss: 0.0533289797604084\n",
      "Iteration 56, Batch: 48, Loss: 0.05411962792277336\n",
      "Iteration 56, Batch: 49, Loss: 0.03935921937227249\n",
      "Iteration 57, Batch: 0, Loss: 0.06239151954650879\n",
      "Iteration 57, Batch: 1, Loss: 0.033339302986860275\n",
      "Iteration 57, Batch: 2, Loss: 0.041133660823106766\n",
      "Iteration 57, Batch: 3, Loss: 0.06581733375787735\n",
      "Iteration 57, Batch: 4, Loss: 0.06013823673129082\n",
      "Iteration 57, Batch: 5, Loss: 0.05787310376763344\n",
      "Iteration 57, Batch: 6, Loss: 0.05259666219353676\n",
      "Iteration 57, Batch: 7, Loss: 0.03580205515027046\n",
      "Iteration 57, Batch: 8, Loss: 0.04619519039988518\n",
      "Iteration 57, Batch: 9, Loss: 0.04241150617599487\n",
      "Iteration 57, Batch: 10, Loss: 0.0633406788110733\n",
      "Iteration 57, Batch: 11, Loss: 0.0498172752559185\n",
      "Iteration 57, Batch: 12, Loss: 0.060499027371406555\n",
      "Iteration 57, Batch: 13, Loss: 0.05297239124774933\n",
      "Iteration 57, Batch: 14, Loss: 0.0535399466753006\n",
      "Iteration 57, Batch: 15, Loss: 0.0634336993098259\n",
      "Iteration 57, Batch: 16, Loss: 0.051175832748413086\n",
      "Iteration 57, Batch: 17, Loss: 0.04708576947450638\n",
      "Iteration 57, Batch: 18, Loss: 0.031845059245824814\n",
      "Iteration 57, Batch: 19, Loss: 0.05538143217563629\n",
      "Iteration 57, Batch: 20, Loss: 0.05060930550098419\n",
      "Iteration 57, Batch: 21, Loss: 0.07278900593519211\n",
      "Iteration 57, Batch: 22, Loss: 0.05605122447013855\n",
      "Iteration 57, Batch: 23, Loss: 0.043331388384103775\n",
      "Iteration 57, Batch: 24, Loss: 0.05435614287853241\n",
      "Iteration 57, Batch: 25, Loss: 0.06739530712366104\n",
      "Iteration 57, Batch: 26, Loss: 0.07296311110258102\n",
      "Iteration 57, Batch: 27, Loss: 0.06878823786973953\n",
      "Iteration 57, Batch: 28, Loss: 0.06325629353523254\n",
      "Iteration 57, Batch: 29, Loss: 0.043188102543354034\n",
      "Iteration 57, Batch: 30, Loss: 0.061357200145721436\n",
      "Iteration 57, Batch: 31, Loss: 0.04219607636332512\n",
      "Iteration 57, Batch: 32, Loss: 0.07362478971481323\n",
      "Iteration 57, Batch: 33, Loss: 0.05713856965303421\n",
      "Iteration 57, Batch: 34, Loss: 0.05576300621032715\n",
      "Iteration 57, Batch: 35, Loss: 0.038166068494319916\n",
      "Iteration 57, Batch: 36, Loss: 0.08416833728551865\n",
      "Iteration 57, Batch: 37, Loss: 0.056999146938323975\n",
      "Iteration 57, Batch: 38, Loss: 0.04379335418343544\n",
      "Iteration 57, Batch: 39, Loss: 0.0584736205637455\n",
      "Iteration 57, Batch: 40, Loss: 0.05124702677130699\n",
      "Iteration 57, Batch: 41, Loss: 0.05470923334360123\n",
      "Iteration 57, Batch: 42, Loss: 0.04137349873781204\n",
      "Iteration 57, Batch: 43, Loss: 0.06692840158939362\n",
      "Iteration 57, Batch: 44, Loss: 0.07092312723398209\n",
      "Iteration 57, Batch: 45, Loss: 0.07616463303565979\n",
      "Iteration 57, Batch: 46, Loss: 0.04764718934893608\n",
      "Iteration 57, Batch: 47, Loss: 0.07949141412973404\n",
      "Iteration 57, Batch: 48, Loss: 0.050302885472774506\n",
      "Iteration 57, Batch: 49, Loss: 0.06681106239557266\n",
      "Iteration 58, Batch: 0, Loss: 0.07064347714185715\n",
      "Iteration 58, Batch: 1, Loss: 0.07173018902540207\n",
      "Iteration 58, Batch: 2, Loss: 0.055050432682037354\n",
      "Iteration 58, Batch: 3, Loss: 0.073549285531044\n",
      "Iteration 58, Batch: 4, Loss: 0.050714630633592606\n",
      "Iteration 58, Batch: 5, Loss: 0.042013075202703476\n",
      "Iteration 58, Batch: 6, Loss: 0.06255989521741867\n",
      "Iteration 58, Batch: 7, Loss: 0.07541391998529434\n",
      "Iteration 58, Batch: 8, Loss: 0.06424897164106369\n",
      "Iteration 58, Batch: 9, Loss: 0.049349345266819\n",
      "Iteration 58, Batch: 10, Loss: 0.07958932965993881\n",
      "Iteration 58, Batch: 11, Loss: 0.06912144273519516\n",
      "Iteration 58, Batch: 12, Loss: 0.06031874567270279\n",
      "Iteration 58, Batch: 13, Loss: 0.06413517892360687\n",
      "Iteration 58, Batch: 14, Loss: 0.030982544645667076\n",
      "Iteration 58, Batch: 15, Loss: 0.05943474546074867\n",
      "Iteration 58, Batch: 16, Loss: 0.04262005537748337\n",
      "Iteration 58, Batch: 17, Loss: 0.0704125240445137\n",
      "Iteration 58, Batch: 18, Loss: 0.06555676460266113\n",
      "Iteration 58, Batch: 19, Loss: 0.051247935742139816\n",
      "Iteration 58, Batch: 20, Loss: 0.054903123527765274\n",
      "Iteration 58, Batch: 21, Loss: 0.03761410713195801\n",
      "Iteration 58, Batch: 22, Loss: 0.042570505291223526\n",
      "Iteration 58, Batch: 23, Loss: 0.09200344234704971\n",
      "Iteration 58, Batch: 24, Loss: 0.07522053271532059\n",
      "Iteration 58, Batch: 25, Loss: 0.043697383254766464\n",
      "Iteration 58, Batch: 26, Loss: 0.044757045805454254\n",
      "Iteration 58, Batch: 27, Loss: 0.0781819149851799\n",
      "Iteration 58, Batch: 28, Loss: 0.08976756781339645\n",
      "Iteration 58, Batch: 29, Loss: 0.07177604734897614\n",
      "Iteration 58, Batch: 30, Loss: 0.08434496074914932\n",
      "Iteration 58, Batch: 31, Loss: 0.08832868188619614\n",
      "Iteration 58, Batch: 32, Loss: 0.07309157401323318\n",
      "Iteration 58, Batch: 33, Loss: 0.06960729509592056\n",
      "Iteration 58, Batch: 34, Loss: 0.0897277370095253\n",
      "Iteration 58, Batch: 35, Loss: 0.07129731774330139\n",
      "Iteration 58, Batch: 36, Loss: 0.07264440506696701\n",
      "Iteration 58, Batch: 37, Loss: 0.06732906401157379\n",
      "Iteration 58, Batch: 38, Loss: 0.06421453505754471\n",
      "Iteration 58, Batch: 39, Loss: 0.05349702760577202\n",
      "Iteration 58, Batch: 40, Loss: 0.07435283064842224\n",
      "Iteration 58, Batch: 41, Loss: 0.05551985651254654\n",
      "Iteration 58, Batch: 42, Loss: 0.07045414298772812\n",
      "Iteration 58, Batch: 43, Loss: 0.05437632277607918\n",
      "Iteration 58, Batch: 44, Loss: 0.04650283604860306\n",
      "Iteration 58, Batch: 45, Loss: 0.09626737982034683\n",
      "Iteration 58, Batch: 46, Loss: 0.08872237801551819\n",
      "Iteration 58, Batch: 47, Loss: 0.11920499801635742\n",
      "Iteration 58, Batch: 48, Loss: 0.08910972625017166\n",
      "Iteration 58, Batch: 49, Loss: 0.06675223261117935\n",
      "Iteration 59, Batch: 0, Loss: 0.08654581755399704\n",
      "Iteration 59, Batch: 1, Loss: 0.04598194360733032\n",
      "Iteration 59, Batch: 2, Loss: 0.07688833773136139\n",
      "Iteration 59, Batch: 3, Loss: 0.08314987272024155\n",
      "Iteration 59, Batch: 4, Loss: 0.05633779242634773\n",
      "Iteration 59, Batch: 5, Loss: 0.09175431728363037\n",
      "Iteration 59, Batch: 6, Loss: 0.05534714460372925\n",
      "Iteration 59, Batch: 7, Loss: 0.05783380940556526\n",
      "Iteration 59, Batch: 8, Loss: 0.0629165917634964\n",
      "Iteration 59, Batch: 9, Loss: 0.058380305767059326\n",
      "Iteration 59, Batch: 10, Loss: 0.08576680719852448\n",
      "Iteration 59, Batch: 11, Loss: 0.041172731667757034\n",
      "Iteration 59, Batch: 12, Loss: 0.05500407889485359\n",
      "Iteration 59, Batch: 13, Loss: 0.03389803692698479\n",
      "Iteration 59, Batch: 14, Loss: 0.06538458168506622\n",
      "Iteration 59, Batch: 15, Loss: 0.04794955253601074\n",
      "Iteration 59, Batch: 16, Loss: 0.07340451329946518\n",
      "Iteration 59, Batch: 17, Loss: 0.06142397224903107\n",
      "Iteration 59, Batch: 18, Loss: 0.0685349628329277\n",
      "Iteration 59, Batch: 19, Loss: 0.07629524171352386\n",
      "Iteration 59, Batch: 20, Loss: 0.06880652904510498\n",
      "Iteration 59, Batch: 21, Loss: 0.0401446633040905\n",
      "Iteration 59, Batch: 22, Loss: 0.12277722358703613\n",
      "Iteration 59, Batch: 23, Loss: 0.09098565578460693\n",
      "Iteration 59, Batch: 24, Loss: 0.08163690567016602\n",
      "Iteration 59, Batch: 25, Loss: 0.05502631142735481\n",
      "Iteration 59, Batch: 26, Loss: 0.0745428279042244\n",
      "Iteration 59, Batch: 27, Loss: 0.07742735743522644\n",
      "Iteration 59, Batch: 28, Loss: 0.07424402236938477\n",
      "Iteration 59, Batch: 29, Loss: 0.06902457028627396\n",
      "Iteration 59, Batch: 30, Loss: 0.08395642787218094\n",
      "Iteration 59, Batch: 31, Loss: 0.060102880001068115\n",
      "Iteration 59, Batch: 32, Loss: 0.07953858375549316\n",
      "Iteration 59, Batch: 33, Loss: 0.0890907421708107\n",
      "Iteration 59, Batch: 34, Loss: 0.09045186638832092\n",
      "Iteration 59, Batch: 35, Loss: 0.05855024233460426\n",
      "Iteration 59, Batch: 36, Loss: 0.07129544019699097\n",
      "Iteration 59, Batch: 37, Loss: 0.06406328827142715\n",
      "Iteration 59, Batch: 38, Loss: 0.08211513608694077\n",
      "Iteration 59, Batch: 39, Loss: 0.08002366125583649\n",
      "Iteration 59, Batch: 40, Loss: 0.0668921247124672\n",
      "Iteration 59, Batch: 41, Loss: 0.0483945868909359\n",
      "Iteration 59, Batch: 42, Loss: 0.07263915240764618\n",
      "Iteration 59, Batch: 43, Loss: 0.09676662087440491\n",
      "Iteration 59, Batch: 44, Loss: 0.066163569688797\n",
      "Iteration 59, Batch: 45, Loss: 0.030188284814357758\n",
      "Iteration 59, Batch: 46, Loss: 0.04653162881731987\n",
      "Iteration 59, Batch: 47, Loss: 0.06097439303994179\n",
      "Iteration 59, Batch: 48, Loss: 0.05861157178878784\n",
      "Iteration 59, Batch: 49, Loss: 0.08541370183229446\n",
      "Iteration 60, Batch: 0, Loss: 0.07198653370141983\n",
      "Iteration 60, Batch: 1, Loss: 0.05826520919799805\n",
      "Iteration 60, Batch: 2, Loss: 0.053708646446466446\n",
      "Iteration 60, Batch: 3, Loss: 0.09209341555833817\n",
      "Iteration 60, Batch: 4, Loss: 0.06468521058559418\n",
      "Iteration 60, Batch: 5, Loss: 0.06143678352236748\n",
      "Iteration 60, Batch: 6, Loss: 0.05321431905031204\n",
      "Iteration 60, Batch: 7, Loss: 0.03568084537982941\n",
      "Iteration 60, Batch: 8, Loss: 0.058595240116119385\n",
      "Iteration 60, Batch: 9, Loss: 0.06909235566854477\n",
      "Iteration 60, Batch: 10, Loss: 0.04879901185631752\n",
      "Iteration 60, Batch: 11, Loss: 0.06142950803041458\n",
      "Iteration 60, Batch: 12, Loss: 0.05234454572200775\n",
      "Iteration 60, Batch: 13, Loss: 0.05100993812084198\n",
      "Iteration 60, Batch: 14, Loss: 0.06579709053039551\n",
      "Iteration 60, Batch: 15, Loss: 0.04411914572119713\n",
      "Iteration 60, Batch: 16, Loss: 0.03937041014432907\n",
      "Iteration 60, Batch: 17, Loss: 0.05927259474992752\n",
      "Iteration 60, Batch: 18, Loss: 0.045226819813251495\n",
      "Iteration 60, Batch: 19, Loss: 0.0531621128320694\n",
      "Iteration 60, Batch: 20, Loss: 0.07483236491680145\n",
      "Iteration 60, Batch: 21, Loss: 0.04208097979426384\n",
      "Iteration 60, Batch: 22, Loss: 0.04075762629508972\n",
      "Iteration 60, Batch: 23, Loss: 0.04527051001787186\n",
      "Iteration 60, Batch: 24, Loss: 0.05387244001030922\n",
      "Iteration 60, Batch: 25, Loss: 0.0509110689163208\n",
      "Iteration 60, Batch: 26, Loss: 0.05835071951150894\n",
      "Iteration 60, Batch: 27, Loss: 0.0430731475353241\n",
      "Iteration 60, Batch: 28, Loss: 0.028276240453124046\n",
      "Iteration 60, Batch: 29, Loss: 0.059176359325647354\n",
      "Iteration 60, Batch: 30, Loss: 0.06879042834043503\n",
      "Iteration 60, Batch: 31, Loss: 0.05605451390147209\n",
      "Iteration 60, Batch: 32, Loss: 0.05987158790230751\n",
      "Iteration 60, Batch: 33, Loss: 0.0722024217247963\n",
      "Iteration 60, Batch: 34, Loss: 0.07259104400873184\n",
      "Iteration 60, Batch: 35, Loss: 0.05839760974049568\n",
      "Iteration 60, Batch: 36, Loss: 0.06409082561731339\n",
      "Iteration 60, Batch: 37, Loss: 0.06199808791279793\n",
      "Iteration 60, Batch: 38, Loss: 0.05062100291252136\n",
      "Iteration 60, Batch: 39, Loss: 0.06331426650285721\n",
      "Iteration 60, Batch: 40, Loss: 0.04607250168919563\n",
      "Iteration 60, Batch: 41, Loss: 0.04950534552335739\n",
      "Iteration 60, Batch: 42, Loss: 0.048059143126010895\n",
      "Iteration 60, Batch: 43, Loss: 0.05193592980504036\n",
      "Iteration 60, Batch: 44, Loss: 0.06828084588050842\n",
      "Iteration 60, Batch: 45, Loss: 0.03985154628753662\n",
      "Iteration 60, Batch: 46, Loss: 0.05746918544173241\n",
      "Iteration 60, Batch: 47, Loss: 0.06927912682294846\n",
      "Iteration 60, Batch: 48, Loss: 0.07317116856575012\n",
      "Iteration 60, Batch: 49, Loss: 0.03294004499912262\n",
      "Iteration 61, Batch: 0, Loss: 0.07002738863229752\n",
      "Iteration 61, Batch: 1, Loss: 0.0517776720225811\n",
      "Iteration 61, Batch: 2, Loss: 0.05156910791993141\n",
      "Iteration 61, Batch: 3, Loss: 0.06916455924510956\n",
      "Iteration 61, Batch: 4, Loss: 0.06218871846795082\n",
      "Iteration 61, Batch: 5, Loss: 0.05003907158970833\n",
      "Iteration 61, Batch: 6, Loss: 0.06318876147270203\n",
      "Iteration 61, Batch: 7, Loss: 0.052495382726192474\n",
      "Iteration 61, Batch: 8, Loss: 0.05065586045384407\n",
      "Iteration 61, Batch: 9, Loss: 0.05789300799369812\n",
      "Iteration 61, Batch: 10, Loss: 0.07942316681146622\n",
      "Iteration 61, Batch: 11, Loss: 0.050643470138311386\n",
      "Iteration 61, Batch: 12, Loss: 0.06832318753004074\n",
      "Iteration 61, Batch: 13, Loss: 0.056819379329681396\n",
      "Iteration 61, Batch: 14, Loss: 0.052374742925167084\n",
      "Iteration 61, Batch: 15, Loss: 0.06939557939767838\n",
      "Iteration 61, Batch: 16, Loss: 0.03196221590042114\n",
      "Iteration 61, Batch: 17, Loss: 0.06474485993385315\n",
      "Iteration 61, Batch: 18, Loss: 0.05931767448782921\n",
      "Iteration 61, Batch: 19, Loss: 0.055847328156232834\n",
      "Iteration 61, Batch: 20, Loss: 0.05517323315143585\n",
      "Iteration 61, Batch: 21, Loss: 0.03310117498040199\n",
      "Iteration 61, Batch: 22, Loss: 0.06930878758430481\n",
      "Iteration 61, Batch: 23, Loss: 0.041450176388025284\n",
      "Iteration 61, Batch: 24, Loss: 0.04055941477417946\n",
      "Iteration 61, Batch: 25, Loss: 0.06556953489780426\n",
      "Iteration 61, Batch: 26, Loss: 0.05586157739162445\n",
      "Iteration 61, Batch: 27, Loss: 0.05392256751656532\n",
      "Iteration 61, Batch: 28, Loss: 0.05825767293572426\n",
      "Iteration 61, Batch: 29, Loss: 0.0451316237449646\n",
      "Iteration 61, Batch: 30, Loss: 0.05752295255661011\n",
      "Iteration 61, Batch: 31, Loss: 0.049947235733270645\n",
      "Iteration 61, Batch: 32, Loss: 0.06292708218097687\n",
      "Iteration 61, Batch: 33, Loss: 0.04205697402358055\n",
      "Iteration 61, Batch: 34, Loss: 0.06323090195655823\n",
      "Iteration 61, Batch: 35, Loss: 0.04622139036655426\n",
      "Iteration 61, Batch: 36, Loss: 0.04799548164010048\n",
      "Iteration 61, Batch: 37, Loss: 0.053893737494945526\n",
      "Iteration 61, Batch: 38, Loss: 0.03666011616587639\n",
      "Iteration 61, Batch: 39, Loss: 0.06711410731077194\n",
      "Iteration 61, Batch: 40, Loss: 0.07382664829492569\n",
      "Iteration 61, Batch: 41, Loss: 0.057092342525720596\n",
      "Iteration 61, Batch: 42, Loss: 0.06753114610910416\n",
      "Iteration 61, Batch: 43, Loss: 0.07815231382846832\n",
      "Iteration 61, Batch: 44, Loss: 0.05888531729578972\n",
      "Iteration 61, Batch: 45, Loss: 0.03765340521931648\n",
      "Iteration 61, Batch: 46, Loss: 0.0574544221162796\n",
      "Iteration 61, Batch: 47, Loss: 0.05613483488559723\n",
      "Iteration 61, Batch: 48, Loss: 0.03479935973882675\n",
      "Iteration 61, Batch: 49, Loss: 0.060893334448337555\n",
      "Iteration 62, Batch: 0, Loss: 0.045248936861753464\n",
      "Iteration 62, Batch: 1, Loss: 0.07905472815036774\n",
      "Iteration 62, Batch: 2, Loss: 0.04937424883246422\n",
      "Iteration 62, Batch: 3, Loss: 0.0750814825296402\n",
      "Iteration 62, Batch: 4, Loss: 0.0664953663945198\n",
      "Iteration 62, Batch: 5, Loss: 0.04133506864309311\n",
      "Iteration 62, Batch: 6, Loss: 0.04053923487663269\n",
      "Iteration 62, Batch: 7, Loss: 0.018192235380411148\n",
      "Iteration 62, Batch: 8, Loss: 0.05045849829912186\n",
      "Iteration 62, Batch: 9, Loss: 0.05008895695209503\n",
      "Iteration 62, Batch: 10, Loss: 0.0682266429066658\n",
      "Iteration 62, Batch: 11, Loss: 0.06220913678407669\n",
      "Iteration 62, Batch: 12, Loss: 0.0822397992014885\n",
      "Iteration 62, Batch: 13, Loss: 0.05636855587363243\n",
      "Iteration 62, Batch: 14, Loss: 0.05679142847657204\n",
      "Iteration 62, Batch: 15, Loss: 0.058756425976753235\n",
      "Iteration 62, Batch: 16, Loss: 0.07968471199274063\n",
      "Iteration 62, Batch: 17, Loss: 0.054424092173576355\n",
      "Iteration 62, Batch: 18, Loss: 0.07617785036563873\n",
      "Iteration 62, Batch: 19, Loss: 0.05395485460758209\n",
      "Iteration 62, Batch: 20, Loss: 0.0655631348490715\n",
      "Iteration 62, Batch: 21, Loss: 0.0417230986058712\n",
      "Iteration 62, Batch: 22, Loss: 0.03348192945122719\n",
      "Iteration 62, Batch: 23, Loss: 0.044109273701906204\n",
      "Iteration 62, Batch: 24, Loss: 0.05559247359633446\n",
      "Iteration 62, Batch: 25, Loss: 0.05368274077773094\n",
      "Iteration 62, Batch: 26, Loss: 0.05479184165596962\n",
      "Iteration 62, Batch: 27, Loss: 0.05406882241368294\n",
      "Iteration 62, Batch: 28, Loss: 0.06903717666864395\n",
      "Iteration 62, Batch: 29, Loss: 0.06757181137800217\n",
      "Iteration 62, Batch: 30, Loss: 0.05644896253943443\n",
      "Iteration 62, Batch: 31, Loss: 0.0642317533493042\n",
      "Iteration 62, Batch: 32, Loss: 0.06974169611930847\n",
      "Iteration 62, Batch: 33, Loss: 0.06444817036390305\n",
      "Iteration 62, Batch: 34, Loss: 0.05887880548834801\n",
      "Iteration 62, Batch: 35, Loss: 0.04925137758255005\n",
      "Iteration 62, Batch: 36, Loss: 0.05006204918026924\n",
      "Iteration 62, Batch: 37, Loss: 0.0713244155049324\n",
      "Iteration 62, Batch: 38, Loss: 0.06619568914175034\n",
      "Iteration 62, Batch: 39, Loss: 0.06352850794792175\n",
      "Iteration 62, Batch: 40, Loss: 0.04998650774359703\n",
      "Iteration 62, Batch: 41, Loss: 0.0742197036743164\n",
      "Iteration 62, Batch: 42, Loss: 0.06547883152961731\n",
      "Iteration 62, Batch: 43, Loss: 0.05741249769926071\n",
      "Iteration 62, Batch: 44, Loss: 0.09257213026285172\n",
      "Iteration 62, Batch: 45, Loss: 0.04811783507466316\n",
      "Iteration 62, Batch: 46, Loss: 0.06017659232020378\n",
      "Iteration 62, Batch: 47, Loss: 0.05077690631151199\n",
      "Iteration 62, Batch: 48, Loss: 0.05157706141471863\n",
      "Iteration 62, Batch: 49, Loss: 0.09232919663190842\n",
      "Iteration 63, Batch: 0, Loss: 0.06327789276838303\n",
      "Iteration 63, Batch: 1, Loss: 0.03897068277001381\n",
      "Iteration 63, Batch: 2, Loss: 0.04102843254804611\n",
      "Iteration 63, Batch: 3, Loss: 0.02807006612420082\n",
      "Iteration 63, Batch: 4, Loss: 0.045724160969257355\n",
      "Iteration 63, Batch: 5, Loss: 0.042177751660346985\n",
      "Iteration 63, Batch: 6, Loss: 0.041411351412534714\n",
      "Iteration 63, Batch: 7, Loss: 0.06632225960493088\n",
      "Iteration 63, Batch: 8, Loss: 0.07032187283039093\n",
      "Iteration 63, Batch: 9, Loss: 0.05134600028395653\n",
      "Iteration 63, Batch: 10, Loss: 0.0708870217204094\n",
      "Iteration 63, Batch: 11, Loss: 0.03864782303571701\n",
      "Iteration 63, Batch: 12, Loss: 0.05238208547234535\n",
      "Iteration 63, Batch: 13, Loss: 0.05413065850734711\n",
      "Iteration 63, Batch: 14, Loss: 0.052152302116155624\n",
      "Iteration 63, Batch: 15, Loss: 0.06459983438253403\n",
      "Iteration 63, Batch: 16, Loss: 0.05834228917956352\n",
      "Iteration 63, Batch: 17, Loss: 0.07525797933340073\n",
      "Iteration 63, Batch: 18, Loss: 0.06478876620531082\n",
      "Iteration 63, Batch: 19, Loss: 0.07307270169258118\n",
      "Iteration 63, Batch: 20, Loss: 0.05191392824053764\n",
      "Iteration 63, Batch: 21, Loss: 0.02705656923353672\n",
      "Iteration 63, Batch: 22, Loss: 0.05363398417830467\n",
      "Iteration 63, Batch: 23, Loss: 0.05540051311254501\n",
      "Iteration 63, Batch: 24, Loss: 0.06751714646816254\n",
      "Iteration 63, Batch: 25, Loss: 0.05604961887001991\n",
      "Iteration 63, Batch: 26, Loss: 0.07549252361059189\n",
      "Iteration 63, Batch: 27, Loss: 0.05925819277763367\n",
      "Iteration 63, Batch: 28, Loss: 0.06221572309732437\n",
      "Iteration 63, Batch: 29, Loss: 0.06630295515060425\n",
      "Iteration 63, Batch: 30, Loss: 0.048541124910116196\n",
      "Iteration 63, Batch: 31, Loss: 0.06845278292894363\n",
      "Iteration 63, Batch: 32, Loss: 0.05996973440051079\n",
      "Iteration 63, Batch: 33, Loss: 0.06196780875325203\n",
      "Iteration 63, Batch: 34, Loss: 0.04589441046118736\n",
      "Iteration 63, Batch: 35, Loss: 0.06850124150514603\n",
      "Iteration 63, Batch: 36, Loss: 0.09377773106098175\n",
      "Iteration 63, Batch: 37, Loss: 0.08327212929725647\n",
      "Iteration 63, Batch: 38, Loss: 0.06436717510223389\n",
      "Iteration 63, Batch: 39, Loss: 0.06503181904554367\n",
      "Iteration 63, Batch: 40, Loss: 0.05641241744160652\n",
      "Iteration 63, Batch: 41, Loss: 0.06278150528669357\n",
      "Iteration 63, Batch: 42, Loss: 0.0811322033405304\n",
      "Iteration 63, Batch: 43, Loss: 0.0595194511115551\n",
      "Iteration 63, Batch: 44, Loss: 0.04379281401634216\n",
      "Iteration 63, Batch: 45, Loss: 0.05745689198374748\n",
      "Iteration 63, Batch: 46, Loss: 0.045845821499824524\n",
      "Iteration 63, Batch: 47, Loss: 0.040859535336494446\n",
      "Iteration 63, Batch: 48, Loss: 0.07027256488800049\n",
      "Iteration 63, Batch: 49, Loss: 0.09074204415082932\n",
      "Iteration 64, Batch: 0, Loss: 0.05555585399270058\n",
      "Iteration 64, Batch: 1, Loss: 0.07954100519418716\n",
      "Iteration 64, Batch: 2, Loss: 0.060505934059619904\n",
      "Iteration 64, Batch: 3, Loss: 0.06250931322574615\n",
      "Iteration 64, Batch: 4, Loss: 0.06176518276333809\n",
      "Iteration 64, Batch: 5, Loss: 0.056091345846652985\n",
      "Iteration 64, Batch: 6, Loss: 0.05252647399902344\n",
      "Iteration 64, Batch: 7, Loss: 0.08446116000413895\n",
      "Iteration 64, Batch: 8, Loss: 0.08074089884757996\n",
      "Iteration 64, Batch: 9, Loss: 0.0693168118596077\n",
      "Iteration 64, Batch: 10, Loss: 0.06285510957241058\n",
      "Iteration 64, Batch: 11, Loss: 0.054373014718294144\n",
      "Iteration 64, Batch: 12, Loss: 0.029864490032196045\n",
      "Iteration 64, Batch: 13, Loss: 0.0642995610833168\n",
      "Iteration 64, Batch: 14, Loss: 0.06454963237047195\n",
      "Iteration 64, Batch: 15, Loss: 0.05664805322885513\n",
      "Iteration 64, Batch: 16, Loss: 0.056794870644807816\n",
      "Iteration 64, Batch: 17, Loss: 0.05692017078399658\n",
      "Iteration 64, Batch: 18, Loss: 0.06058667227625847\n",
      "Iteration 64, Batch: 19, Loss: 0.027169495820999146\n",
      "Iteration 64, Batch: 20, Loss: 0.044797543436288834\n",
      "Iteration 64, Batch: 21, Loss: 0.06623049080371857\n",
      "Iteration 64, Batch: 22, Loss: 0.044220972806215286\n",
      "Iteration 64, Batch: 23, Loss: 0.0335782989859581\n",
      "Iteration 64, Batch: 24, Loss: 0.06689225882291794\n",
      "Iteration 64, Batch: 25, Loss: 0.034886445850133896\n",
      "Iteration 64, Batch: 26, Loss: 0.08278591185808182\n",
      "Iteration 64, Batch: 27, Loss: 0.047099657356739044\n",
      "Iteration 64, Batch: 28, Loss: 0.053755760192871094\n",
      "Iteration 64, Batch: 29, Loss: 0.05462781712412834\n",
      "Iteration 64, Batch: 30, Loss: 0.0439620278775692\n",
      "Iteration 64, Batch: 31, Loss: 0.07657963037490845\n",
      "Iteration 64, Batch: 32, Loss: 0.055325478315353394\n",
      "Iteration 64, Batch: 33, Loss: 0.044894665479660034\n",
      "Iteration 64, Batch: 34, Loss: 0.05314089357852936\n",
      "Iteration 64, Batch: 35, Loss: 0.05479281395673752\n",
      "Iteration 64, Batch: 36, Loss: 0.04948096722364426\n",
      "Iteration 64, Batch: 37, Loss: 0.043678730726242065\n",
      "Iteration 64, Batch: 38, Loss: 0.0550248883664608\n",
      "Iteration 64, Batch: 39, Loss: 0.06176495552062988\n",
      "Iteration 64, Batch: 40, Loss: 0.05949865281581879\n",
      "Iteration 64, Batch: 41, Loss: 0.07967875897884369\n",
      "Iteration 64, Batch: 42, Loss: 0.04360154643654823\n",
      "Iteration 64, Batch: 43, Loss: 0.038372743874788284\n",
      "Iteration 64, Batch: 44, Loss: 0.04858127608895302\n",
      "Iteration 64, Batch: 45, Loss: 0.04866712912917137\n",
      "Iteration 64, Batch: 46, Loss: 0.049335312098264694\n",
      "Iteration 64, Batch: 47, Loss: 0.07547681033611298\n",
      "Iteration 64, Batch: 48, Loss: 0.05411536991596222\n",
      "Iteration 64, Batch: 49, Loss: 0.055166095495224\n",
      "Iteration 65, Batch: 0, Loss: 0.0399334691464901\n",
      "Iteration 65, Batch: 1, Loss: 0.06952475756406784\n",
      "Iteration 65, Batch: 2, Loss: 0.06117844954133034\n",
      "Iteration 65, Batch: 3, Loss: 0.04293195530772209\n",
      "Iteration 65, Batch: 4, Loss: 0.05754409730434418\n",
      "Iteration 65, Batch: 5, Loss: 0.07621654868125916\n",
      "Iteration 65, Batch: 6, Loss: 0.03747617453336716\n",
      "Iteration 65, Batch: 7, Loss: 0.022820396348834038\n",
      "Iteration 65, Batch: 8, Loss: 0.03608666732907295\n",
      "Iteration 65, Batch: 9, Loss: 0.06063418090343475\n",
      "Iteration 65, Batch: 10, Loss: 0.05930521711707115\n",
      "Iteration 65, Batch: 11, Loss: 0.06654077023267746\n",
      "Iteration 65, Batch: 12, Loss: 0.048257071524858475\n",
      "Iteration 65, Batch: 13, Loss: 0.05384748801589012\n",
      "Iteration 65, Batch: 14, Loss: 0.06967349350452423\n",
      "Iteration 65, Batch: 15, Loss: 0.07545264065265656\n",
      "Iteration 65, Batch: 16, Loss: 0.06068405881524086\n",
      "Iteration 65, Batch: 17, Loss: 0.05360376089811325\n",
      "Iteration 65, Batch: 18, Loss: 0.06201435998082161\n",
      "Iteration 65, Batch: 19, Loss: 0.04414667189121246\n",
      "Iteration 65, Batch: 20, Loss: 0.04816065728664398\n",
      "Iteration 65, Batch: 21, Loss: 0.06084437295794487\n",
      "Iteration 65, Batch: 22, Loss: 0.051724545657634735\n",
      "Iteration 65, Batch: 23, Loss: 0.05030049383640289\n",
      "Iteration 65, Batch: 24, Loss: 0.058929193764925\n",
      "Iteration 65, Batch: 25, Loss: 0.04237096756696701\n",
      "Iteration 65, Batch: 26, Loss: 0.04536186158657074\n",
      "Iteration 65, Batch: 27, Loss: 0.05335904657840729\n",
      "Iteration 65, Batch: 28, Loss: 0.050797004252672195\n",
      "Iteration 65, Batch: 29, Loss: 0.041494544595479965\n",
      "Iteration 65, Batch: 30, Loss: 0.05537398159503937\n",
      "Iteration 65, Batch: 31, Loss: 0.08087200671434402\n",
      "Iteration 65, Batch: 32, Loss: 0.06319153308868408\n",
      "Iteration 65, Batch: 33, Loss: 0.06020408123731613\n",
      "Iteration 65, Batch: 34, Loss: 0.058718327432870865\n",
      "Iteration 65, Batch: 35, Loss: 0.06181339919567108\n",
      "Iteration 65, Batch: 36, Loss: 0.04259631037712097\n",
      "Iteration 65, Batch: 37, Loss: 0.07055404782295227\n",
      "Iteration 65, Batch: 38, Loss: 0.05508146062493324\n",
      "Iteration 65, Batch: 39, Loss: 0.04310312122106552\n",
      "Iteration 65, Batch: 40, Loss: 0.04866916686296463\n",
      "Iteration 65, Batch: 41, Loss: 0.0628303587436676\n",
      "Iteration 65, Batch: 42, Loss: 0.09536021947860718\n",
      "Iteration 65, Batch: 43, Loss: 0.08118060976266861\n",
      "Iteration 65, Batch: 44, Loss: 0.04476745426654816\n",
      "Iteration 65, Batch: 45, Loss: 0.05964776873588562\n",
      "Iteration 65, Batch: 46, Loss: 0.04868002608418465\n",
      "Iteration 65, Batch: 47, Loss: 0.07763773947954178\n",
      "Iteration 65, Batch: 48, Loss: 0.07153325527906418\n",
      "Iteration 65, Batch: 49, Loss: 0.04880863055586815\n",
      "Iteration 66, Batch: 0, Loss: 0.054262809455394745\n",
      "Iteration 66, Batch: 1, Loss: 0.05793241783976555\n",
      "Iteration 66, Batch: 2, Loss: 0.04994688928127289\n",
      "Iteration 66, Batch: 3, Loss: 0.06496135890483856\n",
      "Iteration 66, Batch: 4, Loss: 0.08152780681848526\n",
      "Iteration 66, Batch: 5, Loss: 0.07387559115886688\n",
      "Iteration 66, Batch: 6, Loss: 0.0838388055562973\n",
      "Iteration 66, Batch: 7, Loss: 0.10802732408046722\n",
      "Iteration 66, Batch: 8, Loss: 0.08675912767648697\n",
      "Iteration 66, Batch: 9, Loss: 0.08458253741264343\n",
      "Iteration 66, Batch: 10, Loss: 0.056477028876543045\n",
      "Iteration 66, Batch: 11, Loss: 0.08288455754518509\n",
      "Iteration 66, Batch: 12, Loss: 0.06694049388170242\n",
      "Iteration 66, Batch: 13, Loss: 0.06979766488075256\n",
      "Iteration 66, Batch: 14, Loss: 0.029532724991440773\n",
      "Iteration 66, Batch: 15, Loss: 0.05432133376598358\n",
      "Iteration 66, Batch: 16, Loss: 0.05815344676375389\n",
      "Iteration 66, Batch: 17, Loss: 0.06577185541391373\n",
      "Iteration 66, Batch: 18, Loss: 0.041073039174079895\n",
      "Iteration 66, Batch: 19, Loss: 0.05160852149128914\n",
      "Iteration 66, Batch: 20, Loss: 0.061362821608781815\n",
      "Iteration 66, Batch: 21, Loss: 0.07879775017499924\n",
      "Iteration 66, Batch: 22, Loss: 0.05759212374687195\n",
      "Iteration 66, Batch: 23, Loss: 0.055235300213098526\n",
      "Iteration 66, Batch: 24, Loss: 0.0906510129570961\n",
      "Iteration 66, Batch: 25, Loss: 0.07186949998140335\n",
      "Iteration 66, Batch: 26, Loss: 0.07791613787412643\n",
      "Iteration 66, Batch: 27, Loss: 0.04620413854718208\n",
      "Iteration 66, Batch: 28, Loss: 0.07285478711128235\n",
      "Iteration 66, Batch: 29, Loss: 0.0815620943903923\n",
      "Iteration 66, Batch: 30, Loss: 0.074706070125103\n",
      "Iteration 66, Batch: 31, Loss: 0.08690187335014343\n",
      "Iteration 66, Batch: 32, Loss: 0.07540621608495712\n",
      "Iteration 66, Batch: 33, Loss: 0.07526668906211853\n",
      "Iteration 66, Batch: 34, Loss: 0.04075361043214798\n",
      "Iteration 66, Batch: 35, Loss: 0.05856611207127571\n",
      "Iteration 66, Batch: 36, Loss: 0.06645718961954117\n",
      "Iteration 66, Batch: 37, Loss: 0.06877101957798004\n",
      "Iteration 66, Batch: 38, Loss: 0.0858432874083519\n",
      "Iteration 66, Batch: 39, Loss: 0.09446956217288971\n",
      "Iteration 66, Batch: 40, Loss: 0.05210987105965614\n",
      "Iteration 66, Batch: 41, Loss: 0.06856999546289444\n",
      "Iteration 66, Batch: 42, Loss: 0.044611044228076935\n",
      "Iteration 66, Batch: 43, Loss: 0.06204942613840103\n",
      "Iteration 66, Batch: 44, Loss: 0.06426940858364105\n",
      "Iteration 66, Batch: 45, Loss: 0.05877275764942169\n",
      "Iteration 66, Batch: 46, Loss: 0.06995446979999542\n",
      "Iteration 66, Batch: 47, Loss: 0.09724782407283783\n",
      "Iteration 66, Batch: 48, Loss: 0.0985499769449234\n",
      "Iteration 66, Batch: 49, Loss: 0.06875956803560257\n",
      "Iteration 67, Batch: 0, Loss: 0.0753559023141861\n",
      "Iteration 67, Batch: 1, Loss: 0.06587911397218704\n",
      "Iteration 67, Batch: 2, Loss: 0.06124251335859299\n",
      "Iteration 67, Batch: 3, Loss: 0.06825815886259079\n",
      "Iteration 67, Batch: 4, Loss: 0.034847188740968704\n",
      "Iteration 67, Batch: 5, Loss: 0.060130033642053604\n",
      "Iteration 67, Batch: 6, Loss: 0.054241444915533066\n",
      "Iteration 67, Batch: 7, Loss: 0.0749562531709671\n",
      "Iteration 67, Batch: 8, Loss: 0.06223941594362259\n",
      "Iteration 67, Batch: 9, Loss: 0.06776934117078781\n",
      "Iteration 67, Batch: 10, Loss: 0.08008220791816711\n",
      "Iteration 67, Batch: 11, Loss: 0.054526831954717636\n",
      "Iteration 67, Batch: 12, Loss: 0.04064026102423668\n",
      "Iteration 67, Batch: 13, Loss: 0.06386183202266693\n",
      "Iteration 67, Batch: 14, Loss: 0.04764207825064659\n",
      "Iteration 67, Batch: 15, Loss: 0.05979722738265991\n",
      "Iteration 67, Batch: 16, Loss: 0.06801027804613113\n",
      "Iteration 67, Batch: 17, Loss: 0.057759542018175125\n",
      "Iteration 67, Batch: 18, Loss: 0.07743673026561737\n",
      "Iteration 67, Batch: 19, Loss: 0.067364901304245\n",
      "Iteration 67, Batch: 20, Loss: 0.053950656205415726\n",
      "Iteration 67, Batch: 21, Loss: 0.04326903820037842\n",
      "Iteration 67, Batch: 22, Loss: 0.028568459674715996\n",
      "Iteration 67, Batch: 23, Loss: 0.06430371105670929\n",
      "Iteration 67, Batch: 24, Loss: 0.05877666175365448\n",
      "Iteration 67, Batch: 25, Loss: 0.05353858694434166\n",
      "Iteration 67, Batch: 26, Loss: 0.05835834518074989\n",
      "Iteration 67, Batch: 27, Loss: 0.05690581724047661\n",
      "Iteration 67, Batch: 28, Loss: 0.05897161737084389\n",
      "Iteration 67, Batch: 29, Loss: 0.04364306479692459\n",
      "Iteration 67, Batch: 30, Loss: 0.07026117295026779\n",
      "Iteration 67, Batch: 31, Loss: 0.0633644089102745\n",
      "Iteration 67, Batch: 32, Loss: 0.06467834115028381\n",
      "Iteration 67, Batch: 33, Loss: 0.07209303975105286\n",
      "Iteration 67, Batch: 34, Loss: 0.08533412963151932\n",
      "Iteration 67, Batch: 35, Loss: 0.049740228801965714\n",
      "Iteration 67, Batch: 36, Loss: 0.0542314238846302\n",
      "Iteration 67, Batch: 37, Loss: 0.08136127889156342\n",
      "Iteration 67, Batch: 38, Loss: 0.04909102991223335\n",
      "Iteration 67, Batch: 39, Loss: 0.049392230808734894\n",
      "Iteration 67, Batch: 40, Loss: 0.06173630431294441\n",
      "Iteration 67, Batch: 41, Loss: 0.04928917810320854\n",
      "Iteration 67, Batch: 42, Loss: 0.05158938467502594\n",
      "Iteration 67, Batch: 43, Loss: 0.042227134108543396\n",
      "Iteration 67, Batch: 44, Loss: 0.06377635151147842\n",
      "Iteration 67, Batch: 45, Loss: 0.05421024560928345\n",
      "Iteration 67, Batch: 46, Loss: 0.06324302405118942\n",
      "Iteration 67, Batch: 47, Loss: 0.05425142124295235\n",
      "Iteration 67, Batch: 48, Loss: 0.07105717808008194\n",
      "Iteration 67, Batch: 49, Loss: 0.046968765556812286\n",
      "Iteration 68, Batch: 0, Loss: 0.0585939884185791\n",
      "Iteration 68, Batch: 1, Loss: 0.06366123259067535\n",
      "Iteration 68, Batch: 2, Loss: 0.07177691161632538\n",
      "Iteration 68, Batch: 3, Loss: 0.05713840201497078\n",
      "Iteration 68, Batch: 4, Loss: 0.04466000944375992\n",
      "Iteration 68, Batch: 5, Loss: 0.03534629940986633\n",
      "Iteration 68, Batch: 6, Loss: 0.05630038306117058\n",
      "Iteration 68, Batch: 7, Loss: 0.061253029853105545\n",
      "Iteration 68, Batch: 8, Loss: 0.057441405951976776\n",
      "Iteration 68, Batch: 9, Loss: 0.06734270602464676\n",
      "Iteration 68, Batch: 10, Loss: 0.05475630983710289\n",
      "Iteration 68, Batch: 11, Loss: 0.05925684794783592\n",
      "Iteration 68, Batch: 12, Loss: 0.06385338306427002\n",
      "Iteration 68, Batch: 13, Loss: 0.0540117546916008\n",
      "Iteration 68, Batch: 14, Loss: 0.040178146213293076\n",
      "Iteration 68, Batch: 15, Loss: 0.03289686515927315\n",
      "Iteration 68, Batch: 16, Loss: 0.058972280472517014\n",
      "Iteration 68, Batch: 17, Loss: 0.07515939325094223\n",
      "Iteration 68, Batch: 18, Loss: 0.05915552005171776\n",
      "Iteration 68, Batch: 19, Loss: 0.05356169492006302\n",
      "Iteration 68, Batch: 20, Loss: 0.08062735199928284\n",
      "Iteration 68, Batch: 21, Loss: 0.04327678680419922\n",
      "Iteration 68, Batch: 22, Loss: 0.0663771778345108\n",
      "Iteration 68, Batch: 23, Loss: 0.041419219225645065\n",
      "Iteration 68, Batch: 24, Loss: 0.06337860226631165\n",
      "Iteration 68, Batch: 25, Loss: 0.07477027922868729\n",
      "Iteration 68, Batch: 26, Loss: 0.04029465466737747\n",
      "Iteration 68, Batch: 27, Loss: 0.07970631867647171\n",
      "Iteration 68, Batch: 28, Loss: 0.08447273820638657\n",
      "Iteration 68, Batch: 29, Loss: 0.0757891908288002\n",
      "Iteration 68, Batch: 30, Loss: 0.07470684498548508\n",
      "Iteration 68, Batch: 31, Loss: 0.07745908200740814\n",
      "Iteration 68, Batch: 32, Loss: 0.11284807324409485\n",
      "Iteration 68, Batch: 33, Loss: 0.06699739396572113\n",
      "Iteration 68, Batch: 34, Loss: 0.06835877895355225\n",
      "Iteration 68, Batch: 35, Loss: 0.07131658494472504\n",
      "Iteration 68, Batch: 36, Loss: 0.06170554831624031\n",
      "Iteration 68, Batch: 37, Loss: 0.07769842445850372\n",
      "Iteration 68, Batch: 38, Loss: 0.08738000690937042\n",
      "Iteration 68, Batch: 39, Loss: 0.06526762247085571\n",
      "Iteration 68, Batch: 40, Loss: 0.04481177777051926\n",
      "Iteration 68, Batch: 41, Loss: 0.08918031305074692\n",
      "Iteration 68, Batch: 42, Loss: 0.0674419179558754\n",
      "Iteration 68, Batch: 43, Loss: 0.06146712228655815\n",
      "Iteration 68, Batch: 44, Loss: 0.07487484812736511\n",
      "Iteration 68, Batch: 45, Loss: 0.06814416497945786\n",
      "Iteration 68, Batch: 46, Loss: 0.07999495416879654\n",
      "Iteration 68, Batch: 47, Loss: 0.04580299183726311\n",
      "Iteration 68, Batch: 48, Loss: 0.08706746995449066\n",
      "Iteration 68, Batch: 49, Loss: 0.0894797295331955\n",
      "Iteration 69, Batch: 0, Loss: 0.07799693197011948\n",
      "Iteration 69, Batch: 1, Loss: 0.08208970725536346\n",
      "Iteration 69, Batch: 2, Loss: 0.07727480679750443\n",
      "Iteration 69, Batch: 3, Loss: 0.08335261791944504\n",
      "Iteration 69, Batch: 4, Loss: 0.07181376963853836\n",
      "Iteration 69, Batch: 5, Loss: 0.08970905095338821\n",
      "Iteration 69, Batch: 6, Loss: 0.0693463683128357\n",
      "Iteration 69, Batch: 7, Loss: 0.05663272738456726\n",
      "Iteration 69, Batch: 8, Loss: 0.05933941528201103\n",
      "Iteration 69, Batch: 9, Loss: 0.10934179276227951\n",
      "Iteration 69, Batch: 10, Loss: 0.07921459525823593\n",
      "Iteration 69, Batch: 11, Loss: 0.07259553670883179\n",
      "Iteration 69, Batch: 12, Loss: 0.056319329887628555\n",
      "Iteration 69, Batch: 13, Loss: 0.059917282313108444\n",
      "Iteration 69, Batch: 14, Loss: 0.09435737133026123\n",
      "Iteration 69, Batch: 15, Loss: 0.054959915578365326\n",
      "Iteration 69, Batch: 16, Loss: 0.04639717563986778\n",
      "Iteration 69, Batch: 17, Loss: 0.08033797889947891\n",
      "Iteration 69, Batch: 18, Loss: 0.06791254878044128\n",
      "Iteration 69, Batch: 19, Loss: 0.05883277580142021\n",
      "Iteration 69, Batch: 20, Loss: 0.09314874559640884\n",
      "Iteration 69, Batch: 21, Loss: 0.05403269827365875\n",
      "Iteration 69, Batch: 22, Loss: 0.0654517412185669\n",
      "Iteration 69, Batch: 23, Loss: 0.07530263066291809\n",
      "Iteration 69, Batch: 24, Loss: 0.08066047728061676\n",
      "Iteration 69, Batch: 25, Loss: 0.11307965964078903\n",
      "Iteration 69, Batch: 26, Loss: 0.050274934619665146\n",
      "Iteration 69, Batch: 27, Loss: 0.06491551548242569\n",
      "Iteration 69, Batch: 28, Loss: 0.06266295909881592\n",
      "Iteration 69, Batch: 29, Loss: 0.0667882114648819\n",
      "Iteration 69, Batch: 30, Loss: 0.08193347603082657\n",
      "Iteration 69, Batch: 31, Loss: 0.06450874358415604\n",
      "Iteration 69, Batch: 32, Loss: 0.07006756961345673\n",
      "Iteration 69, Batch: 33, Loss: 0.06278447061777115\n",
      "Iteration 69, Batch: 34, Loss: 0.08210452646017075\n",
      "Iteration 69, Batch: 35, Loss: 0.048944223672151566\n",
      "Iteration 69, Batch: 36, Loss: 0.026531048119068146\n",
      "Iteration 69, Batch: 37, Loss: 0.06968185305595398\n",
      "Iteration 69, Batch: 38, Loss: 0.05291622504591942\n",
      "Iteration 69, Batch: 39, Loss: 0.10536056011915207\n",
      "Iteration 69, Batch: 40, Loss: 0.07176485657691956\n",
      "Iteration 69, Batch: 41, Loss: 0.06966861337423325\n",
      "Iteration 69, Batch: 42, Loss: 0.09472419321537018\n",
      "Iteration 69, Batch: 43, Loss: 0.07999885082244873\n",
      "Iteration 69, Batch: 44, Loss: 0.05818067863583565\n",
      "Iteration 69, Batch: 45, Loss: 0.04969019815325737\n",
      "Iteration 69, Batch: 46, Loss: 0.05911654233932495\n",
      "Iteration 69, Batch: 47, Loss: 0.05356387421488762\n",
      "Iteration 69, Batch: 48, Loss: 0.061872050166130066\n",
      "Iteration 69, Batch: 49, Loss: 0.05243078991770744\n",
      "Iteration 70, Batch: 0, Loss: 0.046315986663103104\n",
      "Iteration 70, Batch: 1, Loss: 0.08563032746315002\n",
      "Iteration 70, Batch: 2, Loss: 0.06841645389795303\n",
      "Iteration 70, Batch: 3, Loss: 0.07610885053873062\n",
      "Iteration 70, Batch: 4, Loss: 0.08833187073469162\n",
      "Iteration 70, Batch: 5, Loss: 0.07554206252098083\n",
      "Iteration 70, Batch: 6, Loss: 0.05512695014476776\n",
      "Iteration 70, Batch: 7, Loss: 0.06742732226848602\n",
      "Iteration 70, Batch: 8, Loss: 0.10502083599567413\n",
      "Iteration 70, Batch: 9, Loss: 0.06839577853679657\n",
      "Iteration 70, Batch: 10, Loss: 0.0648457258939743\n",
      "Iteration 70, Batch: 11, Loss: 0.10701589286327362\n",
      "Iteration 70, Batch: 12, Loss: 0.07660716027021408\n",
      "Iteration 70, Batch: 13, Loss: 0.08605631440877914\n",
      "Iteration 70, Batch: 14, Loss: 0.07421988248825073\n",
      "Iteration 70, Batch: 15, Loss: 0.08208415657281876\n",
      "Iteration 70, Batch: 16, Loss: 0.09369847178459167\n",
      "Iteration 70, Batch: 17, Loss: 0.08147840201854706\n",
      "Iteration 70, Batch: 18, Loss: 0.07629112154245377\n",
      "Iteration 70, Batch: 19, Loss: 0.05144500732421875\n",
      "Iteration 70, Batch: 20, Loss: 0.08187399059534073\n",
      "Iteration 70, Batch: 21, Loss: 0.10581550002098083\n",
      "Iteration 70, Batch: 22, Loss: 0.049495138227939606\n",
      "Iteration 70, Batch: 23, Loss: 0.06783587485551834\n",
      "Iteration 70, Batch: 24, Loss: 0.0649760514497757\n",
      "Iteration 70, Batch: 25, Loss: 0.06339135766029358\n",
      "Iteration 70, Batch: 26, Loss: 0.08366895467042923\n",
      "Iteration 70, Batch: 27, Loss: 0.09179213643074036\n",
      "Iteration 70, Batch: 28, Loss: 0.08181749284267426\n",
      "Iteration 70, Batch: 29, Loss: 0.09370749443769455\n",
      "Iteration 70, Batch: 30, Loss: 0.05729670822620392\n",
      "Iteration 70, Batch: 31, Loss: 0.07979081571102142\n",
      "Iteration 70, Batch: 32, Loss: 0.08367885649204254\n",
      "Iteration 70, Batch: 33, Loss: 0.10494688153266907\n",
      "Iteration 70, Batch: 34, Loss: 0.0906451940536499\n",
      "Iteration 70, Batch: 35, Loss: 0.07058887928724289\n",
      "Iteration 70, Batch: 36, Loss: 0.10257840901613235\n",
      "Iteration 70, Batch: 37, Loss: 0.05886766314506531\n",
      "Iteration 70, Batch: 38, Loss: 0.062008537352085114\n",
      "Iteration 70, Batch: 39, Loss: 0.07555323839187622\n",
      "Iteration 70, Batch: 40, Loss: 0.07473703473806381\n",
      "Iteration 70, Batch: 41, Loss: 0.08789335191249847\n",
      "Iteration 70, Batch: 42, Loss: 0.09035727381706238\n",
      "Iteration 70, Batch: 43, Loss: 0.055739060044288635\n",
      "Iteration 70, Batch: 44, Loss: 0.07478002458810806\n",
      "Iteration 70, Batch: 45, Loss: 0.0677264854311943\n",
      "Iteration 70, Batch: 46, Loss: 0.10553065687417984\n",
      "Iteration 70, Batch: 47, Loss: 0.07049159705638885\n",
      "Iteration 70, Batch: 48, Loss: 0.07521649450063705\n",
      "Iteration 70, Batch: 49, Loss: 0.08195143193006516\n",
      "Iteration 71, Batch: 0, Loss: 0.05832225829362869\n",
      "Iteration 71, Batch: 1, Loss: 0.08173280209302902\n",
      "Iteration 71, Batch: 2, Loss: 0.07974590361118317\n",
      "Iteration 71, Batch: 3, Loss: 0.05678391829133034\n",
      "Iteration 71, Batch: 4, Loss: 0.051168784499168396\n",
      "Iteration 71, Batch: 5, Loss: 0.09745870530605316\n",
      "Iteration 71, Batch: 6, Loss: 0.08693307638168335\n",
      "Iteration 71, Batch: 7, Loss: 0.07642928510904312\n",
      "Iteration 71, Batch: 8, Loss: 0.06600157916545868\n",
      "Iteration 71, Batch: 9, Loss: 0.05957129970192909\n",
      "Iteration 71, Batch: 10, Loss: 0.06310445070266724\n",
      "Iteration 71, Batch: 11, Loss: 0.09855995327234268\n",
      "Iteration 71, Batch: 12, Loss: 0.0825401321053505\n",
      "Iteration 71, Batch: 13, Loss: 0.07179895043373108\n",
      "Iteration 71, Batch: 14, Loss: 0.06176063045859337\n",
      "Iteration 71, Batch: 15, Loss: 0.06621269881725311\n",
      "Iteration 71, Batch: 16, Loss: 0.07378684729337692\n",
      "Iteration 71, Batch: 17, Loss: 0.06505041569471359\n",
      "Iteration 71, Batch: 18, Loss: 0.058363012969493866\n",
      "Iteration 71, Batch: 19, Loss: 0.0993831679224968\n",
      "Iteration 71, Batch: 20, Loss: 0.09329194575548172\n",
      "Iteration 71, Batch: 21, Loss: 0.08068258315324783\n",
      "Iteration 71, Batch: 22, Loss: 0.06728339195251465\n",
      "Iteration 71, Batch: 23, Loss: 0.10125988721847534\n",
      "Iteration 71, Batch: 24, Loss: 0.10103824734687805\n",
      "Iteration 71, Batch: 25, Loss: 0.05510023608803749\n",
      "Iteration 71, Batch: 26, Loss: 0.09301093965768814\n",
      "Iteration 71, Batch: 27, Loss: 0.11302340030670166\n",
      "Iteration 71, Batch: 28, Loss: 0.07730219513177872\n",
      "Iteration 71, Batch: 29, Loss: 0.10364457964897156\n",
      "Iteration 71, Batch: 30, Loss: 0.0928022637963295\n",
      "Iteration 71, Batch: 31, Loss: 0.0645233690738678\n",
      "Iteration 71, Batch: 32, Loss: 0.06982452422380447\n",
      "Iteration 71, Batch: 33, Loss: 0.1042911484837532\n",
      "Iteration 71, Batch: 34, Loss: 0.04781016334891319\n",
      "Iteration 71, Batch: 35, Loss: 0.0906599685549736\n",
      "Iteration 71, Batch: 36, Loss: 0.052223023027181625\n",
      "Iteration 71, Batch: 37, Loss: 0.071076899766922\n",
      "Iteration 71, Batch: 38, Loss: 0.059795401990413666\n",
      "Iteration 71, Batch: 39, Loss: 0.09414052963256836\n",
      "Iteration 71, Batch: 40, Loss: 0.08170783519744873\n",
      "Iteration 71, Batch: 41, Loss: 0.10364875197410583\n",
      "Iteration 71, Batch: 42, Loss: 0.11579930782318115\n",
      "Iteration 71, Batch: 43, Loss: 0.12132660299539566\n",
      "Iteration 71, Batch: 44, Loss: 0.06913554668426514\n",
      "Iteration 71, Batch: 45, Loss: 0.05903635919094086\n",
      "Iteration 71, Batch: 46, Loss: 0.09443556517362595\n",
      "Iteration 71, Batch: 47, Loss: 0.04255286604166031\n",
      "Iteration 71, Batch: 48, Loss: 0.10425364971160889\n",
      "Iteration 71, Batch: 49, Loss: 0.0690111443400383\n",
      "Iteration 72, Batch: 0, Loss: 0.06917552649974823\n",
      "Iteration 72, Batch: 1, Loss: 0.09039736539125443\n",
      "Iteration 72, Batch: 2, Loss: 0.10357789695262909\n",
      "Iteration 72, Batch: 3, Loss: 0.06447838246822357\n",
      "Iteration 72, Batch: 4, Loss: 0.08005136251449585\n",
      "Iteration 72, Batch: 5, Loss: 0.07828889787197113\n",
      "Iteration 72, Batch: 6, Loss: 0.05340465530753136\n",
      "Iteration 72, Batch: 7, Loss: 0.06308530271053314\n",
      "Iteration 72, Batch: 8, Loss: 0.07328303158283234\n",
      "Iteration 72, Batch: 9, Loss: 0.09354083985090256\n",
      "Iteration 72, Batch: 10, Loss: 0.08618798851966858\n",
      "Iteration 72, Batch: 11, Loss: 0.11934465914964676\n",
      "Iteration 72, Batch: 12, Loss: 0.0752711221575737\n",
      "Iteration 72, Batch: 13, Loss: 0.057635411620140076\n",
      "Iteration 72, Batch: 14, Loss: 0.10994530469179153\n",
      "Iteration 72, Batch: 15, Loss: 0.119000144302845\n",
      "Iteration 72, Batch: 16, Loss: 0.09944358468055725\n",
      "Iteration 72, Batch: 17, Loss: 0.08779140561819077\n",
      "Iteration 72, Batch: 18, Loss: 0.08196017891168594\n",
      "Iteration 72, Batch: 19, Loss: 0.06718259304761887\n",
      "Iteration 72, Batch: 20, Loss: 0.0710228979587555\n",
      "Iteration 72, Batch: 21, Loss: 0.08157400041818619\n",
      "Iteration 72, Batch: 22, Loss: 0.08798716217279434\n",
      "Iteration 72, Batch: 23, Loss: 0.05866221711039543\n",
      "Iteration 72, Batch: 24, Loss: 0.05571167916059494\n",
      "Iteration 72, Batch: 25, Loss: 0.09984803944826126\n",
      "Iteration 72, Batch: 26, Loss: 0.07034880667924881\n",
      "Iteration 72, Batch: 27, Loss: 0.07560540735721588\n",
      "Iteration 72, Batch: 28, Loss: 0.053336214274168015\n",
      "Iteration 72, Batch: 29, Loss: 0.07970407605171204\n",
      "Iteration 72, Batch: 30, Loss: 0.08411117643117905\n",
      "Iteration 72, Batch: 31, Loss: 0.07191580533981323\n",
      "Iteration 72, Batch: 32, Loss: 0.0622488372027874\n",
      "Iteration 72, Batch: 33, Loss: 0.0830581858754158\n",
      "Iteration 72, Batch: 34, Loss: 0.07551874220371246\n",
      "Iteration 72, Batch: 35, Loss: 0.06930584460496902\n",
      "Iteration 72, Batch: 36, Loss: 0.10880870372056961\n",
      "Iteration 72, Batch: 37, Loss: 0.09090457856655121\n",
      "Iteration 72, Batch: 38, Loss: 0.07582675665616989\n",
      "Iteration 72, Batch: 39, Loss: 0.062000855803489685\n",
      "Iteration 72, Batch: 40, Loss: 0.07287327945232391\n",
      "Iteration 72, Batch: 41, Loss: 0.08355015516281128\n",
      "Iteration 72, Batch: 42, Loss: 0.07878706604242325\n",
      "Iteration 72, Batch: 43, Loss: 0.06883152574300766\n",
      "Iteration 72, Batch: 44, Loss: 0.09442304074764252\n",
      "Iteration 72, Batch: 45, Loss: 0.07420751452445984\n",
      "Iteration 72, Batch: 46, Loss: 0.0636831521987915\n",
      "Iteration 72, Batch: 47, Loss: 0.09948436170816422\n",
      "Iteration 72, Batch: 48, Loss: 0.10132596641778946\n",
      "Iteration 72, Batch: 49, Loss: 0.09129664301872253\n",
      "Iteration 73, Batch: 0, Loss: 0.028594916686415672\n",
      "Iteration 73, Batch: 1, Loss: 0.07913777977228165\n",
      "Iteration 73, Batch: 2, Loss: 0.06364970654249191\n",
      "Iteration 73, Batch: 3, Loss: 0.09689036756753922\n",
      "Iteration 73, Batch: 4, Loss: 0.07970456779003143\n",
      "Iteration 73, Batch: 5, Loss: 0.053465984761714935\n",
      "Iteration 73, Batch: 6, Loss: 0.0941755473613739\n",
      "Iteration 73, Batch: 7, Loss: 0.03930719941854477\n",
      "Iteration 73, Batch: 8, Loss: 0.06605471670627594\n",
      "Iteration 73, Batch: 9, Loss: 0.07255885750055313\n",
      "Iteration 73, Batch: 10, Loss: 0.08329609781503677\n",
      "Iteration 73, Batch: 11, Loss: 0.05023276060819626\n",
      "Iteration 73, Batch: 12, Loss: 0.06760845333337784\n",
      "Iteration 73, Batch: 13, Loss: 0.08078370243310928\n",
      "Iteration 73, Batch: 14, Loss: 0.06044851616024971\n",
      "Iteration 73, Batch: 15, Loss: 0.08648265898227692\n",
      "Iteration 73, Batch: 16, Loss: 0.08162706345319748\n",
      "Iteration 73, Batch: 17, Loss: 0.07425426691770554\n",
      "Iteration 73, Batch: 18, Loss: 0.06867168098688126\n",
      "Iteration 73, Batch: 19, Loss: 0.07072994112968445\n",
      "Iteration 73, Batch: 20, Loss: 0.11518795043230057\n",
      "Iteration 73, Batch: 21, Loss: 0.07349405437707901\n",
      "Iteration 73, Batch: 22, Loss: 0.09083453565835953\n",
      "Iteration 73, Batch: 23, Loss: 0.07047206908464432\n",
      "Iteration 73, Batch: 24, Loss: 0.08305654674768448\n",
      "Iteration 73, Batch: 25, Loss: 0.06332927197217941\n",
      "Iteration 73, Batch: 26, Loss: 0.06453089416027069\n",
      "Iteration 73, Batch: 27, Loss: 0.0704113319516182\n",
      "Iteration 73, Batch: 28, Loss: 0.10445047169923782\n",
      "Iteration 73, Batch: 29, Loss: 0.062356919050216675\n",
      "Iteration 73, Batch: 30, Loss: 0.07365462929010391\n",
      "Iteration 73, Batch: 31, Loss: 0.09115564078092575\n",
      "Iteration 73, Batch: 32, Loss: 0.07056741416454315\n",
      "Iteration 73, Batch: 33, Loss: 0.08734144270420074\n",
      "Iteration 73, Batch: 34, Loss: 0.07075348496437073\n",
      "Iteration 73, Batch: 35, Loss: 0.08201534301042557\n",
      "Iteration 73, Batch: 36, Loss: 0.05051310732960701\n",
      "Iteration 73, Batch: 37, Loss: 0.07712596654891968\n",
      "Iteration 73, Batch: 38, Loss: 0.06554286181926727\n",
      "Iteration 73, Batch: 39, Loss: 0.06775199621915817\n",
      "Iteration 73, Batch: 40, Loss: 0.06934206187725067\n",
      "Iteration 73, Batch: 41, Loss: 0.06481023877859116\n",
      "Iteration 73, Batch: 42, Loss: 0.07413686811923981\n",
      "Iteration 73, Batch: 43, Loss: 0.09528320282697678\n",
      "Iteration 73, Batch: 44, Loss: 0.10958884656429291\n",
      "Iteration 73, Batch: 45, Loss: 0.06557078659534454\n",
      "Iteration 73, Batch: 46, Loss: 0.07055525481700897\n",
      "Iteration 73, Batch: 47, Loss: 0.10371621698141098\n",
      "Iteration 73, Batch: 48, Loss: 0.07228457927703857\n",
      "Iteration 73, Batch: 49, Loss: 0.06317081302404404\n",
      "Iteration 74, Batch: 0, Loss: 0.08964628726243973\n",
      "Iteration 74, Batch: 1, Loss: 0.06701002269983292\n",
      "Iteration 74, Batch: 2, Loss: 0.07156696915626526\n",
      "Iteration 74, Batch: 3, Loss: 0.07606719434261322\n",
      "Iteration 74, Batch: 4, Loss: 0.08883820474147797\n",
      "Iteration 74, Batch: 5, Loss: 0.0810079351067543\n",
      "Iteration 74, Batch: 6, Loss: 0.08350978791713715\n",
      "Iteration 74, Batch: 7, Loss: 0.129299595952034\n",
      "Iteration 74, Batch: 8, Loss: 0.06689437478780746\n",
      "Iteration 74, Batch: 9, Loss: 0.07626746594905853\n",
      "Iteration 74, Batch: 10, Loss: 0.09046217054128647\n",
      "Iteration 74, Batch: 11, Loss: 0.11502622067928314\n",
      "Iteration 74, Batch: 12, Loss: 0.09896040707826614\n",
      "Iteration 74, Batch: 13, Loss: 0.08432291448116302\n",
      "Iteration 74, Batch: 14, Loss: 0.0899823009967804\n",
      "Iteration 74, Batch: 15, Loss: 0.07536991685628891\n",
      "Iteration 74, Batch: 16, Loss: 0.05583588033914566\n",
      "Iteration 74, Batch: 17, Loss: 0.08376676589250565\n",
      "Iteration 74, Batch: 18, Loss: 0.07306578755378723\n",
      "Iteration 74, Batch: 19, Loss: 0.09167066216468811\n",
      "Iteration 74, Batch: 20, Loss: 0.07286571711301804\n",
      "Iteration 74, Batch: 21, Loss: 0.056359946727752686\n",
      "Iteration 74, Batch: 22, Loss: 0.07642924785614014\n",
      "Iteration 74, Batch: 23, Loss: 0.07536652684211731\n",
      "Iteration 74, Batch: 24, Loss: 0.0768469050526619\n",
      "Iteration 74, Batch: 25, Loss: 0.05595816671848297\n",
      "Iteration 74, Batch: 26, Loss: 0.0817428007721901\n",
      "Iteration 74, Batch: 27, Loss: 0.09827934205532074\n",
      "Iteration 74, Batch: 28, Loss: 0.10220790654420853\n",
      "Iteration 74, Batch: 29, Loss: 0.07073130458593369\n",
      "Iteration 74, Batch: 30, Loss: 0.09968066960573196\n",
      "Iteration 74, Batch: 31, Loss: 0.05828455835580826\n",
      "Iteration 74, Batch: 32, Loss: 0.061934079974889755\n",
      "Iteration 74, Batch: 33, Loss: 0.10905487090349197\n",
      "Iteration 74, Batch: 34, Loss: 0.07375259697437286\n",
      "Iteration 74, Batch: 35, Loss: 0.06713245064020157\n",
      "Iteration 74, Batch: 36, Loss: 0.058162350207567215\n",
      "Iteration 74, Batch: 37, Loss: 0.09407152235507965\n",
      "Iteration 74, Batch: 38, Loss: 0.06646113097667694\n",
      "Iteration 74, Batch: 39, Loss: 0.08402083814144135\n",
      "Iteration 74, Batch: 40, Loss: 0.08446381986141205\n",
      "Iteration 74, Batch: 41, Loss: 0.0702514797449112\n",
      "Iteration 74, Batch: 42, Loss: 0.06561630219221115\n",
      "Iteration 74, Batch: 43, Loss: 0.052337996661663055\n",
      "Iteration 74, Batch: 44, Loss: 0.06775631755590439\n",
      "Iteration 74, Batch: 45, Loss: 0.07074355334043503\n",
      "Iteration 74, Batch: 46, Loss: 0.0923009067773819\n",
      "Iteration 74, Batch: 47, Loss: 0.10597007721662521\n",
      "Iteration 74, Batch: 48, Loss: 0.08912598341703415\n",
      "Iteration 74, Batch: 49, Loss: 0.08022759109735489\n",
      "Iteration 75, Batch: 0, Loss: 0.07385475933551788\n",
      "Iteration 75, Batch: 1, Loss: 0.07314623892307281\n",
      "Iteration 75, Batch: 2, Loss: 0.05642576143145561\n",
      "Iteration 75, Batch: 3, Loss: 0.07350660115480423\n",
      "Iteration 75, Batch: 4, Loss: 0.10659154504537582\n",
      "Iteration 75, Batch: 5, Loss: 0.07103966921567917\n",
      "Iteration 75, Batch: 6, Loss: 0.0789574682712555\n",
      "Iteration 75, Batch: 7, Loss: 0.12257727980613708\n",
      "Iteration 75, Batch: 8, Loss: 0.09003203362226486\n",
      "Iteration 75, Batch: 9, Loss: 0.08252328634262085\n",
      "Iteration 75, Batch: 10, Loss: 0.08472810685634613\n",
      "Iteration 75, Batch: 11, Loss: 0.1035652682185173\n",
      "Iteration 75, Batch: 12, Loss: 0.08745181560516357\n",
      "Iteration 75, Batch: 13, Loss: 0.08190765231847763\n",
      "Iteration 75, Batch: 14, Loss: 0.07394222170114517\n",
      "Iteration 75, Batch: 15, Loss: 0.06958256661891937\n",
      "Iteration 75, Batch: 16, Loss: 0.08490011841058731\n",
      "Iteration 75, Batch: 17, Loss: 0.06879530847072601\n",
      "Iteration 75, Batch: 18, Loss: 0.05568134784698486\n",
      "Iteration 75, Batch: 19, Loss: 0.08055916428565979\n",
      "Iteration 75, Batch: 20, Loss: 0.056563641875982285\n",
      "Iteration 75, Batch: 21, Loss: 0.08449368178844452\n",
      "Iteration 75, Batch: 22, Loss: 0.10535536706447601\n",
      "Iteration 75, Batch: 23, Loss: 0.06295870989561081\n",
      "Iteration 75, Batch: 24, Loss: 0.07499611377716064\n",
      "Iteration 75, Batch: 25, Loss: 0.07075607776641846\n",
      "Iteration 75, Batch: 26, Loss: 0.07515336573123932\n",
      "Iteration 75, Batch: 27, Loss: 0.10160096734762192\n",
      "Iteration 75, Batch: 28, Loss: 0.07733605057001114\n",
      "Iteration 75, Batch: 29, Loss: 0.08800166845321655\n",
      "Iteration 75, Batch: 30, Loss: 0.07836826890707016\n",
      "Iteration 75, Batch: 31, Loss: 0.08407051116228104\n",
      "Iteration 75, Batch: 32, Loss: 0.10523493587970734\n",
      "Iteration 75, Batch: 33, Loss: 0.06481640040874481\n",
      "Iteration 75, Batch: 34, Loss: 0.04712804779410362\n",
      "Iteration 75, Batch: 35, Loss: 0.1135723814368248\n",
      "Iteration 75, Batch: 36, Loss: 0.05975598841905594\n",
      "Iteration 75, Batch: 37, Loss: 0.08598746359348297\n",
      "Iteration 75, Batch: 38, Loss: 0.10165474563837051\n",
      "Iteration 75, Batch: 39, Loss: 0.11324457079172134\n",
      "Iteration 75, Batch: 40, Loss: 0.0804200991988182\n",
      "Iteration 75, Batch: 41, Loss: 0.10324043035507202\n",
      "Iteration 75, Batch: 42, Loss: 0.058843519538640976\n",
      "Iteration 75, Batch: 43, Loss: 0.06957373023033142\n",
      "Iteration 75, Batch: 44, Loss: 0.06943619251251221\n",
      "Iteration 75, Batch: 45, Loss: 0.09688887745141983\n",
      "Iteration 75, Batch: 46, Loss: 0.08427856117486954\n",
      "Iteration 75, Batch: 47, Loss: 0.07857941091060638\n",
      "Iteration 75, Batch: 48, Loss: 0.10274522751569748\n",
      "Iteration 75, Batch: 49, Loss: 0.05578894168138504\n",
      "Iteration 76, Batch: 0, Loss: 0.09334412962198257\n",
      "Iteration 76, Batch: 1, Loss: 0.06330212205648422\n",
      "Iteration 76, Batch: 2, Loss: 0.08315137773752213\n",
      "Iteration 76, Batch: 3, Loss: 0.08516097068786621\n",
      "Iteration 76, Batch: 4, Loss: 0.06206921488046646\n",
      "Iteration 76, Batch: 5, Loss: 0.07507936656475067\n",
      "Iteration 76, Batch: 6, Loss: 0.07174823433160782\n",
      "Iteration 76, Batch: 7, Loss: 0.07308347523212433\n",
      "Iteration 76, Batch: 8, Loss: 0.07947755604982376\n",
      "Iteration 76, Batch: 9, Loss: 0.08988387882709503\n",
      "Iteration 76, Batch: 10, Loss: 0.0851578563451767\n",
      "Iteration 76, Batch: 11, Loss: 0.0653635784983635\n",
      "Iteration 76, Batch: 12, Loss: 0.09796496480703354\n",
      "Iteration 76, Batch: 13, Loss: 0.07213929295539856\n",
      "Iteration 76, Batch: 14, Loss: 0.06087058410048485\n",
      "Iteration 76, Batch: 15, Loss: 0.0844406709074974\n",
      "Iteration 76, Batch: 16, Loss: 0.07986491173505783\n",
      "Iteration 76, Batch: 17, Loss: 0.0994795486330986\n",
      "Iteration 76, Batch: 18, Loss: 0.08280693739652634\n",
      "Iteration 76, Batch: 19, Loss: 0.06379511207342148\n",
      "Iteration 76, Batch: 20, Loss: 0.08892469108104706\n",
      "Iteration 76, Batch: 21, Loss: 0.07597363740205765\n",
      "Iteration 76, Batch: 22, Loss: 0.06585513800382614\n",
      "Iteration 76, Batch: 23, Loss: 0.0943806990981102\n",
      "Iteration 76, Batch: 24, Loss: 0.0810583308339119\n",
      "Iteration 76, Batch: 25, Loss: 0.07032455503940582\n",
      "Iteration 76, Batch: 26, Loss: 0.09918766468763351\n",
      "Iteration 76, Batch: 27, Loss: 0.08497834950685501\n",
      "Iteration 76, Batch: 28, Loss: 0.052260879427194595\n",
      "Iteration 76, Batch: 29, Loss: 0.07188180834054947\n",
      "Iteration 76, Batch: 30, Loss: 0.09944527596235275\n",
      "Iteration 76, Batch: 31, Loss: 0.04900268465280533\n",
      "Iteration 76, Batch: 32, Loss: 0.13389717042446136\n",
      "Iteration 76, Batch: 33, Loss: 0.09430303424596786\n",
      "Iteration 76, Batch: 34, Loss: 0.09228808432817459\n",
      "Iteration 76, Batch: 35, Loss: 0.07315754145383835\n",
      "Iteration 76, Batch: 36, Loss: 0.06580854952335358\n",
      "Iteration 76, Batch: 37, Loss: 0.0708886906504631\n",
      "Iteration 76, Batch: 38, Loss: 0.07634307444095612\n",
      "Iteration 76, Batch: 39, Loss: 0.08662298321723938\n",
      "Iteration 76, Batch: 40, Loss: 0.09421441704034805\n",
      "Iteration 76, Batch: 41, Loss: 0.07377655059099197\n",
      "Iteration 76, Batch: 42, Loss: 0.062122657895088196\n",
      "Iteration 76, Batch: 43, Loss: 0.09119825065135956\n",
      "Iteration 76, Batch: 44, Loss: 0.08806440234184265\n",
      "Iteration 76, Batch: 45, Loss: 0.10881160199642181\n",
      "Iteration 76, Batch: 46, Loss: 0.06426937133073807\n",
      "Iteration 76, Batch: 47, Loss: 0.0678466334939003\n",
      "Iteration 76, Batch: 48, Loss: 0.07167506963014603\n",
      "Iteration 76, Batch: 49, Loss: 0.03524922952055931\n",
      "Iteration 77, Batch: 0, Loss: 0.051282405853271484\n",
      "Iteration 77, Batch: 1, Loss: 0.05682138353586197\n",
      "Iteration 77, Batch: 2, Loss: 0.07413969933986664\n",
      "Iteration 77, Batch: 3, Loss: 0.10880426317453384\n",
      "Iteration 77, Batch: 4, Loss: 0.11513783037662506\n",
      "Iteration 77, Batch: 5, Loss: 0.059784382581710815\n",
      "Iteration 77, Batch: 6, Loss: 0.09524267166852951\n",
      "Iteration 77, Batch: 7, Loss: 0.09044618159532547\n",
      "Iteration 77, Batch: 8, Loss: 0.06555936485528946\n",
      "Iteration 77, Batch: 9, Loss: 0.08634017407894135\n",
      "Iteration 77, Batch: 10, Loss: 0.06818297505378723\n",
      "Iteration 77, Batch: 11, Loss: 0.09506839513778687\n",
      "Iteration 77, Batch: 12, Loss: 0.0470987930893898\n",
      "Iteration 77, Batch: 13, Loss: 0.08098714053630829\n",
      "Iteration 77, Batch: 14, Loss: 0.10816197842359543\n",
      "Iteration 77, Batch: 15, Loss: 0.06706468760967255\n",
      "Iteration 77, Batch: 16, Loss: 0.06726561486721039\n",
      "Iteration 77, Batch: 17, Loss: 0.11720304191112518\n",
      "Iteration 77, Batch: 18, Loss: 0.0493980310857296\n",
      "Iteration 77, Batch: 19, Loss: 0.09640203416347504\n",
      "Iteration 77, Batch: 20, Loss: 0.05315261706709862\n",
      "Iteration 77, Batch: 21, Loss: 0.09524697810411453\n",
      "Iteration 77, Batch: 22, Loss: 0.06834175437688828\n",
      "Iteration 77, Batch: 23, Loss: 0.052468087524175644\n",
      "Iteration 77, Batch: 24, Loss: 0.07359479367733002\n",
      "Iteration 77, Batch: 25, Loss: 0.08138803392648697\n",
      "Iteration 77, Batch: 26, Loss: 0.09227931499481201\n",
      "Iteration 77, Batch: 27, Loss: 0.055555570870637894\n",
      "Iteration 77, Batch: 28, Loss: 0.044597238302230835\n",
      "Iteration 77, Batch: 29, Loss: 0.08723316341638565\n",
      "Iteration 77, Batch: 30, Loss: 0.07956083863973618\n",
      "Iteration 77, Batch: 31, Loss: 0.09950046241283417\n",
      "Iteration 77, Batch: 32, Loss: 0.07937081903219223\n",
      "Iteration 77, Batch: 33, Loss: 0.10453477501869202\n",
      "Iteration 77, Batch: 34, Loss: 0.06389349699020386\n",
      "Iteration 77, Batch: 35, Loss: 0.09817665815353394\n",
      "Iteration 77, Batch: 36, Loss: 0.05767068639397621\n",
      "Iteration 77, Batch: 37, Loss: 0.09328366816043854\n",
      "Iteration 77, Batch: 38, Loss: 0.09333238750696182\n",
      "Iteration 77, Batch: 39, Loss: 0.08069133758544922\n",
      "Iteration 77, Batch: 40, Loss: 0.08113586157560349\n",
      "Iteration 77, Batch: 41, Loss: 0.08673280477523804\n",
      "Iteration 77, Batch: 42, Loss: 0.06482835859060287\n",
      "Iteration 77, Batch: 43, Loss: 0.06330351531505585\n",
      "Iteration 77, Batch: 44, Loss: 0.06615889817476273\n",
      "Iteration 77, Batch: 45, Loss: 0.09151424467563629\n",
      "Iteration 77, Batch: 46, Loss: 0.06688192486763\n",
      "Iteration 77, Batch: 47, Loss: 0.08096698671579361\n",
      "Iteration 77, Batch: 48, Loss: 0.0849660262465477\n",
      "Iteration 77, Batch: 49, Loss: 0.09273733198642731\n",
      "Iteration 78, Batch: 0, Loss: 0.05759360268712044\n",
      "Iteration 78, Batch: 1, Loss: 0.06336390972137451\n",
      "Iteration 78, Batch: 2, Loss: 0.08169467747211456\n",
      "Iteration 78, Batch: 3, Loss: 0.07128534466028214\n",
      "Iteration 78, Batch: 4, Loss: 0.11051294207572937\n",
      "Iteration 78, Batch: 5, Loss: 0.08068469911813736\n",
      "Iteration 78, Batch: 6, Loss: 0.07061008363962173\n",
      "Iteration 78, Batch: 7, Loss: 0.08355682343244553\n",
      "Iteration 78, Batch: 8, Loss: 0.11324004828929901\n",
      "Iteration 78, Batch: 9, Loss: 0.09296544641256332\n",
      "Iteration 78, Batch: 10, Loss: 0.08958885818719864\n",
      "Iteration 78, Batch: 11, Loss: 0.07705924659967422\n",
      "Iteration 78, Batch: 12, Loss: 0.0868549719452858\n",
      "Iteration 78, Batch: 13, Loss: 0.09548041969537735\n",
      "Iteration 78, Batch: 14, Loss: 0.07718107849359512\n",
      "Iteration 78, Batch: 15, Loss: 0.07162769138813019\n",
      "Iteration 78, Batch: 16, Loss: 0.09274183958768845\n",
      "Iteration 78, Batch: 17, Loss: 0.09926217794418335\n",
      "Iteration 78, Batch: 18, Loss: 0.07232251763343811\n",
      "Iteration 78, Batch: 19, Loss: 0.07873143255710602\n",
      "Iteration 78, Batch: 20, Loss: 0.08245132863521576\n",
      "Iteration 78, Batch: 21, Loss: 0.07379333674907684\n",
      "Iteration 78, Batch: 22, Loss: 0.07175263017416\n",
      "Iteration 78, Batch: 23, Loss: 0.08285573124885559\n",
      "Iteration 78, Batch: 24, Loss: 0.04873768240213394\n",
      "Iteration 78, Batch: 25, Loss: 0.051613014191389084\n",
      "Iteration 78, Batch: 26, Loss: 0.07108115404844284\n",
      "Iteration 78, Batch: 27, Loss: 0.04933625087141991\n",
      "Iteration 78, Batch: 28, Loss: 0.0911475196480751\n",
      "Iteration 78, Batch: 29, Loss: 0.07138903439044952\n",
      "Iteration 78, Batch: 30, Loss: 0.07735635340213776\n",
      "Iteration 78, Batch: 31, Loss: 0.07528579235076904\n",
      "Iteration 78, Batch: 32, Loss: 0.06610295921564102\n",
      "Iteration 78, Batch: 33, Loss: 0.09339448809623718\n",
      "Iteration 78, Batch: 34, Loss: 0.05678851902484894\n",
      "Iteration 78, Batch: 35, Loss: 0.06344323605298996\n",
      "Iteration 78, Batch: 36, Loss: 0.05124421790242195\n",
      "Iteration 78, Batch: 37, Loss: 0.08076774328947067\n",
      "Iteration 78, Batch: 38, Loss: 0.0867268368601799\n",
      "Iteration 78, Batch: 39, Loss: 0.06309414654970169\n",
      "Iteration 78, Batch: 40, Loss: 0.08462733775377274\n",
      "Iteration 78, Batch: 41, Loss: 0.06515034288167953\n",
      "Iteration 78, Batch: 42, Loss: 0.06739571690559387\n",
      "Iteration 78, Batch: 43, Loss: 0.056854505091905594\n",
      "Iteration 78, Batch: 44, Loss: 0.10127376019954681\n",
      "Iteration 78, Batch: 45, Loss: 0.05295416712760925\n",
      "Iteration 78, Batch: 46, Loss: 0.06601919233798981\n",
      "Iteration 78, Batch: 47, Loss: 0.061531610786914825\n",
      "Iteration 78, Batch: 48, Loss: 0.06598320603370667\n",
      "Iteration 78, Batch: 49, Loss: 0.08992154896259308\n",
      "Iteration 79, Batch: 0, Loss: 0.06982362270355225\n",
      "Iteration 79, Batch: 1, Loss: 0.08344510197639465\n",
      "Iteration 79, Batch: 2, Loss: 0.049760397523641586\n",
      "Iteration 79, Batch: 3, Loss: 0.08050420880317688\n",
      "Iteration 79, Batch: 4, Loss: 0.05630922317504883\n",
      "Iteration 79, Batch: 5, Loss: 0.060466162860393524\n",
      "Iteration 79, Batch: 6, Loss: 0.06107049807906151\n",
      "Iteration 79, Batch: 7, Loss: 0.07162364572286606\n",
      "Iteration 79, Batch: 8, Loss: 0.09165268391370773\n",
      "Iteration 79, Batch: 9, Loss: 0.07406742870807648\n",
      "Iteration 79, Batch: 10, Loss: 0.05638029798865318\n",
      "Iteration 79, Batch: 11, Loss: 0.08833099156618118\n",
      "Iteration 79, Batch: 12, Loss: 0.08840028196573257\n",
      "Iteration 79, Batch: 13, Loss: 0.06060277670621872\n",
      "Iteration 79, Batch: 14, Loss: 0.07023073732852936\n",
      "Iteration 79, Batch: 15, Loss: 0.08546730130910873\n",
      "Iteration 79, Batch: 16, Loss: 0.05630181357264519\n",
      "Iteration 79, Batch: 17, Loss: 0.07558037340641022\n",
      "Iteration 79, Batch: 18, Loss: 0.07669411599636078\n",
      "Iteration 79, Batch: 19, Loss: 0.08166822046041489\n",
      "Iteration 79, Batch: 20, Loss: 0.07853157818317413\n",
      "Iteration 79, Batch: 21, Loss: 0.04625768959522247\n",
      "Iteration 79, Batch: 22, Loss: 0.08755837380886078\n",
      "Iteration 79, Batch: 23, Loss: 0.06965630501508713\n",
      "Iteration 79, Batch: 24, Loss: 0.06308767944574356\n",
      "Iteration 79, Batch: 25, Loss: 0.08446034044027328\n",
      "Iteration 79, Batch: 26, Loss: 0.10120730847120285\n",
      "Iteration 79, Batch: 27, Loss: 0.07713370770215988\n",
      "Iteration 79, Batch: 28, Loss: 0.09519986063241959\n",
      "Iteration 79, Batch: 29, Loss: 0.07811768352985382\n",
      "Iteration 79, Batch: 30, Loss: 0.06617068499326706\n",
      "Iteration 79, Batch: 31, Loss: 0.054728832095861435\n",
      "Iteration 79, Batch: 32, Loss: 0.1006079614162445\n",
      "Iteration 79, Batch: 33, Loss: 0.060210082679986954\n",
      "Iteration 79, Batch: 34, Loss: 0.0706956535577774\n",
      "Iteration 79, Batch: 35, Loss: 0.0841909870505333\n",
      "Iteration 79, Batch: 36, Loss: 0.07901863008737564\n",
      "Iteration 79, Batch: 37, Loss: 0.0558050200343132\n",
      "Iteration 79, Batch: 38, Loss: 0.040781859308481216\n",
      "Iteration 79, Batch: 39, Loss: 0.07532119750976562\n",
      "Iteration 79, Batch: 40, Loss: 0.06420905888080597\n",
      "Iteration 79, Batch: 41, Loss: 0.05303865298628807\n",
      "Iteration 79, Batch: 42, Loss: 0.07204988598823547\n",
      "Iteration 79, Batch: 43, Loss: 0.056666526943445206\n",
      "Iteration 79, Batch: 44, Loss: 0.06008705869317055\n",
      "Iteration 79, Batch: 45, Loss: 0.08313556760549545\n",
      "Iteration 79, Batch: 46, Loss: 0.06825501471757889\n",
      "Iteration 79, Batch: 47, Loss: 0.05797852203249931\n",
      "Iteration 79, Batch: 48, Loss: 0.05242919921875\n",
      "Iteration 79, Batch: 49, Loss: 0.06801716983318329\n",
      "Iteration 80, Batch: 0, Loss: 0.07340352982282639\n",
      "Iteration 80, Batch: 1, Loss: 0.07019466906785965\n",
      "Iteration 80, Batch: 2, Loss: 0.0401076003909111\n",
      "Iteration 80, Batch: 3, Loss: 0.063299261033535\n",
      "Iteration 80, Batch: 4, Loss: 0.06359981000423431\n",
      "Iteration 80, Batch: 5, Loss: 0.0572735071182251\n",
      "Iteration 80, Batch: 6, Loss: 0.08705794811248779\n",
      "Iteration 80, Batch: 7, Loss: 0.10368852317333221\n",
      "Iteration 80, Batch: 8, Loss: 0.07997596263885498\n",
      "Iteration 80, Batch: 9, Loss: 0.04381797835230827\n",
      "Iteration 80, Batch: 10, Loss: 0.07906033098697662\n",
      "Iteration 80, Batch: 11, Loss: 0.06257029622793198\n",
      "Iteration 80, Batch: 12, Loss: 0.059130504727363586\n",
      "Iteration 80, Batch: 13, Loss: 0.07577153295278549\n",
      "Iteration 80, Batch: 14, Loss: 0.06543485075235367\n",
      "Iteration 80, Batch: 15, Loss: 0.07133606821298599\n",
      "Iteration 80, Batch: 16, Loss: 0.071525439620018\n",
      "Iteration 80, Batch: 17, Loss: 0.07074013352394104\n",
      "Iteration 80, Batch: 18, Loss: 0.06369280070066452\n",
      "Iteration 80, Batch: 19, Loss: 0.0865119993686676\n",
      "Iteration 80, Batch: 20, Loss: 0.05449869856238365\n",
      "Iteration 80, Batch: 21, Loss: 0.05809631571173668\n",
      "Iteration 80, Batch: 22, Loss: 0.031470101326704025\n",
      "Iteration 80, Batch: 23, Loss: 0.039591655135154724\n",
      "Iteration 80, Batch: 24, Loss: 0.05298781394958496\n",
      "Iteration 80, Batch: 25, Loss: 0.05256004258990288\n",
      "Iteration 80, Batch: 26, Loss: 0.05832236260175705\n",
      "Iteration 80, Batch: 27, Loss: 0.05670681968331337\n",
      "Iteration 80, Batch: 28, Loss: 0.03892988711595535\n",
      "Iteration 80, Batch: 29, Loss: 0.03820264711976051\n",
      "Iteration 80, Batch: 30, Loss: 0.0468195416033268\n",
      "Iteration 80, Batch: 31, Loss: 0.0696587860584259\n",
      "Iteration 80, Batch: 32, Loss: 0.04478446766734123\n",
      "Iteration 80, Batch: 33, Loss: 0.056635159999132156\n",
      "Iteration 80, Batch: 34, Loss: 0.058735817670822144\n",
      "Iteration 80, Batch: 35, Loss: 0.028822006657719612\n",
      "Iteration 80, Batch: 36, Loss: 0.060288090258836746\n",
      "Iteration 80, Batch: 37, Loss: 0.06293439865112305\n",
      "Iteration 80, Batch: 38, Loss: 0.046673696488142014\n",
      "Iteration 80, Batch: 39, Loss: 0.07747414708137512\n",
      "Iteration 80, Batch: 40, Loss: 0.04222121462225914\n",
      "Iteration 80, Batch: 41, Loss: 0.05647065117955208\n",
      "Iteration 80, Batch: 42, Loss: 0.07637334614992142\n",
      "Iteration 80, Batch: 43, Loss: 0.06204040348529816\n",
      "Iteration 80, Batch: 44, Loss: 0.0538165457546711\n",
      "Iteration 80, Batch: 45, Loss: 0.025300005450844765\n",
      "Iteration 80, Batch: 46, Loss: 0.058811724185943604\n",
      "Iteration 80, Batch: 47, Loss: 0.043184299021959305\n",
      "Iteration 80, Batch: 48, Loss: 0.061625756323337555\n",
      "Iteration 80, Batch: 49, Loss: 0.05496266111731529\n",
      "Iteration 81, Batch: 0, Loss: 0.07380356639623642\n",
      "Iteration 81, Batch: 1, Loss: 0.04504914954304695\n",
      "Iteration 81, Batch: 2, Loss: 0.07588262110948563\n",
      "Iteration 81, Batch: 3, Loss: 0.05836859717965126\n",
      "Iteration 81, Batch: 4, Loss: 0.056955721229314804\n",
      "Iteration 81, Batch: 5, Loss: 0.06084052845835686\n",
      "Iteration 81, Batch: 6, Loss: 0.08484169095754623\n",
      "Iteration 81, Batch: 7, Loss: 0.05776830390095711\n",
      "Iteration 81, Batch: 8, Loss: 0.07003781944513321\n",
      "Iteration 81, Batch: 9, Loss: 0.05634965002536774\n",
      "Iteration 81, Batch: 10, Loss: 0.05650348216295242\n",
      "Iteration 81, Batch: 11, Loss: 0.04699269309639931\n",
      "Iteration 81, Batch: 12, Loss: 0.07644709944725037\n",
      "Iteration 81, Batch: 13, Loss: 0.07449741661548615\n",
      "Iteration 81, Batch: 14, Loss: 0.04882029443979263\n",
      "Iteration 81, Batch: 15, Loss: 0.06104331836104393\n",
      "Iteration 81, Batch: 16, Loss: 0.0662413239479065\n",
      "Iteration 81, Batch: 17, Loss: 0.06699255853891373\n",
      "Iteration 81, Batch: 18, Loss: 0.058041464537382126\n",
      "Iteration 81, Batch: 19, Loss: 0.05824998766183853\n",
      "Iteration 81, Batch: 20, Loss: 0.06560761481523514\n",
      "Iteration 81, Batch: 21, Loss: 0.0845632553100586\n",
      "Iteration 81, Batch: 22, Loss: 0.05250873416662216\n",
      "Iteration 81, Batch: 23, Loss: 0.061635568737983704\n",
      "Iteration 81, Batch: 24, Loss: 0.05175887793302536\n",
      "Iteration 81, Batch: 25, Loss: 0.06850799173116684\n",
      "Iteration 81, Batch: 26, Loss: 0.06006629765033722\n",
      "Iteration 81, Batch: 27, Loss: 0.05138057470321655\n",
      "Iteration 81, Batch: 28, Loss: 0.08272063732147217\n",
      "Iteration 81, Batch: 29, Loss: 0.05659358203411102\n",
      "Iteration 81, Batch: 30, Loss: 0.04340355843305588\n",
      "Iteration 81, Batch: 31, Loss: 0.060480937361717224\n",
      "Iteration 81, Batch: 32, Loss: 0.08098383992910385\n",
      "Iteration 81, Batch: 33, Loss: 0.07744789123535156\n",
      "Iteration 81, Batch: 34, Loss: 0.04476241394877434\n",
      "Iteration 81, Batch: 35, Loss: 0.059768591076135635\n",
      "Iteration 81, Batch: 36, Loss: 0.05119319260120392\n",
      "Iteration 81, Batch: 37, Loss: 0.06699272245168686\n",
      "Iteration 81, Batch: 38, Loss: 0.06233685463666916\n",
      "Iteration 81, Batch: 39, Loss: 0.0820479542016983\n",
      "Iteration 81, Batch: 40, Loss: 0.0617348849773407\n",
      "Iteration 81, Batch: 41, Loss: 0.07204283773899078\n",
      "Iteration 81, Batch: 42, Loss: 0.054765839129686356\n",
      "Iteration 81, Batch: 43, Loss: 0.08094141632318497\n",
      "Iteration 81, Batch: 44, Loss: 0.06127912923693657\n",
      "Iteration 81, Batch: 45, Loss: 0.07360982894897461\n",
      "Iteration 81, Batch: 46, Loss: 0.04885946586728096\n",
      "Iteration 81, Batch: 47, Loss: 0.05090891569852829\n",
      "Iteration 81, Batch: 48, Loss: 0.05478333681821823\n",
      "Iteration 81, Batch: 49, Loss: 0.025396691635251045\n",
      "Iteration 82, Batch: 0, Loss: 0.029435286298394203\n",
      "Iteration 82, Batch: 1, Loss: 0.05529969558119774\n",
      "Iteration 82, Batch: 2, Loss: 0.06285341829061508\n",
      "Iteration 82, Batch: 3, Loss: 0.03143564239144325\n",
      "Iteration 82, Batch: 4, Loss: 0.06284617632627487\n",
      "Iteration 82, Batch: 5, Loss: 0.04590865969657898\n",
      "Iteration 82, Batch: 6, Loss: 0.04940013960003853\n",
      "Iteration 82, Batch: 7, Loss: 0.060189131647348404\n",
      "Iteration 82, Batch: 8, Loss: 0.05482304468750954\n",
      "Iteration 82, Batch: 9, Loss: 0.09906288981437683\n",
      "Iteration 82, Batch: 10, Loss: 0.06433205306529999\n",
      "Iteration 82, Batch: 11, Loss: 0.055060166865587234\n",
      "Iteration 82, Batch: 12, Loss: 0.07046636193990707\n",
      "Iteration 82, Batch: 13, Loss: 0.06924143433570862\n",
      "Iteration 82, Batch: 14, Loss: 0.07690642029047012\n",
      "Iteration 82, Batch: 15, Loss: 0.055344998836517334\n",
      "Iteration 82, Batch: 16, Loss: 0.05840892717242241\n",
      "Iteration 82, Batch: 17, Loss: 0.050216540694236755\n",
      "Iteration 82, Batch: 18, Loss: 0.08731045573949814\n",
      "Iteration 82, Batch: 19, Loss: 0.057592008262872696\n",
      "Iteration 82, Batch: 20, Loss: 0.045305680483579636\n",
      "Iteration 82, Batch: 21, Loss: 0.08827085047960281\n",
      "Iteration 82, Batch: 22, Loss: 0.054531607776880264\n",
      "Iteration 82, Batch: 23, Loss: 0.07848910242319107\n",
      "Iteration 82, Batch: 24, Loss: 0.05623851716518402\n",
      "Iteration 82, Batch: 25, Loss: 0.05340869724750519\n",
      "Iteration 82, Batch: 26, Loss: 0.06858325749635696\n",
      "Iteration 82, Batch: 27, Loss: 0.06638045608997345\n",
      "Iteration 82, Batch: 28, Loss: 0.052438534796237946\n",
      "Iteration 82, Batch: 29, Loss: 0.04748321324586868\n",
      "Iteration 82, Batch: 30, Loss: 0.054161787033081055\n",
      "Iteration 82, Batch: 31, Loss: 0.06702578812837601\n",
      "Iteration 82, Batch: 32, Loss: 0.09381481260061264\n",
      "Iteration 82, Batch: 33, Loss: 0.0568050853908062\n",
      "Iteration 82, Batch: 34, Loss: 0.06596867740154266\n",
      "Iteration 82, Batch: 35, Loss: 0.0526878647506237\n",
      "Iteration 82, Batch: 36, Loss: 0.04173176363110542\n",
      "Iteration 82, Batch: 37, Loss: 0.04869069159030914\n",
      "Iteration 82, Batch: 38, Loss: 0.061758432537317276\n",
      "Iteration 82, Batch: 39, Loss: 0.059341005980968475\n",
      "Iteration 82, Batch: 40, Loss: 0.05398249626159668\n",
      "Iteration 82, Batch: 41, Loss: 0.0899796262383461\n",
      "Iteration 82, Batch: 42, Loss: 0.058375027030706406\n",
      "Iteration 82, Batch: 43, Loss: 0.05196644738316536\n",
      "Iteration 82, Batch: 44, Loss: 0.049556151032447815\n",
      "Iteration 82, Batch: 45, Loss: 0.04091441631317139\n",
      "Iteration 82, Batch: 46, Loss: 0.04006550833582878\n",
      "Iteration 82, Batch: 47, Loss: 0.04966489598155022\n",
      "Iteration 82, Batch: 48, Loss: 0.04902473837137222\n",
      "Iteration 82, Batch: 49, Loss: 0.0765240266919136\n",
      "Iteration 83, Batch: 0, Loss: 0.04814673215150833\n",
      "Iteration 83, Batch: 1, Loss: 0.09251364320516586\n",
      "Iteration 83, Batch: 2, Loss: 0.05496456101536751\n",
      "Iteration 83, Batch: 3, Loss: 0.043742019683122635\n",
      "Iteration 83, Batch: 4, Loss: 0.04312243312597275\n",
      "Iteration 83, Batch: 5, Loss: 0.04394512623548508\n",
      "Iteration 83, Batch: 6, Loss: 0.05427490919828415\n",
      "Iteration 83, Batch: 7, Loss: 0.03530741482973099\n",
      "Iteration 83, Batch: 8, Loss: 0.03618055582046509\n",
      "Iteration 83, Batch: 9, Loss: 0.07610486447811127\n",
      "Iteration 83, Batch: 10, Loss: 0.06754297763109207\n",
      "Iteration 83, Batch: 11, Loss: 0.07690120488405228\n",
      "Iteration 83, Batch: 12, Loss: 0.06506166607141495\n",
      "Iteration 83, Batch: 13, Loss: 0.09023074805736542\n",
      "Iteration 83, Batch: 14, Loss: 0.039182454347610474\n",
      "Iteration 83, Batch: 15, Loss: 0.04561438411474228\n",
      "Iteration 83, Batch: 16, Loss: 0.06502347439527512\n",
      "Iteration 83, Batch: 17, Loss: 0.05429697781801224\n",
      "Iteration 83, Batch: 18, Loss: 0.04186303913593292\n",
      "Iteration 83, Batch: 19, Loss: 0.07425612956285477\n",
      "Iteration 83, Batch: 20, Loss: 0.056454699486494064\n",
      "Iteration 83, Batch: 21, Loss: 0.07512344419956207\n",
      "Iteration 83, Batch: 22, Loss: 0.044114675372838974\n",
      "Iteration 83, Batch: 23, Loss: 0.06004339084029198\n",
      "Iteration 83, Batch: 24, Loss: 0.028371276333928108\n",
      "Iteration 83, Batch: 25, Loss: 0.05249781161546707\n",
      "Iteration 83, Batch: 26, Loss: 0.0657050684094429\n",
      "Iteration 83, Batch: 27, Loss: 0.04840869456529617\n",
      "Iteration 83, Batch: 28, Loss: 0.07447884976863861\n",
      "Iteration 83, Batch: 29, Loss: 0.0717746913433075\n",
      "Iteration 83, Batch: 30, Loss: 0.07798736542463303\n",
      "Iteration 83, Batch: 31, Loss: 0.05305168405175209\n",
      "Iteration 83, Batch: 32, Loss: 0.07540712505578995\n",
      "Iteration 83, Batch: 33, Loss: 0.06185271218419075\n",
      "Iteration 83, Batch: 34, Loss: 0.03980772942304611\n",
      "Iteration 83, Batch: 35, Loss: 0.07382462173700333\n",
      "Iteration 83, Batch: 36, Loss: 0.03593771532177925\n",
      "Iteration 83, Batch: 37, Loss: 0.04845155030488968\n",
      "Iteration 83, Batch: 38, Loss: 0.05196624621748924\n",
      "Iteration 83, Batch: 39, Loss: 0.06788178533315659\n",
      "Iteration 83, Batch: 40, Loss: 0.06041281670331955\n",
      "Iteration 83, Batch: 41, Loss: 0.0572887659072876\n",
      "Iteration 83, Batch: 42, Loss: 0.045447319746017456\n",
      "Iteration 83, Batch: 43, Loss: 0.049758005887269974\n",
      "Iteration 83, Batch: 44, Loss: 0.055600956082344055\n",
      "Iteration 83, Batch: 45, Loss: 0.06226889789104462\n",
      "Iteration 83, Batch: 46, Loss: 0.04733586311340332\n",
      "Iteration 83, Batch: 47, Loss: 0.07299746572971344\n",
      "Iteration 83, Batch: 48, Loss: 0.04709206521511078\n",
      "Iteration 83, Batch: 49, Loss: 0.04433482512831688\n",
      "Iteration 84, Batch: 0, Loss: 0.05596045032143593\n",
      "Iteration 84, Batch: 1, Loss: 0.028438586741685867\n",
      "Iteration 84, Batch: 2, Loss: 0.0698537528514862\n",
      "Iteration 84, Batch: 3, Loss: 0.0543292835354805\n",
      "Iteration 84, Batch: 4, Loss: 0.04554560407996178\n",
      "Iteration 84, Batch: 5, Loss: 0.04789462313055992\n",
      "Iteration 84, Batch: 6, Loss: 0.07124198973178864\n",
      "Iteration 84, Batch: 7, Loss: 0.06055941432714462\n",
      "Iteration 84, Batch: 8, Loss: 0.06108058616518974\n",
      "Iteration 84, Batch: 9, Loss: 0.06811779737472534\n",
      "Iteration 84, Batch: 10, Loss: 0.042345549911260605\n",
      "Iteration 84, Batch: 11, Loss: 0.0295842457562685\n",
      "Iteration 84, Batch: 12, Loss: 0.040756091475486755\n",
      "Iteration 84, Batch: 13, Loss: 0.05204662308096886\n",
      "Iteration 84, Batch: 14, Loss: 0.05242715775966644\n",
      "Iteration 84, Batch: 15, Loss: 0.048755958676338196\n",
      "Iteration 84, Batch: 16, Loss: 0.0595020167529583\n",
      "Iteration 84, Batch: 17, Loss: 0.04593582823872566\n",
      "Iteration 84, Batch: 18, Loss: 0.044099535793066025\n",
      "Iteration 84, Batch: 19, Loss: 0.04700431600213051\n",
      "Iteration 84, Batch: 20, Loss: 0.047559697180986404\n",
      "Iteration 84, Batch: 21, Loss: 0.055198267102241516\n",
      "Iteration 84, Batch: 22, Loss: 0.03231656178832054\n",
      "Iteration 84, Batch: 23, Loss: 0.04503428190946579\n",
      "Iteration 84, Batch: 24, Loss: 0.038146521896123886\n",
      "Iteration 84, Batch: 25, Loss: 0.06694606691598892\n",
      "Iteration 84, Batch: 26, Loss: 0.0416109599173069\n",
      "Iteration 84, Batch: 27, Loss: 0.07002538442611694\n",
      "Iteration 84, Batch: 28, Loss: 0.07874701172113419\n",
      "Iteration 84, Batch: 29, Loss: 0.06147617846727371\n",
      "Iteration 84, Batch: 30, Loss: 0.054685041308403015\n",
      "Iteration 84, Batch: 31, Loss: 0.05076868087053299\n",
      "Iteration 84, Batch: 32, Loss: 0.04150613397359848\n",
      "Iteration 84, Batch: 33, Loss: 0.04634285345673561\n",
      "Iteration 84, Batch: 34, Loss: 0.039331600069999695\n",
      "Iteration 84, Batch: 35, Loss: 0.06110562011599541\n",
      "Iteration 84, Batch: 36, Loss: 0.03464902192354202\n",
      "Iteration 84, Batch: 37, Loss: 0.05831452086567879\n",
      "Iteration 84, Batch: 38, Loss: 0.06050001084804535\n",
      "Iteration 84, Batch: 39, Loss: 0.06521826237440109\n",
      "Iteration 84, Batch: 40, Loss: 0.05007551982998848\n",
      "Iteration 84, Batch: 41, Loss: 0.05367491766810417\n",
      "Iteration 84, Batch: 42, Loss: 0.07302706688642502\n",
      "Iteration 84, Batch: 43, Loss: 0.05075328052043915\n",
      "Iteration 84, Batch: 44, Loss: 0.06329130381345749\n",
      "Iteration 84, Batch: 45, Loss: 0.06818318367004395\n",
      "Iteration 84, Batch: 46, Loss: 0.05701429769396782\n",
      "Iteration 84, Batch: 47, Loss: 0.04568333923816681\n",
      "Iteration 84, Batch: 48, Loss: 0.055886149406433105\n",
      "Iteration 84, Batch: 49, Loss: 0.06756354123353958\n",
      "Iteration 85, Batch: 0, Loss: 0.07334182411432266\n",
      "Iteration 85, Batch: 1, Loss: 0.07217427343130112\n",
      "Iteration 85, Batch: 2, Loss: 0.06583397090435028\n",
      "Iteration 85, Batch: 3, Loss: 0.0669732466340065\n",
      "Iteration 85, Batch: 4, Loss: 0.03538382425904274\n",
      "Iteration 85, Batch: 5, Loss: 0.06195962801575661\n",
      "Iteration 85, Batch: 6, Loss: 0.05719808489084244\n",
      "Iteration 85, Batch: 7, Loss: 0.053381290286779404\n",
      "Iteration 85, Batch: 8, Loss: 0.056199099868535995\n",
      "Iteration 85, Batch: 9, Loss: 0.06135900318622589\n",
      "Iteration 85, Batch: 10, Loss: 0.058405350893735886\n",
      "Iteration 85, Batch: 11, Loss: 0.047769274562597275\n",
      "Iteration 85, Batch: 12, Loss: 0.054086726158857346\n",
      "Iteration 85, Batch: 13, Loss: 0.06936726719141006\n",
      "Iteration 85, Batch: 14, Loss: 0.05582262948155403\n",
      "Iteration 85, Batch: 15, Loss: 0.0733628123998642\n",
      "Iteration 85, Batch: 16, Loss: 0.05478619411587715\n",
      "Iteration 85, Batch: 17, Loss: 0.05094822868704796\n",
      "Iteration 85, Batch: 18, Loss: 0.041091661900281906\n",
      "Iteration 85, Batch: 19, Loss: 0.02753472328186035\n",
      "Iteration 85, Batch: 20, Loss: 0.07428698986768723\n",
      "Iteration 85, Batch: 21, Loss: 0.0409960076212883\n",
      "Iteration 85, Batch: 22, Loss: 0.03295159339904785\n",
      "Iteration 85, Batch: 23, Loss: 0.03701711446046829\n",
      "Iteration 85, Batch: 24, Loss: 0.04009953513741493\n",
      "Iteration 85, Batch: 25, Loss: 0.06700922548770905\n",
      "Iteration 85, Batch: 26, Loss: 0.025740696117281914\n",
      "Iteration 85, Batch: 27, Loss: 0.05564834922552109\n",
      "Iteration 85, Batch: 28, Loss: 0.05393734946846962\n",
      "Iteration 85, Batch: 29, Loss: 0.04404272884130478\n",
      "Iteration 85, Batch: 30, Loss: 0.04606952518224716\n",
      "Iteration 85, Batch: 31, Loss: 0.05402042344212532\n",
      "Iteration 85, Batch: 32, Loss: 0.03949645161628723\n",
      "Iteration 85, Batch: 33, Loss: 0.0765799880027771\n",
      "Iteration 85, Batch: 34, Loss: 0.05402270704507828\n",
      "Iteration 85, Batch: 35, Loss: 0.04569367691874504\n",
      "Iteration 85, Batch: 36, Loss: 0.057892296463251114\n",
      "Iteration 85, Batch: 37, Loss: 0.062664695084095\n",
      "Iteration 85, Batch: 38, Loss: 0.09696552157402039\n",
      "Iteration 85, Batch: 39, Loss: 0.05940469726920128\n",
      "Iteration 85, Batch: 40, Loss: 0.06409545987844467\n",
      "Iteration 85, Batch: 41, Loss: 0.04893117770552635\n",
      "Iteration 85, Batch: 42, Loss: 0.07200973480939865\n",
      "Iteration 85, Batch: 43, Loss: 0.054814908653497696\n",
      "Iteration 85, Batch: 44, Loss: 0.04505505412817001\n",
      "Iteration 85, Batch: 45, Loss: 0.034124281257390976\n",
      "Iteration 85, Batch: 46, Loss: 0.038661565631628036\n",
      "Iteration 85, Batch: 47, Loss: 0.04148661345243454\n",
      "Iteration 85, Batch: 48, Loss: 0.08350268006324768\n",
      "Iteration 85, Batch: 49, Loss: 0.051732633262872696\n",
      "Iteration 86, Batch: 0, Loss: 0.0499207004904747\n",
      "Iteration 86, Batch: 1, Loss: 0.057637471705675125\n",
      "Iteration 86, Batch: 2, Loss: 0.05199797451496124\n",
      "Iteration 86, Batch: 3, Loss: 0.06442474573850632\n",
      "Iteration 86, Batch: 4, Loss: 0.07079530507326126\n",
      "Iteration 86, Batch: 5, Loss: 0.06477239727973938\n",
      "Iteration 86, Batch: 6, Loss: 0.041985753923654556\n",
      "Iteration 86, Batch: 7, Loss: 0.0385158509016037\n",
      "Iteration 86, Batch: 8, Loss: 0.045391883701086044\n",
      "Iteration 86, Batch: 9, Loss: 0.061697013676166534\n",
      "Iteration 86, Batch: 10, Loss: 0.05319838225841522\n",
      "Iteration 86, Batch: 11, Loss: 0.09527255594730377\n",
      "Iteration 86, Batch: 12, Loss: 0.06051280349493027\n",
      "Iteration 86, Batch: 13, Loss: 0.04380832612514496\n",
      "Iteration 86, Batch: 14, Loss: 0.03820106014609337\n",
      "Iteration 86, Batch: 15, Loss: 0.050104860216379166\n",
      "Iteration 86, Batch: 16, Loss: 0.050446104258298874\n",
      "Iteration 86, Batch: 17, Loss: 0.0951465368270874\n",
      "Iteration 86, Batch: 18, Loss: 0.0783674418926239\n",
      "Iteration 86, Batch: 19, Loss: 0.050484657287597656\n",
      "Iteration 86, Batch: 20, Loss: 0.06580343842506409\n",
      "Iteration 86, Batch: 21, Loss: 0.054901428520679474\n",
      "Iteration 86, Batch: 22, Loss: 0.042509861290454865\n",
      "Iteration 86, Batch: 23, Loss: 0.05913099646568298\n",
      "Iteration 86, Batch: 24, Loss: 0.029998142272233963\n",
      "Iteration 86, Batch: 25, Loss: 0.06317368894815445\n",
      "Iteration 86, Batch: 26, Loss: 0.06986323744058609\n",
      "Iteration 86, Batch: 27, Loss: 0.07742532342672348\n",
      "Iteration 86, Batch: 28, Loss: 0.05129985511302948\n",
      "Iteration 86, Batch: 29, Loss: 0.05203792825341225\n",
      "Iteration 86, Batch: 30, Loss: 0.061985936015844345\n",
      "Iteration 86, Batch: 31, Loss: 0.04515671730041504\n",
      "Iteration 86, Batch: 32, Loss: 0.07295213639736176\n",
      "Iteration 86, Batch: 33, Loss: 0.0794140100479126\n",
      "Iteration 86, Batch: 34, Loss: 0.04239610582590103\n",
      "Iteration 86, Batch: 35, Loss: 0.07600322365760803\n",
      "Iteration 86, Batch: 36, Loss: 0.04908495023846626\n",
      "Iteration 86, Batch: 37, Loss: 0.07558394223451614\n",
      "Iteration 86, Batch: 38, Loss: 0.055381420999765396\n",
      "Iteration 86, Batch: 39, Loss: 0.07040265947580338\n",
      "Iteration 86, Batch: 40, Loss: 0.04061569273471832\n",
      "Iteration 86, Batch: 41, Loss: 0.07744989544153214\n",
      "Iteration 86, Batch: 42, Loss: 0.08754601329565048\n",
      "Iteration 86, Batch: 43, Loss: 0.049201685935258865\n",
      "Iteration 86, Batch: 44, Loss: 0.03588016331195831\n",
      "Iteration 86, Batch: 45, Loss: 0.05106383189558983\n",
      "Iteration 86, Batch: 46, Loss: 0.08447674661874771\n",
      "Iteration 86, Batch: 47, Loss: 0.05082156881690025\n",
      "Iteration 86, Batch: 48, Loss: 0.054059095680713654\n",
      "Iteration 86, Batch: 49, Loss: 0.04043637588620186\n",
      "Iteration 87, Batch: 0, Loss: 0.05167621374130249\n",
      "Iteration 87, Batch: 1, Loss: 0.04502201825380325\n",
      "Iteration 87, Batch: 2, Loss: 0.0839775949716568\n",
      "Iteration 87, Batch: 3, Loss: 0.04282355308532715\n",
      "Iteration 87, Batch: 4, Loss: 0.04481874406337738\n",
      "Iteration 87, Batch: 5, Loss: 0.05891921743750572\n",
      "Iteration 87, Batch: 6, Loss: 0.03745872899889946\n",
      "Iteration 87, Batch: 7, Loss: 0.037814997136592865\n",
      "Iteration 87, Batch: 8, Loss: 0.040861766785383224\n",
      "Iteration 87, Batch: 9, Loss: 0.04815975949168205\n",
      "Iteration 87, Batch: 10, Loss: 0.05081497132778168\n",
      "Iteration 87, Batch: 11, Loss: 0.06713707745075226\n",
      "Iteration 87, Batch: 12, Loss: 0.051616501063108444\n",
      "Iteration 87, Batch: 13, Loss: 0.06718096882104874\n",
      "Iteration 87, Batch: 14, Loss: 0.04944558069109917\n",
      "Iteration 87, Batch: 15, Loss: 0.041885875165462494\n",
      "Iteration 87, Batch: 16, Loss: 0.05172183737158775\n",
      "Iteration 87, Batch: 17, Loss: 0.05878039076924324\n",
      "Iteration 87, Batch: 18, Loss: 0.057042159140110016\n",
      "Iteration 87, Batch: 19, Loss: 0.0454760380089283\n",
      "Iteration 87, Batch: 20, Loss: 0.06013612449169159\n",
      "Iteration 87, Batch: 21, Loss: 0.03271283209323883\n",
      "Iteration 87, Batch: 22, Loss: 0.04832255095243454\n",
      "Iteration 87, Batch: 23, Loss: 0.0405825600028038\n",
      "Iteration 87, Batch: 24, Loss: 0.043130744248628616\n",
      "Iteration 87, Batch: 25, Loss: 0.06253456324338913\n",
      "Iteration 87, Batch: 26, Loss: 0.07817600667476654\n",
      "Iteration 87, Batch: 27, Loss: 0.0321800634264946\n",
      "Iteration 87, Batch: 28, Loss: 0.0568062923848629\n",
      "Iteration 87, Batch: 29, Loss: 0.05641796067357063\n",
      "Iteration 87, Batch: 30, Loss: 0.054271843284368515\n",
      "Iteration 87, Batch: 31, Loss: 0.05471210926771164\n",
      "Iteration 87, Batch: 32, Loss: 0.10899887979030609\n",
      "Iteration 87, Batch: 33, Loss: 0.0506853386759758\n",
      "Iteration 87, Batch: 34, Loss: 0.04854248836636543\n",
      "Iteration 87, Batch: 35, Loss: 0.05656803399324417\n",
      "Iteration 87, Batch: 36, Loss: 0.04597897827625275\n",
      "Iteration 87, Batch: 37, Loss: 0.050144512206315994\n",
      "Iteration 87, Batch: 38, Loss: 0.03845621645450592\n",
      "Iteration 87, Batch: 39, Loss: 0.053776469081640244\n",
      "Iteration 87, Batch: 40, Loss: 0.05404891073703766\n",
      "Iteration 87, Batch: 41, Loss: 0.0454629510641098\n",
      "Iteration 87, Batch: 42, Loss: 0.054581835865974426\n",
      "Iteration 87, Batch: 43, Loss: 0.0359649732708931\n",
      "Iteration 87, Batch: 44, Loss: 0.07329140603542328\n",
      "Iteration 87, Batch: 45, Loss: 0.03366104140877724\n",
      "Iteration 87, Batch: 46, Loss: 0.04150056838989258\n",
      "Iteration 87, Batch: 47, Loss: 0.05592988803982735\n",
      "Iteration 87, Batch: 48, Loss: 0.032596345990896225\n",
      "Iteration 87, Batch: 49, Loss: 0.027856310829520226\n",
      "Iteration 88, Batch: 0, Loss: 0.06012428179383278\n",
      "Iteration 88, Batch: 1, Loss: 0.04361319541931152\n",
      "Iteration 88, Batch: 2, Loss: 0.046916864812374115\n",
      "Iteration 88, Batch: 3, Loss: 0.05373772606253624\n",
      "Iteration 88, Batch: 4, Loss: 0.05782325565814972\n",
      "Iteration 88, Batch: 5, Loss: 0.048608772456645966\n",
      "Iteration 88, Batch: 6, Loss: 0.040088485926389694\n",
      "Iteration 88, Batch: 7, Loss: 0.07136277854442596\n",
      "Iteration 88, Batch: 8, Loss: 0.05529475584626198\n",
      "Iteration 88, Batch: 9, Loss: 0.04047340154647827\n",
      "Iteration 88, Batch: 10, Loss: 0.04303726181387901\n",
      "Iteration 88, Batch: 11, Loss: 0.022497348487377167\n",
      "Iteration 88, Batch: 12, Loss: 0.054269541054964066\n",
      "Iteration 88, Batch: 13, Loss: 0.06540904939174652\n",
      "Iteration 88, Batch: 14, Loss: 0.06805631518363953\n",
      "Iteration 88, Batch: 15, Loss: 0.041315604001283646\n",
      "Iteration 88, Batch: 16, Loss: 0.047849029302597046\n",
      "Iteration 88, Batch: 17, Loss: 0.06735634058713913\n",
      "Iteration 88, Batch: 18, Loss: 0.05367011949419975\n",
      "Iteration 88, Batch: 19, Loss: 0.06828120350837708\n",
      "Iteration 88, Batch: 20, Loss: 0.07124605029821396\n",
      "Iteration 88, Batch: 21, Loss: 0.05931251496076584\n",
      "Iteration 88, Batch: 22, Loss: 0.052401069551706314\n",
      "Iteration 88, Batch: 23, Loss: 0.054749924689531326\n",
      "Iteration 88, Batch: 24, Loss: 0.0660252720117569\n",
      "Iteration 88, Batch: 25, Loss: 0.05747528746724129\n",
      "Iteration 88, Batch: 26, Loss: 0.03725680708885193\n",
      "Iteration 88, Batch: 27, Loss: 0.055736567825078964\n",
      "Iteration 88, Batch: 28, Loss: 0.04283132404088974\n",
      "Iteration 88, Batch: 29, Loss: 0.05883106589317322\n",
      "Iteration 88, Batch: 30, Loss: 0.03668684512376785\n",
      "Iteration 88, Batch: 31, Loss: 0.04770883545279503\n",
      "Iteration 88, Batch: 32, Loss: 0.06277263164520264\n",
      "Iteration 88, Batch: 33, Loss: 0.04362212121486664\n",
      "Iteration 88, Batch: 34, Loss: 0.04321640729904175\n",
      "Iteration 88, Batch: 35, Loss: 0.05105901136994362\n",
      "Iteration 88, Batch: 36, Loss: 0.05633164197206497\n",
      "Iteration 88, Batch: 37, Loss: 0.06648766249418259\n",
      "Iteration 88, Batch: 38, Loss: 0.06374144554138184\n",
      "Iteration 88, Batch: 39, Loss: 0.06743191182613373\n",
      "Iteration 88, Batch: 40, Loss: 0.08437079936265945\n",
      "Iteration 88, Batch: 41, Loss: 0.057831645011901855\n",
      "Iteration 88, Batch: 42, Loss: 0.055891599506139755\n",
      "Iteration 88, Batch: 43, Loss: 0.057725485414266586\n",
      "Iteration 88, Batch: 44, Loss: 0.05840461701154709\n",
      "Iteration 88, Batch: 45, Loss: 0.0638163760304451\n",
      "Iteration 88, Batch: 46, Loss: 0.09053660184144974\n",
      "Iteration 88, Batch: 47, Loss: 0.05558561533689499\n",
      "Iteration 88, Batch: 48, Loss: 0.03519633039832115\n",
      "Iteration 88, Batch: 49, Loss: 0.05751187354326248\n",
      "Iteration 89, Batch: 0, Loss: 0.06533535569906235\n",
      "Iteration 89, Batch: 1, Loss: 0.04140828922390938\n",
      "Iteration 89, Batch: 2, Loss: 0.05262615904211998\n",
      "Iteration 89, Batch: 3, Loss: 0.05974621698260307\n",
      "Iteration 89, Batch: 4, Loss: 0.05968421325087547\n",
      "Iteration 89, Batch: 5, Loss: 0.03884290158748627\n",
      "Iteration 89, Batch: 6, Loss: 0.049338243901729584\n",
      "Iteration 89, Batch: 7, Loss: 0.04299147427082062\n",
      "Iteration 89, Batch: 8, Loss: 0.051266543567180634\n",
      "Iteration 89, Batch: 9, Loss: 0.027917589992284775\n",
      "Iteration 89, Batch: 10, Loss: 0.041722267866134644\n",
      "Iteration 89, Batch: 11, Loss: 0.05470074713230133\n",
      "Iteration 89, Batch: 12, Loss: 0.04183853045105934\n",
      "Iteration 89, Batch: 13, Loss: 0.04133390262722969\n",
      "Iteration 89, Batch: 14, Loss: 0.06340871006250381\n",
      "Iteration 89, Batch: 15, Loss: 0.057972297072410583\n",
      "Iteration 89, Batch: 16, Loss: 0.050101183354854584\n",
      "Iteration 89, Batch: 17, Loss: 0.047444283962249756\n",
      "Iteration 89, Batch: 18, Loss: 0.06641625612974167\n",
      "Iteration 89, Batch: 19, Loss: 0.05533584579825401\n",
      "Iteration 89, Batch: 20, Loss: 0.055642422288656235\n",
      "Iteration 89, Batch: 21, Loss: 0.0366813950240612\n",
      "Iteration 89, Batch: 22, Loss: 0.05485142767429352\n",
      "Iteration 89, Batch: 23, Loss: 0.05219731107354164\n",
      "Iteration 89, Batch: 24, Loss: 0.05305857211351395\n",
      "Iteration 89, Batch: 25, Loss: 0.04824405908584595\n",
      "Iteration 89, Batch: 26, Loss: 0.04908426105976105\n",
      "Iteration 89, Batch: 27, Loss: 0.03520580753684044\n",
      "Iteration 89, Batch: 28, Loss: 0.06260383129119873\n",
      "Iteration 89, Batch: 29, Loss: 0.0819023996591568\n",
      "Iteration 89, Batch: 30, Loss: 0.05799408257007599\n",
      "Iteration 89, Batch: 31, Loss: 0.04618749022483826\n",
      "Iteration 89, Batch: 32, Loss: 0.06122205778956413\n",
      "Iteration 89, Batch: 33, Loss: 0.041368551552295685\n",
      "Iteration 89, Batch: 34, Loss: 0.07121170312166214\n",
      "Iteration 89, Batch: 35, Loss: 0.045433394610881805\n",
      "Iteration 89, Batch: 36, Loss: 0.0714050829410553\n",
      "Iteration 89, Batch: 37, Loss: 0.05486277863383293\n",
      "Iteration 89, Batch: 38, Loss: 0.05921505019068718\n",
      "Iteration 89, Batch: 39, Loss: 0.03893555700778961\n",
      "Iteration 89, Batch: 40, Loss: 0.06379687041044235\n",
      "Iteration 89, Batch: 41, Loss: 0.06581542640924454\n",
      "Iteration 89, Batch: 42, Loss: 0.05654484033584595\n",
      "Iteration 89, Batch: 43, Loss: 0.04727866128087044\n",
      "Iteration 89, Batch: 44, Loss: 0.05962992459535599\n",
      "Iteration 89, Batch: 45, Loss: 0.07755498588085175\n",
      "Iteration 89, Batch: 46, Loss: 0.06613108515739441\n",
      "Iteration 89, Batch: 47, Loss: 0.04296720772981644\n",
      "Iteration 89, Batch: 48, Loss: 0.07530977576971054\n",
      "Iteration 89, Batch: 49, Loss: 0.05917644500732422\n",
      "Iteration 90, Batch: 0, Loss: 0.05434735491871834\n",
      "Iteration 90, Batch: 1, Loss: 0.060253627598285675\n",
      "Iteration 90, Batch: 2, Loss: 0.057025518268346786\n",
      "Iteration 90, Batch: 3, Loss: 0.05692807212471962\n",
      "Iteration 90, Batch: 4, Loss: 0.054080717265605927\n",
      "Iteration 90, Batch: 5, Loss: 0.05931742116808891\n",
      "Iteration 90, Batch: 6, Loss: 0.05189163237810135\n",
      "Iteration 90, Batch: 7, Loss: 0.058268409222364426\n",
      "Iteration 90, Batch: 8, Loss: 0.04009411856532097\n",
      "Iteration 90, Batch: 9, Loss: 0.071550652384758\n",
      "Iteration 90, Batch: 10, Loss: 0.057385534048080444\n",
      "Iteration 90, Batch: 11, Loss: 0.042252879589796066\n",
      "Iteration 90, Batch: 12, Loss: 0.0623319186270237\n",
      "Iteration 90, Batch: 13, Loss: 0.06262119114398956\n",
      "Iteration 90, Batch: 14, Loss: 0.04381415620446205\n",
      "Iteration 90, Batch: 15, Loss: 0.05326005816459656\n",
      "Iteration 90, Batch: 16, Loss: 0.026885852217674255\n",
      "Iteration 90, Batch: 17, Loss: 0.0647234171628952\n",
      "Iteration 90, Batch: 18, Loss: 0.07018843293190002\n",
      "Iteration 90, Batch: 19, Loss: 0.05785416066646576\n",
      "Iteration 90, Batch: 20, Loss: 0.036448996514081955\n",
      "Iteration 90, Batch: 21, Loss: 0.059527911245822906\n",
      "Iteration 90, Batch: 22, Loss: 0.06335170567035675\n",
      "Iteration 90, Batch: 23, Loss: 0.034387584775686264\n",
      "Iteration 90, Batch: 24, Loss: 0.045437395572662354\n",
      "Iteration 90, Batch: 25, Loss: 0.055648889392614365\n",
      "Iteration 90, Batch: 26, Loss: 0.05664737522602081\n",
      "Iteration 90, Batch: 27, Loss: 0.05188506096601486\n",
      "Iteration 90, Batch: 28, Loss: 0.05572347342967987\n",
      "Iteration 90, Batch: 29, Loss: 0.05276305228471756\n",
      "Iteration 90, Batch: 30, Loss: 0.05391101539134979\n",
      "Iteration 90, Batch: 31, Loss: 0.03697795048356056\n",
      "Iteration 90, Batch: 32, Loss: 0.037449147552251816\n",
      "Iteration 90, Batch: 33, Loss: 0.0570363886654377\n",
      "Iteration 90, Batch: 34, Loss: 0.02951132319867611\n",
      "Iteration 90, Batch: 35, Loss: 0.04878530278801918\n",
      "Iteration 90, Batch: 36, Loss: 0.06435300409793854\n",
      "Iteration 90, Batch: 37, Loss: 0.056390006095170975\n",
      "Iteration 90, Batch: 38, Loss: 0.05840340629220009\n",
      "Iteration 90, Batch: 39, Loss: 0.06291650980710983\n",
      "Iteration 90, Batch: 40, Loss: 0.055408671498298645\n",
      "Iteration 90, Batch: 41, Loss: 0.063028983771801\n",
      "Iteration 90, Batch: 42, Loss: 0.06173686683177948\n",
      "Iteration 90, Batch: 43, Loss: 0.04371120035648346\n",
      "Iteration 90, Batch: 44, Loss: 0.043514564633369446\n",
      "Iteration 90, Batch: 45, Loss: 0.0548342801630497\n",
      "Iteration 90, Batch: 46, Loss: 0.07303338497877121\n",
      "Iteration 90, Batch: 47, Loss: 0.05012883245944977\n",
      "Iteration 90, Batch: 48, Loss: 0.061922140419483185\n",
      "Iteration 90, Batch: 49, Loss: 0.05575377494096756\n",
      "Iteration 91, Batch: 0, Loss: 0.05719294771552086\n",
      "Iteration 91, Batch: 1, Loss: 0.053065985441207886\n",
      "Iteration 91, Batch: 2, Loss: 0.041595324873924255\n",
      "Iteration 91, Batch: 3, Loss: 0.0647445097565651\n",
      "Iteration 91, Batch: 4, Loss: 0.04506928473711014\n",
      "Iteration 91, Batch: 5, Loss: 0.050780799239873886\n",
      "Iteration 91, Batch: 6, Loss: 0.06085370108485222\n",
      "Iteration 91, Batch: 7, Loss: 0.06310908496379852\n",
      "Iteration 91, Batch: 8, Loss: 0.03359450772404671\n",
      "Iteration 91, Batch: 9, Loss: 0.06726197898387909\n",
      "Iteration 91, Batch: 10, Loss: 0.053974054753780365\n",
      "Iteration 91, Batch: 11, Loss: 0.04214104264974594\n",
      "Iteration 91, Batch: 12, Loss: 0.04336050525307655\n",
      "Iteration 91, Batch: 13, Loss: 0.042188070714473724\n",
      "Iteration 91, Batch: 14, Loss: 0.042391929775476456\n",
      "Iteration 91, Batch: 15, Loss: 0.0518607459962368\n",
      "Iteration 91, Batch: 16, Loss: 0.0439075268805027\n",
      "Iteration 91, Batch: 17, Loss: 0.061432693153619766\n",
      "Iteration 91, Batch: 18, Loss: 0.06921544671058655\n",
      "Iteration 91, Batch: 19, Loss: 0.043331608176231384\n",
      "Iteration 91, Batch: 20, Loss: 0.04432583600282669\n",
      "Iteration 91, Batch: 21, Loss: 0.039207275956869125\n",
      "Iteration 91, Batch: 22, Loss: 0.05196043848991394\n",
      "Iteration 91, Batch: 23, Loss: 0.07738612592220306\n",
      "Iteration 91, Batch: 24, Loss: 0.03974926471710205\n",
      "Iteration 91, Batch: 25, Loss: 0.036835815757513046\n",
      "Iteration 91, Batch: 26, Loss: 0.04044853150844574\n",
      "Iteration 91, Batch: 27, Loss: 0.061149708926677704\n",
      "Iteration 91, Batch: 28, Loss: 0.07603826373815536\n",
      "Iteration 91, Batch: 29, Loss: 0.036230769008398056\n",
      "Iteration 91, Batch: 30, Loss: 0.04744189232587814\n",
      "Iteration 91, Batch: 31, Loss: 0.07267701625823975\n",
      "Iteration 91, Batch: 32, Loss: 0.05535929650068283\n",
      "Iteration 91, Batch: 33, Loss: 0.0511791855096817\n",
      "Iteration 91, Batch: 34, Loss: 0.06720221042633057\n",
      "Iteration 91, Batch: 35, Loss: 0.07139697670936584\n",
      "Iteration 91, Batch: 36, Loss: 0.04556428641080856\n",
      "Iteration 91, Batch: 37, Loss: 0.06169966608285904\n",
      "Iteration 91, Batch: 38, Loss: 0.05764132738113403\n",
      "Iteration 91, Batch: 39, Loss: 0.03922037407755852\n",
      "Iteration 91, Batch: 40, Loss: 0.035079002380371094\n",
      "Iteration 91, Batch: 41, Loss: 0.04681626707315445\n",
      "Iteration 91, Batch: 42, Loss: 0.06289096921682358\n",
      "Iteration 91, Batch: 43, Loss: 0.025904007256031036\n",
      "Iteration 91, Batch: 44, Loss: 0.0647466853260994\n",
      "Iteration 91, Batch: 45, Loss: 0.034847091883420944\n",
      "Iteration 91, Batch: 46, Loss: 0.05726350098848343\n",
      "Iteration 91, Batch: 47, Loss: 0.05128005892038345\n",
      "Iteration 91, Batch: 48, Loss: 0.03019546903669834\n",
      "Iteration 91, Batch: 49, Loss: 0.0793701708316803\n",
      "Iteration 92, Batch: 0, Loss: 0.050426311790943146\n",
      "Iteration 92, Batch: 1, Loss: 0.0419146828353405\n",
      "Iteration 92, Batch: 2, Loss: 0.06349489837884903\n",
      "Iteration 92, Batch: 3, Loss: 0.05876344069838524\n",
      "Iteration 92, Batch: 4, Loss: 0.06815344095230103\n",
      "Iteration 92, Batch: 5, Loss: 0.04570155218243599\n",
      "Iteration 92, Batch: 6, Loss: 0.04277080297470093\n",
      "Iteration 92, Batch: 7, Loss: 0.053454216569662094\n",
      "Iteration 92, Batch: 8, Loss: 0.06367862969636917\n",
      "Iteration 92, Batch: 9, Loss: 0.03943007066845894\n",
      "Iteration 92, Batch: 10, Loss: 0.05072902888059616\n",
      "Iteration 92, Batch: 11, Loss: 0.04861092567443848\n",
      "Iteration 92, Batch: 12, Loss: 0.09516488760709763\n",
      "Iteration 92, Batch: 13, Loss: 0.05202789977192879\n",
      "Iteration 92, Batch: 14, Loss: 0.07231832295656204\n",
      "Iteration 92, Batch: 15, Loss: 0.05860266461968422\n",
      "Iteration 92, Batch: 16, Loss: 0.07168427109718323\n",
      "Iteration 92, Batch: 17, Loss: 0.0868835300207138\n",
      "Iteration 92, Batch: 18, Loss: 0.0831569954752922\n",
      "Iteration 92, Batch: 19, Loss: 0.05541684105992317\n",
      "Iteration 92, Batch: 20, Loss: 0.08155166357755661\n",
      "Iteration 92, Batch: 21, Loss: 0.08699802309274673\n",
      "Iteration 92, Batch: 22, Loss: 0.05422835052013397\n",
      "Iteration 92, Batch: 23, Loss: 0.02562430128455162\n",
      "Iteration 92, Batch: 24, Loss: 0.04831288754940033\n",
      "Iteration 92, Batch: 25, Loss: 0.06779541075229645\n",
      "Iteration 92, Batch: 26, Loss: 0.057339876890182495\n",
      "Iteration 92, Batch: 27, Loss: 0.04921644553542137\n",
      "Iteration 92, Batch: 28, Loss: 0.05189867690205574\n",
      "Iteration 92, Batch: 29, Loss: 0.06816183775663376\n",
      "Iteration 92, Batch: 30, Loss: 0.04859396815299988\n",
      "Iteration 92, Batch: 31, Loss: 0.03652571514248848\n",
      "Iteration 92, Batch: 32, Loss: 0.0410696305334568\n",
      "Iteration 92, Batch: 33, Loss: 0.05649935454130173\n",
      "Iteration 92, Batch: 34, Loss: 0.051319412887096405\n",
      "Iteration 92, Batch: 35, Loss: 0.055323656648397446\n",
      "Iteration 92, Batch: 36, Loss: 0.055696673691272736\n",
      "Iteration 92, Batch: 37, Loss: 0.06006689742207527\n",
      "Iteration 92, Batch: 38, Loss: 0.07746975868940353\n",
      "Iteration 92, Batch: 39, Loss: 0.04669477418065071\n",
      "Iteration 92, Batch: 40, Loss: 0.053487811237573624\n",
      "Iteration 92, Batch: 41, Loss: 0.06340054422616959\n",
      "Iteration 92, Batch: 42, Loss: 0.04754047468304634\n",
      "Iteration 92, Batch: 43, Loss: 0.06794822216033936\n",
      "Iteration 92, Batch: 44, Loss: 0.058144912123680115\n",
      "Iteration 92, Batch: 45, Loss: 0.06591273099184036\n",
      "Iteration 92, Batch: 46, Loss: 0.036969225853681564\n",
      "Iteration 92, Batch: 47, Loss: 0.046326346695423126\n",
      "Iteration 92, Batch: 48, Loss: 0.05010282248258591\n",
      "Iteration 92, Batch: 49, Loss: 0.04878855496644974\n",
      "Iteration 93, Batch: 0, Loss: 0.04441734775900841\n",
      "Iteration 93, Batch: 1, Loss: 0.06028273329138756\n",
      "Iteration 93, Batch: 2, Loss: 0.05756557732820511\n",
      "Iteration 93, Batch: 3, Loss: 0.06575316935777664\n",
      "Iteration 93, Batch: 4, Loss: 0.04709285497665405\n",
      "Iteration 93, Batch: 5, Loss: 0.06064030900597572\n",
      "Iteration 93, Batch: 6, Loss: 0.08369594067335129\n",
      "Iteration 93, Batch: 7, Loss: 0.05950326845049858\n",
      "Iteration 93, Batch: 8, Loss: 0.07418466359376907\n",
      "Iteration 93, Batch: 9, Loss: 0.09418345242738724\n",
      "Iteration 93, Batch: 10, Loss: 0.08612140268087387\n",
      "Iteration 93, Batch: 11, Loss: 0.09480658918619156\n",
      "Iteration 93, Batch: 12, Loss: 0.07569464296102524\n",
      "Iteration 93, Batch: 13, Loss: 0.10361148416996002\n",
      "Iteration 93, Batch: 14, Loss: 0.0584871843457222\n",
      "Iteration 93, Batch: 15, Loss: 0.11327209323644638\n",
      "Iteration 93, Batch: 16, Loss: 0.09537094086408615\n",
      "Iteration 93, Batch: 17, Loss: 0.07337094098329544\n",
      "Iteration 93, Batch: 18, Loss: 0.07203404605388641\n",
      "Iteration 93, Batch: 19, Loss: 0.09749902784824371\n",
      "Iteration 93, Batch: 20, Loss: 0.06755819171667099\n",
      "Iteration 93, Batch: 21, Loss: 0.07741174846887589\n",
      "Iteration 93, Batch: 22, Loss: 0.08887100219726562\n",
      "Iteration 93, Batch: 23, Loss: 0.08461742848157883\n",
      "Iteration 93, Batch: 24, Loss: 0.10008237510919571\n",
      "Iteration 93, Batch: 25, Loss: 0.09202156960964203\n",
      "Iteration 93, Batch: 26, Loss: 0.04485594481229782\n",
      "Iteration 93, Batch: 27, Loss: 0.07551508396863937\n",
      "Iteration 93, Batch: 28, Loss: 0.09866472333669662\n",
      "Iteration 93, Batch: 29, Loss: 0.07181935012340546\n",
      "Iteration 93, Batch: 30, Loss: 0.08878450840711594\n",
      "Iteration 93, Batch: 31, Loss: 0.08169025927782059\n",
      "Iteration 93, Batch: 32, Loss: 0.08317673206329346\n",
      "Iteration 93, Batch: 33, Loss: 0.09934619069099426\n",
      "Iteration 93, Batch: 34, Loss: 0.07518763095140457\n",
      "Iteration 93, Batch: 35, Loss: 0.0778258815407753\n",
      "Iteration 93, Batch: 36, Loss: 0.11421636492013931\n",
      "Iteration 93, Batch: 37, Loss: 0.09129215031862259\n",
      "Iteration 93, Batch: 38, Loss: 0.06829840689897537\n",
      "Iteration 93, Batch: 39, Loss: 0.061308104544878006\n",
      "Iteration 93, Batch: 40, Loss: 0.0989769995212555\n",
      "Iteration 93, Batch: 41, Loss: 0.07677653431892395\n",
      "Iteration 93, Batch: 42, Loss: 0.11136338859796524\n",
      "Iteration 93, Batch: 43, Loss: 0.0696282684803009\n",
      "Iteration 93, Batch: 44, Loss: 0.06410502642393112\n",
      "Iteration 93, Batch: 45, Loss: 0.11190985143184662\n",
      "Iteration 93, Batch: 46, Loss: 0.0873827338218689\n",
      "Iteration 93, Batch: 47, Loss: 0.11570131033658981\n",
      "Iteration 93, Batch: 48, Loss: 0.11605781316757202\n",
      "Iteration 93, Batch: 49, Loss: 0.10109409689903259\n",
      "Iteration 94, Batch: 0, Loss: 0.087569959461689\n",
      "Iteration 94, Batch: 1, Loss: 0.07563219219446182\n",
      "Iteration 94, Batch: 2, Loss: 0.110933318734169\n",
      "Iteration 94, Batch: 3, Loss: 0.12079014629125595\n",
      "Iteration 94, Batch: 4, Loss: 0.08453784883022308\n",
      "Iteration 94, Batch: 5, Loss: 0.08639483153820038\n",
      "Iteration 94, Batch: 6, Loss: 0.0913790687918663\n",
      "Iteration 94, Batch: 7, Loss: 0.09259454160928726\n",
      "Iteration 94, Batch: 8, Loss: 0.07735643535852432\n",
      "Iteration 94, Batch: 9, Loss: 0.0641644299030304\n",
      "Iteration 94, Batch: 10, Loss: 0.09267526119947433\n",
      "Iteration 94, Batch: 11, Loss: 0.0785575732588768\n",
      "Iteration 94, Batch: 12, Loss: 0.061930544674396515\n",
      "Iteration 94, Batch: 13, Loss: 0.11515380442142487\n",
      "Iteration 94, Batch: 14, Loss: 0.06743788719177246\n",
      "Iteration 94, Batch: 15, Loss: 0.06166387349367142\n",
      "Iteration 94, Batch: 16, Loss: 0.0657891035079956\n",
      "Iteration 94, Batch: 17, Loss: 0.09668833762407303\n",
      "Iteration 94, Batch: 18, Loss: 0.06921711564064026\n",
      "Iteration 94, Batch: 19, Loss: 0.06615056097507477\n",
      "Iteration 94, Batch: 20, Loss: 0.05545000731945038\n",
      "Iteration 94, Batch: 21, Loss: 0.06723674386739731\n",
      "Iteration 94, Batch: 22, Loss: 0.10720165073871613\n",
      "Iteration 94, Batch: 23, Loss: 0.07459630072116852\n",
      "Iteration 94, Batch: 24, Loss: 0.07443094998598099\n",
      "Iteration 94, Batch: 25, Loss: 0.0678701400756836\n",
      "Iteration 94, Batch: 26, Loss: 0.06881361454725266\n",
      "Iteration 94, Batch: 27, Loss: 0.06115765497088432\n",
      "Iteration 94, Batch: 28, Loss: 0.04645000398159027\n",
      "Iteration 94, Batch: 29, Loss: 0.06228489801287651\n",
      "Iteration 94, Batch: 30, Loss: 0.07996323704719543\n",
      "Iteration 94, Batch: 31, Loss: 0.0718151405453682\n",
      "Iteration 94, Batch: 32, Loss: 0.07167306542396545\n",
      "Iteration 94, Batch: 33, Loss: 0.06936219334602356\n",
      "Iteration 94, Batch: 34, Loss: 0.06301816552877426\n",
      "Iteration 94, Batch: 35, Loss: 0.04501836746931076\n",
      "Iteration 94, Batch: 36, Loss: 0.07696662843227386\n",
      "Iteration 94, Batch: 37, Loss: 0.0851823017001152\n",
      "Iteration 94, Batch: 38, Loss: 0.08964855968952179\n",
      "Iteration 94, Batch: 39, Loss: 0.06196485459804535\n",
      "Iteration 94, Batch: 40, Loss: 0.061227452009916306\n",
      "Iteration 94, Batch: 41, Loss: 0.07887908816337585\n",
      "Iteration 94, Batch: 42, Loss: 0.08998029679059982\n",
      "Iteration 94, Batch: 43, Loss: 0.08598756790161133\n",
      "Iteration 94, Batch: 44, Loss: 0.0742696151137352\n",
      "Iteration 94, Batch: 45, Loss: 0.05256810039281845\n",
      "Iteration 94, Batch: 46, Loss: 0.07115005701780319\n",
      "Iteration 94, Batch: 47, Loss: 0.07625098526477814\n",
      "Iteration 94, Batch: 48, Loss: 0.05861936882138252\n",
      "Iteration 94, Batch: 49, Loss: 0.10164520144462585\n",
      "Iteration 95, Batch: 0, Loss: 0.09427114576101303\n",
      "Iteration 95, Batch: 1, Loss: 0.09274560958147049\n",
      "Iteration 95, Batch: 2, Loss: 0.06894142925739288\n",
      "Iteration 95, Batch: 3, Loss: 0.09453164786100388\n",
      "Iteration 95, Batch: 4, Loss: 0.10233986377716064\n",
      "Iteration 95, Batch: 5, Loss: 0.057604312896728516\n",
      "Iteration 95, Batch: 6, Loss: 0.06563360244035721\n",
      "Iteration 95, Batch: 7, Loss: 0.07468708604574203\n",
      "Iteration 95, Batch: 8, Loss: 0.06764484941959381\n",
      "Iteration 95, Batch: 9, Loss: 0.10980506986379623\n",
      "Iteration 95, Batch: 10, Loss: 0.05728323757648468\n",
      "Iteration 95, Batch: 11, Loss: 0.06045403331518173\n",
      "Iteration 95, Batch: 12, Loss: 0.069808728992939\n",
      "Iteration 95, Batch: 13, Loss: 0.05233067274093628\n",
      "Iteration 95, Batch: 14, Loss: 0.06576689332723618\n",
      "Iteration 95, Batch: 15, Loss: 0.05916808173060417\n",
      "Iteration 95, Batch: 16, Loss: 0.1258177012205124\n",
      "Iteration 95, Batch: 17, Loss: 0.06327679753303528\n",
      "Iteration 95, Batch: 18, Loss: 0.07521463185548782\n",
      "Iteration 95, Batch: 19, Loss: 0.07657643407583237\n",
      "Iteration 95, Batch: 20, Loss: 0.05654718354344368\n",
      "Iteration 95, Batch: 21, Loss: 0.05169449746608734\n",
      "Iteration 95, Batch: 22, Loss: 0.06139406934380531\n",
      "Iteration 95, Batch: 23, Loss: 0.06252826750278473\n",
      "Iteration 95, Batch: 24, Loss: 0.06272049993276596\n",
      "Iteration 95, Batch: 25, Loss: 0.07294675707817078\n",
      "Iteration 95, Batch: 26, Loss: 0.07766959816217422\n",
      "Iteration 95, Batch: 27, Loss: 0.05981994420289993\n",
      "Iteration 95, Batch: 28, Loss: 0.09175889939069748\n",
      "Iteration 95, Batch: 29, Loss: 0.07858815789222717\n",
      "Iteration 95, Batch: 30, Loss: 0.07323768734931946\n",
      "Iteration 95, Batch: 31, Loss: 0.09617244452238083\n",
      "Iteration 95, Batch: 32, Loss: 0.10977645963430405\n",
      "Iteration 95, Batch: 33, Loss: 0.09755583852529526\n",
      "Iteration 95, Batch: 34, Loss: 0.09092087298631668\n",
      "Iteration 95, Batch: 35, Loss: 0.08148252218961716\n",
      "Iteration 95, Batch: 36, Loss: 0.09364362806081772\n",
      "Iteration 95, Batch: 37, Loss: 0.06791860610246658\n",
      "Iteration 95, Batch: 38, Loss: 0.0838499367237091\n",
      "Iteration 95, Batch: 39, Loss: 0.06078866496682167\n",
      "Iteration 95, Batch: 40, Loss: 0.07746727019548416\n",
      "Iteration 95, Batch: 41, Loss: 0.06964392960071564\n",
      "Iteration 95, Batch: 42, Loss: 0.0918460339307785\n",
      "Iteration 95, Batch: 43, Loss: 0.09274468570947647\n",
      "Iteration 95, Batch: 44, Loss: 0.09221616387367249\n",
      "Iteration 95, Batch: 45, Loss: 0.049074623733758926\n",
      "Iteration 95, Batch: 46, Loss: 0.05956495925784111\n",
      "Iteration 95, Batch: 47, Loss: 0.07246076315641403\n",
      "Iteration 95, Batch: 48, Loss: 0.09528419375419617\n",
      "Iteration 95, Batch: 49, Loss: 0.09059232473373413\n",
      "Iteration 96, Batch: 0, Loss: 0.0686294361948967\n",
      "Iteration 96, Batch: 1, Loss: 0.08075098693370819\n",
      "Iteration 96, Batch: 2, Loss: 0.07864539325237274\n",
      "Iteration 96, Batch: 3, Loss: 0.07585751265287399\n",
      "Iteration 96, Batch: 4, Loss: 0.08290499448776245\n",
      "Iteration 96, Batch: 5, Loss: 0.06969559192657471\n",
      "Iteration 96, Batch: 6, Loss: 0.07450060546398163\n",
      "Iteration 96, Batch: 7, Loss: 0.09946905076503754\n",
      "Iteration 96, Batch: 8, Loss: 0.07063697278499603\n",
      "Iteration 96, Batch: 9, Loss: 0.0720612108707428\n",
      "Iteration 96, Batch: 10, Loss: 0.06870821118354797\n",
      "Iteration 96, Batch: 11, Loss: 0.05122481659054756\n",
      "Iteration 96, Batch: 12, Loss: 0.05829084664583206\n",
      "Iteration 96, Batch: 13, Loss: 0.06336177885532379\n",
      "Iteration 96, Batch: 14, Loss: 0.049287062138319016\n",
      "Iteration 96, Batch: 15, Loss: 0.06421513110399246\n",
      "Iteration 96, Batch: 16, Loss: 0.05599580705165863\n",
      "Iteration 96, Batch: 17, Loss: 0.08435935527086258\n",
      "Iteration 96, Batch: 18, Loss: 0.07064703851938248\n",
      "Iteration 96, Batch: 19, Loss: 0.049211230129003525\n",
      "Iteration 96, Batch: 20, Loss: 0.04824076592922211\n",
      "Iteration 96, Batch: 21, Loss: 0.07487723231315613\n",
      "Iteration 96, Batch: 22, Loss: 0.08359784632921219\n",
      "Iteration 96, Batch: 23, Loss: 0.08890410512685776\n",
      "Iteration 96, Batch: 24, Loss: 0.09039085358381271\n",
      "Iteration 96, Batch: 25, Loss: 0.06630712747573853\n",
      "Iteration 96, Batch: 26, Loss: 0.06082236021757126\n",
      "Iteration 96, Batch: 27, Loss: 0.0943707749247551\n",
      "Iteration 96, Batch: 28, Loss: 0.09052269905805588\n",
      "Iteration 96, Batch: 29, Loss: 0.07100891321897507\n",
      "Iteration 96, Batch: 30, Loss: 0.08391667902469635\n",
      "Iteration 96, Batch: 31, Loss: 0.08211787790060043\n",
      "Iteration 96, Batch: 32, Loss: 0.07543236762285233\n",
      "Iteration 96, Batch: 33, Loss: 0.08150308579206467\n",
      "Iteration 96, Batch: 34, Loss: 0.11708518862724304\n",
      "Iteration 96, Batch: 35, Loss: 0.07770950347185135\n",
      "Iteration 96, Batch: 36, Loss: 0.08311136066913605\n",
      "Iteration 96, Batch: 37, Loss: 0.08337812125682831\n",
      "Iteration 96, Batch: 38, Loss: 0.09933380037546158\n",
      "Iteration 96, Batch: 39, Loss: 0.046285733580589294\n",
      "Iteration 96, Batch: 40, Loss: 0.08027584105730057\n",
      "Iteration 96, Batch: 41, Loss: 0.11187081784009933\n",
      "Iteration 96, Batch: 42, Loss: 0.07615914940834045\n",
      "Iteration 96, Batch: 43, Loss: 0.07998023182153702\n",
      "Iteration 96, Batch: 44, Loss: 0.10904988646507263\n",
      "Iteration 96, Batch: 45, Loss: 0.08719907701015472\n",
      "Iteration 96, Batch: 46, Loss: 0.10873838514089584\n",
      "Iteration 96, Batch: 47, Loss: 0.09682025760412216\n",
      "Iteration 96, Batch: 48, Loss: 0.10610076785087585\n",
      "Iteration 96, Batch: 49, Loss: 0.08052894473075867\n",
      "Iteration 97, Batch: 0, Loss: 0.0781354233622551\n",
      "Iteration 97, Batch: 1, Loss: 0.0932573676109314\n",
      "Iteration 97, Batch: 2, Loss: 0.08263654261827469\n",
      "Iteration 97, Batch: 3, Loss: 0.07904458045959473\n",
      "Iteration 97, Batch: 4, Loss: 0.08295522630214691\n",
      "Iteration 97, Batch: 5, Loss: 0.041358672082424164\n",
      "Iteration 97, Batch: 6, Loss: 0.0757860615849495\n",
      "Iteration 97, Batch: 7, Loss: 0.11780258268117905\n",
      "Iteration 97, Batch: 8, Loss: 0.0786832720041275\n",
      "Iteration 97, Batch: 9, Loss: 0.10005414485931396\n",
      "Iteration 97, Batch: 10, Loss: 0.07366246730089188\n",
      "Iteration 97, Batch: 11, Loss: 0.07009986788034439\n",
      "Iteration 97, Batch: 12, Loss: 0.06800034642219543\n",
      "Iteration 97, Batch: 13, Loss: 0.09719456732273102\n",
      "Iteration 97, Batch: 14, Loss: 0.0706038773059845\n",
      "Iteration 97, Batch: 15, Loss: 0.1093297004699707\n",
      "Iteration 97, Batch: 16, Loss: 0.11766614764928818\n",
      "Iteration 97, Batch: 17, Loss: 0.10074109584093094\n",
      "Iteration 97, Batch: 18, Loss: 0.07028131186962128\n",
      "Iteration 97, Batch: 19, Loss: 0.09393256902694702\n",
      "Iteration 97, Batch: 20, Loss: 0.07600121945142746\n",
      "Iteration 97, Batch: 21, Loss: 0.06610070914030075\n",
      "Iteration 97, Batch: 22, Loss: 0.07978956401348114\n",
      "Iteration 97, Batch: 23, Loss: 0.06919773668050766\n",
      "Iteration 97, Batch: 24, Loss: 0.0615725964307785\n",
      "Iteration 97, Batch: 25, Loss: 0.04433928430080414\n",
      "Iteration 97, Batch: 26, Loss: 0.04724550247192383\n",
      "Iteration 97, Batch: 27, Loss: 0.05818108096718788\n",
      "Iteration 97, Batch: 28, Loss: 0.10007023811340332\n",
      "Iteration 97, Batch: 29, Loss: 0.07654176652431488\n",
      "Iteration 97, Batch: 30, Loss: 0.0807809829711914\n",
      "Iteration 97, Batch: 31, Loss: 0.09031886607408524\n",
      "Iteration 97, Batch: 32, Loss: 0.06443388760089874\n",
      "Iteration 97, Batch: 33, Loss: 0.1338246613740921\n",
      "Iteration 97, Batch: 34, Loss: 0.0847865492105484\n",
      "Iteration 97, Batch: 35, Loss: 0.06701608747243881\n",
      "Iteration 97, Batch: 36, Loss: 0.05542973056435585\n",
      "Iteration 97, Batch: 37, Loss: 0.06500856578350067\n",
      "Iteration 97, Batch: 38, Loss: 0.07989641278982162\n",
      "Iteration 97, Batch: 39, Loss: 0.09382440149784088\n",
      "Iteration 97, Batch: 40, Loss: 0.10148856043815613\n",
      "Iteration 97, Batch: 41, Loss: 0.11064853519201279\n",
      "Iteration 97, Batch: 42, Loss: 0.1234082356095314\n",
      "Iteration 97, Batch: 43, Loss: 0.09374838322401047\n",
      "Iteration 97, Batch: 44, Loss: 0.10226033627986908\n",
      "Iteration 97, Batch: 45, Loss: 0.07511992007493973\n",
      "Iteration 97, Batch: 46, Loss: 0.08626416325569153\n",
      "Iteration 97, Batch: 47, Loss: 0.10354157537221909\n",
      "Iteration 97, Batch: 48, Loss: 0.06991294026374817\n",
      "Iteration 97, Batch: 49, Loss: 0.09077727794647217\n",
      "Iteration 98, Batch: 0, Loss: 0.058531686663627625\n",
      "Iteration 98, Batch: 1, Loss: 0.09867740422487259\n",
      "Iteration 98, Batch: 2, Loss: 0.0834163948893547\n",
      "Iteration 98, Batch: 3, Loss: 0.09216214716434479\n",
      "Iteration 98, Batch: 4, Loss: 0.08820675313472748\n",
      "Iteration 98, Batch: 5, Loss: 0.08142910152673721\n",
      "Iteration 98, Batch: 6, Loss: 0.1039796993136406\n",
      "Iteration 98, Batch: 7, Loss: 0.07412925362586975\n",
      "Iteration 98, Batch: 8, Loss: 0.06682655960321426\n",
      "Iteration 98, Batch: 9, Loss: 0.07170847803354263\n",
      "Iteration 98, Batch: 10, Loss: 0.07694443315267563\n",
      "Iteration 98, Batch: 11, Loss: 0.07015351951122284\n",
      "Iteration 98, Batch: 12, Loss: 0.08368757367134094\n",
      "Iteration 98, Batch: 13, Loss: 0.08643655478954315\n",
      "Iteration 98, Batch: 14, Loss: 0.10304901748895645\n",
      "Iteration 98, Batch: 15, Loss: 0.10948196053504944\n",
      "Iteration 98, Batch: 16, Loss: 0.08763299882411957\n",
      "Iteration 98, Batch: 17, Loss: 0.10359367728233337\n",
      "Iteration 98, Batch: 18, Loss: 0.08497972041368484\n",
      "Iteration 98, Batch: 19, Loss: 0.09205383062362671\n",
      "Iteration 98, Batch: 20, Loss: 0.08159417659044266\n",
      "Iteration 98, Batch: 21, Loss: 0.06603074818849564\n",
      "Iteration 98, Batch: 22, Loss: 0.11610035598278046\n",
      "Iteration 98, Batch: 23, Loss: 0.09056813269853592\n",
      "Iteration 98, Batch: 24, Loss: 0.10006918013095856\n",
      "Iteration 98, Batch: 25, Loss: 0.10366310179233551\n",
      "Iteration 98, Batch: 26, Loss: 0.07635338604450226\n",
      "Iteration 98, Batch: 27, Loss: 0.10442999005317688\n",
      "Iteration 98, Batch: 28, Loss: 0.0760037750005722\n",
      "Iteration 98, Batch: 29, Loss: 0.11485888808965683\n",
      "Iteration 98, Batch: 30, Loss: 0.0836775004863739\n",
      "Iteration 98, Batch: 31, Loss: 0.0554584264755249\n",
      "Iteration 98, Batch: 32, Loss: 0.05657462030649185\n",
      "Iteration 98, Batch: 33, Loss: 0.06577476114034653\n",
      "Iteration 98, Batch: 34, Loss: 0.07582882791757584\n",
      "Iteration 98, Batch: 35, Loss: 0.07628826797008514\n",
      "Iteration 98, Batch: 36, Loss: 0.0700470432639122\n",
      "Iteration 98, Batch: 37, Loss: 0.05277135223150253\n",
      "Iteration 98, Batch: 38, Loss: 0.09161017090082169\n",
      "Iteration 98, Batch: 39, Loss: 0.0620635449886322\n",
      "Iteration 98, Batch: 40, Loss: 0.07493636012077332\n",
      "Iteration 98, Batch: 41, Loss: 0.06673697382211685\n",
      "Iteration 98, Batch: 42, Loss: 0.055106453597545624\n",
      "Iteration 98, Batch: 43, Loss: 0.089237280189991\n",
      "Iteration 98, Batch: 44, Loss: 0.08860605210065842\n",
      "Iteration 98, Batch: 45, Loss: 0.08741654455661774\n",
      "Iteration 98, Batch: 46, Loss: 0.07267626374959946\n",
      "Iteration 98, Batch: 47, Loss: 0.058497264981269836\n",
      "Iteration 98, Batch: 48, Loss: 0.04885933920741081\n",
      "Iteration 98, Batch: 49, Loss: 0.04204239696264267\n",
      "Iteration 99, Batch: 0, Loss: 0.08626392483711243\n",
      "Iteration 99, Batch: 1, Loss: 0.060676489025354385\n",
      "Iteration 99, Batch: 2, Loss: 0.07790055871009827\n",
      "Iteration 99, Batch: 3, Loss: 0.04201089218258858\n",
      "Iteration 99, Batch: 4, Loss: 0.08634139597415924\n",
      "Iteration 99, Batch: 5, Loss: 0.054343461990356445\n",
      "Iteration 99, Batch: 6, Loss: 0.0876166820526123\n",
      "Iteration 99, Batch: 7, Loss: 0.07070627808570862\n",
      "Iteration 99, Batch: 8, Loss: 0.09047972410917282\n",
      "Iteration 99, Batch: 9, Loss: 0.07294358313083649\n",
      "Iteration 99, Batch: 10, Loss: 0.060145772993564606\n",
      "Iteration 99, Batch: 11, Loss: 0.06487658619880676\n",
      "Iteration 99, Batch: 12, Loss: 0.06840173900127411\n",
      "Iteration 99, Batch: 13, Loss: 0.09229400753974915\n",
      "Iteration 99, Batch: 14, Loss: 0.05283447727560997\n",
      "Iteration 99, Batch: 15, Loss: 0.061601437628269196\n",
      "Iteration 99, Batch: 16, Loss: 0.06889861822128296\n",
      "Iteration 99, Batch: 17, Loss: 0.08734513819217682\n",
      "Iteration 99, Batch: 18, Loss: 0.06560217589139938\n",
      "Iteration 99, Batch: 19, Loss: 0.06848442554473877\n",
      "Iteration 99, Batch: 20, Loss: 0.06202741339802742\n",
      "Iteration 99, Batch: 21, Loss: 0.08954845368862152\n",
      "Iteration 99, Batch: 22, Loss: 0.06802177429199219\n",
      "Iteration 99, Batch: 23, Loss: 0.06877941638231277\n",
      "Iteration 99, Batch: 24, Loss: 0.06348317116498947\n",
      "Iteration 99, Batch: 25, Loss: 0.06068001687526703\n",
      "Iteration 99, Batch: 26, Loss: 0.04084111750125885\n",
      "Iteration 99, Batch: 27, Loss: 0.03907531499862671\n",
      "Iteration 99, Batch: 28, Loss: 0.06613750010728836\n",
      "Iteration 99, Batch: 29, Loss: 0.0986064150929451\n",
      "Iteration 99, Batch: 30, Loss: 0.07312306761741638\n",
      "Iteration 99, Batch: 31, Loss: 0.04287274181842804\n",
      "Iteration 99, Batch: 32, Loss: 0.053207311779260635\n",
      "Iteration 99, Batch: 33, Loss: 0.08203758299350739\n",
      "Iteration 99, Batch: 34, Loss: 0.0647599995136261\n",
      "Iteration 99, Batch: 35, Loss: 0.1024065911769867\n",
      "Iteration 99, Batch: 36, Loss: 0.047665517777204514\n",
      "Iteration 99, Batch: 37, Loss: 0.08798585087060928\n",
      "Iteration 99, Batch: 38, Loss: 0.1101660504937172\n",
      "Iteration 99, Batch: 39, Loss: 0.06495533138513565\n",
      "Iteration 99, Batch: 40, Loss: 0.06597137451171875\n",
      "Iteration 99, Batch: 41, Loss: 0.11616764962673187\n",
      "Iteration 99, Batch: 42, Loss: 0.09620401263237\n",
      "Iteration 99, Batch: 43, Loss: 0.07290037721395493\n",
      "Iteration 99, Batch: 44, Loss: 0.06141610071063042\n",
      "Iteration 99, Batch: 45, Loss: 0.07814061641693115\n",
      "Iteration 99, Batch: 46, Loss: 0.09784983098506927\n",
      "Iteration 99, Batch: 47, Loss: 0.0332801379263401\n",
      "Iteration 99, Batch: 48, Loss: 0.046558015048503876\n",
      "Iteration 99, Batch: 49, Loss: 0.04600096121430397\n",
      "Iteration 100, Batch: 0, Loss: 0.08533911406993866\n",
      "Iteration 100, Batch: 1, Loss: 0.06966648250818253\n",
      "Iteration 100, Batch: 2, Loss: 0.07807065546512604\n",
      "Iteration 100, Batch: 3, Loss: 0.10720435529947281\n",
      "Iteration 100, Batch: 4, Loss: 0.1047845259308815\n",
      "Iteration 100, Batch: 5, Loss: 0.08321507275104523\n",
      "Iteration 100, Batch: 6, Loss: 0.08838599920272827\n",
      "Iteration 100, Batch: 7, Loss: 0.08452428877353668\n",
      "Iteration 100, Batch: 8, Loss: 0.07736871391534805\n",
      "Iteration 100, Batch: 9, Loss: 0.059503305703401566\n",
      "Iteration 100, Batch: 10, Loss: 0.06262686848640442\n",
      "Iteration 100, Batch: 11, Loss: 0.08775030076503754\n",
      "Iteration 100, Batch: 12, Loss: 0.05966757982969284\n",
      "Iteration 100, Batch: 13, Loss: 0.09661471843719482\n",
      "Iteration 100, Batch: 14, Loss: 0.07384948432445526\n",
      "Iteration 100, Batch: 15, Loss: 0.0669470950961113\n",
      "Iteration 100, Batch: 16, Loss: 0.06397289037704468\n",
      "Iteration 100, Batch: 17, Loss: 0.06657995283603668\n",
      "Iteration 100, Batch: 18, Loss: 0.07462407648563385\n",
      "Iteration 100, Batch: 19, Loss: 0.05882753059267998\n",
      "Iteration 100, Batch: 20, Loss: 0.06373234838247299\n",
      "Iteration 100, Batch: 21, Loss: 0.07285237312316895\n",
      "Iteration 100, Batch: 22, Loss: 0.10450177639722824\n",
      "Iteration 100, Batch: 23, Loss: 0.06858304142951965\n",
      "Iteration 100, Batch: 24, Loss: 0.06716296821832657\n",
      "Iteration 100, Batch: 25, Loss: 0.0854177251458168\n",
      "Iteration 100, Batch: 26, Loss: 0.0612516924738884\n",
      "Iteration 100, Batch: 27, Loss: 0.0894189402461052\n",
      "Iteration 100, Batch: 28, Loss: 0.06736480444669724\n",
      "Iteration 100, Batch: 29, Loss: 0.09012287110090256\n",
      "Iteration 100, Batch: 30, Loss: 0.09480749070644379\n",
      "Iteration 100, Batch: 31, Loss: 0.078301340341568\n",
      "Iteration 100, Batch: 32, Loss: 0.07154473662376404\n",
      "Iteration 100, Batch: 33, Loss: 0.05987068638205528\n",
      "Iteration 100, Batch: 34, Loss: 0.06608747690916061\n",
      "Iteration 100, Batch: 35, Loss: 0.08535454422235489\n",
      "Iteration 100, Batch: 36, Loss: 0.035972800105810165\n",
      "Iteration 100, Batch: 37, Loss: 0.07446525245904922\n",
      "Iteration 100, Batch: 38, Loss: 0.061244916170835495\n",
      "Iteration 100, Batch: 39, Loss: 0.05844324827194214\n",
      "Iteration 100, Batch: 40, Loss: 0.0779719203710556\n",
      "Iteration 100, Batch: 41, Loss: 0.058016423135995865\n",
      "Iteration 100, Batch: 42, Loss: 0.0491451621055603\n",
      "Iteration 100, Batch: 43, Loss: 0.0746866911649704\n",
      "Iteration 100, Batch: 44, Loss: 0.05321372300386429\n",
      "Iteration 100, Batch: 45, Loss: 0.06773332506418228\n",
      "Iteration 100, Batch: 46, Loss: 0.056767139583826065\n",
      "Iteration 100, Batch: 47, Loss: 0.08196552842855453\n",
      "Iteration 100, Batch: 48, Loss: 0.07958199083805084\n",
      "Iteration 100, Batch: 49, Loss: 0.061219166964292526\n",
      "Iteration 101, Batch: 0, Loss: 0.06936418265104294\n",
      "Iteration 101, Batch: 1, Loss: 0.058587443083524704\n",
      "Iteration 101, Batch: 2, Loss: 0.07788397371768951\n",
      "Iteration 101, Batch: 3, Loss: 0.06587957590818405\n",
      "Iteration 101, Batch: 4, Loss: 0.0514572374522686\n",
      "Iteration 101, Batch: 5, Loss: 0.07306826114654541\n",
      "Iteration 101, Batch: 6, Loss: 0.07400135695934296\n",
      "Iteration 101, Batch: 7, Loss: 0.07695858925580978\n",
      "Iteration 101, Batch: 8, Loss: 0.06780777126550674\n",
      "Iteration 101, Batch: 9, Loss: 0.06149352714419365\n",
      "Iteration 101, Batch: 10, Loss: 0.07989633083343506\n",
      "Iteration 101, Batch: 11, Loss: 0.06784600764513016\n",
      "Iteration 101, Batch: 12, Loss: 0.06929738074541092\n",
      "Iteration 101, Batch: 13, Loss: 0.07005826383829117\n",
      "Iteration 101, Batch: 14, Loss: 0.07304529845714569\n",
      "Iteration 101, Batch: 15, Loss: 0.07527455687522888\n",
      "Iteration 101, Batch: 16, Loss: 0.061121825128793716\n",
      "Iteration 101, Batch: 17, Loss: 0.050024136900901794\n",
      "Iteration 101, Batch: 18, Loss: 0.10519025474786758\n",
      "Iteration 101, Batch: 19, Loss: 0.078240305185318\n",
      "Iteration 101, Batch: 20, Loss: 0.09286057204008102\n",
      "Iteration 101, Batch: 21, Loss: 0.0969577208161354\n",
      "Iteration 101, Batch: 22, Loss: 0.0952538251876831\n",
      "Iteration 101, Batch: 23, Loss: 0.08355748653411865\n",
      "Iteration 101, Batch: 24, Loss: 0.06393742561340332\n",
      "Iteration 101, Batch: 25, Loss: 0.06961055845022202\n",
      "Iteration 101, Batch: 26, Loss: 0.06467103958129883\n",
      "Iteration 101, Batch: 27, Loss: 0.043842535465955734\n",
      "Iteration 101, Batch: 28, Loss: 0.06251803040504456\n",
      "Iteration 101, Batch: 29, Loss: 0.06483973562717438\n",
      "Iteration 101, Batch: 30, Loss: 0.08381462097167969\n",
      "Iteration 101, Batch: 31, Loss: 0.04937048628926277\n",
      "Iteration 101, Batch: 32, Loss: 0.061877284198999405\n",
      "Iteration 101, Batch: 33, Loss: 0.07238972187042236\n",
      "Iteration 101, Batch: 34, Loss: 0.09454574435949326\n",
      "Iteration 101, Batch: 35, Loss: 0.07573113590478897\n",
      "Iteration 101, Batch: 36, Loss: 0.06533292680978775\n",
      "Iteration 101, Batch: 37, Loss: 0.046026963740587234\n",
      "Iteration 101, Batch: 38, Loss: 0.06169847026467323\n",
      "Iteration 101, Batch: 39, Loss: 0.07663751393556595\n",
      "Iteration 101, Batch: 40, Loss: 0.09177951514720917\n",
      "Iteration 101, Batch: 41, Loss: 0.05745800957083702\n",
      "Iteration 101, Batch: 42, Loss: 0.08112825453281403\n",
      "Iteration 101, Batch: 43, Loss: 0.0712132528424263\n",
      "Iteration 101, Batch: 44, Loss: 0.06648905575275421\n",
      "Iteration 101, Batch: 45, Loss: 0.062441129237413406\n",
      "Iteration 101, Batch: 46, Loss: 0.056177251040935516\n",
      "Iteration 101, Batch: 47, Loss: 0.07754562795162201\n",
      "Iteration 101, Batch: 48, Loss: 0.07710631191730499\n",
      "Iteration 101, Batch: 49, Loss: 0.11658122390508652\n",
      "Iteration 102, Batch: 0, Loss: 0.09012912213802338\n",
      "Iteration 102, Batch: 1, Loss: 0.07521968334913254\n",
      "Iteration 102, Batch: 2, Loss: 0.08036024123430252\n",
      "Iteration 102, Batch: 3, Loss: 0.07193893939256668\n",
      "Iteration 102, Batch: 4, Loss: 0.04512167349457741\n",
      "Iteration 102, Batch: 5, Loss: 0.08763507753610611\n",
      "Iteration 102, Batch: 6, Loss: 0.04033828154206276\n",
      "Iteration 102, Batch: 7, Loss: 0.06649566441774368\n",
      "Iteration 102, Batch: 8, Loss: 0.07067389041185379\n",
      "Iteration 102, Batch: 9, Loss: 0.07581109553575516\n",
      "Iteration 102, Batch: 10, Loss: 0.0726727768778801\n",
      "Iteration 102, Batch: 11, Loss: 0.06519394367933273\n",
      "Iteration 102, Batch: 12, Loss: 0.054305218160152435\n",
      "Iteration 102, Batch: 13, Loss: 0.07505262643098831\n",
      "Iteration 102, Batch: 14, Loss: 0.04430625960230827\n",
      "Iteration 102, Batch: 15, Loss: 0.06495029479265213\n",
      "Iteration 102, Batch: 16, Loss: 0.06663651764392853\n",
      "Iteration 102, Batch: 17, Loss: 0.08099189400672913\n",
      "Iteration 102, Batch: 18, Loss: 0.1026567593216896\n",
      "Iteration 102, Batch: 19, Loss: 0.0676647424697876\n",
      "Iteration 102, Batch: 20, Loss: 0.08551251888275146\n",
      "Iteration 102, Batch: 21, Loss: 0.10672634840011597\n",
      "Iteration 102, Batch: 22, Loss: 0.0860772430896759\n",
      "Iteration 102, Batch: 23, Loss: 0.06324046105146408\n",
      "Iteration 102, Batch: 24, Loss: 0.07108139991760254\n",
      "Iteration 102, Batch: 25, Loss: 0.05303814262151718\n",
      "Iteration 102, Batch: 26, Loss: 0.06196312606334686\n",
      "Iteration 102, Batch: 27, Loss: 0.08789090812206268\n",
      "Iteration 102, Batch: 28, Loss: 0.06505469232797623\n",
      "Iteration 102, Batch: 29, Loss: 0.04853718727827072\n",
      "Iteration 102, Batch: 30, Loss: 0.06837170571088791\n",
      "Iteration 102, Batch: 31, Loss: 0.07332093268632889\n",
      "Iteration 102, Batch: 32, Loss: 0.07872119545936584\n",
      "Iteration 102, Batch: 33, Loss: 0.06612753868103027\n",
      "Iteration 102, Batch: 34, Loss: 0.07153849303722382\n",
      "Iteration 102, Batch: 35, Loss: 0.07896189391613007\n",
      "Iteration 102, Batch: 36, Loss: 0.048955220729112625\n",
      "Iteration 102, Batch: 37, Loss: 0.06726609915494919\n",
      "Iteration 102, Batch: 38, Loss: 0.08856029063463211\n",
      "Iteration 102, Batch: 39, Loss: 0.05439416319131851\n",
      "Iteration 102, Batch: 40, Loss: 0.06828884780406952\n",
      "Iteration 102, Batch: 41, Loss: 0.05516696721315384\n",
      "Iteration 102, Batch: 42, Loss: 0.10529972612857819\n",
      "Iteration 102, Batch: 43, Loss: 0.09452472627162933\n",
      "Iteration 102, Batch: 44, Loss: 0.12168385833501816\n",
      "Iteration 102, Batch: 45, Loss: 0.06778208911418915\n",
      "Iteration 102, Batch: 46, Loss: 0.06746605038642883\n",
      "Iteration 102, Batch: 47, Loss: 0.0909510999917984\n",
      "Iteration 102, Batch: 48, Loss: 0.07094921171665192\n",
      "Iteration 102, Batch: 49, Loss: 0.11920730024576187\n",
      "Iteration 103, Batch: 0, Loss: 0.0701601579785347\n",
      "Iteration 103, Batch: 1, Loss: 0.09968183189630508\n",
      "Iteration 103, Batch: 2, Loss: 0.08142881840467453\n",
      "Iteration 103, Batch: 3, Loss: 0.055386122316122055\n",
      "Iteration 103, Batch: 4, Loss: 0.08195826411247253\n",
      "Iteration 103, Batch: 5, Loss: 0.07192779332399368\n",
      "Iteration 103, Batch: 6, Loss: 0.074055977165699\n",
      "Iteration 103, Batch: 7, Loss: 0.08978009223937988\n",
      "Iteration 103, Batch: 8, Loss: 0.08889175206422806\n",
      "Iteration 103, Batch: 9, Loss: 0.04268984869122505\n",
      "Iteration 103, Batch: 10, Loss: 0.048656366765499115\n",
      "Iteration 103, Batch: 11, Loss: 0.0746581181883812\n",
      "Iteration 103, Batch: 12, Loss: 0.10188102722167969\n",
      "Iteration 103, Batch: 13, Loss: 0.06813356280326843\n",
      "Iteration 103, Batch: 14, Loss: 0.0702551081776619\n",
      "Iteration 103, Batch: 15, Loss: 0.09627679735422134\n",
      "Iteration 103, Batch: 16, Loss: 0.08822928369045258\n",
      "Iteration 103, Batch: 17, Loss: 0.0594659298658371\n",
      "Iteration 103, Batch: 18, Loss: 0.09207624942064285\n",
      "Iteration 103, Batch: 19, Loss: 0.06641515344381332\n",
      "Iteration 103, Batch: 20, Loss: 0.05151842162013054\n",
      "Iteration 103, Batch: 21, Loss: 0.0894077867269516\n",
      "Iteration 103, Batch: 22, Loss: 0.034929271787405014\n",
      "Iteration 103, Batch: 23, Loss: 0.07811643183231354\n",
      "Iteration 103, Batch: 24, Loss: 0.05594560503959656\n",
      "Iteration 103, Batch: 25, Loss: 0.09476619213819504\n",
      "Iteration 103, Batch: 26, Loss: 0.07070063054561615\n",
      "Iteration 103, Batch: 27, Loss: 0.05399222671985626\n",
      "Iteration 103, Batch: 28, Loss: 0.05216851085424423\n",
      "Iteration 103, Batch: 29, Loss: 0.037440333515405655\n",
      "Iteration 103, Batch: 30, Loss: 0.052968937903642654\n",
      "Iteration 103, Batch: 31, Loss: 0.0800771564245224\n",
      "Iteration 103, Batch: 32, Loss: 0.055809445679187775\n",
      "Iteration 103, Batch: 33, Loss: 0.08468672633171082\n",
      "Iteration 103, Batch: 34, Loss: 0.06332429498434067\n",
      "Iteration 103, Batch: 35, Loss: 0.09432767331600189\n",
      "Iteration 103, Batch: 36, Loss: 0.06433982402086258\n",
      "Iteration 103, Batch: 37, Loss: 0.07466522604227066\n",
      "Iteration 103, Batch: 38, Loss: 0.07819443196058273\n",
      "Iteration 103, Batch: 39, Loss: 0.046627625823020935\n",
      "Iteration 103, Batch: 40, Loss: 0.07110816240310669\n",
      "Iteration 103, Batch: 41, Loss: 0.07083899527788162\n",
      "Iteration 103, Batch: 42, Loss: 0.06112230569124222\n",
      "Iteration 103, Batch: 43, Loss: 0.10867886990308762\n",
      "Iteration 103, Batch: 44, Loss: 0.09352190047502518\n",
      "Iteration 103, Batch: 45, Loss: 0.06459648162126541\n",
      "Iteration 103, Batch: 46, Loss: 0.07520603388547897\n",
      "Iteration 103, Batch: 47, Loss: 0.10922244191169739\n",
      "Iteration 103, Batch: 48, Loss: 0.07375727593898773\n",
      "Iteration 103, Batch: 49, Loss: 0.07356802374124527\n",
      "Iteration 104, Batch: 0, Loss: 0.11219614744186401\n",
      "Iteration 104, Batch: 1, Loss: 0.05890887975692749\n",
      "Iteration 104, Batch: 2, Loss: 0.057850345969200134\n",
      "Iteration 104, Batch: 3, Loss: 0.05220559984445572\n",
      "Iteration 104, Batch: 4, Loss: 0.06823115795850754\n",
      "Iteration 104, Batch: 5, Loss: 0.07926297187805176\n",
      "Iteration 104, Batch: 6, Loss: 0.07095251977443695\n",
      "Iteration 104, Batch: 7, Loss: 0.05594663321971893\n",
      "Iteration 104, Batch: 8, Loss: 0.07529197633266449\n",
      "Iteration 104, Batch: 9, Loss: 0.06726755946874619\n",
      "Iteration 104, Batch: 10, Loss: 0.06904881447553635\n",
      "Iteration 104, Batch: 11, Loss: 0.06941255182027817\n",
      "Iteration 104, Batch: 12, Loss: 0.07123522460460663\n",
      "Iteration 104, Batch: 13, Loss: 0.09465888887643814\n",
      "Iteration 104, Batch: 14, Loss: 0.07611345499753952\n",
      "Iteration 104, Batch: 15, Loss: 0.08180774748325348\n",
      "Iteration 104, Batch: 16, Loss: 0.10239391028881073\n",
      "Iteration 104, Batch: 17, Loss: 0.07596186548471451\n",
      "Iteration 104, Batch: 18, Loss: 0.07474323362112045\n",
      "Iteration 104, Batch: 19, Loss: 0.07698261737823486\n",
      "Iteration 104, Batch: 20, Loss: 0.05970669910311699\n",
      "Iteration 104, Batch: 21, Loss: 0.04497780650854111\n",
      "Iteration 104, Batch: 22, Loss: 0.06670749187469482\n",
      "Iteration 104, Batch: 23, Loss: 0.06901363283395767\n",
      "Iteration 104, Batch: 24, Loss: 0.0702279806137085\n",
      "Iteration 104, Batch: 25, Loss: 0.0484144389629364\n",
      "Iteration 104, Batch: 26, Loss: 0.06294727325439453\n",
      "Iteration 104, Batch: 27, Loss: 0.08091676235198975\n",
      "Iteration 104, Batch: 28, Loss: 0.08550170809030533\n",
      "Iteration 104, Batch: 29, Loss: 0.07807429134845734\n",
      "Iteration 104, Batch: 30, Loss: 0.0748167410492897\n",
      "Iteration 104, Batch: 31, Loss: 0.07918395102024078\n",
      "Iteration 104, Batch: 32, Loss: 0.07481147348880768\n",
      "Iteration 104, Batch: 33, Loss: 0.11221949011087418\n",
      "Iteration 104, Batch: 34, Loss: 0.07796783000230789\n",
      "Iteration 104, Batch: 35, Loss: 0.05709388479590416\n",
      "Iteration 104, Batch: 36, Loss: 0.06430968642234802\n",
      "Iteration 104, Batch: 37, Loss: 0.07907186448574066\n",
      "Iteration 104, Batch: 38, Loss: 0.08221139013767242\n",
      "Iteration 104, Batch: 39, Loss: 0.0745411217212677\n",
      "Iteration 104, Batch: 40, Loss: 0.06394165009260178\n",
      "Iteration 104, Batch: 41, Loss: 0.06965941190719604\n",
      "Iteration 104, Batch: 42, Loss: 0.06065037474036217\n",
      "Iteration 104, Batch: 43, Loss: 0.06961148977279663\n",
      "Iteration 104, Batch: 44, Loss: 0.06869830191135406\n",
      "Iteration 104, Batch: 45, Loss: 0.07398176193237305\n",
      "Iteration 104, Batch: 46, Loss: 0.093636155128479\n",
      "Iteration 104, Batch: 47, Loss: 0.052781522274017334\n",
      "Iteration 104, Batch: 48, Loss: 0.05136755853891373\n",
      "Iteration 104, Batch: 49, Loss: 0.07298171520233154\n",
      "Iteration 105, Batch: 0, Loss: 0.046312399208545685\n",
      "Iteration 105, Batch: 1, Loss: 0.058322466909885406\n",
      "Iteration 105, Batch: 2, Loss: 0.07882265746593475\n",
      "Iteration 105, Batch: 3, Loss: 0.06824441999197006\n",
      "Iteration 105, Batch: 4, Loss: 0.09003762900829315\n",
      "Iteration 105, Batch: 5, Loss: 0.09312435239553452\n",
      "Iteration 105, Batch: 6, Loss: 0.08089053630828857\n",
      "Iteration 105, Batch: 7, Loss: 0.049513958394527435\n",
      "Iteration 105, Batch: 8, Loss: 0.07200157642364502\n",
      "Iteration 105, Batch: 9, Loss: 0.04606262966990471\n",
      "Iteration 105, Batch: 10, Loss: 0.05446727201342583\n",
      "Iteration 105, Batch: 11, Loss: 0.0979585275053978\n",
      "Iteration 105, Batch: 12, Loss: 0.07115297019481659\n",
      "Iteration 105, Batch: 13, Loss: 0.10800443589687347\n",
      "Iteration 105, Batch: 14, Loss: 0.06973756849765778\n",
      "Iteration 105, Batch: 15, Loss: 0.10461150109767914\n",
      "Iteration 105, Batch: 16, Loss: 0.09722612053155899\n",
      "Iteration 105, Batch: 17, Loss: 0.0685591772198677\n",
      "Iteration 105, Batch: 18, Loss: 0.07989077270030975\n",
      "Iteration 105, Batch: 19, Loss: 0.10563936084508896\n",
      "Iteration 105, Batch: 20, Loss: 0.06746698170900345\n",
      "Iteration 105, Batch: 21, Loss: 0.06221523508429527\n",
      "Iteration 105, Batch: 22, Loss: 0.03923972323536873\n",
      "Iteration 105, Batch: 23, Loss: 0.032943785190582275\n",
      "Iteration 105, Batch: 24, Loss: 0.07547599822282791\n",
      "Iteration 105, Batch: 25, Loss: 0.07792004197835922\n",
      "Iteration 105, Batch: 26, Loss: 0.07350922375917435\n",
      "Iteration 105, Batch: 27, Loss: 0.07443933933973312\n",
      "Iteration 105, Batch: 28, Loss: 0.06781789660453796\n",
      "Iteration 105, Batch: 29, Loss: 0.0782119557261467\n",
      "Iteration 105, Batch: 30, Loss: 0.09292735904455185\n",
      "Iteration 105, Batch: 31, Loss: 0.07232213020324707\n",
      "Iteration 105, Batch: 32, Loss: 0.12291163951158524\n",
      "Iteration 105, Batch: 33, Loss: 0.09656398743391037\n",
      "Iteration 105, Batch: 34, Loss: 0.08117775619029999\n",
      "Iteration 105, Batch: 35, Loss: 0.07960613071918488\n",
      "Iteration 105, Batch: 36, Loss: 0.07723487913608551\n",
      "Iteration 105, Batch: 37, Loss: 0.08533433824777603\n",
      "Iteration 105, Batch: 38, Loss: 0.03512972593307495\n",
      "Iteration 105, Batch: 39, Loss: 0.05505535006523132\n",
      "Iteration 105, Batch: 40, Loss: 0.07754684239625931\n",
      "Iteration 105, Batch: 41, Loss: 0.09553752839565277\n",
      "Iteration 105, Batch: 42, Loss: 0.07882877439260483\n",
      "Iteration 105, Batch: 43, Loss: 0.08859904855489731\n",
      "Iteration 105, Batch: 44, Loss: 0.05525827407836914\n",
      "Iteration 105, Batch: 45, Loss: 0.048053789883852005\n",
      "Iteration 105, Batch: 46, Loss: 0.08270969241857529\n",
      "Iteration 105, Batch: 47, Loss: 0.0836140438914299\n",
      "Iteration 105, Batch: 48, Loss: 0.096816785633564\n",
      "Iteration 105, Batch: 49, Loss: 0.08595208823680878\n",
      "Iteration 106, Batch: 0, Loss: 0.05599071830511093\n",
      "Iteration 106, Batch: 1, Loss: 0.05456210672855377\n",
      "Iteration 106, Batch: 2, Loss: 0.07331981509923935\n",
      "Iteration 106, Batch: 3, Loss: 0.0721096619963646\n",
      "Iteration 106, Batch: 4, Loss: 0.06436058133840561\n",
      "Iteration 106, Batch: 5, Loss: 0.0995732843875885\n",
      "Iteration 106, Batch: 6, Loss: 0.10474108159542084\n",
      "Iteration 106, Batch: 7, Loss: 0.05692499876022339\n",
      "Iteration 106, Batch: 8, Loss: 0.09475317597389221\n",
      "Iteration 106, Batch: 9, Loss: 0.07561841607093811\n",
      "Iteration 106, Batch: 10, Loss: 0.08216821402311325\n",
      "Iteration 106, Batch: 11, Loss: 0.06843162328004837\n",
      "Iteration 106, Batch: 12, Loss: 0.07973681390285492\n",
      "Iteration 106, Batch: 13, Loss: 0.08185391128063202\n",
      "Iteration 106, Batch: 14, Loss: 0.06486838310956955\n",
      "Iteration 106, Batch: 15, Loss: 0.09732852876186371\n",
      "Iteration 106, Batch: 16, Loss: 0.059661492705345154\n",
      "Iteration 106, Batch: 17, Loss: 0.059178341180086136\n",
      "Iteration 106, Batch: 18, Loss: 0.07114831358194351\n",
      "Iteration 106, Batch: 19, Loss: 0.07106679677963257\n",
      "Iteration 106, Batch: 20, Loss: 0.0984044224023819\n",
      "Iteration 106, Batch: 21, Loss: 0.075084388256073\n",
      "Iteration 106, Batch: 22, Loss: 0.08010878413915634\n",
      "Iteration 106, Batch: 23, Loss: 0.08479773998260498\n",
      "Iteration 106, Batch: 24, Loss: 0.06764046847820282\n",
      "Iteration 106, Batch: 25, Loss: 0.045456662774086\n",
      "Iteration 106, Batch: 26, Loss: 0.08568290621042252\n",
      "Iteration 106, Batch: 27, Loss: 0.04935838654637337\n",
      "Iteration 106, Batch: 28, Loss: 0.07085347175598145\n",
      "Iteration 106, Batch: 29, Loss: 0.07849901914596558\n",
      "Iteration 106, Batch: 30, Loss: 0.09892641007900238\n",
      "Iteration 106, Batch: 31, Loss: 0.061755310744047165\n",
      "Iteration 106, Batch: 32, Loss: 0.042207736521959305\n",
      "Iteration 106, Batch: 33, Loss: 0.08807367831468582\n",
      "Iteration 106, Batch: 34, Loss: 0.0581807978451252\n",
      "Iteration 106, Batch: 35, Loss: 0.08288135379552841\n",
      "Iteration 106, Batch: 36, Loss: 0.083579421043396\n",
      "Iteration 106, Batch: 37, Loss: 0.06225287914276123\n",
      "Iteration 106, Batch: 38, Loss: 0.07850062847137451\n",
      "Iteration 106, Batch: 39, Loss: 0.0879780575633049\n",
      "Iteration 106, Batch: 40, Loss: 0.10520987957715988\n",
      "Iteration 106, Batch: 41, Loss: 0.05997157841920853\n",
      "Iteration 106, Batch: 42, Loss: 0.06978750973939896\n",
      "Iteration 106, Batch: 43, Loss: 0.07418979704380035\n",
      "Iteration 106, Batch: 44, Loss: 0.056196995079517365\n",
      "Iteration 106, Batch: 45, Loss: 0.05299115926027298\n",
      "Iteration 106, Batch: 46, Loss: 0.07187267392873764\n",
      "Iteration 106, Batch: 47, Loss: 0.07174509763717651\n",
      "Iteration 106, Batch: 48, Loss: 0.08296901732683182\n",
      "Iteration 106, Batch: 49, Loss: 0.07075922936201096\n",
      "Iteration 107, Batch: 0, Loss: 0.06689963489770889\n",
      "Iteration 107, Batch: 1, Loss: 0.06312248855829239\n",
      "Iteration 107, Batch: 2, Loss: 0.0510433167219162\n",
      "Iteration 107, Batch: 3, Loss: 0.057576730847358704\n",
      "Iteration 107, Batch: 4, Loss: 0.07379075139760971\n",
      "Iteration 107, Batch: 5, Loss: 0.08371701091527939\n",
      "Iteration 107, Batch: 6, Loss: 0.0983549952507019\n",
      "Iteration 107, Batch: 7, Loss: 0.08043653517961502\n",
      "Iteration 107, Batch: 8, Loss: 0.10792563110589981\n",
      "Iteration 107, Batch: 9, Loss: 0.05508258566260338\n",
      "Iteration 107, Batch: 10, Loss: 0.06280606985092163\n",
      "Iteration 107, Batch: 11, Loss: 0.04411197081208229\n",
      "Iteration 107, Batch: 12, Loss: 0.06515838205814362\n",
      "Iteration 107, Batch: 13, Loss: 0.08619125932455063\n",
      "Iteration 107, Batch: 14, Loss: 0.06341071426868439\n",
      "Iteration 107, Batch: 15, Loss: 0.07830400764942169\n",
      "Iteration 107, Batch: 16, Loss: 0.08586512506008148\n",
      "Iteration 107, Batch: 17, Loss: 0.07934001833200455\n",
      "Iteration 107, Batch: 18, Loss: 0.06986517459154129\n",
      "Iteration 107, Batch: 19, Loss: 0.05380988121032715\n",
      "Iteration 107, Batch: 20, Loss: 0.0714862048625946\n",
      "Iteration 107, Batch: 21, Loss: 0.07484646886587143\n",
      "Iteration 107, Batch: 22, Loss: 0.10173697024583817\n",
      "Iteration 107, Batch: 23, Loss: 0.10232646763324738\n",
      "Iteration 107, Batch: 24, Loss: 0.06795135140419006\n",
      "Iteration 107, Batch: 25, Loss: 0.09508586674928665\n",
      "Iteration 107, Batch: 26, Loss: 0.05904964730143547\n",
      "Iteration 107, Batch: 27, Loss: 0.056295622140169144\n",
      "Iteration 107, Batch: 28, Loss: 0.05318628251552582\n",
      "Iteration 107, Batch: 29, Loss: 0.11377975344657898\n",
      "Iteration 107, Batch: 30, Loss: 0.11357288807630539\n",
      "Iteration 107, Batch: 31, Loss: 0.09192640334367752\n",
      "Iteration 107, Batch: 32, Loss: 0.06995277106761932\n",
      "Iteration 107, Batch: 33, Loss: 0.06963567435741425\n",
      "Iteration 107, Batch: 34, Loss: 0.08782666176557541\n",
      "Iteration 107, Batch: 35, Loss: 0.06306134164333344\n",
      "Iteration 107, Batch: 36, Loss: 0.0704621896147728\n",
      "Iteration 107, Batch: 37, Loss: 0.07773103564977646\n",
      "Iteration 107, Batch: 38, Loss: 0.0765397921204567\n",
      "Iteration 107, Batch: 39, Loss: 0.07540181279182434\n",
      "Iteration 107, Batch: 40, Loss: 0.09580665081739426\n",
      "Iteration 107, Batch: 41, Loss: 0.07445774972438812\n",
      "Iteration 107, Batch: 42, Loss: 0.07503460347652435\n",
      "Iteration 107, Batch: 43, Loss: 0.07317176461219788\n",
      "Iteration 107, Batch: 44, Loss: 0.0575263574719429\n",
      "Iteration 107, Batch: 45, Loss: 0.1114555075764656\n",
      "Iteration 107, Batch: 46, Loss: 0.0620429702103138\n",
      "Iteration 107, Batch: 47, Loss: 0.07144057750701904\n",
      "Iteration 107, Batch: 48, Loss: 0.077361099421978\n",
      "Iteration 107, Batch: 49, Loss: 0.04823725298047066\n",
      "Iteration 108, Batch: 0, Loss: 0.08645715564489365\n",
      "Iteration 108, Batch: 1, Loss: 0.10359165817499161\n",
      "Iteration 108, Batch: 2, Loss: 0.06953577697277069\n",
      "Iteration 108, Batch: 3, Loss: 0.06455983221530914\n",
      "Iteration 108, Batch: 4, Loss: 0.09663692861795425\n",
      "Iteration 108, Batch: 5, Loss: 0.08447787165641785\n",
      "Iteration 108, Batch: 6, Loss: 0.1065070703625679\n",
      "Iteration 108, Batch: 7, Loss: 0.0936422124505043\n",
      "Iteration 108, Batch: 8, Loss: 0.08834123611450195\n",
      "Iteration 108, Batch: 9, Loss: 0.06923217326402664\n",
      "Iteration 108, Batch: 10, Loss: 0.07224644720554352\n",
      "Iteration 108, Batch: 11, Loss: 0.07415124028921127\n",
      "Iteration 108, Batch: 12, Loss: 0.06218734011054039\n",
      "Iteration 108, Batch: 13, Loss: 0.061481256037950516\n",
      "Iteration 108, Batch: 14, Loss: 0.09239797294139862\n",
      "Iteration 108, Batch: 15, Loss: 0.06523287296295166\n",
      "Iteration 108, Batch: 16, Loss: 0.06290625780820847\n",
      "Iteration 108, Batch: 17, Loss: 0.07616516947746277\n",
      "Iteration 108, Batch: 18, Loss: 0.05417368933558464\n",
      "Iteration 108, Batch: 19, Loss: 0.06900427490472794\n",
      "Iteration 108, Batch: 20, Loss: 0.07747412472963333\n",
      "Iteration 108, Batch: 21, Loss: 0.06637910008430481\n",
      "Iteration 108, Batch: 22, Loss: 0.06922698765993118\n",
      "Iteration 108, Batch: 23, Loss: 0.07297677546739578\n",
      "Iteration 108, Batch: 24, Loss: 0.06152402609586716\n",
      "Iteration 108, Batch: 25, Loss: 0.10619315505027771\n",
      "Iteration 108, Batch: 26, Loss: 0.05979222059249878\n",
      "Iteration 108, Batch: 27, Loss: 0.09621083736419678\n",
      "Iteration 108, Batch: 28, Loss: 0.08201467245817184\n",
      "Iteration 108, Batch: 29, Loss: 0.07846470922231674\n",
      "Iteration 108, Batch: 30, Loss: 0.0647643432021141\n",
      "Iteration 108, Batch: 31, Loss: 0.06304053217172623\n",
      "Iteration 108, Batch: 32, Loss: 0.0752546489238739\n",
      "Iteration 108, Batch: 33, Loss: 0.0678495392203331\n",
      "Iteration 108, Batch: 34, Loss: 0.08016757667064667\n",
      "Iteration 108, Batch: 35, Loss: 0.07572987675666809\n",
      "Iteration 108, Batch: 36, Loss: 0.08401899039745331\n",
      "Iteration 108, Batch: 37, Loss: 0.07228650152683258\n",
      "Iteration 108, Batch: 38, Loss: 0.0804869532585144\n",
      "Iteration 108, Batch: 39, Loss: 0.09359664469957352\n",
      "Iteration 108, Batch: 40, Loss: 0.07988845556974411\n",
      "Iteration 108, Batch: 41, Loss: 0.06483963131904602\n",
      "Iteration 108, Batch: 42, Loss: 0.06359338015317917\n",
      "Iteration 108, Batch: 43, Loss: 0.10014217346906662\n",
      "Iteration 108, Batch: 44, Loss: 0.06300663948059082\n",
      "Iteration 108, Batch: 45, Loss: 0.10027504712343216\n",
      "Iteration 108, Batch: 46, Loss: 0.07225317507982254\n",
      "Iteration 108, Batch: 47, Loss: 0.06362473964691162\n",
      "Iteration 108, Batch: 48, Loss: 0.10027579963207245\n",
      "Iteration 108, Batch: 49, Loss: 0.05708391219377518\n",
      "Iteration 109, Batch: 0, Loss: 0.08410327881574631\n",
      "Iteration 109, Batch: 1, Loss: 0.07530342042446136\n",
      "Iteration 109, Batch: 2, Loss: 0.07471605390310287\n",
      "Iteration 109, Batch: 3, Loss: 0.07766679674386978\n",
      "Iteration 109, Batch: 4, Loss: 0.08194267749786377\n",
      "Iteration 109, Batch: 5, Loss: 0.06370028108358383\n",
      "Iteration 109, Batch: 6, Loss: 0.09013841301202774\n",
      "Iteration 109, Batch: 7, Loss: 0.11733264476060867\n",
      "Iteration 109, Batch: 8, Loss: 0.10986251384019852\n",
      "Iteration 109, Batch: 9, Loss: 0.0707380548119545\n",
      "Iteration 109, Batch: 10, Loss: 0.07552526891231537\n",
      "Iteration 109, Batch: 11, Loss: 0.06067555397748947\n",
      "Iteration 109, Batch: 12, Loss: 0.08499176055192947\n",
      "Iteration 109, Batch: 13, Loss: 0.08185146749019623\n",
      "Iteration 109, Batch: 14, Loss: 0.08607098460197449\n",
      "Iteration 109, Batch: 15, Loss: 0.0835605189204216\n",
      "Iteration 109, Batch: 16, Loss: 0.07046804577112198\n",
      "Iteration 109, Batch: 17, Loss: 0.05676250159740448\n",
      "Iteration 109, Batch: 18, Loss: 0.07311154156923294\n",
      "Iteration 109, Batch: 19, Loss: 0.07687931507825851\n",
      "Iteration 109, Batch: 20, Loss: 0.08393663167953491\n",
      "Iteration 109, Batch: 21, Loss: 0.07102050632238388\n",
      "Iteration 109, Batch: 22, Loss: 0.05200684443116188\n",
      "Iteration 109, Batch: 23, Loss: 0.07282151281833649\n",
      "Iteration 109, Batch: 24, Loss: 0.08454125374555588\n",
      "Iteration 109, Batch: 25, Loss: 0.08573608100414276\n",
      "Iteration 109, Batch: 26, Loss: 0.07305730879306793\n",
      "Iteration 109, Batch: 27, Loss: 0.07440497726202011\n",
      "Iteration 109, Batch: 28, Loss: 0.04645174741744995\n",
      "Iteration 109, Batch: 29, Loss: 0.06716189533472061\n",
      "Iteration 109, Batch: 30, Loss: 0.06010359525680542\n",
      "Iteration 109, Batch: 31, Loss: 0.07741676270961761\n",
      "Iteration 109, Batch: 32, Loss: 0.03710949420928955\n",
      "Iteration 109, Batch: 33, Loss: 0.05874670296907425\n",
      "Iteration 109, Batch: 34, Loss: 0.07442181557416916\n",
      "Iteration 109, Batch: 35, Loss: 0.07118283212184906\n",
      "Iteration 109, Batch: 36, Loss: 0.06736817955970764\n",
      "Iteration 109, Batch: 37, Loss: 0.05428756773471832\n",
      "Iteration 109, Batch: 38, Loss: 0.09542714059352875\n",
      "Iteration 109, Batch: 39, Loss: 0.03376103937625885\n",
      "Iteration 109, Batch: 40, Loss: 0.07350211590528488\n",
      "Iteration 109, Batch: 41, Loss: 0.03842104971408844\n",
      "Iteration 109, Batch: 42, Loss: 0.09659390896558762\n",
      "Iteration 109, Batch: 43, Loss: 0.048590630292892456\n",
      "Iteration 109, Batch: 44, Loss: 0.06651338189840317\n",
      "Iteration 109, Batch: 45, Loss: 0.08784166723489761\n",
      "Iteration 109, Batch: 46, Loss: 0.08282701671123505\n",
      "Iteration 109, Batch: 47, Loss: 0.05556059628725052\n",
      "Iteration 109, Batch: 48, Loss: 0.07767026126384735\n",
      "Iteration 109, Batch: 49, Loss: 0.06961049884557724\n",
      "Iteration 110, Batch: 0, Loss: 0.060064151883125305\n",
      "Iteration 110, Batch: 1, Loss: 0.04903187230229378\n",
      "Iteration 110, Batch: 2, Loss: 0.06778885424137115\n",
      "Iteration 110, Batch: 3, Loss: 0.09386083483695984\n",
      "Iteration 110, Batch: 4, Loss: 0.0650501698255539\n",
      "Iteration 110, Batch: 5, Loss: 0.05055582523345947\n",
      "Iteration 110, Batch: 6, Loss: 0.06338226050138474\n",
      "Iteration 110, Batch: 7, Loss: 0.06419086456298828\n",
      "Iteration 110, Batch: 8, Loss: 0.07062610238790512\n",
      "Iteration 110, Batch: 9, Loss: 0.05045066773891449\n",
      "Iteration 110, Batch: 10, Loss: 0.05840213596820831\n",
      "Iteration 110, Batch: 11, Loss: 0.05827702209353447\n",
      "Iteration 110, Batch: 12, Loss: 0.0791686549782753\n",
      "Iteration 110, Batch: 13, Loss: 0.07345873862504959\n",
      "Iteration 110, Batch: 14, Loss: 0.049267664551734924\n",
      "Iteration 110, Batch: 15, Loss: 0.09014454483985901\n",
      "Iteration 110, Batch: 16, Loss: 0.07626458257436752\n",
      "Iteration 110, Batch: 17, Loss: 0.0756978690624237\n",
      "Iteration 110, Batch: 18, Loss: 0.08369668573141098\n",
      "Iteration 110, Batch: 19, Loss: 0.08490163832902908\n",
      "Iteration 110, Batch: 20, Loss: 0.044475212693214417\n",
      "Iteration 110, Batch: 21, Loss: 0.05884598568081856\n",
      "Iteration 110, Batch: 22, Loss: 0.05375856161117554\n",
      "Iteration 110, Batch: 23, Loss: 0.05416954308748245\n",
      "Iteration 110, Batch: 24, Loss: 0.06234912574291229\n",
      "Iteration 110, Batch: 25, Loss: 0.08931752294301987\n",
      "Iteration 110, Batch: 26, Loss: 0.07415366172790527\n",
      "Iteration 110, Batch: 27, Loss: 0.062401968985795975\n",
      "Iteration 110, Batch: 28, Loss: 0.0727921724319458\n",
      "Iteration 110, Batch: 29, Loss: 0.05161207169294357\n",
      "Iteration 110, Batch: 30, Loss: 0.07341580092906952\n",
      "Iteration 110, Batch: 31, Loss: 0.06485307961702347\n",
      "Iteration 110, Batch: 32, Loss: 0.05348993092775345\n",
      "Iteration 110, Batch: 33, Loss: 0.0744316577911377\n",
      "Iteration 110, Batch: 34, Loss: 0.07278693467378616\n",
      "Iteration 110, Batch: 35, Loss: 0.06179173290729523\n",
      "Iteration 110, Batch: 36, Loss: 0.0605049692094326\n",
      "Iteration 110, Batch: 37, Loss: 0.0744762048125267\n",
      "Iteration 110, Batch: 38, Loss: 0.058140337467193604\n",
      "Iteration 110, Batch: 39, Loss: 0.041942089796066284\n",
      "Iteration 110, Batch: 40, Loss: 0.09058772027492523\n",
      "Iteration 110, Batch: 41, Loss: 0.07158510386943817\n",
      "Iteration 110, Batch: 42, Loss: 0.06733774393796921\n",
      "Iteration 110, Batch: 43, Loss: 0.06100080534815788\n",
      "Iteration 110, Batch: 44, Loss: 0.07799271494150162\n",
      "Iteration 110, Batch: 45, Loss: 0.09558500349521637\n",
      "Iteration 110, Batch: 46, Loss: 0.07241560518741608\n",
      "Iteration 110, Batch: 47, Loss: 0.08601042628288269\n",
      "Iteration 110, Batch: 48, Loss: 0.06602741032838821\n",
      "Iteration 110, Batch: 49, Loss: 0.0665033683180809\n",
      "Iteration 111, Batch: 0, Loss: 0.08288625627756119\n",
      "Iteration 111, Batch: 1, Loss: 0.06500106304883957\n",
      "Iteration 111, Batch: 2, Loss: 0.05104618892073631\n",
      "Iteration 111, Batch: 3, Loss: 0.08591639995574951\n",
      "Iteration 111, Batch: 4, Loss: 0.07690692692995071\n",
      "Iteration 111, Batch: 5, Loss: 0.08020027726888657\n",
      "Iteration 111, Batch: 6, Loss: 0.05334336310625076\n",
      "Iteration 111, Batch: 7, Loss: 0.09787517040967941\n",
      "Iteration 111, Batch: 8, Loss: 0.04594903439283371\n",
      "Iteration 111, Batch: 9, Loss: 0.048680104315280914\n",
      "Iteration 111, Batch: 10, Loss: 0.06715045124292374\n",
      "Iteration 111, Batch: 11, Loss: 0.06318399310112\n",
      "Iteration 111, Batch: 12, Loss: 0.05414985492825508\n",
      "Iteration 111, Batch: 13, Loss: 0.06856869906187057\n",
      "Iteration 111, Batch: 14, Loss: 0.04303014650940895\n",
      "Iteration 111, Batch: 15, Loss: 0.09237228333950043\n",
      "Iteration 111, Batch: 16, Loss: 0.07744906842708588\n",
      "Iteration 111, Batch: 17, Loss: 0.0825987383723259\n",
      "Iteration 111, Batch: 18, Loss: 0.04573582485318184\n",
      "Iteration 111, Batch: 19, Loss: 0.06346069276332855\n",
      "Iteration 111, Batch: 20, Loss: 0.08341745287179947\n",
      "Iteration 111, Batch: 21, Loss: 0.09715955704450607\n",
      "Iteration 111, Batch: 22, Loss: 0.08181385695934296\n",
      "Iteration 111, Batch: 23, Loss: 0.0612604022026062\n",
      "Iteration 111, Batch: 24, Loss: 0.04721305891871452\n",
      "Iteration 111, Batch: 25, Loss: 0.061318811029195786\n",
      "Iteration 111, Batch: 26, Loss: 0.0726270079612732\n",
      "Iteration 111, Batch: 27, Loss: 0.08627435564994812\n",
      "Iteration 111, Batch: 28, Loss: 0.06681820005178452\n",
      "Iteration 111, Batch: 29, Loss: 0.06150854751467705\n",
      "Iteration 111, Batch: 30, Loss: 0.07043783366680145\n",
      "Iteration 111, Batch: 31, Loss: 0.04956962168216705\n",
      "Iteration 111, Batch: 32, Loss: 0.08217145502567291\n",
      "Iteration 111, Batch: 33, Loss: 0.07426872849464417\n",
      "Iteration 111, Batch: 34, Loss: 0.06966762244701385\n",
      "Iteration 111, Batch: 35, Loss: 0.0546918585896492\n",
      "Iteration 111, Batch: 36, Loss: 0.08397648483514786\n",
      "Iteration 111, Batch: 37, Loss: 0.06202908977866173\n",
      "Iteration 111, Batch: 38, Loss: 0.08518879115581512\n",
      "Iteration 111, Batch: 39, Loss: 0.04203181341290474\n",
      "Iteration 111, Batch: 40, Loss: 0.058657433837652206\n",
      "Iteration 111, Batch: 41, Loss: 0.05064341425895691\n",
      "Iteration 111, Batch: 42, Loss: 0.06278204917907715\n",
      "Iteration 111, Batch: 43, Loss: 0.0749916210770607\n",
      "Iteration 111, Batch: 44, Loss: 0.041121285408735275\n",
      "Iteration 111, Batch: 45, Loss: 0.08034634590148926\n",
      "Iteration 111, Batch: 46, Loss: 0.05087494105100632\n",
      "Iteration 111, Batch: 47, Loss: 0.07801233232021332\n",
      "Iteration 111, Batch: 48, Loss: 0.05360205098986626\n",
      "Iteration 111, Batch: 49, Loss: 0.0802336186170578\n",
      "Iteration 112, Batch: 0, Loss: 0.07281361520290375\n",
      "Iteration 112, Batch: 1, Loss: 0.06293223053216934\n",
      "Iteration 112, Batch: 2, Loss: 0.04178321734070778\n",
      "Iteration 112, Batch: 3, Loss: 0.08402296155691147\n",
      "Iteration 112, Batch: 4, Loss: 0.07771213352680206\n",
      "Iteration 112, Batch: 5, Loss: 0.06832440942525864\n",
      "Iteration 112, Batch: 6, Loss: 0.07302331179380417\n",
      "Iteration 112, Batch: 7, Loss: 0.09370093792676926\n",
      "Iteration 112, Batch: 8, Loss: 0.05167798325419426\n",
      "Iteration 112, Batch: 9, Loss: 0.06886385381221771\n",
      "Iteration 112, Batch: 10, Loss: 0.05202552676200867\n",
      "Iteration 112, Batch: 11, Loss: 0.08647268265485764\n",
      "Iteration 112, Batch: 12, Loss: 0.07114563137292862\n",
      "Iteration 112, Batch: 13, Loss: 0.06039022281765938\n",
      "Iteration 112, Batch: 14, Loss: 0.05765004828572273\n",
      "Iteration 112, Batch: 15, Loss: 0.056259505450725555\n",
      "Iteration 112, Batch: 16, Loss: 0.06844009459018707\n",
      "Iteration 112, Batch: 17, Loss: 0.05657031014561653\n",
      "Iteration 112, Batch: 18, Loss: 0.07251935452222824\n",
      "Iteration 112, Batch: 19, Loss: 0.05233648046851158\n",
      "Iteration 112, Batch: 20, Loss: 0.060562532395124435\n",
      "Iteration 112, Batch: 21, Loss: 0.04816730320453644\n",
      "Iteration 112, Batch: 22, Loss: 0.08022672683000565\n",
      "Iteration 112, Batch: 23, Loss: 0.05519290640950203\n",
      "Iteration 112, Batch: 24, Loss: 0.0592794306576252\n",
      "Iteration 112, Batch: 25, Loss: 0.06476563960313797\n",
      "Iteration 112, Batch: 26, Loss: 0.05412178859114647\n",
      "Iteration 112, Batch: 27, Loss: 0.09817416965961456\n",
      "Iteration 112, Batch: 28, Loss: 0.06997336447238922\n",
      "Iteration 112, Batch: 29, Loss: 0.06914085149765015\n",
      "Iteration 112, Batch: 30, Loss: 0.05595356971025467\n",
      "Iteration 112, Batch: 31, Loss: 0.04718436300754547\n",
      "Iteration 112, Batch: 32, Loss: 0.05610385909676552\n",
      "Iteration 112, Batch: 33, Loss: 0.06515733897686005\n",
      "Iteration 112, Batch: 34, Loss: 0.06289906799793243\n",
      "Iteration 112, Batch: 35, Loss: 0.057304468005895615\n",
      "Iteration 112, Batch: 36, Loss: 0.07495743781328201\n",
      "Iteration 112, Batch: 37, Loss: 0.025617823004722595\n",
      "Iteration 112, Batch: 38, Loss: 0.056725502014160156\n",
      "Iteration 112, Batch: 39, Loss: 0.061978451907634735\n",
      "Iteration 112, Batch: 40, Loss: 0.06687390059232712\n",
      "Iteration 112, Batch: 41, Loss: 0.06468252092599869\n",
      "Iteration 112, Batch: 42, Loss: 0.06898008286952972\n",
      "Iteration 112, Batch: 43, Loss: 0.07659615576267242\n",
      "Iteration 112, Batch: 44, Loss: 0.09733258932828903\n",
      "Iteration 112, Batch: 45, Loss: 0.08276128768920898\n",
      "Iteration 112, Batch: 46, Loss: 0.08040248602628708\n",
      "Iteration 112, Batch: 47, Loss: 0.10716939717531204\n",
      "Iteration 112, Batch: 48, Loss: 0.0954667329788208\n",
      "Iteration 112, Batch: 49, Loss: 0.0669039636850357\n",
      "Iteration 113, Batch: 0, Loss: 0.10400990396738052\n",
      "Iteration 113, Batch: 1, Loss: 0.0653637945652008\n",
      "Iteration 113, Batch: 2, Loss: 0.06284009665250778\n",
      "Iteration 113, Batch: 3, Loss: 0.07134764641523361\n",
      "Iteration 113, Batch: 4, Loss: 0.05721363052725792\n",
      "Iteration 113, Batch: 5, Loss: 0.06049073860049248\n",
      "Iteration 113, Batch: 6, Loss: 0.09598442167043686\n",
      "Iteration 113, Batch: 7, Loss: 0.05033619701862335\n",
      "Iteration 113, Batch: 8, Loss: 0.08203771710395813\n",
      "Iteration 113, Batch: 9, Loss: 0.09567203372716904\n",
      "Iteration 113, Batch: 10, Loss: 0.05880684405565262\n",
      "Iteration 113, Batch: 11, Loss: 0.06988343596458435\n",
      "Iteration 113, Batch: 12, Loss: 0.0568624772131443\n",
      "Iteration 113, Batch: 13, Loss: 0.07300131022930145\n",
      "Iteration 113, Batch: 14, Loss: 0.046398792415857315\n",
      "Iteration 113, Batch: 15, Loss: 0.07844902575016022\n",
      "Iteration 113, Batch: 16, Loss: 0.08238240331411362\n",
      "Iteration 113, Batch: 17, Loss: 0.07670748233795166\n",
      "Iteration 113, Batch: 18, Loss: 0.055450476706027985\n",
      "Iteration 113, Batch: 19, Loss: 0.0741613432765007\n",
      "Iteration 113, Batch: 20, Loss: 0.061358608305454254\n",
      "Iteration 113, Batch: 21, Loss: 0.06374541670084\n",
      "Iteration 113, Batch: 22, Loss: 0.052298180758953094\n",
      "Iteration 113, Batch: 23, Loss: 0.053677551448345184\n",
      "Iteration 113, Batch: 24, Loss: 0.05609291046857834\n",
      "Iteration 113, Batch: 25, Loss: 0.053092096000909805\n",
      "Iteration 113, Batch: 26, Loss: 0.0645771399140358\n",
      "Iteration 113, Batch: 27, Loss: 0.09596961736679077\n",
      "Iteration 113, Batch: 28, Loss: 0.07185892760753632\n",
      "Iteration 113, Batch: 29, Loss: 0.06143438071012497\n",
      "Iteration 113, Batch: 30, Loss: 0.0811956450343132\n",
      "Iteration 113, Batch: 31, Loss: 0.08154354244470596\n",
      "Iteration 113, Batch: 32, Loss: 0.05866704881191254\n",
      "Iteration 113, Batch: 33, Loss: 0.05663435906171799\n",
      "Iteration 113, Batch: 34, Loss: 0.0776960477232933\n",
      "Iteration 113, Batch: 35, Loss: 0.07744826376438141\n",
      "Iteration 113, Batch: 36, Loss: 0.09106960892677307\n",
      "Iteration 113, Batch: 37, Loss: 0.07009568810462952\n",
      "Iteration 113, Batch: 38, Loss: 0.07083041220903397\n",
      "Iteration 113, Batch: 39, Loss: 0.07387234270572662\n",
      "Iteration 113, Batch: 40, Loss: 0.09956703335046768\n",
      "Iteration 113, Batch: 41, Loss: 0.12030944228172302\n",
      "Iteration 113, Batch: 42, Loss: 0.09785262495279312\n",
      "Iteration 113, Batch: 43, Loss: 0.12852689623832703\n",
      "Iteration 113, Batch: 44, Loss: 0.13564732670783997\n",
      "Iteration 113, Batch: 45, Loss: 0.08337313681840897\n",
      "Iteration 113, Batch: 46, Loss: 0.10021095722913742\n",
      "Iteration 113, Batch: 47, Loss: 0.1430341750383377\n",
      "Iteration 113, Batch: 48, Loss: 0.11562871932983398\n",
      "Iteration 113, Batch: 49, Loss: 0.08553546667098999\n",
      "Iteration 114, Batch: 0, Loss: 0.09315065294504166\n",
      "Iteration 114, Batch: 1, Loss: 0.08190511167049408\n",
      "Iteration 114, Batch: 2, Loss: 0.09387068450450897\n",
      "Iteration 114, Batch: 3, Loss: 0.07287684082984924\n",
      "Iteration 114, Batch: 4, Loss: 0.06976983696222305\n",
      "Iteration 114, Batch: 5, Loss: 0.06616470217704773\n",
      "Iteration 114, Batch: 6, Loss: 0.07198124378919601\n",
      "Iteration 114, Batch: 7, Loss: 0.0664186105132103\n",
      "Iteration 114, Batch: 8, Loss: 0.06900648027658463\n",
      "Iteration 114, Batch: 9, Loss: 0.039689525961875916\n",
      "Iteration 114, Batch: 10, Loss: 0.09410665929317474\n",
      "Iteration 114, Batch: 11, Loss: 0.053140707314014435\n",
      "Iteration 114, Batch: 12, Loss: 0.05872524157166481\n",
      "Iteration 114, Batch: 13, Loss: 0.07080857455730438\n",
      "Iteration 114, Batch: 14, Loss: 0.06112254783511162\n",
      "Iteration 114, Batch: 15, Loss: 0.06615909934043884\n",
      "Iteration 114, Batch: 16, Loss: 0.06116018071770668\n",
      "Iteration 114, Batch: 17, Loss: 0.04583614692091942\n",
      "Iteration 114, Batch: 18, Loss: 0.07068073004484177\n",
      "Iteration 114, Batch: 19, Loss: 0.06749508529901505\n",
      "Iteration 114, Batch: 20, Loss: 0.08102386444807053\n",
      "Iteration 114, Batch: 21, Loss: 0.06783536076545715\n",
      "Iteration 114, Batch: 22, Loss: 0.08246264606714249\n",
      "Iteration 114, Batch: 23, Loss: 0.061922356486320496\n",
      "Iteration 114, Batch: 24, Loss: 0.08116302639245987\n",
      "Iteration 114, Batch: 25, Loss: 0.06763956695795059\n",
      "Iteration 114, Batch: 26, Loss: 0.0622207447886467\n",
      "Iteration 114, Batch: 27, Loss: 0.04367990046739578\n",
      "Iteration 114, Batch: 28, Loss: 0.04521341249346733\n",
      "Iteration 114, Batch: 29, Loss: 0.06479298323392868\n",
      "Iteration 114, Batch: 30, Loss: 0.07719142735004425\n",
      "Iteration 114, Batch: 31, Loss: 0.07696718722581863\n",
      "Iteration 114, Batch: 32, Loss: 0.06320784240961075\n",
      "Iteration 114, Batch: 33, Loss: 0.06232687830924988\n",
      "Iteration 114, Batch: 34, Loss: 0.06033019721508026\n",
      "Iteration 114, Batch: 35, Loss: 0.0778602585196495\n",
      "Iteration 114, Batch: 36, Loss: 0.0824185237288475\n",
      "Iteration 114, Batch: 37, Loss: 0.07106858491897583\n",
      "Iteration 114, Batch: 38, Loss: 0.08409734815359116\n",
      "Iteration 114, Batch: 39, Loss: 0.05506165325641632\n",
      "Iteration 114, Batch: 40, Loss: 0.04094984009861946\n",
      "Iteration 114, Batch: 41, Loss: 0.07286647707223892\n",
      "Iteration 114, Batch: 42, Loss: 0.06421256065368652\n",
      "Iteration 114, Batch: 43, Loss: 0.07990293204784393\n",
      "Iteration 114, Batch: 44, Loss: 0.07215502113103867\n",
      "Iteration 114, Batch: 45, Loss: 0.07098333537578583\n",
      "Iteration 114, Batch: 46, Loss: 0.09688857942819595\n",
      "Iteration 114, Batch: 47, Loss: 0.06108134239912033\n",
      "Iteration 114, Batch: 48, Loss: 0.07021743804216385\n",
      "Iteration 114, Batch: 49, Loss: 0.06696396321058273\n",
      "Iteration 115, Batch: 0, Loss: 0.08365152031183243\n",
      "Iteration 115, Batch: 1, Loss: 0.07703443616628647\n",
      "Iteration 115, Batch: 2, Loss: 0.11143369227647781\n",
      "Iteration 115, Batch: 3, Loss: 0.037816185504198074\n",
      "Iteration 115, Batch: 4, Loss: 0.0761287584900856\n",
      "Iteration 115, Batch: 5, Loss: 0.08119288086891174\n",
      "Iteration 115, Batch: 6, Loss: 0.06569626927375793\n",
      "Iteration 115, Batch: 7, Loss: 0.07517952471971512\n",
      "Iteration 115, Batch: 8, Loss: 0.07495300471782684\n",
      "Iteration 115, Batch: 9, Loss: 0.06167582795023918\n",
      "Iteration 115, Batch: 10, Loss: 0.05490092188119888\n",
      "Iteration 115, Batch: 11, Loss: 0.08607321232557297\n",
      "Iteration 115, Batch: 12, Loss: 0.10170747339725494\n",
      "Iteration 115, Batch: 13, Loss: 0.06706423312425613\n",
      "Iteration 115, Batch: 14, Loss: 0.06695978343486786\n",
      "Iteration 115, Batch: 15, Loss: 0.0641946792602539\n",
      "Iteration 115, Batch: 16, Loss: 0.06845682114362717\n",
      "Iteration 115, Batch: 17, Loss: 0.06638047099113464\n",
      "Iteration 115, Batch: 18, Loss: 0.07479187101125717\n",
      "Iteration 115, Batch: 19, Loss: 0.06485091149806976\n",
      "Iteration 115, Batch: 20, Loss: 0.05645492300391197\n",
      "Iteration 115, Batch: 21, Loss: 0.0881105437874794\n",
      "Iteration 115, Batch: 22, Loss: 0.04778657853603363\n",
      "Iteration 115, Batch: 23, Loss: 0.05656968802213669\n",
      "Iteration 115, Batch: 24, Loss: 0.09856489300727844\n",
      "Iteration 115, Batch: 25, Loss: 0.04721174016594887\n",
      "Iteration 115, Batch: 26, Loss: 0.04754408821463585\n",
      "Iteration 115, Batch: 27, Loss: 0.06355564296245575\n",
      "Iteration 115, Batch: 28, Loss: 0.05314396321773529\n",
      "Iteration 115, Batch: 29, Loss: 0.04561644420027733\n",
      "Iteration 115, Batch: 30, Loss: 0.0711430236697197\n",
      "Iteration 115, Batch: 31, Loss: 0.04610322043299675\n",
      "Iteration 115, Batch: 32, Loss: 0.055621035397052765\n",
      "Iteration 115, Batch: 33, Loss: 0.06979698687791824\n",
      "Iteration 115, Batch: 34, Loss: 0.06987735629081726\n",
      "Iteration 115, Batch: 35, Loss: 0.08587804436683655\n",
      "Iteration 115, Batch: 36, Loss: 0.07143093645572662\n",
      "Iteration 115, Batch: 37, Loss: 0.0671144351363182\n",
      "Iteration 115, Batch: 38, Loss: 0.05078938230872154\n",
      "Iteration 115, Batch: 39, Loss: 0.08618971705436707\n",
      "Iteration 115, Batch: 40, Loss: 0.06193982809782028\n",
      "Iteration 115, Batch: 41, Loss: 0.055899012833833694\n",
      "Iteration 115, Batch: 42, Loss: 0.05065013840794563\n",
      "Iteration 115, Batch: 43, Loss: 0.05139559134840965\n",
      "Iteration 115, Batch: 44, Loss: 0.05652814731001854\n",
      "Iteration 115, Batch: 45, Loss: 0.060867276042699814\n",
      "Iteration 115, Batch: 46, Loss: 0.0636041983962059\n",
      "Iteration 115, Batch: 47, Loss: 0.05038917437195778\n",
      "Iteration 115, Batch: 48, Loss: 0.09426325559616089\n",
      "Iteration 115, Batch: 49, Loss: 0.04096316173672676\n",
      "Iteration 116, Batch: 0, Loss: 0.07850799709558487\n",
      "Iteration 116, Batch: 1, Loss: 0.10438363999128342\n",
      "Iteration 116, Batch: 2, Loss: 0.08636049926280975\n",
      "Iteration 116, Batch: 3, Loss: 0.06419333070516586\n",
      "Iteration 116, Batch: 4, Loss: 0.05159212648868561\n",
      "Iteration 116, Batch: 5, Loss: 0.08384697884321213\n",
      "Iteration 116, Batch: 6, Loss: 0.09363096952438354\n",
      "Iteration 116, Batch: 7, Loss: 0.06486927717924118\n",
      "Iteration 116, Batch: 8, Loss: 0.0698387548327446\n",
      "Iteration 116, Batch: 9, Loss: 0.07999138534069061\n",
      "Iteration 116, Batch: 10, Loss: 0.05570027232170105\n",
      "Iteration 116, Batch: 11, Loss: 0.08060826361179352\n",
      "Iteration 116, Batch: 12, Loss: 0.09079500287771225\n",
      "Iteration 116, Batch: 13, Loss: 0.05107202008366585\n",
      "Iteration 116, Batch: 14, Loss: 0.06833994388580322\n",
      "Iteration 116, Batch: 15, Loss: 0.05281580239534378\n",
      "Iteration 116, Batch: 16, Loss: 0.036699745804071426\n",
      "Iteration 116, Batch: 17, Loss: 0.07327265292406082\n",
      "Iteration 116, Batch: 18, Loss: 0.05443401262164116\n",
      "Iteration 116, Batch: 19, Loss: 0.05488903075456619\n",
      "Iteration 116, Batch: 20, Loss: 0.08735654503107071\n",
      "Iteration 116, Batch: 21, Loss: 0.04492681473493576\n",
      "Iteration 116, Batch: 22, Loss: 0.08663320541381836\n",
      "Iteration 116, Batch: 23, Loss: 0.052429914474487305\n",
      "Iteration 116, Batch: 24, Loss: 0.06000762805342674\n",
      "Iteration 116, Batch: 25, Loss: 0.0808757022023201\n",
      "Iteration 116, Batch: 26, Loss: 0.06986171752214432\n",
      "Iteration 116, Batch: 27, Loss: 0.04348459094762802\n",
      "Iteration 116, Batch: 28, Loss: 0.09691301733255386\n",
      "Iteration 116, Batch: 29, Loss: 0.05531522259116173\n",
      "Iteration 116, Batch: 30, Loss: 0.046109188348054886\n",
      "Iteration 116, Batch: 31, Loss: 0.06318427622318268\n",
      "Iteration 116, Batch: 32, Loss: 0.06240034103393555\n",
      "Iteration 116, Batch: 33, Loss: 0.0835476666688919\n",
      "Iteration 116, Batch: 34, Loss: 0.03354466333985329\n",
      "Iteration 116, Batch: 35, Loss: 0.06530390679836273\n",
      "Iteration 116, Batch: 36, Loss: 0.06478147953748703\n",
      "Iteration 116, Batch: 37, Loss: 0.031573813408613205\n",
      "Iteration 116, Batch: 38, Loss: 0.07708714157342911\n",
      "Iteration 116, Batch: 39, Loss: 0.05401178076863289\n",
      "Iteration 116, Batch: 40, Loss: 0.05882801488041878\n",
      "Iteration 116, Batch: 41, Loss: 0.05395977944135666\n",
      "Iteration 116, Batch: 42, Loss: 0.07248777896165848\n",
      "Iteration 116, Batch: 43, Loss: 0.07308907061815262\n",
      "Iteration 116, Batch: 44, Loss: 0.07008533179759979\n",
      "Iteration 116, Batch: 45, Loss: 0.10256890952587128\n",
      "Iteration 116, Batch: 46, Loss: 0.08534162491559982\n",
      "Iteration 116, Batch: 47, Loss: 0.09649191051721573\n",
      "Iteration 116, Batch: 48, Loss: 0.06499958783388138\n",
      "Iteration 116, Batch: 49, Loss: 0.08020523935556412\n",
      "Iteration 117, Batch: 0, Loss: 0.11092379689216614\n",
      "Iteration 117, Batch: 1, Loss: 0.07313123345375061\n",
      "Iteration 117, Batch: 2, Loss: 0.08809352666139603\n",
      "Iteration 117, Batch: 3, Loss: 0.06539366394281387\n",
      "Iteration 117, Batch: 4, Loss: 0.0696488618850708\n",
      "Iteration 117, Batch: 5, Loss: 0.046913083642721176\n",
      "Iteration 117, Batch: 6, Loss: 0.045819882303476334\n",
      "Iteration 117, Batch: 7, Loss: 0.07228674739599228\n",
      "Iteration 117, Batch: 8, Loss: 0.05965637043118477\n",
      "Iteration 117, Batch: 9, Loss: 0.10803595930337906\n",
      "Iteration 117, Batch: 10, Loss: 0.10242490470409393\n",
      "Iteration 117, Batch: 11, Loss: 0.05992504581809044\n",
      "Iteration 117, Batch: 12, Loss: 0.08078710734844208\n",
      "Iteration 117, Batch: 13, Loss: 0.0719379112124443\n",
      "Iteration 117, Batch: 14, Loss: 0.06880774348974228\n",
      "Iteration 117, Batch: 15, Loss: 0.07908572256565094\n",
      "Iteration 117, Batch: 16, Loss: 0.08673511445522308\n",
      "Iteration 117, Batch: 17, Loss: 0.06656327843666077\n",
      "Iteration 117, Batch: 18, Loss: 0.0409979484975338\n",
      "Iteration 117, Batch: 19, Loss: 0.04981826990842819\n",
      "Iteration 117, Batch: 20, Loss: 0.07901658862829208\n",
      "Iteration 117, Batch: 21, Loss: 0.09780962020158768\n",
      "Iteration 117, Batch: 22, Loss: 0.06991215795278549\n",
      "Iteration 117, Batch: 23, Loss: 0.06760579347610474\n",
      "Iteration 117, Batch: 24, Loss: 0.06331890821456909\n",
      "Iteration 117, Batch: 25, Loss: 0.061063725501298904\n",
      "Iteration 117, Batch: 26, Loss: 0.11225101351737976\n",
      "Iteration 117, Batch: 27, Loss: 0.07636752724647522\n",
      "Iteration 117, Batch: 28, Loss: 0.06705320626497269\n",
      "Iteration 117, Batch: 29, Loss: 0.07132364809513092\n",
      "Iteration 117, Batch: 30, Loss: 0.06924563646316528\n",
      "Iteration 117, Batch: 31, Loss: 0.06948409229516983\n",
      "Iteration 117, Batch: 32, Loss: 0.06500684469938278\n",
      "Iteration 117, Batch: 33, Loss: 0.05884934216737747\n",
      "Iteration 117, Batch: 34, Loss: 0.07751820236444473\n",
      "Iteration 117, Batch: 35, Loss: 0.06951075047254562\n",
      "Iteration 117, Batch: 36, Loss: 0.07418179512023926\n",
      "Iteration 117, Batch: 37, Loss: 0.08987053483724594\n",
      "Iteration 117, Batch: 38, Loss: 0.0705035924911499\n",
      "Iteration 117, Batch: 39, Loss: 0.08051691949367523\n",
      "Iteration 117, Batch: 40, Loss: 0.04369393736124039\n",
      "Iteration 117, Batch: 41, Loss: 0.05457483232021332\n",
      "Iteration 117, Batch: 42, Loss: 0.08416345715522766\n",
      "Iteration 117, Batch: 43, Loss: 0.03424253687262535\n",
      "Iteration 117, Batch: 44, Loss: 0.06233274191617966\n",
      "Iteration 117, Batch: 45, Loss: 0.05830180644989014\n",
      "Iteration 117, Batch: 46, Loss: 0.06659150123596191\n",
      "Iteration 117, Batch: 47, Loss: 0.07207535952329636\n",
      "Iteration 117, Batch: 48, Loss: 0.08895125240087509\n",
      "Iteration 117, Batch: 49, Loss: 0.04111744835972786\n",
      "Iteration 118, Batch: 0, Loss: 0.06594722718000412\n",
      "Iteration 118, Batch: 1, Loss: 0.06157444790005684\n",
      "Iteration 118, Batch: 2, Loss: 0.05760153755545616\n",
      "Iteration 118, Batch: 3, Loss: 0.06831918656826019\n",
      "Iteration 118, Batch: 4, Loss: 0.06709121912717819\n",
      "Iteration 118, Batch: 5, Loss: 0.06330670416355133\n",
      "Iteration 118, Batch: 6, Loss: 0.06921990215778351\n",
      "Iteration 118, Batch: 7, Loss: 0.07109212875366211\n",
      "Iteration 118, Batch: 8, Loss: 0.06945335119962692\n",
      "Iteration 118, Batch: 9, Loss: 0.06969492137432098\n",
      "Iteration 118, Batch: 10, Loss: 0.03874678164720535\n",
      "Iteration 118, Batch: 11, Loss: 0.06196485459804535\n",
      "Iteration 118, Batch: 12, Loss: 0.06888967007398605\n",
      "Iteration 118, Batch: 13, Loss: 0.08222262561321259\n",
      "Iteration 118, Batch: 14, Loss: 0.04610380902886391\n",
      "Iteration 118, Batch: 15, Loss: 0.03823436051607132\n",
      "Iteration 118, Batch: 16, Loss: 0.09368203580379486\n",
      "Iteration 118, Batch: 17, Loss: 0.048725757747888565\n",
      "Iteration 118, Batch: 18, Loss: 0.09524646401405334\n",
      "Iteration 118, Batch: 19, Loss: 0.06545635312795639\n",
      "Iteration 118, Batch: 20, Loss: 0.04079794883728027\n",
      "Iteration 118, Batch: 21, Loss: 0.07600314915180206\n",
      "Iteration 118, Batch: 22, Loss: 0.06874583661556244\n",
      "Iteration 118, Batch: 23, Loss: 0.1072000116109848\n",
      "Iteration 118, Batch: 24, Loss: 0.07444107532501221\n",
      "Iteration 118, Batch: 25, Loss: 0.069374680519104\n",
      "Iteration 118, Batch: 26, Loss: 0.06102079153060913\n",
      "Iteration 118, Batch: 27, Loss: 0.06170320883393288\n",
      "Iteration 118, Batch: 28, Loss: 0.06320208311080933\n",
      "Iteration 118, Batch: 29, Loss: 0.07287391275167465\n",
      "Iteration 118, Batch: 30, Loss: 0.05222533270716667\n",
      "Iteration 118, Batch: 31, Loss: 0.0830601379275322\n",
      "Iteration 118, Batch: 32, Loss: 0.10223396122455597\n",
      "Iteration 118, Batch: 33, Loss: 0.06496662646532059\n",
      "Iteration 118, Batch: 34, Loss: 0.05627523362636566\n",
      "Iteration 118, Batch: 35, Loss: 0.06212034076452255\n",
      "Iteration 118, Batch: 36, Loss: 0.07999811321496964\n",
      "Iteration 118, Batch: 37, Loss: 0.0691542774438858\n",
      "Iteration 118, Batch: 38, Loss: 0.10197459906339645\n",
      "Iteration 118, Batch: 39, Loss: 0.049449510872364044\n",
      "Iteration 118, Batch: 40, Loss: 0.0923573300242424\n",
      "Iteration 118, Batch: 41, Loss: 0.040604785084724426\n",
      "Iteration 118, Batch: 42, Loss: 0.053981006145477295\n",
      "Iteration 118, Batch: 43, Loss: 0.06626668572425842\n",
      "Iteration 118, Batch: 44, Loss: 0.0627247616648674\n",
      "Iteration 118, Batch: 45, Loss: 0.06810812652111053\n",
      "Iteration 118, Batch: 46, Loss: 0.07247570157051086\n",
      "Iteration 118, Batch: 47, Loss: 0.060589998960494995\n",
      "Iteration 118, Batch: 48, Loss: 0.07114332169294357\n",
      "Iteration 118, Batch: 49, Loss: 0.07324254512786865\n",
      "Iteration 119, Batch: 0, Loss: 0.05239427462220192\n",
      "Iteration 119, Batch: 1, Loss: 0.07759135216474533\n",
      "Iteration 119, Batch: 2, Loss: 0.05133642256259918\n",
      "Iteration 119, Batch: 3, Loss: 0.07032868266105652\n",
      "Iteration 119, Batch: 4, Loss: 0.037753332406282425\n",
      "Iteration 119, Batch: 5, Loss: 0.05948788672685623\n",
      "Iteration 119, Batch: 6, Loss: 0.03917267546057701\n",
      "Iteration 119, Batch: 7, Loss: 0.06795169413089752\n",
      "Iteration 119, Batch: 8, Loss: 0.06668931990861893\n",
      "Iteration 119, Batch: 9, Loss: 0.0427531898021698\n",
      "Iteration 119, Batch: 10, Loss: 0.05437011644244194\n",
      "Iteration 119, Batch: 11, Loss: 0.05475059524178505\n",
      "Iteration 119, Batch: 12, Loss: 0.06387227773666382\n",
      "Iteration 119, Batch: 13, Loss: 0.0392625592648983\n",
      "Iteration 119, Batch: 14, Loss: 0.06690043956041336\n",
      "Iteration 119, Batch: 15, Loss: 0.06335559487342834\n",
      "Iteration 119, Batch: 16, Loss: 0.06846412271261215\n",
      "Iteration 119, Batch: 17, Loss: 0.05300317704677582\n",
      "Iteration 119, Batch: 18, Loss: 0.06003935635089874\n",
      "Iteration 119, Batch: 19, Loss: 0.05794578418135643\n",
      "Iteration 119, Batch: 20, Loss: 0.04651821032166481\n",
      "Iteration 119, Batch: 21, Loss: 0.07101202756166458\n",
      "Iteration 119, Batch: 22, Loss: 0.055301547050476074\n",
      "Iteration 119, Batch: 23, Loss: 0.07123734056949615\n",
      "Iteration 119, Batch: 24, Loss: 0.0558600015938282\n",
      "Iteration 119, Batch: 25, Loss: 0.06765910983085632\n",
      "Iteration 119, Batch: 26, Loss: 0.06738081574440002\n",
      "Iteration 119, Batch: 27, Loss: 0.06614699959754944\n",
      "Iteration 119, Batch: 28, Loss: 0.08551423996686935\n",
      "Iteration 119, Batch: 29, Loss: 0.0751216784119606\n",
      "Iteration 119, Batch: 30, Loss: 0.062286294996738434\n",
      "Iteration 119, Batch: 31, Loss: 0.02965286187827587\n",
      "Iteration 119, Batch: 32, Loss: 0.08189146965742111\n",
      "Iteration 119, Batch: 33, Loss: 0.0660749301314354\n",
      "Iteration 119, Batch: 34, Loss: 0.058389730751514435\n",
      "Iteration 119, Batch: 35, Loss: 0.037335023283958435\n",
      "Iteration 119, Batch: 36, Loss: 0.04863068461418152\n",
      "Iteration 119, Batch: 37, Loss: 0.0833522379398346\n",
      "Iteration 119, Batch: 38, Loss: 0.08079569786787033\n",
      "Iteration 119, Batch: 39, Loss: 0.05830422416329384\n",
      "Iteration 119, Batch: 40, Loss: 0.08497291058301926\n",
      "Iteration 119, Batch: 41, Loss: 0.05183980241417885\n",
      "Iteration 119, Batch: 42, Loss: 0.03249124065041542\n",
      "Iteration 119, Batch: 43, Loss: 0.05909768119454384\n",
      "Iteration 119, Batch: 44, Loss: 0.08449218422174454\n",
      "Iteration 119, Batch: 45, Loss: 0.06429215520620346\n",
      "Iteration 119, Batch: 46, Loss: 0.050069186836481094\n",
      "Iteration 119, Batch: 47, Loss: 0.07163010537624359\n",
      "Iteration 119, Batch: 48, Loss: 0.05583742633461952\n",
      "Iteration 119, Batch: 49, Loss: 0.039705634117126465\n",
      "Iteration 120, Batch: 0, Loss: 0.06824266910552979\n",
      "Iteration 120, Batch: 1, Loss: 0.06265414506196976\n",
      "Iteration 120, Batch: 2, Loss: 0.06935740262269974\n",
      "Iteration 120, Batch: 3, Loss: 0.07772620767354965\n",
      "Iteration 120, Batch: 4, Loss: 0.05606861039996147\n",
      "Iteration 120, Batch: 5, Loss: 0.05216923728585243\n",
      "Iteration 120, Batch: 6, Loss: 0.07427362352609634\n",
      "Iteration 120, Batch: 7, Loss: 0.10462015122175217\n",
      "Iteration 120, Batch: 8, Loss: 0.054323673248291016\n",
      "Iteration 120, Batch: 9, Loss: 0.08338237553834915\n",
      "Iteration 120, Batch: 10, Loss: 0.06988871097564697\n",
      "Iteration 120, Batch: 11, Loss: 0.04181068018078804\n",
      "Iteration 120, Batch: 12, Loss: 0.04883531481027603\n",
      "Iteration 120, Batch: 13, Loss: 0.0754389837384224\n",
      "Iteration 120, Batch: 14, Loss: 0.06492260843515396\n",
      "Iteration 120, Batch: 15, Loss: 0.08194528520107269\n",
      "Iteration 120, Batch: 16, Loss: 0.06915270537137985\n",
      "Iteration 120, Batch: 17, Loss: 0.11022384464740753\n",
      "Iteration 120, Batch: 18, Loss: 0.10490265488624573\n",
      "Iteration 120, Batch: 19, Loss: 0.06629318743944168\n",
      "Iteration 120, Batch: 20, Loss: 0.06476061791181564\n",
      "Iteration 120, Batch: 21, Loss: 0.05865997076034546\n",
      "Iteration 120, Batch: 22, Loss: 0.1066179946064949\n",
      "Iteration 120, Batch: 23, Loss: 0.06061522290110588\n",
      "Iteration 120, Batch: 24, Loss: 0.07186718285083771\n",
      "Iteration 120, Batch: 25, Loss: 0.06884169578552246\n",
      "Iteration 120, Batch: 26, Loss: 0.11051574349403381\n",
      "Iteration 120, Batch: 27, Loss: 0.06594168394804001\n",
      "Iteration 120, Batch: 28, Loss: 0.07071597874164581\n",
      "Iteration 120, Batch: 29, Loss: 0.07303257286548615\n",
      "Iteration 120, Batch: 30, Loss: 0.04119366407394409\n",
      "Iteration 120, Batch: 31, Loss: 0.0723104476928711\n",
      "Iteration 120, Batch: 32, Loss: 0.05607324838638306\n",
      "Iteration 120, Batch: 33, Loss: 0.06157594919204712\n",
      "Iteration 120, Batch: 34, Loss: 0.06700969487428665\n",
      "Iteration 120, Batch: 35, Loss: 0.07349207997322083\n",
      "Iteration 120, Batch: 36, Loss: 0.05311354622244835\n",
      "Iteration 120, Batch: 37, Loss: 0.12317178398370743\n",
      "Iteration 120, Batch: 38, Loss: 0.04786868393421173\n",
      "Iteration 120, Batch: 39, Loss: 0.06261264532804489\n",
      "Iteration 120, Batch: 40, Loss: 0.07676046341657639\n",
      "Iteration 120, Batch: 41, Loss: 0.10792603343725204\n",
      "Iteration 120, Batch: 42, Loss: 0.07819496840238571\n",
      "Iteration 120, Batch: 43, Loss: 0.054925162345170975\n",
      "Iteration 120, Batch: 44, Loss: 0.08843259513378143\n",
      "Iteration 120, Batch: 45, Loss: 0.053050439804792404\n",
      "Iteration 120, Batch: 46, Loss: 0.0748603343963623\n",
      "Iteration 120, Batch: 47, Loss: 0.059861525893211365\n",
      "Iteration 120, Batch: 48, Loss: 0.06264050304889679\n",
      "Iteration 120, Batch: 49, Loss: 0.10286428034305573\n",
      "Iteration 121, Batch: 0, Loss: 0.1203785389661789\n",
      "Iteration 121, Batch: 1, Loss: 0.07802942395210266\n",
      "Iteration 121, Batch: 2, Loss: 0.11429982632398605\n",
      "Iteration 121, Batch: 3, Loss: 0.105087049305439\n",
      "Iteration 121, Batch: 4, Loss: 0.10521993041038513\n",
      "Iteration 121, Batch: 5, Loss: 0.09065166115760803\n",
      "Iteration 121, Batch: 6, Loss: 0.06971583515405655\n",
      "Iteration 121, Batch: 7, Loss: 0.08058087527751923\n",
      "Iteration 121, Batch: 8, Loss: 0.05289879068732262\n",
      "Iteration 121, Batch: 9, Loss: 0.08314251899719238\n",
      "Iteration 121, Batch: 10, Loss: 0.07416197657585144\n",
      "Iteration 121, Batch: 11, Loss: 0.05505780130624771\n",
      "Iteration 121, Batch: 12, Loss: 0.07189837098121643\n",
      "Iteration 121, Batch: 13, Loss: 0.07580647617578506\n",
      "Iteration 121, Batch: 14, Loss: 0.048200950026512146\n",
      "Iteration 121, Batch: 15, Loss: 0.06938374042510986\n",
      "Iteration 121, Batch: 16, Loss: 0.0839005634188652\n",
      "Iteration 121, Batch: 17, Loss: 0.0708044096827507\n",
      "Iteration 121, Batch: 18, Loss: 0.12847037613391876\n",
      "Iteration 121, Batch: 19, Loss: 0.06092463433742523\n",
      "Iteration 121, Batch: 20, Loss: 0.10476876050233841\n",
      "Iteration 121, Batch: 21, Loss: 0.0747898668050766\n",
      "Iteration 121, Batch: 22, Loss: 0.09132342040538788\n",
      "Iteration 121, Batch: 23, Loss: 0.08414789289236069\n",
      "Iteration 121, Batch: 24, Loss: 0.06232999637722969\n",
      "Iteration 121, Batch: 25, Loss: 0.08942052721977234\n",
      "Iteration 121, Batch: 26, Loss: 0.0517137311398983\n",
      "Iteration 121, Batch: 27, Loss: 0.0645940750837326\n",
      "Iteration 121, Batch: 28, Loss: 0.06024816259741783\n",
      "Iteration 121, Batch: 29, Loss: 0.05701163038611412\n",
      "Iteration 121, Batch: 30, Loss: 0.10130555182695389\n",
      "Iteration 121, Batch: 31, Loss: 0.06783223152160645\n",
      "Iteration 121, Batch: 32, Loss: 0.06429319083690643\n",
      "Iteration 121, Batch: 33, Loss: 0.07803661376237869\n",
      "Iteration 121, Batch: 34, Loss: 0.10030665993690491\n",
      "Iteration 121, Batch: 35, Loss: 0.07256024330854416\n",
      "Iteration 121, Batch: 36, Loss: 0.08641380071640015\n",
      "Iteration 121, Batch: 37, Loss: 0.11123716086149216\n",
      "Iteration 121, Batch: 38, Loss: 0.07534704357385635\n",
      "Iteration 121, Batch: 39, Loss: 0.06645270437002182\n",
      "Iteration 121, Batch: 40, Loss: 0.06956233084201813\n",
      "Iteration 121, Batch: 41, Loss: 0.06878796219825745\n",
      "Iteration 121, Batch: 42, Loss: 0.0856965035200119\n",
      "Iteration 121, Batch: 43, Loss: 0.07019152492284775\n",
      "Iteration 121, Batch: 44, Loss: 0.05255166068673134\n",
      "Iteration 121, Batch: 45, Loss: 0.04385422542691231\n",
      "Iteration 121, Batch: 46, Loss: 0.05653449892997742\n",
      "Iteration 121, Batch: 47, Loss: 0.06778746098279953\n",
      "Iteration 121, Batch: 48, Loss: 0.10558030754327774\n",
      "Iteration 121, Batch: 49, Loss: 0.0466565266251564\n",
      "Iteration 122, Batch: 0, Loss: 0.10789582133293152\n",
      "Iteration 122, Batch: 1, Loss: 0.06564004719257355\n",
      "Iteration 122, Batch: 2, Loss: 0.05474737659096718\n",
      "Iteration 122, Batch: 3, Loss: 0.039954811334609985\n",
      "Iteration 122, Batch: 4, Loss: 0.05016203597187996\n",
      "Iteration 122, Batch: 5, Loss: 0.057369597256183624\n",
      "Iteration 122, Batch: 6, Loss: 0.04242255911231041\n",
      "Iteration 122, Batch: 7, Loss: 0.04553622007369995\n",
      "Iteration 122, Batch: 8, Loss: 0.051498401910066605\n",
      "Iteration 122, Batch: 9, Loss: 0.07280495762825012\n",
      "Iteration 122, Batch: 10, Loss: 0.06940710544586182\n",
      "Iteration 122, Batch: 11, Loss: 0.09344799071550369\n",
      "Iteration 122, Batch: 12, Loss: 0.0796838104724884\n",
      "Iteration 122, Batch: 13, Loss: 0.06800860166549683\n",
      "Iteration 122, Batch: 14, Loss: 0.07550199329853058\n",
      "Iteration 122, Batch: 15, Loss: 0.06960706412792206\n",
      "Iteration 122, Batch: 16, Loss: 0.08402814716100693\n",
      "Iteration 122, Batch: 17, Loss: 0.06043269485235214\n",
      "Iteration 122, Batch: 18, Loss: 0.06539002060890198\n",
      "Iteration 122, Batch: 19, Loss: 0.04663274809718132\n",
      "Iteration 122, Batch: 20, Loss: 0.0733942985534668\n",
      "Iteration 122, Batch: 21, Loss: 0.12572607398033142\n",
      "Iteration 122, Batch: 22, Loss: 0.11510615050792694\n",
      "Iteration 122, Batch: 23, Loss: 0.11737117171287537\n",
      "Iteration 122, Batch: 24, Loss: 0.10556378960609436\n",
      "Iteration 122, Batch: 25, Loss: 0.104977548122406\n",
      "Iteration 122, Batch: 26, Loss: 0.12094476073980331\n",
      "Iteration 122, Batch: 27, Loss: 0.12803243100643158\n",
      "Iteration 122, Batch: 28, Loss: 0.06494724750518799\n",
      "Iteration 122, Batch: 29, Loss: 0.11528629064559937\n",
      "Iteration 122, Batch: 30, Loss: 0.07444202899932861\n",
      "Iteration 122, Batch: 31, Loss: 0.0717158243060112\n",
      "Iteration 122, Batch: 32, Loss: 0.0647912546992302\n",
      "Iteration 122, Batch: 33, Loss: 0.05247804522514343\n",
      "Iteration 122, Batch: 34, Loss: 0.06299062818288803\n",
      "Iteration 122, Batch: 35, Loss: 0.08729267120361328\n",
      "Iteration 122, Batch: 36, Loss: 0.056205857545137405\n",
      "Iteration 122, Batch: 37, Loss: 0.05911920592188835\n",
      "Iteration 122, Batch: 38, Loss: 0.08731390535831451\n",
      "Iteration 122, Batch: 39, Loss: 0.04400790110230446\n",
      "Iteration 122, Batch: 40, Loss: 0.08375328034162521\n",
      "Iteration 122, Batch: 41, Loss: 0.041258152574300766\n",
      "Iteration 122, Batch: 42, Loss: 0.05201064795255661\n",
      "Iteration 122, Batch: 43, Loss: 0.0698876678943634\n",
      "Iteration 122, Batch: 44, Loss: 0.07036121189594269\n",
      "Iteration 122, Batch: 45, Loss: 0.04897518828511238\n",
      "Iteration 122, Batch: 46, Loss: 0.03637335076928139\n",
      "Iteration 122, Batch: 47, Loss: 0.054516296833753586\n",
      "Iteration 122, Batch: 48, Loss: 0.058822620660066605\n",
      "Iteration 122, Batch: 49, Loss: 0.03879876807332039\n",
      "Iteration 123, Batch: 0, Loss: 0.04970959201455116\n",
      "Iteration 123, Batch: 1, Loss: 0.07152451574802399\n",
      "Iteration 123, Batch: 2, Loss: 0.03843948245048523\n",
      "Iteration 123, Batch: 3, Loss: 0.07034234702587128\n",
      "Iteration 123, Batch: 4, Loss: 0.03021072968840599\n",
      "Iteration 123, Batch: 5, Loss: 0.04961350932717323\n",
      "Iteration 123, Batch: 6, Loss: 0.06309418380260468\n",
      "Iteration 123, Batch: 7, Loss: 0.08810308575630188\n",
      "Iteration 123, Batch: 8, Loss: 0.04381607100367546\n",
      "Iteration 123, Batch: 9, Loss: 0.050295572727918625\n",
      "Iteration 123, Batch: 10, Loss: 0.04978809878230095\n",
      "Iteration 123, Batch: 11, Loss: 0.07127296179533005\n",
      "Iteration 123, Batch: 12, Loss: 0.0882801041007042\n",
      "Iteration 123, Batch: 13, Loss: 0.06802376359701157\n",
      "Iteration 123, Batch: 14, Loss: 0.07589171081781387\n",
      "Iteration 123, Batch: 15, Loss: 0.06382662057876587\n",
      "Iteration 123, Batch: 16, Loss: 0.04832203686237335\n",
      "Iteration 123, Batch: 17, Loss: 0.070341557264328\n",
      "Iteration 123, Batch: 18, Loss: 0.035050179809331894\n",
      "Iteration 123, Batch: 19, Loss: 0.06357739865779877\n",
      "Iteration 123, Batch: 20, Loss: 0.06550916284322739\n",
      "Iteration 123, Batch: 21, Loss: 0.07050642371177673\n",
      "Iteration 123, Batch: 22, Loss: 0.05507431924343109\n",
      "Iteration 123, Batch: 23, Loss: 0.04688500240445137\n",
      "Iteration 123, Batch: 24, Loss: 0.1070997565984726\n",
      "Iteration 123, Batch: 25, Loss: 0.06481471657752991\n",
      "Iteration 123, Batch: 26, Loss: 0.06392078846693039\n",
      "Iteration 123, Batch: 27, Loss: 0.06091545149683952\n",
      "Iteration 123, Batch: 28, Loss: 0.07285232096910477\n",
      "Iteration 123, Batch: 29, Loss: 0.043911904096603394\n",
      "Iteration 123, Batch: 30, Loss: 0.04852576181292534\n",
      "Iteration 123, Batch: 31, Loss: 0.0703352838754654\n",
      "Iteration 123, Batch: 32, Loss: 0.05376814678311348\n",
      "Iteration 123, Batch: 33, Loss: 0.06264322251081467\n",
      "Iteration 123, Batch: 34, Loss: 0.061541441828012466\n",
      "Iteration 123, Batch: 35, Loss: 0.049708783626556396\n",
      "Iteration 123, Batch: 36, Loss: 0.0639408677816391\n",
      "Iteration 123, Batch: 37, Loss: 0.0631781741976738\n",
      "Iteration 123, Batch: 38, Loss: 0.043678224086761475\n",
      "Iteration 123, Batch: 39, Loss: 0.07149333506822586\n",
      "Iteration 123, Batch: 40, Loss: 0.06250112503767014\n",
      "Iteration 123, Batch: 41, Loss: 0.06720037013292313\n",
      "Iteration 123, Batch: 42, Loss: 0.08111677318811417\n",
      "Iteration 123, Batch: 43, Loss: 0.04336579144001007\n",
      "Iteration 123, Batch: 44, Loss: 0.04299277812242508\n",
      "Iteration 123, Batch: 45, Loss: 0.06252487003803253\n",
      "Iteration 123, Batch: 46, Loss: 0.04569563642144203\n",
      "Iteration 123, Batch: 47, Loss: 0.07241901010274887\n",
      "Iteration 123, Batch: 48, Loss: 0.08353101462125778\n",
      "Iteration 123, Batch: 49, Loss: 0.052518442273139954\n",
      "Iteration 124, Batch: 0, Loss: 0.04495957866311073\n",
      "Iteration 124, Batch: 1, Loss: 0.06762788444757462\n",
      "Iteration 124, Batch: 2, Loss: 0.08182685077190399\n",
      "Iteration 124, Batch: 3, Loss: 0.06078619509935379\n",
      "Iteration 124, Batch: 4, Loss: 0.054671332240104675\n",
      "Iteration 124, Batch: 5, Loss: 0.05934908613562584\n",
      "Iteration 124, Batch: 6, Loss: 0.06854577362537384\n",
      "Iteration 124, Batch: 7, Loss: 0.08516315370798111\n",
      "Iteration 124, Batch: 8, Loss: 0.09572026133537292\n",
      "Iteration 124, Batch: 9, Loss: 0.05163965001702309\n",
      "Iteration 124, Batch: 10, Loss: 0.0711127445101738\n",
      "Iteration 124, Batch: 11, Loss: 0.07958939671516418\n",
      "Iteration 124, Batch: 12, Loss: 0.03942941129207611\n",
      "Iteration 124, Batch: 13, Loss: 0.04478463530540466\n",
      "Iteration 124, Batch: 14, Loss: 0.060151826590299606\n",
      "Iteration 124, Batch: 15, Loss: 0.08777713775634766\n",
      "Iteration 124, Batch: 16, Loss: 0.0769210085272789\n",
      "Iteration 124, Batch: 17, Loss: 0.07070879638195038\n",
      "Iteration 124, Batch: 18, Loss: 0.04543672502040863\n",
      "Iteration 124, Batch: 19, Loss: 0.0750374048948288\n",
      "Iteration 124, Batch: 20, Loss: 0.06097535789012909\n",
      "Iteration 124, Batch: 21, Loss: 0.05427360534667969\n",
      "Iteration 124, Batch: 22, Loss: 0.050300002098083496\n",
      "Iteration 124, Batch: 23, Loss: 0.04511158913373947\n",
      "Iteration 124, Batch: 24, Loss: 0.10013660788536072\n",
      "Iteration 124, Batch: 25, Loss: 0.048397649079561234\n",
      "Iteration 124, Batch: 26, Loss: 0.07647638022899628\n",
      "Iteration 124, Batch: 27, Loss: 0.042951397597789764\n",
      "Iteration 124, Batch: 28, Loss: 0.06711618602275848\n",
      "Iteration 124, Batch: 29, Loss: 0.05516660585999489\n",
      "Iteration 124, Batch: 30, Loss: 0.054606951773166656\n",
      "Iteration 124, Batch: 31, Loss: 0.06578028202056885\n",
      "Iteration 124, Batch: 32, Loss: 0.07200860977172852\n",
      "Iteration 124, Batch: 33, Loss: 0.03511355444788933\n",
      "Iteration 124, Batch: 34, Loss: 0.07822811603546143\n",
      "Iteration 124, Batch: 35, Loss: 0.07176555693149567\n",
      "Iteration 124, Batch: 36, Loss: 0.058798857033252716\n",
      "Iteration 124, Batch: 37, Loss: 0.06471949070692062\n",
      "Iteration 124, Batch: 38, Loss: 0.06054236367344856\n",
      "Iteration 124, Batch: 39, Loss: 0.0640353113412857\n",
      "Iteration 124, Batch: 40, Loss: 0.04707338660955429\n",
      "Iteration 124, Batch: 41, Loss: 0.05358543246984482\n",
      "Iteration 124, Batch: 42, Loss: 0.0670529380440712\n",
      "Iteration 124, Batch: 43, Loss: 0.07153709977865219\n",
      "Iteration 124, Batch: 44, Loss: 0.08486852794885635\n",
      "Iteration 124, Batch: 45, Loss: 0.0599825382232666\n",
      "Iteration 124, Batch: 46, Loss: 0.09908914566040039\n",
      "Iteration 124, Batch: 47, Loss: 0.07801010459661484\n",
      "Iteration 124, Batch: 48, Loss: 0.0676083192229271\n",
      "Iteration 124, Batch: 49, Loss: 0.06652361154556274\n",
      "Iteration 125, Batch: 0, Loss: 0.0947955772280693\n",
      "Iteration 125, Batch: 1, Loss: 0.043461885303258896\n",
      "Iteration 125, Batch: 2, Loss: 0.09781837463378906\n",
      "Iteration 125, Batch: 3, Loss: 0.049856942147016525\n",
      "Iteration 125, Batch: 4, Loss: 0.060605887323617935\n",
      "Iteration 125, Batch: 5, Loss: 0.0480799674987793\n",
      "Iteration 125, Batch: 6, Loss: 0.06158551201224327\n",
      "Iteration 125, Batch: 7, Loss: 0.08111320436000824\n",
      "Iteration 125, Batch: 8, Loss: 0.05692557245492935\n",
      "Iteration 125, Batch: 9, Loss: 0.06705829501152039\n",
      "Iteration 125, Batch: 10, Loss: 0.0813477486371994\n",
      "Iteration 125, Batch: 11, Loss: 0.07936851680278778\n",
      "Iteration 125, Batch: 12, Loss: 0.08279049396514893\n",
      "Iteration 125, Batch: 13, Loss: 0.07338228821754456\n",
      "Iteration 125, Batch: 14, Loss: 0.07988790422677994\n",
      "Iteration 125, Batch: 15, Loss: 0.05515637248754501\n",
      "Iteration 125, Batch: 16, Loss: 0.06606017053127289\n",
      "Iteration 125, Batch: 17, Loss: 0.047057442367076874\n",
      "Iteration 125, Batch: 18, Loss: 0.06809651851654053\n",
      "Iteration 125, Batch: 19, Loss: 0.045344337821006775\n",
      "Iteration 125, Batch: 20, Loss: 0.055269259959459305\n",
      "Iteration 125, Batch: 21, Loss: 0.07514297962188721\n",
      "Iteration 125, Batch: 22, Loss: 0.06084882467985153\n",
      "Iteration 125, Batch: 23, Loss: 0.052321404218673706\n",
      "Iteration 125, Batch: 24, Loss: 0.05348135530948639\n",
      "Iteration 125, Batch: 25, Loss: 0.08239202946424484\n",
      "Iteration 125, Batch: 26, Loss: 0.05701911821961403\n",
      "Iteration 125, Batch: 27, Loss: 0.07557307183742523\n",
      "Iteration 125, Batch: 28, Loss: 0.044471368193626404\n",
      "Iteration 125, Batch: 29, Loss: 0.03860827535390854\n",
      "Iteration 125, Batch: 30, Loss: 0.04245670139789581\n",
      "Iteration 125, Batch: 31, Loss: 0.0653795674443245\n",
      "Iteration 125, Batch: 32, Loss: 0.04428451508283615\n",
      "Iteration 125, Batch: 33, Loss: 0.052078284323215485\n",
      "Iteration 125, Batch: 34, Loss: 0.08649982511997223\n",
      "Iteration 125, Batch: 35, Loss: 0.04074956849217415\n",
      "Iteration 125, Batch: 36, Loss: 0.05562978610396385\n",
      "Iteration 125, Batch: 37, Loss: 0.0736645981669426\n",
      "Iteration 125, Batch: 38, Loss: 0.07191254198551178\n",
      "Iteration 125, Batch: 39, Loss: 0.07783877104520798\n",
      "Iteration 125, Batch: 40, Loss: 0.04996850714087486\n",
      "Iteration 125, Batch: 41, Loss: 0.07187788188457489\n",
      "Iteration 125, Batch: 42, Loss: 0.0652657225728035\n",
      "Iteration 125, Batch: 43, Loss: 0.06098592281341553\n",
      "Iteration 125, Batch: 44, Loss: 0.08770827949047089\n",
      "Iteration 125, Batch: 45, Loss: 0.07187385112047195\n",
      "Iteration 125, Batch: 46, Loss: 0.062028512358665466\n",
      "Iteration 125, Batch: 47, Loss: 0.05065048113465309\n",
      "Iteration 125, Batch: 48, Loss: 0.1218249574303627\n",
      "Iteration 125, Batch: 49, Loss: 0.08488988876342773\n",
      "Iteration 126, Batch: 0, Loss: 0.09118161350488663\n",
      "Iteration 126, Batch: 1, Loss: 0.08092816174030304\n",
      "Iteration 126, Batch: 2, Loss: 0.0676841214299202\n",
      "Iteration 126, Batch: 3, Loss: 0.08067385852336884\n",
      "Iteration 126, Batch: 4, Loss: 0.07202132046222687\n",
      "Iteration 126, Batch: 5, Loss: 0.0800667330622673\n",
      "Iteration 126, Batch: 6, Loss: 0.05992341786623001\n",
      "Iteration 126, Batch: 7, Loss: 0.07852981984615326\n",
      "Iteration 126, Batch: 8, Loss: 0.04942687973380089\n",
      "Iteration 126, Batch: 9, Loss: 0.06312768161296844\n",
      "Iteration 126, Batch: 10, Loss: 0.04110318049788475\n",
      "Iteration 126, Batch: 11, Loss: 0.05555793270468712\n",
      "Iteration 126, Batch: 12, Loss: 0.05609753355383873\n",
      "Iteration 126, Batch: 13, Loss: 0.050414618104696274\n",
      "Iteration 126, Batch: 14, Loss: 0.04434924200177193\n",
      "Iteration 126, Batch: 15, Loss: 0.044553592801094055\n",
      "Iteration 126, Batch: 16, Loss: 0.04550981521606445\n",
      "Iteration 126, Batch: 17, Loss: 0.05820072442293167\n",
      "Iteration 126, Batch: 18, Loss: 0.056595105677843094\n",
      "Iteration 126, Batch: 19, Loss: 0.045476216822862625\n",
      "Iteration 126, Batch: 20, Loss: 0.0631885975599289\n",
      "Iteration 126, Batch: 21, Loss: 0.044483888894319534\n",
      "Iteration 126, Batch: 22, Loss: 0.05987027660012245\n",
      "Iteration 126, Batch: 23, Loss: 0.07176809757947922\n",
      "Iteration 126, Batch: 24, Loss: 0.05169658362865448\n",
      "Iteration 126, Batch: 25, Loss: 0.05002022162079811\n",
      "Iteration 126, Batch: 26, Loss: 0.04689352586865425\n",
      "Iteration 126, Batch: 27, Loss: 0.0880495086312294\n",
      "Iteration 126, Batch: 28, Loss: 0.08422666043043137\n",
      "Iteration 126, Batch: 29, Loss: 0.05646580457687378\n",
      "Iteration 126, Batch: 30, Loss: 0.05506496876478195\n",
      "Iteration 126, Batch: 31, Loss: 0.06473246216773987\n",
      "Iteration 126, Batch: 32, Loss: 0.06962038576602936\n",
      "Iteration 126, Batch: 33, Loss: 0.09797165542840958\n",
      "Iteration 126, Batch: 34, Loss: 0.06313958019018173\n",
      "Iteration 126, Batch: 35, Loss: 0.05560164153575897\n",
      "Iteration 126, Batch: 36, Loss: 0.060776907950639725\n",
      "Iteration 126, Batch: 37, Loss: 0.06622522324323654\n",
      "Iteration 126, Batch: 38, Loss: 0.09217576682567596\n",
      "Iteration 126, Batch: 39, Loss: 0.06654494255781174\n",
      "Iteration 126, Batch: 40, Loss: 0.058563508093357086\n",
      "Iteration 126, Batch: 41, Loss: 0.052680909633636475\n",
      "Iteration 126, Batch: 42, Loss: 0.06962106376886368\n",
      "Iteration 126, Batch: 43, Loss: 0.09222278743982315\n",
      "Iteration 126, Batch: 44, Loss: 0.04434686154127121\n",
      "Iteration 126, Batch: 45, Loss: 0.048447754234075546\n",
      "Iteration 126, Batch: 46, Loss: 0.053740959614515305\n",
      "Iteration 126, Batch: 47, Loss: 0.06984606385231018\n",
      "Iteration 126, Batch: 48, Loss: 0.06115658953785896\n",
      "Iteration 126, Batch: 49, Loss: 0.06686099618673325\n",
      "Iteration 127, Batch: 0, Loss: 0.07363763451576233\n",
      "Iteration 127, Batch: 1, Loss: 0.06997551023960114\n",
      "Iteration 127, Batch: 2, Loss: 0.06480245292186737\n",
      "Iteration 127, Batch: 3, Loss: 0.061207883059978485\n",
      "Iteration 127, Batch: 4, Loss: 0.07143191248178482\n",
      "Iteration 127, Batch: 5, Loss: 0.08165905624628067\n",
      "Iteration 127, Batch: 6, Loss: 0.06698167324066162\n",
      "Iteration 127, Batch: 7, Loss: 0.09041043370962143\n",
      "Iteration 127, Batch: 8, Loss: 0.062455710023641586\n",
      "Iteration 127, Batch: 9, Loss: 0.09072046726942062\n",
      "Iteration 127, Batch: 10, Loss: 0.12361281365156174\n",
      "Iteration 127, Batch: 11, Loss: 0.08175885677337646\n",
      "Iteration 127, Batch: 12, Loss: 0.0994613990187645\n",
      "Iteration 127, Batch: 13, Loss: 0.07787998765707016\n",
      "Iteration 127, Batch: 14, Loss: 0.0839255154132843\n",
      "Iteration 127, Batch: 15, Loss: 0.0951385647058487\n",
      "Iteration 127, Batch: 16, Loss: 0.07878284901380539\n",
      "Iteration 127, Batch: 17, Loss: 0.07246758043766022\n",
      "Iteration 127, Batch: 18, Loss: 0.08154744654893875\n",
      "Iteration 127, Batch: 19, Loss: 0.04891412332653999\n",
      "Iteration 127, Batch: 20, Loss: 0.046760864555835724\n",
      "Iteration 127, Batch: 21, Loss: 0.06488850712776184\n",
      "Iteration 127, Batch: 22, Loss: 0.06396442651748657\n",
      "Iteration 127, Batch: 23, Loss: 0.08673356473445892\n",
      "Iteration 127, Batch: 24, Loss: 0.059459883719682693\n",
      "Iteration 127, Batch: 25, Loss: 0.054228801280260086\n",
      "Iteration 127, Batch: 26, Loss: 0.08132165670394897\n",
      "Iteration 127, Batch: 27, Loss: 0.07240915298461914\n",
      "Iteration 127, Batch: 28, Loss: 0.04680909216403961\n",
      "Iteration 127, Batch: 29, Loss: 0.06318260729312897\n",
      "Iteration 127, Batch: 30, Loss: 0.07896311581134796\n",
      "Iteration 127, Batch: 31, Loss: 0.042944908142089844\n",
      "Iteration 127, Batch: 32, Loss: 0.0608571358025074\n",
      "Iteration 127, Batch: 33, Loss: 0.06256985664367676\n",
      "Iteration 127, Batch: 34, Loss: 0.11878751218318939\n",
      "Iteration 127, Batch: 35, Loss: 0.04120884835720062\n",
      "Iteration 127, Batch: 36, Loss: 0.07522337883710861\n",
      "Iteration 127, Batch: 37, Loss: 0.07764417678117752\n",
      "Iteration 127, Batch: 38, Loss: 0.07496017962694168\n",
      "Iteration 127, Batch: 39, Loss: 0.06352727860212326\n",
      "Iteration 127, Batch: 40, Loss: 0.05930785834789276\n",
      "Iteration 127, Batch: 41, Loss: 0.06623600423336029\n",
      "Iteration 127, Batch: 42, Loss: 0.06795103847980499\n",
      "Iteration 127, Batch: 43, Loss: 0.05859692394733429\n",
      "Iteration 127, Batch: 44, Loss: 0.053199298679828644\n",
      "Iteration 127, Batch: 45, Loss: 0.05905991420149803\n",
      "Iteration 127, Batch: 46, Loss: 0.08298135548830032\n",
      "Iteration 127, Batch: 47, Loss: 0.052066344767808914\n",
      "Iteration 127, Batch: 48, Loss: 0.05285554751753807\n",
      "Iteration 127, Batch: 49, Loss: 0.04353829100728035\n",
      "Iteration 128, Batch: 0, Loss: 0.07387275248765945\n",
      "Iteration 128, Batch: 1, Loss: 0.05809852480888367\n",
      "Iteration 128, Batch: 2, Loss: 0.04952164366841316\n",
      "Iteration 128, Batch: 3, Loss: 0.0583333782851696\n",
      "Iteration 128, Batch: 4, Loss: 0.07596176862716675\n",
      "Iteration 128, Batch: 5, Loss: 0.04904840886592865\n",
      "Iteration 128, Batch: 6, Loss: 0.03872481733560562\n",
      "Iteration 128, Batch: 7, Loss: 0.06579861789941788\n",
      "Iteration 128, Batch: 8, Loss: 0.03258882835507393\n",
      "Iteration 128, Batch: 9, Loss: 0.05412350222468376\n",
      "Iteration 128, Batch: 10, Loss: 0.0469820536673069\n",
      "Iteration 128, Batch: 11, Loss: 0.05942842364311218\n",
      "Iteration 128, Batch: 12, Loss: 0.08511032164096832\n",
      "Iteration 128, Batch: 13, Loss: 0.06455068290233612\n",
      "Iteration 128, Batch: 14, Loss: 0.06344950199127197\n",
      "Iteration 128, Batch: 15, Loss: 0.0665515586733818\n",
      "Iteration 128, Batch: 16, Loss: 0.07437406480312347\n",
      "Iteration 128, Batch: 17, Loss: 0.0564834289252758\n",
      "Iteration 128, Batch: 18, Loss: 0.07092492282390594\n",
      "Iteration 128, Batch: 19, Loss: 0.0658629909157753\n",
      "Iteration 128, Batch: 20, Loss: 0.07965899258852005\n",
      "Iteration 128, Batch: 21, Loss: 0.07232897728681564\n",
      "Iteration 128, Batch: 22, Loss: 0.07756081223487854\n",
      "Iteration 128, Batch: 23, Loss: 0.0740514025092125\n",
      "Iteration 128, Batch: 24, Loss: 0.06822389364242554\n",
      "Iteration 128, Batch: 25, Loss: 0.0943845734000206\n",
      "Iteration 128, Batch: 26, Loss: 0.09566741436719894\n",
      "Iteration 128, Batch: 27, Loss: 0.12011110782623291\n",
      "Iteration 128, Batch: 28, Loss: 0.08732262998819351\n",
      "Iteration 128, Batch: 29, Loss: 0.10979657620191574\n",
      "Iteration 128, Batch: 30, Loss: 0.06450483202934265\n",
      "Iteration 128, Batch: 31, Loss: 0.08408905565738678\n",
      "Iteration 128, Batch: 32, Loss: 0.077786885201931\n",
      "Iteration 128, Batch: 33, Loss: 0.060191914439201355\n",
      "Iteration 128, Batch: 34, Loss: 0.05398549139499664\n",
      "Iteration 128, Batch: 35, Loss: 0.06203740835189819\n",
      "Iteration 128, Batch: 36, Loss: 0.06140484660863876\n",
      "Iteration 128, Batch: 37, Loss: 0.0686621144413948\n",
      "Iteration 128, Batch: 38, Loss: 0.04886547848582268\n",
      "Iteration 128, Batch: 39, Loss: 0.05687901750206947\n",
      "Iteration 128, Batch: 40, Loss: 0.07187877595424652\n",
      "Iteration 128, Batch: 41, Loss: 0.08356954157352448\n",
      "Iteration 128, Batch: 42, Loss: 0.06377476453781128\n",
      "Iteration 128, Batch: 43, Loss: 0.0796976089477539\n",
      "Iteration 128, Batch: 44, Loss: 0.058000098913908005\n",
      "Iteration 128, Batch: 45, Loss: 0.06309953331947327\n",
      "Iteration 128, Batch: 46, Loss: 0.06620613485574722\n",
      "Iteration 128, Batch: 47, Loss: 0.039230555295944214\n",
      "Iteration 128, Batch: 48, Loss: 0.03293813019990921\n",
      "Iteration 128, Batch: 49, Loss: 0.08144085854291916\n",
      "Iteration 129, Batch: 0, Loss: 0.07827156037092209\n",
      "Iteration 129, Batch: 1, Loss: 0.05424809455871582\n",
      "Iteration 129, Batch: 2, Loss: 0.06672792881727219\n",
      "Iteration 129, Batch: 3, Loss: 0.08118215203285217\n",
      "Iteration 129, Batch: 4, Loss: 0.04968857765197754\n",
      "Iteration 129, Batch: 5, Loss: 0.07950300723314285\n",
      "Iteration 129, Batch: 6, Loss: 0.06787161529064178\n",
      "Iteration 129, Batch: 7, Loss: 0.05018181353807449\n",
      "Iteration 129, Batch: 8, Loss: 0.05606384947896004\n",
      "Iteration 129, Batch: 9, Loss: 0.08727884292602539\n",
      "Iteration 129, Batch: 10, Loss: 0.07564429938793182\n",
      "Iteration 129, Batch: 11, Loss: 0.09501434862613678\n",
      "Iteration 129, Batch: 12, Loss: 0.08408224582672119\n",
      "Iteration 129, Batch: 13, Loss: 0.08600492030382156\n",
      "Iteration 129, Batch: 14, Loss: 0.08806730061769485\n",
      "Iteration 129, Batch: 15, Loss: 0.07283617556095123\n",
      "Iteration 129, Batch: 16, Loss: 0.0534590519964695\n",
      "Iteration 129, Batch: 17, Loss: 0.047008588910102844\n",
      "Iteration 129, Batch: 18, Loss: 0.07561228424310684\n",
      "Iteration 129, Batch: 19, Loss: 0.0560876838862896\n",
      "Iteration 129, Batch: 20, Loss: 0.0815131738781929\n",
      "Iteration 129, Batch: 21, Loss: 0.07007109373807907\n",
      "Iteration 129, Batch: 22, Loss: 0.03910907357931137\n",
      "Iteration 129, Batch: 23, Loss: 0.05664687976241112\n",
      "Iteration 129, Batch: 24, Loss: 0.061841659247875214\n",
      "Iteration 129, Batch: 25, Loss: 0.05983160808682442\n",
      "Iteration 129, Batch: 26, Loss: 0.0380462110042572\n",
      "Iteration 129, Batch: 27, Loss: 0.07504308223724365\n",
      "Iteration 129, Batch: 28, Loss: 0.0783361867070198\n",
      "Iteration 129, Batch: 29, Loss: 0.04522480070590973\n",
      "Iteration 129, Batch: 30, Loss: 0.0433942936360836\n",
      "Iteration 129, Batch: 31, Loss: 0.08546718209981918\n",
      "Iteration 129, Batch: 32, Loss: 0.052723731845617294\n",
      "Iteration 129, Batch: 33, Loss: 0.08653654158115387\n",
      "Iteration 129, Batch: 34, Loss: 0.04537298157811165\n",
      "Iteration 129, Batch: 35, Loss: 0.05040602385997772\n",
      "Iteration 129, Batch: 36, Loss: 0.08806362003087997\n",
      "Iteration 129, Batch: 37, Loss: 0.05285066366195679\n",
      "Iteration 129, Batch: 38, Loss: 0.08831120282411575\n",
      "Iteration 129, Batch: 39, Loss: 0.06000128760933876\n",
      "Iteration 129, Batch: 40, Loss: 0.07480452954769135\n",
      "Iteration 129, Batch: 41, Loss: 0.09817492216825485\n",
      "Iteration 129, Batch: 42, Loss: 0.06581640988588333\n",
      "Iteration 129, Batch: 43, Loss: 0.04721508547663689\n",
      "Iteration 129, Batch: 44, Loss: 0.04838485270738602\n",
      "Iteration 129, Batch: 45, Loss: 0.07620024681091309\n",
      "Iteration 129, Batch: 46, Loss: 0.0294086541980505\n",
      "Iteration 129, Batch: 47, Loss: 0.05024900287389755\n",
      "Iteration 129, Batch: 48, Loss: 0.07111351191997528\n",
      "Iteration 129, Batch: 49, Loss: 0.049902550876140594\n",
      "Iteration 130, Batch: 0, Loss: 0.08459903299808502\n",
      "Iteration 130, Batch: 1, Loss: 0.06269805133342743\n",
      "Iteration 130, Batch: 2, Loss: 0.037473224103450775\n",
      "Iteration 130, Batch: 3, Loss: 0.055634673684835434\n",
      "Iteration 130, Batch: 4, Loss: 0.07562960684299469\n",
      "Iteration 130, Batch: 5, Loss: 0.04504426196217537\n",
      "Iteration 130, Batch: 6, Loss: 0.05214454606175423\n",
      "Iteration 130, Batch: 7, Loss: 0.04384790360927582\n",
      "Iteration 130, Batch: 8, Loss: 0.09706062078475952\n",
      "Iteration 130, Batch: 9, Loss: 0.08027316629886627\n",
      "Iteration 130, Batch: 10, Loss: 0.03671892732381821\n",
      "Iteration 130, Batch: 11, Loss: 0.0522208996117115\n",
      "Iteration 130, Batch: 12, Loss: 0.04630112648010254\n",
      "Iteration 130, Batch: 13, Loss: 0.057822924107313156\n",
      "Iteration 130, Batch: 14, Loss: 0.054681047797203064\n",
      "Iteration 130, Batch: 15, Loss: 0.07157327979803085\n",
      "Iteration 130, Batch: 16, Loss: 0.06727608293294907\n",
      "Iteration 130, Batch: 17, Loss: 0.03154486045241356\n",
      "Iteration 130, Batch: 18, Loss: 0.06937925517559052\n",
      "Iteration 130, Batch: 19, Loss: 0.05110780894756317\n",
      "Iteration 130, Batch: 20, Loss: 0.07662251591682434\n",
      "Iteration 130, Batch: 21, Loss: 0.05794297158718109\n",
      "Iteration 130, Batch: 22, Loss: 0.07786580175161362\n",
      "Iteration 130, Batch: 23, Loss: 0.06875775009393692\n",
      "Iteration 130, Batch: 24, Loss: 0.05490918830037117\n",
      "Iteration 130, Batch: 25, Loss: 0.049413103610277176\n",
      "Iteration 130, Batch: 26, Loss: 0.0509643629193306\n",
      "Iteration 130, Batch: 27, Loss: 0.08209342509508133\n",
      "Iteration 130, Batch: 28, Loss: 0.053391437977552414\n",
      "Iteration 130, Batch: 29, Loss: 0.04290279373526573\n",
      "Iteration 130, Batch: 30, Loss: 0.06440072506666183\n",
      "Iteration 130, Batch: 31, Loss: 0.04992518201470375\n",
      "Iteration 130, Batch: 32, Loss: 0.07194900512695312\n",
      "Iteration 130, Batch: 33, Loss: 0.07200760394334793\n",
      "Iteration 130, Batch: 34, Loss: 0.0550931841135025\n",
      "Iteration 130, Batch: 35, Loss: 0.06262476742267609\n",
      "Iteration 130, Batch: 36, Loss: 0.04901692271232605\n",
      "Iteration 130, Batch: 37, Loss: 0.06689424067735672\n",
      "Iteration 130, Batch: 38, Loss: 0.06886857002973557\n",
      "Iteration 130, Batch: 39, Loss: 0.04023061692714691\n",
      "Iteration 130, Batch: 40, Loss: 0.02911202795803547\n",
      "Iteration 130, Batch: 41, Loss: 0.04963324964046478\n",
      "Iteration 130, Batch: 42, Loss: 0.07299723476171494\n",
      "Iteration 130, Batch: 43, Loss: 0.050487324595451355\n",
      "Iteration 130, Batch: 44, Loss: 0.060137879103422165\n",
      "Iteration 130, Batch: 45, Loss: 0.06609213352203369\n",
      "Iteration 130, Batch: 46, Loss: 0.058589741587638855\n",
      "Iteration 130, Batch: 47, Loss: 0.07904598861932755\n",
      "Iteration 130, Batch: 48, Loss: 0.0495365709066391\n",
      "Iteration 130, Batch: 49, Loss: 0.05159265920519829\n",
      "Iteration 131, Batch: 0, Loss: 0.037980929017066956\n",
      "Iteration 131, Batch: 1, Loss: 0.06225799024105072\n",
      "Iteration 131, Batch: 2, Loss: 0.04521036893129349\n",
      "Iteration 131, Batch: 3, Loss: 0.06781228631734848\n",
      "Iteration 131, Batch: 4, Loss: 0.06258376687765121\n",
      "Iteration 131, Batch: 5, Loss: 0.04953157156705856\n",
      "Iteration 131, Batch: 6, Loss: 0.041591890156269073\n",
      "Iteration 131, Batch: 7, Loss: 0.06926102191209793\n",
      "Iteration 131, Batch: 8, Loss: 0.04864498972892761\n",
      "Iteration 131, Batch: 9, Loss: 0.05044203996658325\n",
      "Iteration 131, Batch: 10, Loss: 0.04615616798400879\n",
      "Iteration 131, Batch: 11, Loss: 0.0604664608836174\n",
      "Iteration 131, Batch: 12, Loss: 0.04221155121922493\n",
      "Iteration 131, Batch: 13, Loss: 0.03512907773256302\n",
      "Iteration 131, Batch: 14, Loss: 0.04128025099635124\n",
      "Iteration 131, Batch: 15, Loss: 0.04059018939733505\n",
      "Iteration 131, Batch: 16, Loss: 0.04809463396668434\n",
      "Iteration 131, Batch: 17, Loss: 0.07782559096813202\n",
      "Iteration 131, Batch: 18, Loss: 0.06479300558567047\n",
      "Iteration 131, Batch: 19, Loss: 0.033257339149713516\n",
      "Iteration 131, Batch: 20, Loss: 0.02815583162009716\n",
      "Iteration 131, Batch: 21, Loss: 0.052046507596969604\n",
      "Iteration 131, Batch: 22, Loss: 0.06179129332304001\n",
      "Iteration 131, Batch: 23, Loss: 0.051223743706941605\n",
      "Iteration 131, Batch: 24, Loss: 0.06361368298530579\n",
      "Iteration 131, Batch: 25, Loss: 0.06078018620610237\n",
      "Iteration 131, Batch: 26, Loss: 0.045436613261699677\n",
      "Iteration 131, Batch: 27, Loss: 0.02592603676021099\n",
      "Iteration 131, Batch: 28, Loss: 0.03699711337685585\n",
      "Iteration 131, Batch: 29, Loss: 0.06018821895122528\n",
      "Iteration 131, Batch: 30, Loss: 0.04900793358683586\n",
      "Iteration 131, Batch: 31, Loss: 0.05100441724061966\n",
      "Iteration 131, Batch: 32, Loss: 0.07023109495639801\n",
      "Iteration 131, Batch: 33, Loss: 0.06525956839323044\n",
      "Iteration 131, Batch: 34, Loss: 0.06423179060220718\n",
      "Iteration 131, Batch: 35, Loss: 0.04572247341275215\n",
      "Iteration 131, Batch: 36, Loss: 0.059508662670850754\n",
      "Iteration 131, Batch: 37, Loss: 0.054976217448711395\n",
      "Iteration 131, Batch: 38, Loss: 0.031193111091852188\n",
      "Iteration 131, Batch: 39, Loss: 0.04990903660655022\n",
      "Iteration 131, Batch: 40, Loss: 0.06861095875501633\n",
      "Iteration 131, Batch: 41, Loss: 0.06766390800476074\n",
      "Iteration 131, Batch: 42, Loss: 0.042913250625133514\n",
      "Iteration 131, Batch: 43, Loss: 0.08007951825857162\n",
      "Iteration 131, Batch: 44, Loss: 0.06469786167144775\n",
      "Iteration 131, Batch: 45, Loss: 0.058259572833776474\n",
      "Iteration 131, Batch: 46, Loss: 0.06715994328260422\n",
      "Iteration 131, Batch: 47, Loss: 0.05472126975655556\n",
      "Iteration 131, Batch: 48, Loss: 0.055544450879096985\n",
      "Iteration 131, Batch: 49, Loss: 0.09314469248056412\n",
      "Iteration 132, Batch: 0, Loss: 0.09448062628507614\n",
      "Iteration 132, Batch: 1, Loss: 0.08121640235185623\n",
      "Iteration 132, Batch: 2, Loss: 0.0834832563996315\n",
      "Iteration 132, Batch: 3, Loss: 0.09526270627975464\n",
      "Iteration 132, Batch: 4, Loss: 0.09034402668476105\n",
      "Iteration 132, Batch: 5, Loss: 0.06509339809417725\n",
      "Iteration 132, Batch: 6, Loss: 0.09368649870157242\n",
      "Iteration 132, Batch: 7, Loss: 0.08329791575670242\n",
      "Iteration 132, Batch: 8, Loss: 0.07051514834165573\n",
      "Iteration 132, Batch: 9, Loss: 0.07038604468107224\n",
      "Iteration 132, Batch: 10, Loss: 0.06346922367811203\n",
      "Iteration 132, Batch: 11, Loss: 0.06613831967115402\n",
      "Iteration 132, Batch: 12, Loss: 0.08172662556171417\n",
      "Iteration 132, Batch: 13, Loss: 0.051085565239191055\n",
      "Iteration 132, Batch: 14, Loss: 0.05459678918123245\n",
      "Iteration 132, Batch: 15, Loss: 0.06889523565769196\n",
      "Iteration 132, Batch: 16, Loss: 0.05802519619464874\n",
      "Iteration 132, Batch: 17, Loss: 0.06613457202911377\n",
      "Iteration 132, Batch: 18, Loss: 0.08075559139251709\n",
      "Iteration 132, Batch: 19, Loss: 0.09711913764476776\n",
      "Iteration 132, Batch: 20, Loss: 0.0720939114689827\n",
      "Iteration 132, Batch: 21, Loss: 0.08326194435358047\n",
      "Iteration 132, Batch: 22, Loss: 0.07136543840169907\n",
      "Iteration 132, Batch: 23, Loss: 0.06743276119232178\n",
      "Iteration 132, Batch: 24, Loss: 0.062369588762521744\n",
      "Iteration 132, Batch: 25, Loss: 0.07888983190059662\n",
      "Iteration 132, Batch: 26, Loss: 0.10327976197004318\n",
      "Iteration 132, Batch: 27, Loss: 0.07279536128044128\n",
      "Iteration 132, Batch: 28, Loss: 0.09967104345560074\n",
      "Iteration 132, Batch: 29, Loss: 0.060702208429574966\n",
      "Iteration 132, Batch: 30, Loss: 0.040447644889354706\n",
      "Iteration 132, Batch: 31, Loss: 0.07499073445796967\n",
      "Iteration 132, Batch: 32, Loss: 0.071444071829319\n",
      "Iteration 132, Batch: 33, Loss: 0.07344750314950943\n",
      "Iteration 132, Batch: 34, Loss: 0.06989937275648117\n",
      "Iteration 132, Batch: 35, Loss: 0.050713710486888885\n",
      "Iteration 132, Batch: 36, Loss: 0.04713304340839386\n",
      "Iteration 132, Batch: 37, Loss: 0.05590955540537834\n",
      "Iteration 132, Batch: 38, Loss: 0.03955668956041336\n",
      "Iteration 132, Batch: 39, Loss: 0.04630310833454132\n",
      "Iteration 132, Batch: 40, Loss: 0.05872250720858574\n",
      "Iteration 132, Batch: 41, Loss: 0.05690791830420494\n",
      "Iteration 132, Batch: 42, Loss: 0.029293034225702286\n",
      "Iteration 132, Batch: 43, Loss: 0.06671979278326035\n",
      "Iteration 132, Batch: 44, Loss: 0.06447143107652664\n",
      "Iteration 132, Batch: 45, Loss: 0.03422732651233673\n",
      "Iteration 132, Batch: 46, Loss: 0.05243164300918579\n",
      "Iteration 132, Batch: 47, Loss: 0.0584244579076767\n",
      "Iteration 132, Batch: 48, Loss: 0.06331053376197815\n",
      "Iteration 132, Batch: 49, Loss: 0.04698921740055084\n",
      "Iteration 133, Batch: 0, Loss: 0.061599891632795334\n",
      "Iteration 133, Batch: 1, Loss: 0.05511397868394852\n",
      "Iteration 133, Batch: 2, Loss: 0.0585080124437809\n",
      "Iteration 133, Batch: 3, Loss: 0.05264635011553764\n",
      "Iteration 133, Batch: 4, Loss: 0.05896443873643875\n",
      "Iteration 133, Batch: 5, Loss: 0.05074330046772957\n",
      "Iteration 133, Batch: 6, Loss: 0.05560080334544182\n",
      "Iteration 133, Batch: 7, Loss: 0.061158422380685806\n",
      "Iteration 133, Batch: 8, Loss: 0.08443232625722885\n",
      "Iteration 133, Batch: 9, Loss: 0.05188724771142006\n",
      "Iteration 133, Batch: 10, Loss: 0.0411568246781826\n",
      "Iteration 133, Batch: 11, Loss: 0.05601101741194725\n",
      "Iteration 133, Batch: 12, Loss: 0.06941666454076767\n",
      "Iteration 133, Batch: 13, Loss: 0.05116976797580719\n",
      "Iteration 133, Batch: 14, Loss: 0.04647236689925194\n",
      "Iteration 133, Batch: 15, Loss: 0.04208822548389435\n",
      "Iteration 133, Batch: 16, Loss: 0.04926499351859093\n",
      "Iteration 133, Batch: 17, Loss: 0.048586826771497726\n",
      "Iteration 133, Batch: 18, Loss: 0.06452998518943787\n",
      "Iteration 133, Batch: 19, Loss: 0.07406968623399734\n",
      "Iteration 133, Batch: 20, Loss: 0.0548270009458065\n",
      "Iteration 133, Batch: 21, Loss: 0.08329242467880249\n",
      "Iteration 133, Batch: 22, Loss: 0.07107885181903839\n",
      "Iteration 133, Batch: 23, Loss: 0.07928256690502167\n",
      "Iteration 133, Batch: 24, Loss: 0.062253039330244064\n",
      "Iteration 133, Batch: 25, Loss: 0.048706892877817154\n",
      "Iteration 133, Batch: 26, Loss: 0.07457989454269409\n",
      "Iteration 133, Batch: 27, Loss: 0.050000518560409546\n",
      "Iteration 133, Batch: 28, Loss: 0.0684891939163208\n",
      "Iteration 133, Batch: 29, Loss: 0.04590735211968422\n",
      "Iteration 133, Batch: 30, Loss: 0.047809820622205734\n",
      "Iteration 133, Batch: 31, Loss: 0.048218514770269394\n",
      "Iteration 133, Batch: 32, Loss: 0.049334507435560226\n",
      "Iteration 133, Batch: 33, Loss: 0.05903581529855728\n",
      "Iteration 133, Batch: 34, Loss: 0.06951749324798584\n",
      "Iteration 133, Batch: 35, Loss: 0.054289765655994415\n",
      "Iteration 133, Batch: 36, Loss: 0.051395587623119354\n",
      "Iteration 133, Batch: 37, Loss: 0.06785561144351959\n",
      "Iteration 133, Batch: 38, Loss: 0.043949563056230545\n",
      "Iteration 133, Batch: 39, Loss: 0.059611719101667404\n",
      "Iteration 133, Batch: 40, Loss: 0.0260913223028183\n",
      "Iteration 133, Batch: 41, Loss: 0.04306955263018608\n",
      "Iteration 133, Batch: 42, Loss: 0.05595231056213379\n",
      "Iteration 133, Batch: 43, Loss: 0.057140473276376724\n",
      "Iteration 133, Batch: 44, Loss: 0.0668356865644455\n",
      "Iteration 133, Batch: 45, Loss: 0.034374870359897614\n",
      "Iteration 133, Batch: 46, Loss: 0.049967210739851\n",
      "Iteration 133, Batch: 47, Loss: 0.05976337194442749\n",
      "Iteration 133, Batch: 48, Loss: 0.035271599888801575\n",
      "Iteration 133, Batch: 49, Loss: 0.05605946108698845\n",
      "Iteration 134, Batch: 0, Loss: 0.06671272218227386\n",
      "Iteration 134, Batch: 1, Loss: 0.07395247370004654\n",
      "Iteration 134, Batch: 2, Loss: 0.046591028571128845\n",
      "Iteration 134, Batch: 3, Loss: 0.04220372065901756\n",
      "Iteration 134, Batch: 4, Loss: 0.0752386525273323\n",
      "Iteration 134, Batch: 5, Loss: 0.05119136720895767\n",
      "Iteration 134, Batch: 6, Loss: 0.04482489079236984\n",
      "Iteration 134, Batch: 7, Loss: 0.034113749861717224\n",
      "Iteration 134, Batch: 8, Loss: 0.06257316470146179\n",
      "Iteration 134, Batch: 9, Loss: 0.04229981452226639\n",
      "Iteration 134, Batch: 10, Loss: 0.0659162625670433\n",
      "Iteration 134, Batch: 11, Loss: 0.06289638578891754\n",
      "Iteration 134, Batch: 12, Loss: 0.03357651084661484\n",
      "Iteration 134, Batch: 13, Loss: 0.03763747215270996\n",
      "Iteration 134, Batch: 14, Loss: 0.05670372396707535\n",
      "Iteration 134, Batch: 15, Loss: 0.04155171662569046\n",
      "Iteration 134, Batch: 16, Loss: 0.05384473130106926\n",
      "Iteration 134, Batch: 17, Loss: 0.0653233751654625\n",
      "Iteration 134, Batch: 18, Loss: 0.05166419968008995\n",
      "Iteration 134, Batch: 19, Loss: 0.05145330727100372\n",
      "Iteration 134, Batch: 20, Loss: 0.0674695298075676\n",
      "Iteration 134, Batch: 21, Loss: 0.09025219082832336\n",
      "Iteration 134, Batch: 22, Loss: 0.07423911988735199\n",
      "Iteration 134, Batch: 23, Loss: 0.08264584094285965\n",
      "Iteration 134, Batch: 24, Loss: 0.06351812183856964\n",
      "Iteration 134, Batch: 25, Loss: 0.050174713134765625\n",
      "Iteration 134, Batch: 26, Loss: 0.05663332715630531\n",
      "Iteration 134, Batch: 27, Loss: 0.06464587897062302\n",
      "Iteration 134, Batch: 28, Loss: 0.08258193731307983\n",
      "Iteration 134, Batch: 29, Loss: 0.05992398038506508\n",
      "Iteration 134, Batch: 30, Loss: 0.0661352127790451\n",
      "Iteration 134, Batch: 31, Loss: 0.0761791318655014\n",
      "Iteration 134, Batch: 32, Loss: 0.09785861521959305\n",
      "Iteration 134, Batch: 33, Loss: 0.06814372539520264\n",
      "Iteration 134, Batch: 34, Loss: 0.08387095481157303\n",
      "Iteration 134, Batch: 35, Loss: 0.086344413459301\n",
      "Iteration 134, Batch: 36, Loss: 0.07080372422933578\n",
      "Iteration 134, Batch: 37, Loss: 0.05628449469804764\n",
      "Iteration 134, Batch: 38, Loss: 0.07271786779165268\n",
      "Iteration 134, Batch: 39, Loss: 0.05434952676296234\n",
      "Iteration 134, Batch: 40, Loss: 0.06214074790477753\n",
      "Iteration 134, Batch: 41, Loss: 0.07797610759735107\n",
      "Iteration 134, Batch: 42, Loss: 0.0755319744348526\n",
      "Iteration 134, Batch: 43, Loss: 0.06578358262777328\n",
      "Iteration 134, Batch: 44, Loss: 0.04344148188829422\n",
      "Iteration 134, Batch: 45, Loss: 0.07194200158119202\n",
      "Iteration 134, Batch: 46, Loss: 0.03668830171227455\n",
      "Iteration 134, Batch: 47, Loss: 0.061140451580286026\n",
      "Iteration 134, Batch: 48, Loss: 0.07036209851503372\n",
      "Iteration 134, Batch: 49, Loss: 0.050905741751194\n",
      "Iteration 135, Batch: 0, Loss: 0.053310588002204895\n",
      "Iteration 135, Batch: 1, Loss: 0.04549785330891609\n",
      "Iteration 135, Batch: 2, Loss: 0.04836111143231392\n",
      "Iteration 135, Batch: 3, Loss: 0.04034750536084175\n",
      "Iteration 135, Batch: 4, Loss: 0.05163063853979111\n",
      "Iteration 135, Batch: 5, Loss: 0.08532415330410004\n",
      "Iteration 135, Batch: 6, Loss: 0.06639173626899719\n",
      "Iteration 135, Batch: 7, Loss: 0.0790175125002861\n",
      "Iteration 135, Batch: 8, Loss: 0.08881344646215439\n",
      "Iteration 135, Batch: 9, Loss: 0.06577685475349426\n",
      "Iteration 135, Batch: 10, Loss: 0.03557520732283592\n",
      "Iteration 135, Batch: 11, Loss: 0.06581200659275055\n",
      "Iteration 135, Batch: 12, Loss: 0.044891394674777985\n",
      "Iteration 135, Batch: 13, Loss: 0.06738745421171188\n",
      "Iteration 135, Batch: 14, Loss: 0.037829961627721786\n",
      "Iteration 135, Batch: 15, Loss: 0.053816452622413635\n",
      "Iteration 135, Batch: 16, Loss: 0.05186808481812477\n",
      "Iteration 135, Batch: 17, Loss: 0.09336326271295547\n",
      "Iteration 135, Batch: 18, Loss: 0.06362935900688171\n",
      "Iteration 135, Batch: 19, Loss: 0.0619112104177475\n",
      "Iteration 135, Batch: 20, Loss: 0.04351431876420975\n",
      "Iteration 135, Batch: 21, Loss: 0.029606269672513008\n",
      "Iteration 135, Batch: 22, Loss: 0.06867463141679764\n",
      "Iteration 135, Batch: 23, Loss: 0.048048391938209534\n",
      "Iteration 135, Batch: 24, Loss: 0.07004166394472122\n",
      "Iteration 135, Batch: 25, Loss: 0.09710758924484253\n",
      "Iteration 135, Batch: 26, Loss: 0.05623295530676842\n",
      "Iteration 135, Batch: 27, Loss: 0.05220102518796921\n",
      "Iteration 135, Batch: 28, Loss: 0.07889943569898605\n",
      "Iteration 135, Batch: 29, Loss: 0.07727370411157608\n",
      "Iteration 135, Batch: 30, Loss: 0.07162807881832123\n",
      "Iteration 135, Batch: 31, Loss: 0.0839000716805458\n",
      "Iteration 135, Batch: 32, Loss: 0.06064334884285927\n",
      "Iteration 135, Batch: 33, Loss: 0.029134001582860947\n",
      "Iteration 135, Batch: 34, Loss: 0.06206980720162392\n",
      "Iteration 135, Batch: 35, Loss: 0.06866253912448883\n",
      "Iteration 135, Batch: 36, Loss: 0.045208681374788284\n",
      "Iteration 135, Batch: 37, Loss: 0.04852600395679474\n",
      "Iteration 135, Batch: 38, Loss: 0.06966903805732727\n",
      "Iteration 135, Batch: 39, Loss: 0.05281072482466698\n",
      "Iteration 135, Batch: 40, Loss: 0.04018282890319824\n",
      "Iteration 135, Batch: 41, Loss: 0.034036874771118164\n",
      "Iteration 135, Batch: 42, Loss: 0.06051042303442955\n",
      "Iteration 135, Batch: 43, Loss: 0.041361793875694275\n",
      "Iteration 135, Batch: 44, Loss: 0.045027680695056915\n",
      "Iteration 135, Batch: 45, Loss: 0.032972414046525955\n",
      "Iteration 135, Batch: 46, Loss: 0.07287103682756424\n",
      "Iteration 135, Batch: 47, Loss: 0.06598954647779465\n",
      "Iteration 135, Batch: 48, Loss: 0.049049012362957\n",
      "Iteration 135, Batch: 49, Loss: 0.06829111278057098\n",
      "Iteration 136, Batch: 0, Loss: 0.0535578615963459\n",
      "Iteration 136, Batch: 1, Loss: 0.08570139110088348\n",
      "Iteration 136, Batch: 2, Loss: 0.04429381340742111\n",
      "Iteration 136, Batch: 3, Loss: 0.05056184530258179\n",
      "Iteration 136, Batch: 4, Loss: 0.05991778150200844\n",
      "Iteration 136, Batch: 5, Loss: 0.047636643052101135\n",
      "Iteration 136, Batch: 6, Loss: 0.04331665858626366\n",
      "Iteration 136, Batch: 7, Loss: 0.029252564534544945\n",
      "Iteration 136, Batch: 8, Loss: 0.06067930907011032\n",
      "Iteration 136, Batch: 9, Loss: 0.06249840557575226\n",
      "Iteration 136, Batch: 10, Loss: 0.061739325523376465\n",
      "Iteration 136, Batch: 11, Loss: 0.04775652661919594\n",
      "Iteration 136, Batch: 12, Loss: 0.06827870011329651\n",
      "Iteration 136, Batch: 13, Loss: 0.07438633590936661\n",
      "Iteration 136, Batch: 14, Loss: 0.05669672042131424\n",
      "Iteration 136, Batch: 15, Loss: 0.07787900418043137\n",
      "Iteration 136, Batch: 16, Loss: 0.04846318066120148\n",
      "Iteration 136, Batch: 17, Loss: 0.09369955211877823\n",
      "Iteration 136, Batch: 18, Loss: 0.06297750025987625\n",
      "Iteration 136, Batch: 19, Loss: 0.08301462978124619\n",
      "Iteration 136, Batch: 20, Loss: 0.08176484704017639\n",
      "Iteration 136, Batch: 21, Loss: 0.05083954334259033\n",
      "Iteration 136, Batch: 22, Loss: 0.060309771448373795\n",
      "Iteration 136, Batch: 23, Loss: 0.04667153209447861\n",
      "Iteration 136, Batch: 24, Loss: 0.04756544902920723\n",
      "Iteration 136, Batch: 25, Loss: 0.05425141379237175\n",
      "Iteration 136, Batch: 26, Loss: 0.0413183718919754\n",
      "Iteration 136, Batch: 27, Loss: 0.04331213980913162\n",
      "Iteration 136, Batch: 28, Loss: 0.0537138506770134\n",
      "Iteration 136, Batch: 29, Loss: 0.07455003261566162\n",
      "Iteration 136, Batch: 30, Loss: 0.04029330983757973\n",
      "Iteration 136, Batch: 31, Loss: 0.07123999297618866\n",
      "Iteration 136, Batch: 32, Loss: 0.0601259209215641\n",
      "Iteration 136, Batch: 33, Loss: 0.05106942355632782\n",
      "Iteration 136, Batch: 34, Loss: 0.03128150478005409\n",
      "Iteration 136, Batch: 35, Loss: 0.07175073027610779\n",
      "Iteration 136, Batch: 36, Loss: 0.06109905242919922\n",
      "Iteration 136, Batch: 37, Loss: 0.09076282382011414\n",
      "Iteration 136, Batch: 38, Loss: 0.08034007996320724\n",
      "Iteration 136, Batch: 39, Loss: 0.04514934867620468\n",
      "Iteration 136, Batch: 40, Loss: 0.046706151217222214\n",
      "Iteration 136, Batch: 41, Loss: 0.06499487906694412\n",
      "Iteration 136, Batch: 42, Loss: 0.04488975554704666\n",
      "Iteration 136, Batch: 43, Loss: 0.055762071162462234\n",
      "Iteration 136, Batch: 44, Loss: 0.04660004377365112\n",
      "Iteration 136, Batch: 45, Loss: 0.0649222582578659\n",
      "Iteration 136, Batch: 46, Loss: 0.07164555788040161\n",
      "Iteration 136, Batch: 47, Loss: 0.047937873750925064\n",
      "Iteration 136, Batch: 48, Loss: 0.06492605060338974\n",
      "Iteration 136, Batch: 49, Loss: 0.05786088854074478\n",
      "Iteration 137, Batch: 0, Loss: 0.06636202335357666\n",
      "Iteration 137, Batch: 1, Loss: 0.039981406182050705\n",
      "Iteration 137, Batch: 2, Loss: 0.035870376974344254\n",
      "Iteration 137, Batch: 3, Loss: 0.05159052833914757\n",
      "Iteration 137, Batch: 4, Loss: 0.07568909227848053\n",
      "Iteration 137, Batch: 5, Loss: 0.038742318749427795\n",
      "Iteration 137, Batch: 6, Loss: 0.07301460951566696\n",
      "Iteration 137, Batch: 7, Loss: 0.05766821280121803\n",
      "Iteration 137, Batch: 8, Loss: 0.05582553148269653\n",
      "Iteration 137, Batch: 9, Loss: 0.043904151767492294\n",
      "Iteration 137, Batch: 10, Loss: 0.04521961137652397\n",
      "Iteration 137, Batch: 11, Loss: 0.03770860657095909\n",
      "Iteration 137, Batch: 12, Loss: 0.054783400148153305\n",
      "Iteration 137, Batch: 13, Loss: 0.06740868091583252\n",
      "Iteration 137, Batch: 14, Loss: 0.046923331916332245\n",
      "Iteration 137, Batch: 15, Loss: 0.06597980111837387\n",
      "Iteration 137, Batch: 16, Loss: 0.04623117297887802\n",
      "Iteration 137, Batch: 17, Loss: 0.0814034715294838\n",
      "Iteration 137, Batch: 18, Loss: 0.027971021831035614\n",
      "Iteration 137, Batch: 19, Loss: 0.05540001764893532\n",
      "Iteration 137, Batch: 20, Loss: 0.06053658947348595\n",
      "Iteration 137, Batch: 21, Loss: 0.05083842948079109\n",
      "Iteration 137, Batch: 22, Loss: 0.03665805980563164\n",
      "Iteration 137, Batch: 23, Loss: 0.07106219232082367\n",
      "Iteration 137, Batch: 24, Loss: 0.06871522963047028\n",
      "Iteration 137, Batch: 25, Loss: 0.05719401314854622\n",
      "Iteration 137, Batch: 26, Loss: 0.04811621084809303\n",
      "Iteration 137, Batch: 27, Loss: 0.05404346436262131\n",
      "Iteration 137, Batch: 28, Loss: 0.038641028106212616\n",
      "Iteration 137, Batch: 29, Loss: 0.03311152756214142\n",
      "Iteration 137, Batch: 30, Loss: 0.04101438447833061\n",
      "Iteration 137, Batch: 31, Loss: 0.06290328502655029\n",
      "Iteration 137, Batch: 32, Loss: 0.056813325732946396\n",
      "Iteration 137, Batch: 33, Loss: 0.05121906101703644\n",
      "Iteration 137, Batch: 34, Loss: 0.06145700812339783\n",
      "Iteration 137, Batch: 35, Loss: 0.05746065080165863\n",
      "Iteration 137, Batch: 36, Loss: 0.032843489199876785\n",
      "Iteration 137, Batch: 37, Loss: 0.037198591977357864\n",
      "Iteration 137, Batch: 38, Loss: 0.05370726436376572\n",
      "Iteration 137, Batch: 39, Loss: 0.07563674449920654\n",
      "Iteration 137, Batch: 40, Loss: 0.06301330775022507\n",
      "Iteration 137, Batch: 41, Loss: 0.0413050577044487\n",
      "Iteration 137, Batch: 42, Loss: 0.03789546340703964\n",
      "Iteration 137, Batch: 43, Loss: 0.05519000440835953\n",
      "Iteration 137, Batch: 44, Loss: 0.03121359832584858\n",
      "Iteration 137, Batch: 45, Loss: 0.057642485946416855\n",
      "Iteration 137, Batch: 46, Loss: 0.05032658576965332\n",
      "Iteration 137, Batch: 47, Loss: 0.05375215783715248\n",
      "Iteration 137, Batch: 48, Loss: 0.045674003660678864\n",
      "Iteration 137, Batch: 49, Loss: 0.053531039506196976\n",
      "Iteration 138, Batch: 0, Loss: 0.049747858196496964\n",
      "Iteration 138, Batch: 1, Loss: 0.0711774155497551\n",
      "Iteration 138, Batch: 2, Loss: 0.04781287536025047\n",
      "Iteration 138, Batch: 3, Loss: 0.045034751296043396\n",
      "Iteration 138, Batch: 4, Loss: 0.07557260990142822\n",
      "Iteration 138, Batch: 5, Loss: 0.05101627856492996\n",
      "Iteration 138, Batch: 6, Loss: 0.05958889052271843\n",
      "Iteration 138, Batch: 7, Loss: 0.07176431268453598\n",
      "Iteration 138, Batch: 8, Loss: 0.048755109310150146\n",
      "Iteration 138, Batch: 9, Loss: 0.05930702015757561\n",
      "Iteration 138, Batch: 10, Loss: 0.05665949732065201\n",
      "Iteration 138, Batch: 11, Loss: 0.05027643218636513\n",
      "Iteration 138, Batch: 12, Loss: 0.03716054558753967\n",
      "Iteration 138, Batch: 13, Loss: 0.06811723858118057\n",
      "Iteration 138, Batch: 14, Loss: 0.06330473721027374\n",
      "Iteration 138, Batch: 15, Loss: 0.03593241795897484\n",
      "Iteration 138, Batch: 16, Loss: 0.05781007558107376\n",
      "Iteration 138, Batch: 17, Loss: 0.04405707120895386\n",
      "Iteration 138, Batch: 18, Loss: 0.035337891429662704\n",
      "Iteration 138, Batch: 19, Loss: 0.046201933175325394\n",
      "Iteration 138, Batch: 20, Loss: 0.04481255263090134\n",
      "Iteration 138, Batch: 21, Loss: 0.07397811114788055\n",
      "Iteration 138, Batch: 22, Loss: 0.04732389375567436\n",
      "Iteration 138, Batch: 23, Loss: 0.06032334268093109\n",
      "Iteration 138, Batch: 24, Loss: 0.057590264827013016\n",
      "Iteration 138, Batch: 25, Loss: 0.04228256270289421\n",
      "Iteration 138, Batch: 26, Loss: 0.05699441209435463\n",
      "Iteration 138, Batch: 27, Loss: 0.03667813166975975\n",
      "Iteration 138, Batch: 28, Loss: 0.05523524060845375\n",
      "Iteration 138, Batch: 29, Loss: 0.04169266298413277\n",
      "Iteration 138, Batch: 30, Loss: 0.05518343672156334\n",
      "Iteration 138, Batch: 31, Loss: 0.042212143540382385\n",
      "Iteration 138, Batch: 32, Loss: 0.06843812763690948\n",
      "Iteration 138, Batch: 33, Loss: 0.04617001488804817\n",
      "Iteration 138, Batch: 34, Loss: 0.05547390878200531\n",
      "Iteration 138, Batch: 35, Loss: 0.047622956335544586\n",
      "Iteration 138, Batch: 36, Loss: 0.04665879160165787\n",
      "Iteration 138, Batch: 37, Loss: 0.056383341550827026\n",
      "Iteration 138, Batch: 38, Loss: 0.04818866029381752\n",
      "Iteration 138, Batch: 39, Loss: 0.03784151375293732\n",
      "Iteration 138, Batch: 40, Loss: 0.04942942410707474\n",
      "Iteration 138, Batch: 41, Loss: 0.04274560883641243\n",
      "Iteration 138, Batch: 42, Loss: 0.04898214712738991\n",
      "Iteration 138, Batch: 43, Loss: 0.05158628523349762\n",
      "Iteration 138, Batch: 44, Loss: 0.04464554786682129\n",
      "Iteration 138, Batch: 45, Loss: 0.039540812373161316\n",
      "Iteration 138, Batch: 46, Loss: 0.058445800095796585\n",
      "Iteration 138, Batch: 47, Loss: 0.05388915538787842\n",
      "Iteration 138, Batch: 48, Loss: 0.04954184591770172\n",
      "Iteration 138, Batch: 49, Loss: 0.057628557085990906\n",
      "Iteration 139, Batch: 0, Loss: 0.04346582293510437\n",
      "Iteration 139, Batch: 1, Loss: 0.059749048203229904\n",
      "Iteration 139, Batch: 2, Loss: 0.03343164548277855\n",
      "Iteration 139, Batch: 3, Loss: 0.06979676336050034\n",
      "Iteration 139, Batch: 4, Loss: 0.05589865520596504\n",
      "Iteration 139, Batch: 5, Loss: 0.040898021310567856\n",
      "Iteration 139, Batch: 6, Loss: 0.05787891149520874\n",
      "Iteration 139, Batch: 7, Loss: 0.045615795999765396\n",
      "Iteration 139, Batch: 8, Loss: 0.05802175775170326\n",
      "Iteration 139, Batch: 9, Loss: 0.05925833433866501\n",
      "Iteration 139, Batch: 10, Loss: 0.07953940331935883\n",
      "Iteration 139, Batch: 11, Loss: 0.07048937678337097\n",
      "Iteration 139, Batch: 12, Loss: 0.048162657767534256\n",
      "Iteration 139, Batch: 13, Loss: 0.04582088068127632\n",
      "Iteration 139, Batch: 14, Loss: 0.03886690363287926\n",
      "Iteration 139, Batch: 15, Loss: 0.055820636451244354\n",
      "Iteration 139, Batch: 16, Loss: 0.052652452141046524\n",
      "Iteration 139, Batch: 17, Loss: 0.0400446355342865\n",
      "Iteration 139, Batch: 18, Loss: 0.03421732410788536\n",
      "Iteration 139, Batch: 19, Loss: 0.06437525898218155\n",
      "Iteration 139, Batch: 20, Loss: 0.052606724202632904\n",
      "Iteration 139, Batch: 21, Loss: 0.031985893845558167\n",
      "Iteration 139, Batch: 22, Loss: 0.061874501407146454\n",
      "Iteration 139, Batch: 23, Loss: 0.04916103184223175\n",
      "Iteration 139, Batch: 24, Loss: 0.0681559294462204\n",
      "Iteration 139, Batch: 25, Loss: 0.06029341742396355\n",
      "Iteration 139, Batch: 26, Loss: 0.07427974045276642\n",
      "Iteration 139, Batch: 27, Loss: 0.06765790283679962\n",
      "Iteration 139, Batch: 28, Loss: 0.044485729187726974\n",
      "Iteration 139, Batch: 29, Loss: 0.05765541270375252\n",
      "Iteration 139, Batch: 30, Loss: 0.06017781049013138\n",
      "Iteration 139, Batch: 31, Loss: 0.05085618048906326\n",
      "Iteration 139, Batch: 32, Loss: 0.05378011614084244\n",
      "Iteration 139, Batch: 33, Loss: 0.05581498146057129\n",
      "Iteration 139, Batch: 34, Loss: 0.0500187911093235\n",
      "Iteration 139, Batch: 35, Loss: 0.054002340883016586\n",
      "Iteration 139, Batch: 36, Loss: 0.07180614769458771\n",
      "Iteration 139, Batch: 37, Loss: 0.05414276942610741\n",
      "Iteration 139, Batch: 38, Loss: 0.04663126915693283\n",
      "Iteration 139, Batch: 39, Loss: 0.07218225300312042\n",
      "Iteration 139, Batch: 40, Loss: 0.041761789470911026\n",
      "Iteration 139, Batch: 41, Loss: 0.07146140187978745\n",
      "Iteration 139, Batch: 42, Loss: 0.04952123388648033\n",
      "Iteration 139, Batch: 43, Loss: 0.056476667523384094\n",
      "Iteration 139, Batch: 44, Loss: 0.0565904937684536\n",
      "Iteration 139, Batch: 45, Loss: 0.034916505217552185\n",
      "Iteration 139, Batch: 46, Loss: 0.08820991963148117\n",
      "Iteration 139, Batch: 47, Loss: 0.05517066270112991\n",
      "Iteration 139, Batch: 48, Loss: 0.0465339794754982\n",
      "Iteration 139, Batch: 49, Loss: 0.04054741933941841\n",
      "Iteration 140, Batch: 0, Loss: 0.03655722364783287\n",
      "Iteration 140, Batch: 1, Loss: 0.06742798537015915\n",
      "Iteration 140, Batch: 2, Loss: 0.05591626837849617\n",
      "Iteration 140, Batch: 3, Loss: 0.06468270719051361\n",
      "Iteration 140, Batch: 4, Loss: 0.04909687861800194\n",
      "Iteration 140, Batch: 5, Loss: 0.05690803378820419\n",
      "Iteration 140, Batch: 6, Loss: 0.022420605644583702\n",
      "Iteration 140, Batch: 7, Loss: 0.06823845952749252\n",
      "Iteration 140, Batch: 8, Loss: 0.017747648060321808\n",
      "Iteration 140, Batch: 9, Loss: 0.053565673530101776\n",
      "Iteration 140, Batch: 10, Loss: 0.045905496925115585\n",
      "Iteration 140, Batch: 11, Loss: 0.041752927005290985\n",
      "Iteration 140, Batch: 12, Loss: 0.04115517437458038\n",
      "Iteration 140, Batch: 13, Loss: 0.05862634629011154\n",
      "Iteration 140, Batch: 14, Loss: 0.05136805400252342\n",
      "Iteration 140, Batch: 15, Loss: 0.05694014951586723\n",
      "Iteration 140, Batch: 16, Loss: 0.05399496853351593\n",
      "Iteration 140, Batch: 17, Loss: 0.047136832028627396\n",
      "Iteration 140, Batch: 18, Loss: 0.053550150245428085\n",
      "Iteration 140, Batch: 19, Loss: 0.053200721740722656\n",
      "Iteration 140, Batch: 20, Loss: 0.06129033863544464\n",
      "Iteration 140, Batch: 21, Loss: 0.04530554264783859\n",
      "Iteration 140, Batch: 22, Loss: 0.05142985284328461\n",
      "Iteration 140, Batch: 23, Loss: 0.06990747153759003\n",
      "Iteration 140, Batch: 24, Loss: 0.05901459977030754\n",
      "Iteration 140, Batch: 25, Loss: 0.05569169297814369\n",
      "Iteration 140, Batch: 26, Loss: 0.0783153623342514\n",
      "Iteration 140, Batch: 27, Loss: 0.06370912492275238\n",
      "Iteration 140, Batch: 28, Loss: 0.04262213036417961\n",
      "Iteration 140, Batch: 29, Loss: 0.02960510179400444\n",
      "Iteration 140, Batch: 30, Loss: 0.05381637066602707\n",
      "Iteration 140, Batch: 31, Loss: 0.05549008399248123\n",
      "Iteration 140, Batch: 32, Loss: 0.0759255588054657\n",
      "Iteration 140, Batch: 33, Loss: 0.06393185257911682\n",
      "Iteration 140, Batch: 34, Loss: 0.052936945110559464\n",
      "Iteration 140, Batch: 35, Loss: 0.038637008517980576\n",
      "Iteration 140, Batch: 36, Loss: 0.05159694701433182\n",
      "Iteration 140, Batch: 37, Loss: 0.0657745748758316\n",
      "Iteration 140, Batch: 38, Loss: 0.02922547049820423\n",
      "Iteration 140, Batch: 39, Loss: 0.0551067590713501\n",
      "Iteration 140, Batch: 40, Loss: 0.03383396938443184\n",
      "Iteration 140, Batch: 41, Loss: 0.04194309934973717\n",
      "Iteration 140, Batch: 42, Loss: 0.050817862153053284\n",
      "Iteration 140, Batch: 43, Loss: 0.0491388700902462\n",
      "Iteration 140, Batch: 44, Loss: 0.03925870731472969\n",
      "Iteration 140, Batch: 45, Loss: 0.05841105431318283\n",
      "Iteration 140, Batch: 46, Loss: 0.07573919743299484\n",
      "Iteration 140, Batch: 47, Loss: 0.0599430687725544\n",
      "Iteration 140, Batch: 48, Loss: 0.07191328704357147\n",
      "Iteration 140, Batch: 49, Loss: 0.05539049580693245\n",
      "Iteration 141, Batch: 0, Loss: 0.04546277970075607\n",
      "Iteration 141, Batch: 1, Loss: 0.0641963854432106\n",
      "Iteration 141, Batch: 2, Loss: 0.06537269800901413\n",
      "Iteration 141, Batch: 3, Loss: 0.03842277452349663\n",
      "Iteration 141, Batch: 4, Loss: 0.03275489807128906\n",
      "Iteration 141, Batch: 5, Loss: 0.07659637928009033\n",
      "Iteration 141, Batch: 6, Loss: 0.04898660257458687\n",
      "Iteration 141, Batch: 7, Loss: 0.058701179921627045\n",
      "Iteration 141, Batch: 8, Loss: 0.034357041120529175\n",
      "Iteration 141, Batch: 9, Loss: 0.0782511755824089\n",
      "Iteration 141, Batch: 10, Loss: 0.04947977885603905\n",
      "Iteration 141, Batch: 11, Loss: 0.06933499872684479\n",
      "Iteration 141, Batch: 12, Loss: 0.07216524332761765\n",
      "Iteration 141, Batch: 13, Loss: 0.03660060465335846\n",
      "Iteration 141, Batch: 14, Loss: 0.052407559007406235\n",
      "Iteration 141, Batch: 15, Loss: 0.04793321341276169\n",
      "Iteration 141, Batch: 16, Loss: 0.05005575716495514\n",
      "Iteration 141, Batch: 17, Loss: 0.05448419600725174\n",
      "Iteration 141, Batch: 18, Loss: 0.04717087000608444\n",
      "Iteration 141, Batch: 19, Loss: 0.045313119888305664\n",
      "Iteration 141, Batch: 20, Loss: 0.06855833530426025\n",
      "Iteration 141, Batch: 21, Loss: 0.06399617344141006\n",
      "Iteration 141, Batch: 22, Loss: 0.03318210691213608\n",
      "Iteration 141, Batch: 23, Loss: 0.07067187875509262\n",
      "Iteration 141, Batch: 24, Loss: 0.031096577644348145\n",
      "Iteration 141, Batch: 25, Loss: 0.06419189274311066\n",
      "Iteration 141, Batch: 26, Loss: 0.06362924724817276\n",
      "Iteration 141, Batch: 27, Loss: 0.06229637190699577\n",
      "Iteration 141, Batch: 28, Loss: 0.05318698659539223\n",
      "Iteration 141, Batch: 29, Loss: 0.028111491352319717\n",
      "Iteration 141, Batch: 30, Loss: 0.07000898569822311\n",
      "Iteration 141, Batch: 31, Loss: 0.06221453845500946\n",
      "Iteration 141, Batch: 32, Loss: 0.04509103670716286\n",
      "Iteration 141, Batch: 33, Loss: 0.03747881203889847\n",
      "Iteration 141, Batch: 34, Loss: 0.03919891640543938\n",
      "Iteration 141, Batch: 35, Loss: 0.05206793546676636\n",
      "Iteration 141, Batch: 36, Loss: 0.05190218985080719\n",
      "Iteration 141, Batch: 37, Loss: 0.057212311774492264\n",
      "Iteration 141, Batch: 38, Loss: 0.05440947413444519\n",
      "Iteration 141, Batch: 39, Loss: 0.04995500296354294\n",
      "Iteration 141, Batch: 40, Loss: 0.06635823845863342\n",
      "Iteration 141, Batch: 41, Loss: 0.036763306707143784\n",
      "Iteration 141, Batch: 42, Loss: 0.04255233705043793\n",
      "Iteration 141, Batch: 43, Loss: 0.056252069771289825\n",
      "Iteration 141, Batch: 44, Loss: 0.039358705282211304\n",
      "Iteration 141, Batch: 45, Loss: 0.04019314795732498\n",
      "Iteration 141, Batch: 46, Loss: 0.05168406292796135\n",
      "Iteration 141, Batch: 47, Loss: 0.051942795515060425\n",
      "Iteration 141, Batch: 48, Loss: 0.0447184257209301\n",
      "Iteration 141, Batch: 49, Loss: 0.057843998074531555\n",
      "Iteration 142, Batch: 0, Loss: 0.0767427459359169\n",
      "Iteration 142, Batch: 1, Loss: 0.06141822785139084\n",
      "Iteration 142, Batch: 2, Loss: 0.05188945680856705\n",
      "Iteration 142, Batch: 3, Loss: 0.05732521787285805\n",
      "Iteration 142, Batch: 4, Loss: 0.047517355531454086\n",
      "Iteration 142, Batch: 5, Loss: 0.055175770074129105\n",
      "Iteration 142, Batch: 6, Loss: 0.03580096736550331\n",
      "Iteration 142, Batch: 7, Loss: 0.04170004650950432\n",
      "Iteration 142, Batch: 8, Loss: 0.05274953693151474\n",
      "Iteration 142, Batch: 9, Loss: 0.05537157878279686\n",
      "Iteration 142, Batch: 10, Loss: 0.05632852762937546\n",
      "Iteration 142, Batch: 11, Loss: 0.04503640905022621\n",
      "Iteration 142, Batch: 12, Loss: 0.04283231496810913\n",
      "Iteration 142, Batch: 13, Loss: 0.05660222843289375\n",
      "Iteration 142, Batch: 14, Loss: 0.0438821017742157\n",
      "Iteration 142, Batch: 15, Loss: 0.024568727239966393\n",
      "Iteration 142, Batch: 16, Loss: 0.06971342116594315\n",
      "Iteration 142, Batch: 17, Loss: 0.0484263114631176\n",
      "Iteration 142, Batch: 18, Loss: 0.048819709569215775\n",
      "Iteration 142, Batch: 19, Loss: 0.04126366600394249\n",
      "Iteration 142, Batch: 20, Loss: 0.05414557456970215\n",
      "Iteration 142, Batch: 21, Loss: 0.052000273019075394\n",
      "Iteration 142, Batch: 22, Loss: 0.051797326654195786\n",
      "Iteration 142, Batch: 23, Loss: 0.04864473268389702\n",
      "Iteration 142, Batch: 24, Loss: 0.049017056822776794\n",
      "Iteration 142, Batch: 25, Loss: 0.04076777771115303\n",
      "Iteration 142, Batch: 26, Loss: 0.07153663784265518\n",
      "Iteration 142, Batch: 27, Loss: 0.051678769290447235\n",
      "Iteration 142, Batch: 28, Loss: 0.04858182743191719\n",
      "Iteration 142, Batch: 29, Loss: 0.04770984500646591\n",
      "Iteration 142, Batch: 30, Loss: 0.067215695977211\n",
      "Iteration 142, Batch: 31, Loss: 0.04577357694506645\n",
      "Iteration 142, Batch: 32, Loss: 0.07390502840280533\n",
      "Iteration 142, Batch: 33, Loss: 0.050933387130498886\n",
      "Iteration 142, Batch: 34, Loss: 0.05370179936289787\n",
      "Iteration 142, Batch: 35, Loss: 0.06003047898411751\n",
      "Iteration 142, Batch: 36, Loss: 0.059636738151311874\n",
      "Iteration 142, Batch: 37, Loss: 0.04941848665475845\n",
      "Iteration 142, Batch: 38, Loss: 0.04200049489736557\n",
      "Iteration 142, Batch: 39, Loss: 0.045229118317365646\n",
      "Iteration 142, Batch: 40, Loss: 0.028000084683299065\n",
      "Iteration 142, Batch: 41, Loss: 0.05716041475534439\n",
      "Iteration 142, Batch: 42, Loss: 0.07539775222539902\n",
      "Iteration 142, Batch: 43, Loss: 0.04366216063499451\n",
      "Iteration 142, Batch: 44, Loss: 0.07285265624523163\n",
      "Iteration 142, Batch: 45, Loss: 0.028515296056866646\n",
      "Iteration 142, Batch: 46, Loss: 0.05521955341100693\n",
      "Iteration 142, Batch: 47, Loss: 0.05341975390911102\n",
      "Iteration 142, Batch: 48, Loss: 0.07140741497278214\n",
      "Iteration 142, Batch: 49, Loss: 0.05296816676855087\n",
      "Iteration 143, Batch: 0, Loss: 0.05059768632054329\n",
      "Iteration 143, Batch: 1, Loss: 0.0401042178273201\n",
      "Iteration 143, Batch: 2, Loss: 0.0699668750166893\n",
      "Iteration 143, Batch: 3, Loss: 0.05088113248348236\n",
      "Iteration 143, Batch: 4, Loss: 0.03951634466648102\n",
      "Iteration 143, Batch: 5, Loss: 0.05690721794962883\n",
      "Iteration 143, Batch: 6, Loss: 0.04130582883954048\n",
      "Iteration 143, Batch: 7, Loss: 0.033887725323438644\n",
      "Iteration 143, Batch: 8, Loss: 0.04522884264588356\n",
      "Iteration 143, Batch: 9, Loss: 0.057046882808208466\n",
      "Iteration 143, Batch: 10, Loss: 0.05456465482711792\n",
      "Iteration 143, Batch: 11, Loss: 0.041963156312704086\n",
      "Iteration 143, Batch: 12, Loss: 0.07899987697601318\n",
      "Iteration 143, Batch: 13, Loss: 0.06650000065565109\n",
      "Iteration 143, Batch: 14, Loss: 0.04787515476346016\n",
      "Iteration 143, Batch: 15, Loss: 0.058539945632219315\n",
      "Iteration 143, Batch: 16, Loss: 0.06058351323008537\n",
      "Iteration 143, Batch: 17, Loss: 0.04813341051340103\n",
      "Iteration 143, Batch: 18, Loss: 0.035902976989746094\n",
      "Iteration 143, Batch: 19, Loss: 0.04888549819588661\n",
      "Iteration 143, Batch: 20, Loss: 0.05663438141345978\n",
      "Iteration 143, Batch: 21, Loss: 0.027109267190098763\n",
      "Iteration 143, Batch: 22, Loss: 0.04564822465181351\n",
      "Iteration 143, Batch: 23, Loss: 0.04159771651029587\n",
      "Iteration 143, Batch: 24, Loss: 0.05339173972606659\n",
      "Iteration 143, Batch: 25, Loss: 0.040278296917676926\n",
      "Iteration 143, Batch: 26, Loss: 0.052311018109321594\n",
      "Iteration 143, Batch: 27, Loss: 0.03237833455204964\n",
      "Iteration 143, Batch: 28, Loss: 0.05484835430979729\n",
      "Iteration 143, Batch: 29, Loss: 0.05888749286532402\n",
      "Iteration 143, Batch: 30, Loss: 0.04929184168577194\n",
      "Iteration 143, Batch: 31, Loss: 0.05438283830881119\n",
      "Iteration 143, Batch: 32, Loss: 0.04541982710361481\n",
      "Iteration 143, Batch: 33, Loss: 0.05870160833001137\n",
      "Iteration 143, Batch: 34, Loss: 0.05279191955924034\n",
      "Iteration 143, Batch: 35, Loss: 0.05018675699830055\n",
      "Iteration 143, Batch: 36, Loss: 0.07459770888090134\n",
      "Iteration 143, Batch: 37, Loss: 0.0796106830239296\n",
      "Iteration 143, Batch: 38, Loss: 0.04117273539304733\n",
      "Iteration 143, Batch: 39, Loss: 0.05068228021264076\n",
      "Iteration 143, Batch: 40, Loss: 0.051949623972177505\n",
      "Iteration 143, Batch: 41, Loss: 0.03476914018392563\n",
      "Iteration 143, Batch: 42, Loss: 0.06431355327367783\n",
      "Iteration 143, Batch: 43, Loss: 0.0456908643245697\n",
      "Iteration 143, Batch: 44, Loss: 0.04364272952079773\n",
      "Iteration 143, Batch: 45, Loss: 0.050268638879060745\n",
      "Iteration 143, Batch: 46, Loss: 0.05243004858493805\n",
      "Iteration 143, Batch: 47, Loss: 0.050052981823682785\n",
      "Iteration 143, Batch: 48, Loss: 0.06287318468093872\n",
      "Iteration 143, Batch: 49, Loss: 0.03934561088681221\n",
      "Iteration 144, Batch: 0, Loss: 0.05304576829075813\n",
      "Iteration 144, Batch: 1, Loss: 0.05388611555099487\n",
      "Iteration 144, Batch: 2, Loss: 0.0674559473991394\n",
      "Iteration 144, Batch: 3, Loss: 0.027242906391620636\n",
      "Iteration 144, Batch: 4, Loss: 0.05876109004020691\n",
      "Iteration 144, Batch: 5, Loss: 0.05088949576020241\n",
      "Iteration 144, Batch: 6, Loss: 0.07314877212047577\n",
      "Iteration 144, Batch: 7, Loss: 0.07317697256803513\n",
      "Iteration 144, Batch: 8, Loss: 0.06485388427972794\n",
      "Iteration 144, Batch: 9, Loss: 0.06831514835357666\n",
      "Iteration 144, Batch: 10, Loss: 0.050313811749219894\n",
      "Iteration 144, Batch: 11, Loss: 0.03259981423616409\n",
      "Iteration 144, Batch: 12, Loss: 0.03811077028512955\n",
      "Iteration 144, Batch: 13, Loss: 0.04943721741437912\n",
      "Iteration 144, Batch: 14, Loss: 0.06105991080403328\n",
      "Iteration 144, Batch: 15, Loss: 0.047661423683166504\n",
      "Iteration 144, Batch: 16, Loss: 0.06280545145273209\n",
      "Iteration 144, Batch: 17, Loss: 0.059859149158000946\n",
      "Iteration 144, Batch: 18, Loss: 0.08301858603954315\n",
      "Iteration 144, Batch: 19, Loss: 0.03729221597313881\n",
      "Iteration 144, Batch: 20, Loss: 0.04596371576189995\n",
      "Iteration 144, Batch: 21, Loss: 0.06476320326328278\n",
      "Iteration 144, Batch: 22, Loss: 0.04322213679552078\n",
      "Iteration 144, Batch: 23, Loss: 0.0631178468465805\n",
      "Iteration 144, Batch: 24, Loss: 0.05415615066885948\n",
      "Iteration 144, Batch: 25, Loss: 0.03202108293771744\n",
      "Iteration 144, Batch: 26, Loss: 0.055539730936288834\n",
      "Iteration 144, Batch: 27, Loss: 0.05716337636113167\n",
      "Iteration 144, Batch: 28, Loss: 0.05020090565085411\n",
      "Iteration 144, Batch: 29, Loss: 0.08965614438056946\n",
      "Iteration 144, Batch: 30, Loss: 0.05489368364214897\n",
      "Iteration 144, Batch: 31, Loss: 0.0412806011736393\n",
      "Iteration 144, Batch: 32, Loss: 0.03870806470513344\n",
      "Iteration 144, Batch: 33, Loss: 0.052137307822704315\n",
      "Iteration 144, Batch: 34, Loss: 0.05909605696797371\n",
      "Iteration 144, Batch: 35, Loss: 0.0730997696518898\n",
      "Iteration 144, Batch: 36, Loss: 0.05978884920477867\n",
      "Iteration 144, Batch: 37, Loss: 0.0613032691180706\n",
      "Iteration 144, Batch: 38, Loss: 0.07181094586849213\n",
      "Iteration 144, Batch: 39, Loss: 0.04646794870495796\n",
      "Iteration 144, Batch: 40, Loss: 0.045741137117147446\n",
      "Iteration 144, Batch: 41, Loss: 0.04260046035051346\n",
      "Iteration 144, Batch: 42, Loss: 0.037382304668426514\n",
      "Iteration 144, Batch: 43, Loss: 0.05863630026578903\n",
      "Iteration 144, Batch: 44, Loss: 0.055144332349300385\n",
      "Iteration 144, Batch: 45, Loss: 0.0602840781211853\n",
      "Iteration 144, Batch: 46, Loss: 0.0566541962325573\n",
      "Iteration 144, Batch: 47, Loss: 0.06625345349311829\n",
      "Iteration 144, Batch: 48, Loss: 0.0576649084687233\n",
      "Iteration 144, Batch: 49, Loss: 0.06487270444631577\n",
      "Iteration 145, Batch: 0, Loss: 0.058690957725048065\n",
      "Iteration 145, Batch: 1, Loss: 0.04448530077934265\n",
      "Iteration 145, Batch: 2, Loss: 0.06502752751111984\n",
      "Iteration 145, Batch: 3, Loss: 0.06477505713701248\n",
      "Iteration 145, Batch: 4, Loss: 0.08029045164585114\n",
      "Iteration 145, Batch: 5, Loss: 0.048936743289232254\n",
      "Iteration 145, Batch: 6, Loss: 0.05382828414440155\n",
      "Iteration 145, Batch: 7, Loss: 0.06287831813097\n",
      "Iteration 145, Batch: 8, Loss: 0.0708959698677063\n",
      "Iteration 145, Batch: 9, Loss: 0.06280503422021866\n",
      "Iteration 145, Batch: 10, Loss: 0.09227073192596436\n",
      "Iteration 145, Batch: 11, Loss: 0.04744695872068405\n",
      "Iteration 145, Batch: 12, Loss: 0.061764683574438095\n",
      "Iteration 145, Batch: 13, Loss: 0.06723218411207199\n",
      "Iteration 145, Batch: 14, Loss: 0.061473242938518524\n",
      "Iteration 145, Batch: 15, Loss: 0.06741742789745331\n",
      "Iteration 145, Batch: 16, Loss: 0.05632459372282028\n",
      "Iteration 145, Batch: 17, Loss: 0.04552268981933594\n",
      "Iteration 145, Batch: 18, Loss: 0.04813966155052185\n",
      "Iteration 145, Batch: 19, Loss: 0.06736783683300018\n",
      "Iteration 145, Batch: 20, Loss: 0.04303546994924545\n",
      "Iteration 145, Batch: 21, Loss: 0.06931376457214355\n",
      "Iteration 145, Batch: 22, Loss: 0.08087091892957687\n",
      "Iteration 145, Batch: 23, Loss: 0.021470511332154274\n",
      "Iteration 145, Batch: 24, Loss: 0.028571294620633125\n",
      "Iteration 145, Batch: 25, Loss: 0.06903190910816193\n",
      "Iteration 145, Batch: 26, Loss: 0.04919232428073883\n",
      "Iteration 145, Batch: 27, Loss: 0.06489776074886322\n",
      "Iteration 145, Batch: 28, Loss: 0.06901977211236954\n",
      "Iteration 145, Batch: 29, Loss: 0.042102329432964325\n",
      "Iteration 145, Batch: 30, Loss: 0.032624151557683945\n",
      "Iteration 145, Batch: 31, Loss: 0.04567401856184006\n",
      "Iteration 145, Batch: 32, Loss: 0.018714120611548424\n",
      "Iteration 145, Batch: 33, Loss: 0.048610296100378036\n",
      "Iteration 145, Batch: 34, Loss: 0.06815600395202637\n",
      "Iteration 145, Batch: 35, Loss: 0.07138658314943314\n",
      "Iteration 145, Batch: 36, Loss: 0.0732562318444252\n",
      "Iteration 145, Batch: 37, Loss: 0.04230048134922981\n",
      "Iteration 145, Batch: 38, Loss: 0.034084610641002655\n",
      "Iteration 145, Batch: 39, Loss: 0.06128926947712898\n",
      "Iteration 145, Batch: 40, Loss: 0.06321033090353012\n",
      "Iteration 145, Batch: 41, Loss: 0.05524267256259918\n",
      "Iteration 145, Batch: 42, Loss: 0.06786997616291046\n",
      "Iteration 145, Batch: 43, Loss: 0.048855822533369064\n",
      "Iteration 145, Batch: 44, Loss: 0.04552629217505455\n",
      "Iteration 145, Batch: 45, Loss: 0.060605280101299286\n",
      "Iteration 145, Batch: 46, Loss: 0.07185974717140198\n",
      "Iteration 145, Batch: 47, Loss: 0.03509998321533203\n",
      "Iteration 145, Batch: 48, Loss: 0.05783649906516075\n",
      "Iteration 145, Batch: 49, Loss: 0.05849804729223251\n",
      "Iteration 146, Batch: 0, Loss: 0.03272654116153717\n",
      "Iteration 146, Batch: 1, Loss: 0.044552046805620193\n",
      "Iteration 146, Batch: 2, Loss: 0.07043543457984924\n",
      "Iteration 146, Batch: 3, Loss: 0.04996378347277641\n",
      "Iteration 146, Batch: 4, Loss: 0.05098506808280945\n",
      "Iteration 146, Batch: 5, Loss: 0.06808590143918991\n",
      "Iteration 146, Batch: 6, Loss: 0.06096123531460762\n",
      "Iteration 146, Batch: 7, Loss: 0.04562637209892273\n",
      "Iteration 146, Batch: 8, Loss: 0.07160906493663788\n",
      "Iteration 146, Batch: 9, Loss: 0.07755504548549652\n",
      "Iteration 146, Batch: 10, Loss: 0.07874380797147751\n",
      "Iteration 146, Batch: 11, Loss: 0.09062753617763519\n",
      "Iteration 146, Batch: 12, Loss: 0.05329502373933792\n",
      "Iteration 146, Batch: 13, Loss: 0.06390993297100067\n",
      "Iteration 146, Batch: 14, Loss: 0.065960593521595\n",
      "Iteration 146, Batch: 15, Loss: 0.060784537345170975\n",
      "Iteration 146, Batch: 16, Loss: 0.039102982729673386\n",
      "Iteration 146, Batch: 17, Loss: 0.04777834936976433\n",
      "Iteration 146, Batch: 18, Loss: 0.07691830396652222\n",
      "Iteration 146, Batch: 19, Loss: 0.05780492722988129\n",
      "Iteration 146, Batch: 20, Loss: 0.05872220918536186\n",
      "Iteration 146, Batch: 21, Loss: 0.059022556990385056\n",
      "Iteration 146, Batch: 22, Loss: 0.061932943761348724\n",
      "Iteration 146, Batch: 23, Loss: 0.08667097985744476\n",
      "Iteration 146, Batch: 24, Loss: 0.0630945935845375\n",
      "Iteration 146, Batch: 25, Loss: 0.04874175786972046\n",
      "Iteration 146, Batch: 26, Loss: 0.06886943429708481\n",
      "Iteration 146, Batch: 27, Loss: 0.057114582508802414\n",
      "Iteration 146, Batch: 28, Loss: 0.048960380256175995\n",
      "Iteration 146, Batch: 29, Loss: 0.04887465015053749\n",
      "Iteration 146, Batch: 30, Loss: 0.06329910457134247\n",
      "Iteration 146, Batch: 31, Loss: 0.043350715190172195\n",
      "Iteration 146, Batch: 32, Loss: 0.06166943535208702\n",
      "Iteration 146, Batch: 33, Loss: 0.04315061867237091\n",
      "Iteration 146, Batch: 34, Loss: 0.053870391100645065\n",
      "Iteration 146, Batch: 35, Loss: 0.07709065079689026\n",
      "Iteration 146, Batch: 36, Loss: 0.04621642455458641\n",
      "Iteration 146, Batch: 37, Loss: 0.04660974070429802\n",
      "Iteration 146, Batch: 38, Loss: 0.05936586111783981\n",
      "Iteration 146, Batch: 39, Loss: 0.04736640304327011\n",
      "Iteration 146, Batch: 40, Loss: 0.048609960824251175\n",
      "Iteration 146, Batch: 41, Loss: 0.06401818245649338\n",
      "Iteration 146, Batch: 42, Loss: 0.04131108149886131\n",
      "Iteration 146, Batch: 43, Loss: 0.07082909345626831\n",
      "Iteration 146, Batch: 44, Loss: 0.06339132785797119\n",
      "Iteration 146, Batch: 45, Loss: 0.06761214137077332\n",
      "Iteration 146, Batch: 46, Loss: 0.06306831538677216\n",
      "Iteration 146, Batch: 47, Loss: 0.06369482725858688\n",
      "Iteration 146, Batch: 48, Loss: 0.08262775093317032\n",
      "Iteration 146, Batch: 49, Loss: 0.049862828105688095\n",
      "Iteration 147, Batch: 0, Loss: 0.05018732324242592\n",
      "Iteration 147, Batch: 1, Loss: 0.05641157925128937\n",
      "Iteration 147, Batch: 2, Loss: 0.060895517468452454\n",
      "Iteration 147, Batch: 3, Loss: 0.046481966972351074\n",
      "Iteration 147, Batch: 4, Loss: 0.08782035857439041\n",
      "Iteration 147, Batch: 5, Loss: 0.057593345642089844\n",
      "Iteration 147, Batch: 6, Loss: 0.06777919828891754\n",
      "Iteration 147, Batch: 7, Loss: 0.04606049507856369\n",
      "Iteration 147, Batch: 8, Loss: 0.07598000019788742\n",
      "Iteration 147, Batch: 9, Loss: 0.056718096137046814\n",
      "Iteration 147, Batch: 10, Loss: 0.02887166477739811\n",
      "Iteration 147, Batch: 11, Loss: 0.06529734283685684\n",
      "Iteration 147, Batch: 12, Loss: 0.07997865229845047\n",
      "Iteration 147, Batch: 13, Loss: 0.060799550265073776\n",
      "Iteration 147, Batch: 14, Loss: 0.08040045201778412\n",
      "Iteration 147, Batch: 15, Loss: 0.05170866847038269\n",
      "Iteration 147, Batch: 16, Loss: 0.08657421916723251\n",
      "Iteration 147, Batch: 17, Loss: 0.06725288182497025\n",
      "Iteration 147, Batch: 18, Loss: 0.04400600492954254\n",
      "Iteration 147, Batch: 19, Loss: 0.08266107738018036\n",
      "Iteration 147, Batch: 20, Loss: 0.08556579053401947\n",
      "Iteration 147, Batch: 21, Loss: 0.09203551709651947\n",
      "Iteration 147, Batch: 22, Loss: 0.07732730358839035\n",
      "Iteration 147, Batch: 23, Loss: 0.05811595916748047\n",
      "Iteration 147, Batch: 24, Loss: 0.06574254482984543\n",
      "Iteration 147, Batch: 25, Loss: 0.055824164301157\n",
      "Iteration 147, Batch: 26, Loss: 0.07365524023771286\n",
      "Iteration 147, Batch: 27, Loss: 0.07072872668504715\n",
      "Iteration 147, Batch: 28, Loss: 0.0683872178196907\n",
      "Iteration 147, Batch: 29, Loss: 0.0905124843120575\n",
      "Iteration 147, Batch: 30, Loss: 0.05358598753809929\n",
      "Iteration 147, Batch: 31, Loss: 0.053685665130615234\n",
      "Iteration 147, Batch: 32, Loss: 0.06732738018035889\n",
      "Iteration 147, Batch: 33, Loss: 0.07305020093917847\n",
      "Iteration 147, Batch: 34, Loss: 0.057083096355199814\n",
      "Iteration 147, Batch: 35, Loss: 0.057748354971408844\n",
      "Iteration 147, Batch: 36, Loss: 0.04626405984163284\n",
      "Iteration 147, Batch: 37, Loss: 0.07428421825170517\n",
      "Iteration 147, Batch: 38, Loss: 0.06662678718566895\n",
      "Iteration 147, Batch: 39, Loss: 0.07576717436313629\n",
      "Iteration 147, Batch: 40, Loss: 0.05639524385333061\n",
      "Iteration 147, Batch: 41, Loss: 0.059083860367536545\n",
      "Iteration 147, Batch: 42, Loss: 0.05354120209813118\n",
      "Iteration 147, Batch: 43, Loss: 0.03637472167611122\n",
      "Iteration 147, Batch: 44, Loss: 0.05989718437194824\n",
      "Iteration 147, Batch: 45, Loss: 0.04535587131977081\n",
      "Iteration 147, Batch: 46, Loss: 0.05199809744954109\n",
      "Iteration 147, Batch: 47, Loss: 0.04905872419476509\n",
      "Iteration 147, Batch: 48, Loss: 0.04256436228752136\n",
      "Iteration 147, Batch: 49, Loss: 0.0733715295791626\n",
      "Iteration 148, Batch: 0, Loss: 0.06440233439207077\n",
      "Iteration 148, Batch: 1, Loss: 0.0667324885725975\n",
      "Iteration 148, Batch: 2, Loss: 0.07285148650407791\n",
      "Iteration 148, Batch: 3, Loss: 0.054563894867897034\n",
      "Iteration 148, Batch: 4, Loss: 0.07839777320623398\n",
      "Iteration 148, Batch: 5, Loss: 0.04938318952918053\n",
      "Iteration 148, Batch: 6, Loss: 0.04135219380259514\n",
      "Iteration 148, Batch: 7, Loss: 0.09438386559486389\n",
      "Iteration 148, Batch: 8, Loss: 0.03369897976517677\n",
      "Iteration 148, Batch: 9, Loss: 0.08699686080217361\n",
      "Iteration 148, Batch: 10, Loss: 0.06392243504524231\n",
      "Iteration 148, Batch: 11, Loss: 0.07680949568748474\n",
      "Iteration 148, Batch: 12, Loss: 0.06362780928611755\n",
      "Iteration 148, Batch: 13, Loss: 0.06943761557340622\n",
      "Iteration 148, Batch: 14, Loss: 0.09308244287967682\n",
      "Iteration 148, Batch: 15, Loss: 0.06454487890005112\n",
      "Iteration 148, Batch: 16, Loss: 0.08607176691293716\n",
      "Iteration 148, Batch: 17, Loss: 0.08518164604902267\n",
      "Iteration 148, Batch: 18, Loss: 0.06617080420255661\n",
      "Iteration 148, Batch: 19, Loss: 0.06381361186504364\n",
      "Iteration 148, Batch: 20, Loss: 0.06293108314275742\n",
      "Iteration 148, Batch: 21, Loss: 0.07459138333797455\n",
      "Iteration 148, Batch: 22, Loss: 0.057392675429582596\n",
      "Iteration 148, Batch: 23, Loss: 0.061330780386924744\n",
      "Iteration 148, Batch: 24, Loss: 0.1066899374127388\n",
      "Iteration 148, Batch: 25, Loss: 0.07873344421386719\n",
      "Iteration 148, Batch: 26, Loss: 0.0781574696302414\n",
      "Iteration 148, Batch: 27, Loss: 0.07842430472373962\n",
      "Iteration 148, Batch: 28, Loss: 0.04808959737420082\n",
      "Iteration 148, Batch: 29, Loss: 0.07235870510339737\n",
      "Iteration 148, Batch: 30, Loss: 0.06806707382202148\n",
      "Iteration 148, Batch: 31, Loss: 0.07283943146467209\n",
      "Iteration 148, Batch: 32, Loss: 0.03779445216059685\n",
      "Iteration 148, Batch: 33, Loss: 0.06508388370275497\n",
      "Iteration 148, Batch: 34, Loss: 0.06339820474386215\n",
      "Iteration 148, Batch: 35, Loss: 0.07430499792098999\n",
      "Iteration 148, Batch: 36, Loss: 0.08251497894525528\n",
      "Iteration 148, Batch: 37, Loss: 0.06493578106164932\n",
      "Iteration 148, Batch: 38, Loss: 0.08026504516601562\n",
      "Iteration 148, Batch: 39, Loss: 0.09688790142536163\n",
      "Iteration 148, Batch: 40, Loss: 0.08991813659667969\n",
      "Iteration 148, Batch: 41, Loss: 0.07776962220668793\n",
      "Iteration 148, Batch: 42, Loss: 0.09112565964460373\n",
      "Iteration 148, Batch: 43, Loss: 0.06513142585754395\n",
      "Iteration 148, Batch: 44, Loss: 0.08090415596961975\n",
      "Iteration 148, Batch: 45, Loss: 0.059680309146642685\n",
      "Iteration 148, Batch: 46, Loss: 0.07670063525438309\n",
      "Iteration 148, Batch: 47, Loss: 0.08074712753295898\n",
      "Iteration 148, Batch: 48, Loss: 0.06811611354351044\n",
      "Iteration 148, Batch: 49, Loss: 0.07154018431901932\n",
      "Iteration 149, Batch: 0, Loss: 0.07094459980726242\n",
      "Iteration 149, Batch: 1, Loss: 0.058074310421943665\n",
      "Iteration 149, Batch: 2, Loss: 0.08477877080440521\n",
      "Iteration 149, Batch: 3, Loss: 0.08102825284004211\n",
      "Iteration 149, Batch: 4, Loss: 0.08662107586860657\n",
      "Iteration 149, Batch: 5, Loss: 0.08067069202661514\n",
      "Iteration 149, Batch: 6, Loss: 0.06921844184398651\n",
      "Iteration 149, Batch: 7, Loss: 0.03721198812127113\n",
      "Iteration 149, Batch: 8, Loss: 0.0703025683760643\n",
      "Iteration 149, Batch: 9, Loss: 0.06249993294477463\n",
      "Iteration 149, Batch: 10, Loss: 0.05763085186481476\n",
      "Iteration 149, Batch: 11, Loss: 0.03595703840255737\n",
      "Iteration 149, Batch: 12, Loss: 0.07827234268188477\n",
      "Iteration 149, Batch: 13, Loss: 0.07844848930835724\n",
      "Iteration 149, Batch: 14, Loss: 0.05202183499932289\n",
      "Iteration 149, Batch: 15, Loss: 0.06860944628715515\n",
      "Iteration 149, Batch: 16, Loss: 0.03846300393342972\n",
      "Iteration 149, Batch: 17, Loss: 0.07640574872493744\n",
      "Iteration 149, Batch: 18, Loss: 0.07464832812547684\n",
      "Iteration 149, Batch: 19, Loss: 0.05716397985816002\n",
      "Iteration 149, Batch: 20, Loss: 0.0771329253911972\n",
      "Iteration 149, Batch: 21, Loss: 0.061403874307870865\n",
      "Iteration 149, Batch: 22, Loss: 0.038784805685281754\n",
      "Iteration 149, Batch: 23, Loss: 0.05840379744768143\n",
      "Iteration 149, Batch: 24, Loss: 0.07204045355319977\n",
      "Iteration 149, Batch: 25, Loss: 0.033421535044908524\n",
      "Iteration 149, Batch: 26, Loss: 0.044600553810596466\n",
      "Iteration 149, Batch: 27, Loss: 0.047799814492464066\n",
      "Iteration 149, Batch: 28, Loss: 0.07431472837924957\n",
      "Iteration 149, Batch: 29, Loss: 0.06815383583307266\n",
      "Iteration 149, Batch: 30, Loss: 0.05997607111930847\n",
      "Iteration 149, Batch: 31, Loss: 0.0676771029829979\n",
      "Iteration 149, Batch: 32, Loss: 0.09405501186847687\n",
      "Iteration 149, Batch: 33, Loss: 0.09009114652872086\n",
      "Iteration 149, Batch: 34, Loss: 0.05434829741716385\n",
      "Iteration 149, Batch: 35, Loss: 0.05351150408387184\n",
      "Iteration 149, Batch: 36, Loss: 0.07700272649526596\n",
      "Iteration 149, Batch: 37, Loss: 0.08387637883424759\n",
      "Iteration 149, Batch: 38, Loss: 0.06357607990503311\n",
      "Iteration 149, Batch: 39, Loss: 0.05789163336157799\n",
      "Iteration 149, Batch: 40, Loss: 0.07022330909967422\n",
      "Iteration 149, Batch: 41, Loss: 0.0667702704668045\n",
      "Iteration 149, Batch: 42, Loss: 0.06945348531007767\n",
      "Iteration 149, Batch: 43, Loss: 0.04841618612408638\n",
      "Iteration 149, Batch: 44, Loss: 0.07575815171003342\n",
      "Iteration 149, Batch: 45, Loss: 0.08534324169158936\n",
      "Iteration 149, Batch: 46, Loss: 0.04871838539838791\n",
      "Iteration 149, Batch: 47, Loss: 0.07068295031785965\n",
      "Iteration 149, Batch: 48, Loss: 0.059317197650671005\n",
      "Iteration 149, Batch: 49, Loss: 0.04680899530649185\n",
      "Iteration 150, Batch: 0, Loss: 0.07056935131549835\n",
      "Iteration 150, Batch: 1, Loss: 0.04949604347348213\n",
      "Iteration 150, Batch: 2, Loss: 0.04950699582695961\n",
      "Iteration 150, Batch: 3, Loss: 0.06552953273057938\n",
      "Iteration 150, Batch: 4, Loss: 0.06596825271844864\n",
      "Iteration 150, Batch: 5, Loss: 0.08480192720890045\n",
      "Iteration 150, Batch: 6, Loss: 0.08190488815307617\n",
      "Iteration 150, Batch: 7, Loss: 0.05925926938652992\n",
      "Iteration 150, Batch: 8, Loss: 0.0712745189666748\n",
      "Iteration 150, Batch: 9, Loss: 0.08229198306798935\n",
      "Iteration 150, Batch: 10, Loss: 0.040182288736104965\n",
      "Iteration 150, Batch: 11, Loss: 0.04084663838148117\n",
      "Iteration 150, Batch: 12, Loss: 0.08307304233312607\n",
      "Iteration 150, Batch: 13, Loss: 0.09050100296735764\n",
      "Iteration 150, Batch: 14, Loss: 0.06080241873860359\n",
      "Iteration 150, Batch: 15, Loss: 0.04512462392449379\n",
      "Iteration 150, Batch: 16, Loss: 0.06632092595100403\n",
      "Iteration 150, Batch: 17, Loss: 0.0661371573805809\n",
      "Iteration 150, Batch: 18, Loss: 0.08479044586420059\n",
      "Iteration 150, Batch: 19, Loss: 0.09286666661500931\n",
      "Iteration 150, Batch: 20, Loss: 0.0670279860496521\n",
      "Iteration 150, Batch: 21, Loss: 0.04636668041348457\n",
      "Iteration 150, Batch: 22, Loss: 0.05596108362078667\n",
      "Iteration 150, Batch: 23, Loss: 0.08898498117923737\n",
      "Iteration 150, Batch: 24, Loss: 0.049914922565221786\n",
      "Iteration 150, Batch: 25, Loss: 0.04685814678668976\n",
      "Iteration 150, Batch: 26, Loss: 0.04638823866844177\n",
      "Iteration 150, Batch: 27, Loss: 0.0602332279086113\n",
      "Iteration 150, Batch: 28, Loss: 0.04208090528845787\n",
      "Iteration 150, Batch: 29, Loss: 0.05331007018685341\n",
      "Iteration 150, Batch: 30, Loss: 0.07020562887191772\n",
      "Iteration 150, Batch: 31, Loss: 0.060230765491724014\n",
      "Iteration 150, Batch: 32, Loss: 0.07415186613798141\n",
      "Iteration 150, Batch: 33, Loss: 0.06481236964464188\n",
      "Iteration 150, Batch: 34, Loss: 0.08008665591478348\n",
      "Iteration 150, Batch: 35, Loss: 0.06252919882535934\n",
      "Iteration 150, Batch: 36, Loss: 0.0843559056520462\n",
      "Iteration 150, Batch: 37, Loss: 0.05649957433342934\n",
      "Iteration 150, Batch: 38, Loss: 0.04630180820822716\n",
      "Iteration 150, Batch: 39, Loss: 0.04977691173553467\n",
      "Iteration 150, Batch: 40, Loss: 0.04709528759121895\n",
      "Iteration 150, Batch: 41, Loss: 0.05965633690357208\n",
      "Iteration 150, Batch: 42, Loss: 0.04735659062862396\n",
      "Iteration 150, Batch: 43, Loss: 0.06291001290082932\n",
      "Iteration 150, Batch: 44, Loss: 0.07623158395290375\n",
      "Iteration 150, Batch: 45, Loss: 0.0865318775177002\n",
      "Iteration 150, Batch: 46, Loss: 0.04652300849556923\n",
      "Iteration 150, Batch: 47, Loss: 0.06783459335565567\n",
      "Iteration 150, Batch: 48, Loss: 0.09275873005390167\n",
      "Iteration 150, Batch: 49, Loss: 0.0672125518321991\n",
      "Iteration 151, Batch: 0, Loss: 0.08116358518600464\n",
      "Iteration 151, Batch: 1, Loss: 0.043893907219171524\n",
      "Iteration 151, Batch: 2, Loss: 0.056680094450712204\n",
      "Iteration 151, Batch: 3, Loss: 0.06500737369060516\n",
      "Iteration 151, Batch: 4, Loss: 0.07915851473808289\n",
      "Iteration 151, Batch: 5, Loss: 0.07283736765384674\n",
      "Iteration 151, Batch: 6, Loss: 0.05925978347659111\n",
      "Iteration 151, Batch: 7, Loss: 0.09977858513593674\n",
      "Iteration 151, Batch: 8, Loss: 0.06720224022865295\n",
      "Iteration 151, Batch: 9, Loss: 0.07419507950544357\n",
      "Iteration 151, Batch: 10, Loss: 0.09619638323783875\n",
      "Iteration 151, Batch: 11, Loss: 0.0535486675798893\n",
      "Iteration 151, Batch: 12, Loss: 0.04909395053982735\n",
      "Iteration 151, Batch: 13, Loss: 0.0732002779841423\n",
      "Iteration 151, Batch: 14, Loss: 0.07390771806240082\n",
      "Iteration 151, Batch: 15, Loss: 0.043174780905246735\n",
      "Iteration 151, Batch: 16, Loss: 0.06074832007288933\n",
      "Iteration 151, Batch: 17, Loss: 0.07378554344177246\n",
      "Iteration 151, Batch: 18, Loss: 0.05332787334918976\n",
      "Iteration 151, Batch: 19, Loss: 0.08036088943481445\n",
      "Iteration 151, Batch: 20, Loss: 0.06010061129927635\n",
      "Iteration 151, Batch: 21, Loss: 0.08991483598947525\n",
      "Iteration 151, Batch: 22, Loss: 0.06814718246459961\n",
      "Iteration 151, Batch: 23, Loss: 0.07718470692634583\n",
      "Iteration 151, Batch: 24, Loss: 0.09722345322370529\n",
      "Iteration 151, Batch: 25, Loss: 0.05269499123096466\n",
      "Iteration 151, Batch: 26, Loss: 0.06233115866780281\n",
      "Iteration 151, Batch: 27, Loss: 0.04750155285000801\n",
      "Iteration 151, Batch: 28, Loss: 0.09884124249219894\n",
      "Iteration 151, Batch: 29, Loss: 0.08022390305995941\n",
      "Iteration 151, Batch: 30, Loss: 0.09143344312906265\n",
      "Iteration 151, Batch: 31, Loss: 0.09060561656951904\n",
      "Iteration 151, Batch: 32, Loss: 0.09836133569478989\n",
      "Iteration 151, Batch: 33, Loss: 0.08221503347158432\n",
      "Iteration 151, Batch: 34, Loss: 0.06925434619188309\n",
      "Iteration 151, Batch: 35, Loss: 0.07877884805202484\n",
      "Iteration 151, Batch: 36, Loss: 0.0892980769276619\n",
      "Iteration 151, Batch: 37, Loss: 0.05709759518504143\n",
      "Iteration 151, Batch: 38, Loss: 0.07574654370546341\n",
      "Iteration 151, Batch: 39, Loss: 0.08427057415246964\n",
      "Iteration 151, Batch: 40, Loss: 0.034769318997859955\n",
      "Iteration 151, Batch: 41, Loss: 0.08705351501703262\n",
      "Iteration 151, Batch: 42, Loss: 0.05396125838160515\n",
      "Iteration 151, Batch: 43, Loss: 0.06201155111193657\n",
      "Iteration 151, Batch: 44, Loss: 0.08413205295801163\n",
      "Iteration 151, Batch: 45, Loss: 0.08790870755910873\n",
      "Iteration 151, Batch: 46, Loss: 0.06999345868825912\n",
      "Iteration 151, Batch: 47, Loss: 0.061444949358701706\n",
      "Iteration 151, Batch: 48, Loss: 0.07991272956132889\n",
      "Iteration 151, Batch: 49, Loss: 0.09067779034376144\n",
      "Iteration 152, Batch: 0, Loss: 0.0780012235045433\n",
      "Iteration 152, Batch: 1, Loss: 0.06765377521514893\n",
      "Iteration 152, Batch: 2, Loss: 0.0916471853852272\n",
      "Iteration 152, Batch: 3, Loss: 0.07817892730236053\n",
      "Iteration 152, Batch: 4, Loss: 0.0727902203798294\n",
      "Iteration 152, Batch: 5, Loss: 0.0608762726187706\n",
      "Iteration 152, Batch: 6, Loss: 0.08427181094884872\n",
      "Iteration 152, Batch: 7, Loss: 0.05596219003200531\n",
      "Iteration 152, Batch: 8, Loss: 0.06508731096982956\n",
      "Iteration 152, Batch: 9, Loss: 0.12415744364261627\n",
      "Iteration 152, Batch: 10, Loss: 0.12257124483585358\n",
      "Iteration 152, Batch: 11, Loss: 0.09321755915880203\n",
      "Iteration 152, Batch: 12, Loss: 0.06844368577003479\n",
      "Iteration 152, Batch: 13, Loss: 0.07691822946071625\n",
      "Iteration 152, Batch: 14, Loss: 0.07727549225091934\n",
      "Iteration 152, Batch: 15, Loss: 0.09250875562429428\n",
      "Iteration 152, Batch: 16, Loss: 0.10309669375419617\n",
      "Iteration 152, Batch: 17, Loss: 0.04942762106657028\n",
      "Iteration 152, Batch: 18, Loss: 0.06224113702774048\n",
      "Iteration 152, Batch: 19, Loss: 0.08954383432865143\n",
      "Iteration 152, Batch: 20, Loss: 0.05687814578413963\n",
      "Iteration 152, Batch: 21, Loss: 0.0914633572101593\n",
      "Iteration 152, Batch: 22, Loss: 0.0716947615146637\n",
      "Iteration 152, Batch: 23, Loss: 0.05513901263475418\n",
      "Iteration 152, Batch: 24, Loss: 0.0646200031042099\n",
      "Iteration 152, Batch: 25, Loss: 0.058413874357938766\n",
      "Iteration 152, Batch: 26, Loss: 0.06395920366048813\n",
      "Iteration 152, Batch: 27, Loss: 0.06444934010505676\n",
      "Iteration 152, Batch: 28, Loss: 0.05970299243927002\n",
      "Iteration 152, Batch: 29, Loss: 0.0848621129989624\n",
      "Iteration 152, Batch: 30, Loss: 0.07089071720838547\n",
      "Iteration 152, Batch: 31, Loss: 0.06956855952739716\n",
      "Iteration 152, Batch: 32, Loss: 0.08569833636283875\n",
      "Iteration 152, Batch: 33, Loss: 0.10379139333963394\n",
      "Iteration 152, Batch: 34, Loss: 0.07004749774932861\n",
      "Iteration 152, Batch: 35, Loss: 0.09524469077587128\n",
      "Iteration 152, Batch: 36, Loss: 0.08852775394916534\n",
      "Iteration 152, Batch: 37, Loss: 0.06365934014320374\n",
      "Iteration 152, Batch: 38, Loss: 0.11196277290582657\n",
      "Iteration 152, Batch: 39, Loss: 0.11642933636903763\n",
      "Iteration 152, Batch: 40, Loss: 0.07626280188560486\n",
      "Iteration 152, Batch: 41, Loss: 0.06110704317688942\n",
      "Iteration 152, Batch: 42, Loss: 0.04518037289381027\n",
      "Iteration 152, Batch: 43, Loss: 0.06552564352750778\n",
      "Iteration 152, Batch: 44, Loss: 0.08136990666389465\n",
      "Iteration 152, Batch: 45, Loss: 0.09135850518941879\n",
      "Iteration 152, Batch: 46, Loss: 0.09447858482599258\n",
      "Iteration 152, Batch: 47, Loss: 0.05939977988600731\n",
      "Iteration 152, Batch: 48, Loss: 0.05148199945688248\n",
      "Iteration 152, Batch: 49, Loss: 0.08401387929916382\n",
      "Iteration 153, Batch: 0, Loss: 0.054355502128601074\n",
      "Iteration 153, Batch: 1, Loss: 0.06198027357459068\n",
      "Iteration 153, Batch: 2, Loss: 0.09015628695487976\n",
      "Iteration 153, Batch: 3, Loss: 0.09699761867523193\n",
      "Iteration 153, Batch: 4, Loss: 0.05530642345547676\n",
      "Iteration 153, Batch: 5, Loss: 0.05989876762032509\n",
      "Iteration 153, Batch: 6, Loss: 0.0708230659365654\n",
      "Iteration 153, Batch: 7, Loss: 0.04739933833479881\n",
      "Iteration 153, Batch: 8, Loss: 0.06704450398683548\n",
      "Iteration 153, Batch: 9, Loss: 0.11290400475263596\n",
      "Iteration 153, Batch: 10, Loss: 0.04504748806357384\n",
      "Iteration 153, Batch: 11, Loss: 0.05124537646770477\n",
      "Iteration 153, Batch: 12, Loss: 0.08440025895833969\n",
      "Iteration 153, Batch: 13, Loss: 0.1086495965719223\n",
      "Iteration 153, Batch: 14, Loss: 0.0633649080991745\n",
      "Iteration 153, Batch: 15, Loss: 0.06968513131141663\n",
      "Iteration 153, Batch: 16, Loss: 0.04849141836166382\n",
      "Iteration 153, Batch: 17, Loss: 0.07685574144124985\n",
      "Iteration 153, Batch: 18, Loss: 0.10595280677080154\n",
      "Iteration 153, Batch: 19, Loss: 0.06786736100912094\n",
      "Iteration 153, Batch: 20, Loss: 0.08023638278245926\n",
      "Iteration 153, Batch: 21, Loss: 0.0766209289431572\n",
      "Iteration 153, Batch: 22, Loss: 0.06186751276254654\n",
      "Iteration 153, Batch: 23, Loss: 0.052363280206918716\n",
      "Iteration 153, Batch: 24, Loss: 0.07130201160907745\n",
      "Iteration 153, Batch: 25, Loss: 0.07592393457889557\n",
      "Iteration 153, Batch: 26, Loss: 0.051586978137493134\n",
      "Iteration 153, Batch: 27, Loss: 0.08056768029928207\n",
      "Iteration 153, Batch: 28, Loss: 0.040816765278577805\n",
      "Iteration 153, Batch: 29, Loss: 0.050396401435136795\n",
      "Iteration 153, Batch: 30, Loss: 0.0881570428609848\n",
      "Iteration 153, Batch: 31, Loss: 0.05180854722857475\n",
      "Iteration 153, Batch: 32, Loss: 0.0717063695192337\n",
      "Iteration 153, Batch: 33, Loss: 0.05698968097567558\n",
      "Iteration 153, Batch: 34, Loss: 0.07999525964260101\n",
      "Iteration 153, Batch: 35, Loss: 0.045550111681222916\n",
      "Iteration 153, Batch: 36, Loss: 0.07876269519329071\n",
      "Iteration 153, Batch: 37, Loss: 0.0670696496963501\n",
      "Iteration 153, Batch: 38, Loss: 0.07102402299642563\n",
      "Iteration 153, Batch: 39, Loss: 0.07666134834289551\n",
      "Iteration 153, Batch: 40, Loss: 0.056591298431158066\n",
      "Iteration 153, Batch: 41, Loss: 0.10057789832353592\n",
      "Iteration 153, Batch: 42, Loss: 0.05244188755750656\n",
      "Iteration 153, Batch: 43, Loss: 0.04205133393406868\n",
      "Iteration 153, Batch: 44, Loss: 0.05021091178059578\n",
      "Iteration 153, Batch: 45, Loss: 0.07331375032663345\n",
      "Iteration 153, Batch: 46, Loss: 0.05108547955751419\n",
      "Iteration 153, Batch: 47, Loss: 0.056681063026189804\n",
      "Iteration 153, Batch: 48, Loss: 0.04500092566013336\n",
      "Iteration 153, Batch: 49, Loss: 0.09143560379743576\n",
      "Iteration 154, Batch: 0, Loss: 0.06953264027833939\n",
      "Iteration 154, Batch: 1, Loss: 0.0924782082438469\n",
      "Iteration 154, Batch: 2, Loss: 0.07170102000236511\n",
      "Iteration 154, Batch: 3, Loss: 0.07607246935367584\n",
      "Iteration 154, Batch: 4, Loss: 0.059322111308574677\n",
      "Iteration 154, Batch: 5, Loss: 0.08305169641971588\n",
      "Iteration 154, Batch: 6, Loss: 0.04121369868516922\n",
      "Iteration 154, Batch: 7, Loss: 0.08491859585046768\n",
      "Iteration 154, Batch: 8, Loss: 0.04123441129922867\n",
      "Iteration 154, Batch: 9, Loss: 0.07863569259643555\n",
      "Iteration 154, Batch: 10, Loss: 0.07771195471286774\n",
      "Iteration 154, Batch: 11, Loss: 0.04366740956902504\n",
      "Iteration 154, Batch: 12, Loss: 0.06112902984023094\n",
      "Iteration 154, Batch: 13, Loss: 0.07345876842737198\n",
      "Iteration 154, Batch: 14, Loss: 0.049250707030296326\n",
      "Iteration 154, Batch: 15, Loss: 0.07998716831207275\n",
      "Iteration 154, Batch: 16, Loss: 0.07517465949058533\n",
      "Iteration 154, Batch: 17, Loss: 0.04037865623831749\n",
      "Iteration 154, Batch: 18, Loss: 0.0914369598031044\n",
      "Iteration 154, Batch: 19, Loss: 0.046006277203559875\n",
      "Iteration 154, Batch: 20, Loss: 0.07958243787288666\n",
      "Iteration 154, Batch: 21, Loss: 0.09699033200740814\n",
      "Iteration 154, Batch: 22, Loss: 0.06228528916835785\n",
      "Iteration 154, Batch: 23, Loss: 0.0630377009510994\n",
      "Iteration 154, Batch: 24, Loss: 0.048686735332012177\n",
      "Iteration 154, Batch: 25, Loss: 0.054540399461984634\n",
      "Iteration 154, Batch: 26, Loss: 0.04267830774188042\n",
      "Iteration 154, Batch: 27, Loss: 0.06664784997701645\n",
      "Iteration 154, Batch: 28, Loss: 0.0659508928656578\n",
      "Iteration 154, Batch: 29, Loss: 0.057415734976530075\n",
      "Iteration 154, Batch: 30, Loss: 0.06195475533604622\n",
      "Iteration 154, Batch: 31, Loss: 0.0906219482421875\n",
      "Iteration 154, Batch: 32, Loss: 0.060236282646656036\n",
      "Iteration 154, Batch: 33, Loss: 0.07550458610057831\n",
      "Iteration 154, Batch: 34, Loss: 0.08330491930246353\n",
      "Iteration 154, Batch: 35, Loss: 0.05549278110265732\n",
      "Iteration 154, Batch: 36, Loss: 0.060120582580566406\n",
      "Iteration 154, Batch: 37, Loss: 0.05798398330807686\n",
      "Iteration 154, Batch: 38, Loss: 0.09591502696275711\n",
      "Iteration 154, Batch: 39, Loss: 0.0713033452630043\n",
      "Iteration 154, Batch: 40, Loss: 0.07650034874677658\n",
      "Iteration 154, Batch: 41, Loss: 0.07976432889699936\n",
      "Iteration 154, Batch: 42, Loss: 0.06141526997089386\n",
      "Iteration 154, Batch: 43, Loss: 0.030619706958532333\n",
      "Iteration 154, Batch: 44, Loss: 0.048720214515924454\n",
      "Iteration 154, Batch: 45, Loss: 0.048266105353832245\n",
      "Iteration 154, Batch: 46, Loss: 0.05781325325369835\n",
      "Iteration 154, Batch: 47, Loss: 0.0711461752653122\n",
      "Iteration 154, Batch: 48, Loss: 0.05823517590761185\n",
      "Iteration 154, Batch: 49, Loss: 0.07228509336709976\n",
      "Iteration 155, Batch: 0, Loss: 0.059205312281847\n",
      "Iteration 155, Batch: 1, Loss: 0.04912332445383072\n",
      "Iteration 155, Batch: 2, Loss: 0.052607130259275436\n",
      "Iteration 155, Batch: 3, Loss: 0.060390301048755646\n",
      "Iteration 155, Batch: 4, Loss: 0.04116540029644966\n",
      "Iteration 155, Batch: 5, Loss: 0.0716060921549797\n",
      "Iteration 155, Batch: 6, Loss: 0.051696594804525375\n",
      "Iteration 155, Batch: 7, Loss: 0.07453294843435287\n",
      "Iteration 155, Batch: 8, Loss: 0.08480829000473022\n",
      "Iteration 155, Batch: 9, Loss: 0.08466487377882004\n",
      "Iteration 155, Batch: 10, Loss: 0.07131032645702362\n",
      "Iteration 155, Batch: 11, Loss: 0.06044217571616173\n",
      "Iteration 155, Batch: 12, Loss: 0.048950016498565674\n",
      "Iteration 155, Batch: 13, Loss: 0.07784749567508698\n",
      "Iteration 155, Batch: 14, Loss: 0.062094707041978836\n",
      "Iteration 155, Batch: 15, Loss: 0.06083560734987259\n",
      "Iteration 155, Batch: 16, Loss: 0.06767004728317261\n",
      "Iteration 155, Batch: 17, Loss: 0.07043878734111786\n",
      "Iteration 155, Batch: 18, Loss: 0.05162447690963745\n",
      "Iteration 155, Batch: 19, Loss: 0.07509440928697586\n",
      "Iteration 155, Batch: 20, Loss: 0.0757426992058754\n",
      "Iteration 155, Batch: 21, Loss: 0.07749956846237183\n",
      "Iteration 155, Batch: 22, Loss: 0.07258320599794388\n",
      "Iteration 155, Batch: 23, Loss: 0.08209824562072754\n",
      "Iteration 155, Batch: 24, Loss: 0.061824195086956024\n",
      "Iteration 155, Batch: 25, Loss: 0.05582302063703537\n",
      "Iteration 155, Batch: 26, Loss: 0.058769844472408295\n",
      "Iteration 155, Batch: 27, Loss: 0.06668397039175034\n",
      "Iteration 155, Batch: 28, Loss: 0.07751164585351944\n",
      "Iteration 155, Batch: 29, Loss: 0.10361525416374207\n",
      "Iteration 155, Batch: 30, Loss: 0.09924133121967316\n",
      "Iteration 155, Batch: 31, Loss: 0.07863159477710724\n",
      "Iteration 155, Batch: 32, Loss: 0.04731104150414467\n",
      "Iteration 155, Batch: 33, Loss: 0.06262068450450897\n",
      "Iteration 155, Batch: 34, Loss: 0.07318700850009918\n",
      "Iteration 155, Batch: 35, Loss: 0.07827842235565186\n",
      "Iteration 155, Batch: 36, Loss: 0.0756651982665062\n",
      "Iteration 155, Batch: 37, Loss: 0.04906325787305832\n",
      "Iteration 155, Batch: 38, Loss: 0.07281409204006195\n",
      "Iteration 155, Batch: 39, Loss: 0.051911771297454834\n",
      "Iteration 155, Batch: 40, Loss: 0.047626886516809464\n",
      "Iteration 155, Batch: 41, Loss: 0.06395070999860764\n",
      "Iteration 155, Batch: 42, Loss: 0.0776909738779068\n",
      "Iteration 155, Batch: 43, Loss: 0.04795533046126366\n",
      "Iteration 155, Batch: 44, Loss: 0.039525941014289856\n",
      "Iteration 155, Batch: 45, Loss: 0.09900303930044174\n",
      "Iteration 155, Batch: 46, Loss: 0.06094059720635414\n",
      "Iteration 155, Batch: 47, Loss: 0.07538120448589325\n",
      "Iteration 155, Batch: 48, Loss: 0.07849554717540741\n",
      "Iteration 155, Batch: 49, Loss: 0.08379924297332764\n",
      "Iteration 156, Batch: 0, Loss: 0.022259755060076714\n",
      "Iteration 156, Batch: 1, Loss: 0.06263761967420578\n",
      "Iteration 156, Batch: 2, Loss: 0.053349483758211136\n",
      "Iteration 156, Batch: 3, Loss: 0.06223851069808006\n",
      "Iteration 156, Batch: 4, Loss: 0.0702490508556366\n",
      "Iteration 156, Batch: 5, Loss: 0.06735267490148544\n",
      "Iteration 156, Batch: 6, Loss: 0.038346774876117706\n",
      "Iteration 156, Batch: 7, Loss: 0.07150916755199432\n",
      "Iteration 156, Batch: 8, Loss: 0.06328009814023972\n",
      "Iteration 156, Batch: 9, Loss: 0.04542706534266472\n",
      "Iteration 156, Batch: 10, Loss: 0.05886467918753624\n",
      "Iteration 156, Batch: 11, Loss: 0.0673631876707077\n",
      "Iteration 156, Batch: 12, Loss: 0.10500314831733704\n",
      "Iteration 156, Batch: 13, Loss: 0.07238365709781647\n",
      "Iteration 156, Batch: 14, Loss: 0.06479303538799286\n",
      "Iteration 156, Batch: 15, Loss: 0.060601748526096344\n",
      "Iteration 156, Batch: 16, Loss: 0.06466127187013626\n",
      "Iteration 156, Batch: 17, Loss: 0.07457627356052399\n",
      "Iteration 156, Batch: 18, Loss: 0.0797899067401886\n",
      "Iteration 156, Batch: 19, Loss: 0.0413336418569088\n",
      "Iteration 156, Batch: 20, Loss: 0.05636066198348999\n",
      "Iteration 156, Batch: 21, Loss: 0.0841284766793251\n",
      "Iteration 156, Batch: 22, Loss: 0.04573521390557289\n",
      "Iteration 156, Batch: 23, Loss: 0.039202120155096054\n",
      "Iteration 156, Batch: 24, Loss: 0.07235124707221985\n",
      "Iteration 156, Batch: 25, Loss: 0.051820628345012665\n",
      "Iteration 156, Batch: 26, Loss: 0.07445168495178223\n",
      "Iteration 156, Batch: 27, Loss: 0.05838082358241081\n",
      "Iteration 156, Batch: 28, Loss: 0.08180619776248932\n",
      "Iteration 156, Batch: 29, Loss: 0.08026815205812454\n",
      "Iteration 156, Batch: 30, Loss: 0.029517611488699913\n",
      "Iteration 156, Batch: 31, Loss: 0.08334130793809891\n",
      "Iteration 156, Batch: 32, Loss: 0.050653599202632904\n",
      "Iteration 156, Batch: 33, Loss: 0.07351455837488174\n",
      "Iteration 156, Batch: 34, Loss: 0.043653275817632675\n",
      "Iteration 156, Batch: 35, Loss: 0.07213713228702545\n",
      "Iteration 156, Batch: 36, Loss: 0.04853074997663498\n",
      "Iteration 156, Batch: 37, Loss: 0.06982779502868652\n",
      "Iteration 156, Batch: 38, Loss: 0.04499858617782593\n",
      "Iteration 156, Batch: 39, Loss: 0.0637911707162857\n",
      "Iteration 156, Batch: 40, Loss: 0.09106448292732239\n",
      "Iteration 156, Batch: 41, Loss: 0.05308135971426964\n",
      "Iteration 156, Batch: 42, Loss: 0.07538843899965286\n",
      "Iteration 156, Batch: 43, Loss: 0.07111584395170212\n",
      "Iteration 156, Batch: 44, Loss: 0.07114534825086594\n",
      "Iteration 156, Batch: 45, Loss: 0.05171971023082733\n",
      "Iteration 156, Batch: 46, Loss: 0.08469709008932114\n",
      "Iteration 156, Batch: 47, Loss: 0.02207755111157894\n",
      "Iteration 156, Batch: 48, Loss: 0.0887126624584198\n",
      "Iteration 156, Batch: 49, Loss: 0.05228811129927635\n",
      "Iteration 157, Batch: 0, Loss: 0.06802968680858612\n",
      "Iteration 157, Batch: 1, Loss: 0.07687018811702728\n",
      "Iteration 157, Batch: 2, Loss: 0.054564304649829865\n",
      "Iteration 157, Batch: 3, Loss: 0.05977613478899002\n",
      "Iteration 157, Batch: 4, Loss: 0.07443259656429291\n",
      "Iteration 157, Batch: 5, Loss: 0.07420055568218231\n",
      "Iteration 157, Batch: 6, Loss: 0.05712803825736046\n",
      "Iteration 157, Batch: 7, Loss: 0.08296316862106323\n",
      "Iteration 157, Batch: 8, Loss: 0.07691636681556702\n",
      "Iteration 157, Batch: 9, Loss: 0.05364164337515831\n",
      "Iteration 157, Batch: 10, Loss: 0.09133187681436539\n",
      "Iteration 157, Batch: 11, Loss: 0.1074310690164566\n",
      "Iteration 157, Batch: 12, Loss: 0.05771910399198532\n",
      "Iteration 157, Batch: 13, Loss: 0.08728180825710297\n",
      "Iteration 157, Batch: 14, Loss: 0.05183413252234459\n",
      "Iteration 157, Batch: 15, Loss: 0.06229433789849281\n",
      "Iteration 157, Batch: 16, Loss: 0.0775102972984314\n",
      "Iteration 157, Batch: 17, Loss: 0.05416509136557579\n",
      "Iteration 157, Batch: 18, Loss: 0.04485991224646568\n",
      "Iteration 157, Batch: 19, Loss: 0.10439898073673248\n",
      "Iteration 157, Batch: 20, Loss: 0.05775841698050499\n",
      "Iteration 157, Batch: 21, Loss: 0.06339454650878906\n",
      "Iteration 157, Batch: 22, Loss: 0.07406909763813019\n",
      "Iteration 157, Batch: 23, Loss: 0.05037982761859894\n",
      "Iteration 157, Batch: 24, Loss: 0.04063444957137108\n",
      "Iteration 157, Batch: 25, Loss: 0.05581015348434448\n",
      "Iteration 157, Batch: 26, Loss: 0.06525404751300812\n",
      "Iteration 157, Batch: 27, Loss: 0.06022237613797188\n",
      "Iteration 157, Batch: 28, Loss: 0.05323595926165581\n",
      "Iteration 157, Batch: 29, Loss: 0.04303162917494774\n",
      "Iteration 157, Batch: 30, Loss: 0.05020354315638542\n",
      "Iteration 157, Batch: 31, Loss: 0.05175698176026344\n",
      "Iteration 157, Batch: 32, Loss: 0.0658058151602745\n",
      "Iteration 157, Batch: 33, Loss: 0.05268891155719757\n",
      "Iteration 157, Batch: 34, Loss: 0.1138891875743866\n",
      "Iteration 157, Batch: 35, Loss: 0.12898603081703186\n",
      "Iteration 157, Batch: 36, Loss: 0.04769434034824371\n",
      "Iteration 157, Batch: 37, Loss: 0.07972735911607742\n",
      "Iteration 157, Batch: 38, Loss: 0.06485525518655777\n",
      "Iteration 157, Batch: 39, Loss: 0.07593069970607758\n",
      "Iteration 157, Batch: 40, Loss: 0.06056096404790878\n",
      "Iteration 157, Batch: 41, Loss: 0.05755733698606491\n",
      "Iteration 157, Batch: 42, Loss: 0.056559886783361435\n",
      "Iteration 157, Batch: 43, Loss: 0.06670410186052322\n",
      "Iteration 157, Batch: 44, Loss: 0.09320440888404846\n",
      "Iteration 157, Batch: 45, Loss: 0.05309094861149788\n",
      "Iteration 157, Batch: 46, Loss: 0.05826621502637863\n",
      "Iteration 157, Batch: 47, Loss: 0.04064515605568886\n",
      "Iteration 157, Batch: 48, Loss: 0.062221381813287735\n",
      "Iteration 157, Batch: 49, Loss: 0.07737654447555542\n",
      "Iteration 158, Batch: 0, Loss: 0.06613317131996155\n",
      "Iteration 158, Batch: 1, Loss: 0.07668690383434296\n",
      "Iteration 158, Batch: 2, Loss: 0.09835141897201538\n",
      "Iteration 158, Batch: 3, Loss: 0.07269406318664551\n",
      "Iteration 158, Batch: 4, Loss: 0.06745246797800064\n",
      "Iteration 158, Batch: 5, Loss: 0.06812102347612381\n",
      "Iteration 158, Batch: 6, Loss: 0.08428142964839935\n",
      "Iteration 158, Batch: 7, Loss: 0.06666336208581924\n",
      "Iteration 158, Batch: 8, Loss: 0.053316984325647354\n",
      "Iteration 158, Batch: 9, Loss: 0.06089503690600395\n",
      "Iteration 158, Batch: 10, Loss: 0.05182981491088867\n",
      "Iteration 158, Batch: 11, Loss: 0.06296874582767487\n",
      "Iteration 158, Batch: 12, Loss: 0.06599675118923187\n",
      "Iteration 158, Batch: 13, Loss: 0.06259419023990631\n",
      "Iteration 158, Batch: 14, Loss: 0.08086885511875153\n",
      "Iteration 158, Batch: 15, Loss: 0.0584595613181591\n",
      "Iteration 158, Batch: 16, Loss: 0.056778013706207275\n",
      "Iteration 158, Batch: 17, Loss: 0.06259775906801224\n",
      "Iteration 158, Batch: 18, Loss: 0.07231247425079346\n",
      "Iteration 158, Batch: 19, Loss: 0.053486742079257965\n",
      "Iteration 158, Batch: 20, Loss: 0.06063878536224365\n",
      "Iteration 158, Batch: 21, Loss: 0.08450248837471008\n",
      "Iteration 158, Batch: 22, Loss: 0.08574415743350983\n",
      "Iteration 158, Batch: 23, Loss: 0.09051446616649628\n",
      "Iteration 158, Batch: 24, Loss: 0.04426063597202301\n",
      "Iteration 158, Batch: 25, Loss: 0.05841252580285072\n",
      "Iteration 158, Batch: 26, Loss: 0.08088445663452148\n",
      "Iteration 158, Batch: 27, Loss: 0.05359014496207237\n",
      "Iteration 158, Batch: 28, Loss: 0.06248636171221733\n",
      "Iteration 158, Batch: 29, Loss: 0.05435328185558319\n",
      "Iteration 158, Batch: 30, Loss: 0.02529655583202839\n",
      "Iteration 158, Batch: 31, Loss: 0.0466456264257431\n",
      "Iteration 158, Batch: 32, Loss: 0.08347491174936295\n",
      "Iteration 158, Batch: 33, Loss: 0.07605886459350586\n",
      "Iteration 158, Batch: 34, Loss: 0.09388018399477005\n",
      "Iteration 158, Batch: 35, Loss: 0.08555036038160324\n",
      "Iteration 158, Batch: 36, Loss: 0.05063102766871452\n",
      "Iteration 158, Batch: 37, Loss: 0.07544351369142532\n",
      "Iteration 158, Batch: 38, Loss: 0.04848212003707886\n",
      "Iteration 158, Batch: 39, Loss: 0.05496479943394661\n",
      "Iteration 158, Batch: 40, Loss: 0.0545225590467453\n",
      "Iteration 158, Batch: 41, Loss: 0.0654122605919838\n",
      "Iteration 158, Batch: 42, Loss: 0.07269414514303207\n",
      "Iteration 158, Batch: 43, Loss: 0.05446857213973999\n",
      "Iteration 158, Batch: 44, Loss: 0.035014208406209946\n",
      "Iteration 158, Batch: 45, Loss: 0.0761166661977768\n",
      "Iteration 158, Batch: 46, Loss: 0.0764722228050232\n",
      "Iteration 158, Batch: 47, Loss: 0.03505265340209007\n",
      "Iteration 158, Batch: 48, Loss: 0.07261672616004944\n",
      "Iteration 158, Batch: 49, Loss: 0.05597667023539543\n",
      "Iteration 159, Batch: 0, Loss: 0.04516173154115677\n",
      "Iteration 159, Batch: 1, Loss: 0.07410962134599686\n",
      "Iteration 159, Batch: 2, Loss: 0.05888134241104126\n",
      "Iteration 159, Batch: 3, Loss: 0.052210357040166855\n",
      "Iteration 159, Batch: 4, Loss: 0.06957154721021652\n",
      "Iteration 159, Batch: 5, Loss: 0.07951682806015015\n",
      "Iteration 159, Batch: 6, Loss: 0.07476939260959625\n",
      "Iteration 159, Batch: 7, Loss: 0.07803285121917725\n",
      "Iteration 159, Batch: 8, Loss: 0.09519360214471817\n",
      "Iteration 159, Batch: 9, Loss: 0.07210639864206314\n",
      "Iteration 159, Batch: 10, Loss: 0.06736525148153305\n",
      "Iteration 159, Batch: 11, Loss: 0.05951525643467903\n",
      "Iteration 159, Batch: 12, Loss: 0.05482275038957596\n",
      "Iteration 159, Batch: 13, Loss: 0.056778281927108765\n",
      "Iteration 159, Batch: 14, Loss: 0.048066433519124985\n",
      "Iteration 159, Batch: 15, Loss: 0.06952429562807083\n",
      "Iteration 159, Batch: 16, Loss: 0.060973137617111206\n",
      "Iteration 159, Batch: 17, Loss: 0.09657735377550125\n",
      "Iteration 159, Batch: 18, Loss: 0.05043575167655945\n",
      "Iteration 159, Batch: 19, Loss: 0.05577274039387703\n",
      "Iteration 159, Batch: 20, Loss: 0.048204295337200165\n",
      "Iteration 159, Batch: 21, Loss: 0.06150424852967262\n",
      "Iteration 159, Batch: 22, Loss: 0.03214601054787636\n",
      "Iteration 159, Batch: 23, Loss: 0.07342424988746643\n",
      "Iteration 159, Batch: 24, Loss: 0.06180322915315628\n",
      "Iteration 159, Batch: 25, Loss: 0.04873500391840935\n",
      "Iteration 159, Batch: 26, Loss: 0.04752369597554207\n",
      "Iteration 159, Batch: 27, Loss: 0.05121232569217682\n",
      "Iteration 159, Batch: 28, Loss: 0.06804956495761871\n",
      "Iteration 159, Batch: 29, Loss: 0.07786273211240768\n",
      "Iteration 159, Batch: 30, Loss: 0.060063522309064865\n",
      "Iteration 159, Batch: 31, Loss: 0.06657736748456955\n",
      "Iteration 159, Batch: 32, Loss: 0.06277373433113098\n",
      "Iteration 159, Batch: 33, Loss: 0.08894369006156921\n",
      "Iteration 159, Batch: 34, Loss: 0.061723463237285614\n",
      "Iteration 159, Batch: 35, Loss: 0.10813340544700623\n",
      "Iteration 159, Batch: 36, Loss: 0.06973111629486084\n",
      "Iteration 159, Batch: 37, Loss: 0.054784756153821945\n",
      "Iteration 159, Batch: 38, Loss: 0.06353750824928284\n",
      "Iteration 159, Batch: 39, Loss: 0.06526678055524826\n",
      "Iteration 159, Batch: 40, Loss: 0.0779530331492424\n",
      "Iteration 159, Batch: 41, Loss: 0.07336205989122391\n",
      "Iteration 159, Batch: 42, Loss: 0.04615789279341698\n",
      "Iteration 159, Batch: 43, Loss: 0.030819140374660492\n",
      "Iteration 159, Batch: 44, Loss: 0.03688376024365425\n",
      "Iteration 159, Batch: 45, Loss: 0.06696849316358566\n",
      "Iteration 159, Batch: 46, Loss: 0.09454738348722458\n",
      "Iteration 159, Batch: 47, Loss: 0.05925777181982994\n",
      "Iteration 159, Batch: 48, Loss: 0.07652131468057632\n",
      "Iteration 159, Batch: 49, Loss: 0.07734902948141098\n",
      "Iteration 160, Batch: 0, Loss: 0.06971461325883865\n",
      "Iteration 160, Batch: 1, Loss: 0.05001850798726082\n",
      "Iteration 160, Batch: 2, Loss: 0.06920444965362549\n",
      "Iteration 160, Batch: 3, Loss: 0.03423793241381645\n",
      "Iteration 160, Batch: 4, Loss: 0.08067283779382706\n",
      "Iteration 160, Batch: 5, Loss: 0.04825093224644661\n",
      "Iteration 160, Batch: 6, Loss: 0.03287201747298241\n",
      "Iteration 160, Batch: 7, Loss: 0.06508229672908783\n",
      "Iteration 160, Batch: 8, Loss: 0.056813377887010574\n",
      "Iteration 160, Batch: 9, Loss: 0.08066464960575104\n",
      "Iteration 160, Batch: 10, Loss: 0.059785179793834686\n",
      "Iteration 160, Batch: 11, Loss: 0.06957467645406723\n",
      "Iteration 160, Batch: 12, Loss: 0.08846591413021088\n",
      "Iteration 160, Batch: 13, Loss: 0.046102385967969894\n",
      "Iteration 160, Batch: 14, Loss: 0.04725934565067291\n",
      "Iteration 160, Batch: 15, Loss: 0.0817612037062645\n",
      "Iteration 160, Batch: 16, Loss: 0.07399078458547592\n",
      "Iteration 160, Batch: 17, Loss: 0.08407995849847794\n",
      "Iteration 160, Batch: 18, Loss: 0.05332610011100769\n",
      "Iteration 160, Batch: 19, Loss: 0.05069417878985405\n",
      "Iteration 160, Batch: 20, Loss: 0.10871908068656921\n",
      "Iteration 160, Batch: 21, Loss: 0.07986205071210861\n",
      "Iteration 160, Batch: 22, Loss: 0.0869966372847557\n",
      "Iteration 160, Batch: 23, Loss: 0.05072817578911781\n",
      "Iteration 160, Batch: 24, Loss: 0.05846453085541725\n",
      "Iteration 160, Batch: 25, Loss: 0.09605466574430466\n",
      "Iteration 160, Batch: 26, Loss: 0.04566840082406998\n",
      "Iteration 160, Batch: 27, Loss: 0.06224536895751953\n",
      "Iteration 160, Batch: 28, Loss: 0.05834423378109932\n",
      "Iteration 160, Batch: 29, Loss: 0.07240185886621475\n",
      "Iteration 160, Batch: 30, Loss: 0.06947310268878937\n",
      "Iteration 160, Batch: 31, Loss: 0.08255904912948608\n",
      "Iteration 160, Batch: 32, Loss: 0.06742208451032639\n",
      "Iteration 160, Batch: 33, Loss: 0.058694373816251755\n",
      "Iteration 160, Batch: 34, Loss: 0.06283213198184967\n",
      "Iteration 160, Batch: 35, Loss: 0.07513659447431564\n",
      "Iteration 160, Batch: 36, Loss: 0.07754085958003998\n",
      "Iteration 160, Batch: 37, Loss: 0.07596408575773239\n",
      "Iteration 160, Batch: 38, Loss: 0.04648980498313904\n",
      "Iteration 160, Batch: 39, Loss: 0.05588020384311676\n",
      "Iteration 160, Batch: 40, Loss: 0.07062437385320663\n",
      "Iteration 160, Batch: 41, Loss: 0.05684725567698479\n",
      "Iteration 160, Batch: 42, Loss: 0.07205454260110855\n",
      "Iteration 160, Batch: 43, Loss: 0.06779439002275467\n",
      "Iteration 160, Batch: 44, Loss: 0.057391099631786346\n",
      "Iteration 160, Batch: 45, Loss: 0.04272734373807907\n",
      "Iteration 160, Batch: 46, Loss: 0.09000347554683685\n",
      "Iteration 160, Batch: 47, Loss: 0.05750178173184395\n",
      "Iteration 160, Batch: 48, Loss: 0.05236147716641426\n",
      "Iteration 160, Batch: 49, Loss: 0.06416404247283936\n",
      "Iteration 161, Batch: 0, Loss: 0.07723946124315262\n",
      "Iteration 161, Batch: 1, Loss: 0.0811881422996521\n",
      "Iteration 161, Batch: 2, Loss: 0.050103526562452316\n",
      "Iteration 161, Batch: 3, Loss: 0.04265480861067772\n",
      "Iteration 161, Batch: 4, Loss: 0.05132101848721504\n",
      "Iteration 161, Batch: 5, Loss: 0.09273242950439453\n",
      "Iteration 161, Batch: 6, Loss: 0.10081194341182709\n",
      "Iteration 161, Batch: 7, Loss: 0.07015003263950348\n",
      "Iteration 161, Batch: 8, Loss: 0.06419725716114044\n",
      "Iteration 161, Batch: 9, Loss: 0.05530199036002159\n",
      "Iteration 161, Batch: 10, Loss: 0.05858973041176796\n",
      "Iteration 161, Batch: 11, Loss: 0.07054886221885681\n",
      "Iteration 161, Batch: 12, Loss: 0.06489915400743484\n",
      "Iteration 161, Batch: 13, Loss: 0.060743510723114014\n",
      "Iteration 161, Batch: 14, Loss: 0.07418456673622131\n",
      "Iteration 161, Batch: 15, Loss: 0.08938416093587875\n",
      "Iteration 161, Batch: 16, Loss: 0.06540548801422119\n",
      "Iteration 161, Batch: 17, Loss: 0.07016380876302719\n",
      "Iteration 161, Batch: 18, Loss: 0.053543128073215485\n",
      "Iteration 161, Batch: 19, Loss: 0.0783548429608345\n",
      "Iteration 161, Batch: 20, Loss: 0.05748802050948143\n",
      "Iteration 161, Batch: 21, Loss: 0.04560944437980652\n",
      "Iteration 161, Batch: 22, Loss: 0.02239411510527134\n",
      "Iteration 161, Batch: 23, Loss: 0.06758669018745422\n",
      "Iteration 161, Batch: 24, Loss: 0.0543084591627121\n",
      "Iteration 161, Batch: 25, Loss: 0.06662987917661667\n",
      "Iteration 161, Batch: 26, Loss: 0.03934119641780853\n",
      "Iteration 161, Batch: 27, Loss: 0.061432283371686935\n",
      "Iteration 161, Batch: 28, Loss: 0.07737024873495102\n",
      "Iteration 161, Batch: 29, Loss: 0.0767252966761589\n",
      "Iteration 161, Batch: 30, Loss: 0.07693468034267426\n",
      "Iteration 161, Batch: 31, Loss: 0.08678120374679565\n",
      "Iteration 161, Batch: 32, Loss: 0.06611622124910355\n",
      "Iteration 161, Batch: 33, Loss: 0.0558021143078804\n",
      "Iteration 161, Batch: 34, Loss: 0.07385183870792389\n",
      "Iteration 161, Batch: 35, Loss: 0.051327917724847794\n",
      "Iteration 161, Batch: 36, Loss: 0.05962233990430832\n",
      "Iteration 161, Batch: 37, Loss: 0.057664655148983\n",
      "Iteration 161, Batch: 38, Loss: 0.06287634372711182\n",
      "Iteration 161, Batch: 39, Loss: 0.05566275492310524\n",
      "Iteration 161, Batch: 40, Loss: 0.06721928715705872\n",
      "Iteration 161, Batch: 41, Loss: 0.06275774538516998\n",
      "Iteration 161, Batch: 42, Loss: 0.03789449483156204\n",
      "Iteration 161, Batch: 43, Loss: 0.0668054074048996\n",
      "Iteration 161, Batch: 44, Loss: 0.05974176153540611\n",
      "Iteration 161, Batch: 45, Loss: 0.06071382388472557\n",
      "Iteration 161, Batch: 46, Loss: 0.08197042346000671\n",
      "Iteration 161, Batch: 47, Loss: 0.06390256434679031\n",
      "Iteration 161, Batch: 48, Loss: 0.05767250806093216\n",
      "Iteration 161, Batch: 49, Loss: 0.07622018456459045\n",
      "Iteration 162, Batch: 0, Loss: 0.06515169888734818\n",
      "Iteration 162, Batch: 1, Loss: 0.07492286711931229\n",
      "Iteration 162, Batch: 2, Loss: 0.06879834830760956\n",
      "Iteration 162, Batch: 3, Loss: 0.08575495332479477\n",
      "Iteration 162, Batch: 4, Loss: 0.060829710215330124\n",
      "Iteration 162, Batch: 5, Loss: 0.07056750357151031\n",
      "Iteration 162, Batch: 6, Loss: 0.054362256079912186\n",
      "Iteration 162, Batch: 7, Loss: 0.05925701931118965\n",
      "Iteration 162, Batch: 8, Loss: 0.03570040315389633\n",
      "Iteration 162, Batch: 9, Loss: 0.07032667845487595\n",
      "Iteration 162, Batch: 10, Loss: 0.06064727157354355\n",
      "Iteration 162, Batch: 11, Loss: 0.039937570691108704\n",
      "Iteration 162, Batch: 12, Loss: 0.0590398870408535\n",
      "Iteration 162, Batch: 13, Loss: 0.06539640575647354\n",
      "Iteration 162, Batch: 14, Loss: 0.08179788291454315\n",
      "Iteration 162, Batch: 15, Loss: 0.0905587375164032\n",
      "Iteration 162, Batch: 16, Loss: 0.07719089090824127\n",
      "Iteration 162, Batch: 17, Loss: 0.06463678926229477\n",
      "Iteration 162, Batch: 18, Loss: 0.08850163966417313\n",
      "Iteration 162, Batch: 19, Loss: 0.056056320667266846\n",
      "Iteration 162, Batch: 20, Loss: 0.06695089489221573\n",
      "Iteration 162, Batch: 21, Loss: 0.0942232683300972\n",
      "Iteration 162, Batch: 22, Loss: 0.09098445624113083\n",
      "Iteration 162, Batch: 23, Loss: 0.0757642313838005\n",
      "Iteration 162, Batch: 24, Loss: 0.10819895565509796\n",
      "Iteration 162, Batch: 25, Loss: 0.06903243809938431\n",
      "Iteration 162, Batch: 26, Loss: 0.08805986493825912\n",
      "Iteration 162, Batch: 27, Loss: 0.09889320284128189\n",
      "Iteration 162, Batch: 28, Loss: 0.07355733960866928\n",
      "Iteration 162, Batch: 29, Loss: 0.0615977942943573\n",
      "Iteration 162, Batch: 30, Loss: 0.05629061535000801\n",
      "Iteration 162, Batch: 31, Loss: 0.05196423828601837\n",
      "Iteration 162, Batch: 32, Loss: 0.05448226258158684\n",
      "Iteration 162, Batch: 33, Loss: 0.07439971715211868\n",
      "Iteration 162, Batch: 34, Loss: 0.08195465058088303\n",
      "Iteration 162, Batch: 35, Loss: 0.07308327406644821\n",
      "Iteration 162, Batch: 36, Loss: 0.047345153987407684\n",
      "Iteration 162, Batch: 37, Loss: 0.06170498579740524\n",
      "Iteration 162, Batch: 38, Loss: 0.07376187294721603\n",
      "Iteration 162, Batch: 39, Loss: 0.06221974268555641\n",
      "Iteration 162, Batch: 40, Loss: 0.04846959933638573\n",
      "Iteration 162, Batch: 41, Loss: 0.06654077023267746\n",
      "Iteration 162, Batch: 42, Loss: 0.05946933478116989\n",
      "Iteration 162, Batch: 43, Loss: 0.05674224719405174\n",
      "Iteration 162, Batch: 44, Loss: 0.07417261600494385\n",
      "Iteration 162, Batch: 45, Loss: 0.10019172728061676\n",
      "Iteration 162, Batch: 46, Loss: 0.06128472089767456\n",
      "Iteration 162, Batch: 47, Loss: 0.06701626628637314\n",
      "Iteration 162, Batch: 48, Loss: 0.045131485909223557\n",
      "Iteration 162, Batch: 49, Loss: 0.05856425315141678\n",
      "Iteration 163, Batch: 0, Loss: 0.08339425921440125\n",
      "Iteration 163, Batch: 1, Loss: 0.05310818925499916\n",
      "Iteration 163, Batch: 2, Loss: 0.06444744020700455\n",
      "Iteration 163, Batch: 3, Loss: 0.08895360678434372\n",
      "Iteration 163, Batch: 4, Loss: 0.06836223602294922\n",
      "Iteration 163, Batch: 5, Loss: 0.049413856118917465\n",
      "Iteration 163, Batch: 6, Loss: 0.0575314499437809\n",
      "Iteration 163, Batch: 7, Loss: 0.05738833174109459\n",
      "Iteration 163, Batch: 8, Loss: 0.07086633890867233\n",
      "Iteration 163, Batch: 9, Loss: 0.06447046995162964\n",
      "Iteration 163, Batch: 10, Loss: 0.048671722412109375\n",
      "Iteration 163, Batch: 11, Loss: 0.08663465082645416\n",
      "Iteration 163, Batch: 12, Loss: 0.07866153120994568\n",
      "Iteration 163, Batch: 13, Loss: 0.06526575237512589\n",
      "Iteration 163, Batch: 14, Loss: 0.06158144026994705\n",
      "Iteration 163, Batch: 15, Loss: 0.07813595235347748\n",
      "Iteration 163, Batch: 16, Loss: 0.08128108829259872\n",
      "Iteration 163, Batch: 17, Loss: 0.08863694965839386\n",
      "Iteration 163, Batch: 18, Loss: 0.08111324161291122\n",
      "Iteration 163, Batch: 19, Loss: 0.0733126699924469\n",
      "Iteration 163, Batch: 20, Loss: 0.04484342038631439\n",
      "Iteration 163, Batch: 21, Loss: 0.09497272223234177\n",
      "Iteration 163, Batch: 22, Loss: 0.07597678154706955\n",
      "Iteration 163, Batch: 23, Loss: 0.07472474873065948\n",
      "Iteration 163, Batch: 24, Loss: 0.08165016025304794\n",
      "Iteration 163, Batch: 25, Loss: 0.07600434124469757\n",
      "Iteration 163, Batch: 26, Loss: 0.07799874246120453\n",
      "Iteration 163, Batch: 27, Loss: 0.06895473599433899\n",
      "Iteration 163, Batch: 28, Loss: 0.0667010024189949\n",
      "Iteration 163, Batch: 29, Loss: 0.07875405997037888\n",
      "Iteration 163, Batch: 30, Loss: 0.07801852375268936\n",
      "Iteration 163, Batch: 31, Loss: 0.08247315138578415\n",
      "Iteration 163, Batch: 32, Loss: 0.045088157057762146\n",
      "Iteration 163, Batch: 33, Loss: 0.07064386457204819\n",
      "Iteration 163, Batch: 34, Loss: 0.06161437928676605\n",
      "Iteration 163, Batch: 35, Loss: 0.06418760865926743\n",
      "Iteration 163, Batch: 36, Loss: 0.09769121557474136\n",
      "Iteration 163, Batch: 37, Loss: 0.06903146952390671\n",
      "Iteration 163, Batch: 38, Loss: 0.09666719287633896\n",
      "Iteration 163, Batch: 39, Loss: 0.08391901850700378\n",
      "Iteration 163, Batch: 40, Loss: 0.07438613474369049\n",
      "Iteration 163, Batch: 41, Loss: 0.07800410687923431\n",
      "Iteration 163, Batch: 42, Loss: 0.07546525448560715\n",
      "Iteration 163, Batch: 43, Loss: 0.11597216129302979\n",
      "Iteration 163, Batch: 44, Loss: 0.08718480914831161\n",
      "Iteration 163, Batch: 45, Loss: 0.07420070469379425\n",
      "Iteration 163, Batch: 46, Loss: 0.07848621904850006\n",
      "Iteration 163, Batch: 47, Loss: 0.07137235999107361\n",
      "Iteration 163, Batch: 48, Loss: 0.12686605751514435\n",
      "Iteration 163, Batch: 49, Loss: 0.10229577124118805\n",
      "Iteration 164, Batch: 0, Loss: 0.0806846022605896\n",
      "Iteration 164, Batch: 1, Loss: 0.08566706627607346\n",
      "Iteration 164, Batch: 2, Loss: 0.07823656499385834\n",
      "Iteration 164, Batch: 3, Loss: 0.08929973095655441\n",
      "Iteration 164, Batch: 4, Loss: 0.09329328685998917\n",
      "Iteration 164, Batch: 5, Loss: 0.08688293397426605\n",
      "Iteration 164, Batch: 6, Loss: 0.09376925975084305\n",
      "Iteration 164, Batch: 7, Loss: 0.042025983333587646\n",
      "Iteration 164, Batch: 8, Loss: 0.07681987434625626\n",
      "Iteration 164, Batch: 9, Loss: 0.07818582653999329\n",
      "Iteration 164, Batch: 10, Loss: 0.08640377223491669\n",
      "Iteration 164, Batch: 11, Loss: 0.07814259082078934\n",
      "Iteration 164, Batch: 12, Loss: 0.06889370828866959\n",
      "Iteration 164, Batch: 13, Loss: 0.08433965593576431\n",
      "Iteration 164, Batch: 14, Loss: 0.08133076131343842\n",
      "Iteration 164, Batch: 15, Loss: 0.0640583261847496\n",
      "Iteration 164, Batch: 16, Loss: 0.04835046827793121\n",
      "Iteration 164, Batch: 17, Loss: 0.06412521004676819\n",
      "Iteration 164, Batch: 18, Loss: 0.04742829129099846\n",
      "Iteration 164, Batch: 19, Loss: 0.06827807426452637\n",
      "Iteration 164, Batch: 20, Loss: 0.06259576976299286\n",
      "Iteration 164, Batch: 21, Loss: 0.04316512122750282\n",
      "Iteration 164, Batch: 22, Loss: 0.0516316257417202\n",
      "Iteration 164, Batch: 23, Loss: 0.06536123901605606\n",
      "Iteration 164, Batch: 24, Loss: 0.05787220969796181\n",
      "Iteration 164, Batch: 25, Loss: 0.058195993304252625\n",
      "Iteration 164, Batch: 26, Loss: 0.05272889882326126\n",
      "Iteration 164, Batch: 27, Loss: 0.046908602118492126\n",
      "Iteration 164, Batch: 28, Loss: 0.08609883487224579\n",
      "Iteration 164, Batch: 29, Loss: 0.0567607618868351\n",
      "Iteration 164, Batch: 30, Loss: 0.0660988986492157\n",
      "Iteration 164, Batch: 31, Loss: 0.06524600833654404\n",
      "Iteration 164, Batch: 32, Loss: 0.0676480382680893\n",
      "Iteration 164, Batch: 33, Loss: 0.058940447866916656\n",
      "Iteration 164, Batch: 34, Loss: 0.0435163676738739\n",
      "Iteration 164, Batch: 35, Loss: 0.05310770869255066\n",
      "Iteration 164, Batch: 36, Loss: 0.0551157183945179\n",
      "Iteration 164, Batch: 37, Loss: 0.06610052287578583\n",
      "Iteration 164, Batch: 38, Loss: 0.05329984426498413\n",
      "Iteration 164, Batch: 39, Loss: 0.06579648703336716\n",
      "Iteration 164, Batch: 40, Loss: 0.04681931436061859\n",
      "Iteration 164, Batch: 41, Loss: 0.06603476405143738\n",
      "Iteration 164, Batch: 42, Loss: 0.03804735466837883\n",
      "Iteration 164, Batch: 43, Loss: 0.06623118370771408\n",
      "Iteration 164, Batch: 44, Loss: 0.06126350164413452\n",
      "Iteration 164, Batch: 45, Loss: 0.05559205263853073\n",
      "Iteration 164, Batch: 46, Loss: 0.0860816240310669\n",
      "Iteration 164, Batch: 47, Loss: 0.039904188364744186\n",
      "Iteration 164, Batch: 48, Loss: 0.07278928905725479\n",
      "Iteration 164, Batch: 49, Loss: 0.0765586793422699\n",
      "Iteration 165, Batch: 0, Loss: 0.07012510299682617\n",
      "Iteration 165, Batch: 1, Loss: 0.07373065501451492\n",
      "Iteration 165, Batch: 2, Loss: 0.06650689244270325\n",
      "Iteration 165, Batch: 3, Loss: 0.07669807225465775\n",
      "Iteration 165, Batch: 4, Loss: 0.08248776197433472\n",
      "Iteration 165, Batch: 5, Loss: 0.07235920429229736\n",
      "Iteration 165, Batch: 6, Loss: 0.07263367623090744\n",
      "Iteration 165, Batch: 7, Loss: 0.050650596618652344\n",
      "Iteration 165, Batch: 8, Loss: 0.05676104500889778\n",
      "Iteration 165, Batch: 9, Loss: 0.06703614443540573\n",
      "Iteration 165, Batch: 10, Loss: 0.06508363038301468\n",
      "Iteration 165, Batch: 11, Loss: 0.0747504010796547\n",
      "Iteration 165, Batch: 12, Loss: 0.12101156264543533\n",
      "Iteration 165, Batch: 13, Loss: 0.09544561058282852\n",
      "Iteration 165, Batch: 14, Loss: 0.06824199855327606\n",
      "Iteration 165, Batch: 15, Loss: 0.0702841579914093\n",
      "Iteration 165, Batch: 16, Loss: 0.08662839978933334\n",
      "Iteration 165, Batch: 17, Loss: 0.043434321880340576\n",
      "Iteration 165, Batch: 18, Loss: 0.0852321982383728\n",
      "Iteration 165, Batch: 19, Loss: 0.07719950377941132\n",
      "Iteration 165, Batch: 20, Loss: 0.07623475790023804\n",
      "Iteration 165, Batch: 21, Loss: 0.06669389456510544\n",
      "Iteration 165, Batch: 22, Loss: 0.04034499078989029\n",
      "Iteration 165, Batch: 23, Loss: 0.07894913107156754\n",
      "Iteration 165, Batch: 24, Loss: 0.11222755163908005\n",
      "Iteration 165, Batch: 25, Loss: 0.08394180983304977\n",
      "Iteration 165, Batch: 26, Loss: 0.08425205945968628\n",
      "Iteration 165, Batch: 27, Loss: 0.07606546580791473\n",
      "Iteration 165, Batch: 28, Loss: 0.06761711835861206\n",
      "Iteration 165, Batch: 29, Loss: 0.07486803829669952\n",
      "Iteration 165, Batch: 30, Loss: 0.073079913854599\n",
      "Iteration 165, Batch: 31, Loss: 0.09112784266471863\n",
      "Iteration 165, Batch: 32, Loss: 0.05740760266780853\n",
      "Iteration 165, Batch: 33, Loss: 0.043036088347435\n",
      "Iteration 165, Batch: 34, Loss: 0.04254234582185745\n",
      "Iteration 165, Batch: 35, Loss: 0.035991087555885315\n",
      "Iteration 165, Batch: 36, Loss: 0.07443128526210785\n",
      "Iteration 165, Batch: 37, Loss: 0.06179204210639\n",
      "Iteration 165, Batch: 38, Loss: 0.03282003477215767\n",
      "Iteration 165, Batch: 39, Loss: 0.06609083712100983\n",
      "Iteration 165, Batch: 40, Loss: 0.08199331164360046\n",
      "Iteration 165, Batch: 41, Loss: 0.045450687408447266\n",
      "Iteration 165, Batch: 42, Loss: 0.07085856795310974\n",
      "Iteration 165, Batch: 43, Loss: 0.0766591802239418\n",
      "Iteration 165, Batch: 44, Loss: 0.11051183193922043\n",
      "Iteration 165, Batch: 45, Loss: 0.05292138084769249\n",
      "Iteration 165, Batch: 46, Loss: 0.0705011710524559\n",
      "Iteration 165, Batch: 47, Loss: 0.09440445899963379\n",
      "Iteration 165, Batch: 48, Loss: 0.0835651382803917\n",
      "Iteration 165, Batch: 49, Loss: 0.07776064425706863\n",
      "Iteration 166, Batch: 0, Loss: 0.08970868587493896\n",
      "Iteration 166, Batch: 1, Loss: 0.045630957931280136\n",
      "Iteration 166, Batch: 2, Loss: 0.06419586390256882\n",
      "Iteration 166, Batch: 3, Loss: 0.060960572212934494\n",
      "Iteration 166, Batch: 4, Loss: 0.0520564466714859\n",
      "Iteration 166, Batch: 5, Loss: 0.08823337405920029\n",
      "Iteration 166, Batch: 6, Loss: 0.07364294677972794\n",
      "Iteration 166, Batch: 7, Loss: 0.0749659314751625\n",
      "Iteration 166, Batch: 8, Loss: 0.05920342728495598\n",
      "Iteration 166, Batch: 9, Loss: 0.06916777789592743\n",
      "Iteration 166, Batch: 10, Loss: 0.045373812317848206\n",
      "Iteration 166, Batch: 11, Loss: 0.04780644178390503\n",
      "Iteration 166, Batch: 12, Loss: 0.06780543178319931\n",
      "Iteration 166, Batch: 13, Loss: 0.07406647503376007\n",
      "Iteration 166, Batch: 14, Loss: 0.0788118988275528\n",
      "Iteration 166, Batch: 15, Loss: 0.08992789685726166\n",
      "Iteration 166, Batch: 16, Loss: 0.059775952249765396\n",
      "Iteration 166, Batch: 17, Loss: 0.04942214861512184\n",
      "Iteration 166, Batch: 18, Loss: 0.0731903463602066\n",
      "Iteration 166, Batch: 19, Loss: 0.028397301211953163\n",
      "Iteration 166, Batch: 20, Loss: 0.06440287828445435\n",
      "Iteration 166, Batch: 21, Loss: 0.04988078400492668\n",
      "Iteration 166, Batch: 22, Loss: 0.07998955249786377\n",
      "Iteration 166, Batch: 23, Loss: 0.0799190104007721\n",
      "Iteration 166, Batch: 24, Loss: 0.0699356198310852\n",
      "Iteration 166, Batch: 25, Loss: 0.06098616495728493\n",
      "Iteration 166, Batch: 26, Loss: 0.037779245525598526\n",
      "Iteration 166, Batch: 27, Loss: 0.08617217093706131\n",
      "Iteration 166, Batch: 28, Loss: 0.06192820519208908\n",
      "Iteration 166, Batch: 29, Loss: 0.06205027922987938\n",
      "Iteration 166, Batch: 30, Loss: 0.06241895630955696\n",
      "Iteration 166, Batch: 31, Loss: 0.05913021042943001\n",
      "Iteration 166, Batch: 32, Loss: 0.05789535492658615\n",
      "Iteration 166, Batch: 33, Loss: 0.08734741806983948\n",
      "Iteration 166, Batch: 34, Loss: 0.07707735151052475\n",
      "Iteration 166, Batch: 35, Loss: 0.055778853595256805\n",
      "Iteration 166, Batch: 36, Loss: 0.062095291912555695\n",
      "Iteration 166, Batch: 37, Loss: 0.07181426882743835\n",
      "Iteration 166, Batch: 38, Loss: 0.07396108657121658\n",
      "Iteration 166, Batch: 39, Loss: 0.06233229488134384\n",
      "Iteration 166, Batch: 40, Loss: 0.056849583983421326\n",
      "Iteration 166, Batch: 41, Loss: 0.046813853085041046\n",
      "Iteration 166, Batch: 42, Loss: 0.04450979083776474\n",
      "Iteration 166, Batch: 43, Loss: 0.03140464797616005\n",
      "Iteration 166, Batch: 44, Loss: 0.04856112226843834\n",
      "Iteration 166, Batch: 45, Loss: 0.07953044772148132\n",
      "Iteration 166, Batch: 46, Loss: 0.052037522196769714\n",
      "Iteration 166, Batch: 47, Loss: 0.033057332038879395\n",
      "Iteration 166, Batch: 48, Loss: 0.0697147473692894\n",
      "Iteration 166, Batch: 49, Loss: 0.06824474781751633\n",
      "Iteration 167, Batch: 0, Loss: 0.0671241283416748\n",
      "Iteration 167, Batch: 1, Loss: 0.03816003352403641\n",
      "Iteration 167, Batch: 2, Loss: 0.04468433931469917\n",
      "Iteration 167, Batch: 3, Loss: 0.06273477524518967\n",
      "Iteration 167, Batch: 4, Loss: 0.019920838996767998\n",
      "Iteration 167, Batch: 5, Loss: 0.03988140821456909\n",
      "Iteration 167, Batch: 6, Loss: 0.07003944367170334\n",
      "Iteration 167, Batch: 7, Loss: 0.06338600814342499\n",
      "Iteration 167, Batch: 8, Loss: 0.06935062259435654\n",
      "Iteration 167, Batch: 9, Loss: 0.057736605405807495\n",
      "Iteration 167, Batch: 10, Loss: 0.059023551642894745\n",
      "Iteration 167, Batch: 11, Loss: 0.07552662491798401\n",
      "Iteration 167, Batch: 12, Loss: 0.0576927550137043\n",
      "Iteration 167, Batch: 13, Loss: 0.0715530514717102\n",
      "Iteration 167, Batch: 14, Loss: 0.08682500571012497\n",
      "Iteration 167, Batch: 15, Loss: 0.08322729170322418\n",
      "Iteration 167, Batch: 16, Loss: 0.07588410377502441\n",
      "Iteration 167, Batch: 17, Loss: 0.03836345672607422\n",
      "Iteration 167, Batch: 18, Loss: 0.05712338909506798\n",
      "Iteration 167, Batch: 19, Loss: 0.08092565089464188\n",
      "Iteration 167, Batch: 20, Loss: 0.05407797545194626\n",
      "Iteration 167, Batch: 21, Loss: 0.06886718422174454\n",
      "Iteration 167, Batch: 22, Loss: 0.07984673976898193\n",
      "Iteration 167, Batch: 23, Loss: 0.05540509149432182\n",
      "Iteration 167, Batch: 24, Loss: 0.04580539092421532\n",
      "Iteration 167, Batch: 25, Loss: 0.08214326947927475\n",
      "Iteration 167, Batch: 26, Loss: 0.06669256836175919\n",
      "Iteration 167, Batch: 27, Loss: 0.06043602153658867\n",
      "Iteration 167, Batch: 28, Loss: 0.05074004828929901\n",
      "Iteration 167, Batch: 29, Loss: 0.06138802692294121\n",
      "Iteration 167, Batch: 30, Loss: 0.05883399024605751\n",
      "Iteration 167, Batch: 31, Loss: 0.08200918138027191\n",
      "Iteration 167, Batch: 32, Loss: 0.06604444980621338\n",
      "Iteration 167, Batch: 33, Loss: 0.06274167448282242\n",
      "Iteration 167, Batch: 34, Loss: 0.06564405560493469\n",
      "Iteration 167, Batch: 35, Loss: 0.050082024186849594\n",
      "Iteration 167, Batch: 36, Loss: 0.06645198911428452\n",
      "Iteration 167, Batch: 37, Loss: 0.05254027247428894\n",
      "Iteration 167, Batch: 38, Loss: 0.07513700425624847\n",
      "Iteration 167, Batch: 39, Loss: 0.05238712206482887\n",
      "Iteration 167, Batch: 40, Loss: 0.029496721923351288\n",
      "Iteration 167, Batch: 41, Loss: 0.06922102719545364\n",
      "Iteration 167, Batch: 42, Loss: 0.05288157984614372\n",
      "Iteration 167, Batch: 43, Loss: 0.054424140602350235\n",
      "Iteration 167, Batch: 44, Loss: 0.07761276513338089\n",
      "Iteration 167, Batch: 45, Loss: 0.07216174900531769\n",
      "Iteration 167, Batch: 46, Loss: 0.03763965144753456\n",
      "Iteration 167, Batch: 47, Loss: 0.05378558486700058\n",
      "Iteration 167, Batch: 48, Loss: 0.07676449418067932\n",
      "Iteration 167, Batch: 49, Loss: 0.08781573921442032\n",
      "Iteration 168, Batch: 0, Loss: 0.067349873483181\n",
      "Iteration 168, Batch: 1, Loss: 0.08562088012695312\n",
      "Iteration 168, Batch: 2, Loss: 0.0630202442407608\n",
      "Iteration 168, Batch: 3, Loss: 0.060722894966602325\n",
      "Iteration 168, Batch: 4, Loss: 0.06482318043708801\n",
      "Iteration 168, Batch: 5, Loss: 0.04039526730775833\n",
      "Iteration 168, Batch: 6, Loss: 0.06918124109506607\n",
      "Iteration 168, Batch: 7, Loss: 0.06796132773160934\n",
      "Iteration 168, Batch: 8, Loss: 0.08003412187099457\n",
      "Iteration 168, Batch: 9, Loss: 0.07097705453634262\n",
      "Iteration 168, Batch: 10, Loss: 0.07906579971313477\n",
      "Iteration 168, Batch: 11, Loss: 0.06988737732172012\n",
      "Iteration 168, Batch: 12, Loss: 0.054440688341856\n",
      "Iteration 168, Batch: 13, Loss: 0.03921990469098091\n",
      "Iteration 168, Batch: 14, Loss: 0.06414254754781723\n",
      "Iteration 168, Batch: 15, Loss: 0.03945785388350487\n",
      "Iteration 168, Batch: 16, Loss: 0.05248350650072098\n",
      "Iteration 168, Batch: 17, Loss: 0.049273088574409485\n",
      "Iteration 168, Batch: 18, Loss: 0.04739506170153618\n",
      "Iteration 168, Batch: 19, Loss: 0.06287150830030441\n",
      "Iteration 168, Batch: 20, Loss: 0.050637826323509216\n",
      "Iteration 168, Batch: 21, Loss: 0.0718916803598404\n",
      "Iteration 168, Batch: 22, Loss: 0.09359916299581528\n",
      "Iteration 168, Batch: 23, Loss: 0.06306653469800949\n",
      "Iteration 168, Batch: 24, Loss: 0.07322810590267181\n",
      "Iteration 168, Batch: 25, Loss: 0.057355575263500214\n",
      "Iteration 168, Batch: 26, Loss: 0.0601448193192482\n",
      "Iteration 168, Batch: 27, Loss: 0.04143242910504341\n",
      "Iteration 168, Batch: 28, Loss: 0.07990829646587372\n",
      "Iteration 168, Batch: 29, Loss: 0.03823849931359291\n",
      "Iteration 168, Batch: 30, Loss: 0.04924633353948593\n",
      "Iteration 168, Batch: 31, Loss: 0.0819324180483818\n",
      "Iteration 168, Batch: 32, Loss: 0.0411798357963562\n",
      "Iteration 168, Batch: 33, Loss: 0.05157716944813728\n",
      "Iteration 168, Batch: 34, Loss: 0.04643761366605759\n",
      "Iteration 168, Batch: 35, Loss: 0.06407355517148972\n",
      "Iteration 168, Batch: 36, Loss: 0.05454333499073982\n",
      "Iteration 168, Batch: 37, Loss: 0.08520669490098953\n",
      "Iteration 168, Batch: 38, Loss: 0.05260559543967247\n",
      "Iteration 168, Batch: 39, Loss: 0.0912441611289978\n",
      "Iteration 168, Batch: 40, Loss: 0.041282158344984055\n",
      "Iteration 168, Batch: 41, Loss: 0.059499677270650864\n",
      "Iteration 168, Batch: 42, Loss: 0.06712665408849716\n",
      "Iteration 168, Batch: 43, Loss: 0.06604132056236267\n",
      "Iteration 168, Batch: 44, Loss: 0.06914833188056946\n",
      "Iteration 168, Batch: 45, Loss: 0.06518789380788803\n",
      "Iteration 168, Batch: 46, Loss: 0.04294387623667717\n",
      "Iteration 168, Batch: 47, Loss: 0.08309865742921829\n",
      "Iteration 168, Batch: 48, Loss: 0.04659319669008255\n",
      "Iteration 168, Batch: 49, Loss: 0.06708025932312012\n",
      "Iteration 169, Batch: 0, Loss: 0.04579651728272438\n",
      "Iteration 169, Batch: 1, Loss: 0.05173669755458832\n",
      "Iteration 169, Batch: 2, Loss: 0.07561926543712616\n",
      "Iteration 169, Batch: 3, Loss: 0.040805213153362274\n",
      "Iteration 169, Batch: 4, Loss: 0.10370923578739166\n",
      "Iteration 169, Batch: 5, Loss: 0.06837005913257599\n",
      "Iteration 169, Batch: 6, Loss: 0.08364392071962357\n",
      "Iteration 169, Batch: 7, Loss: 0.06629019230604172\n",
      "Iteration 169, Batch: 8, Loss: 0.07919156551361084\n",
      "Iteration 169, Batch: 9, Loss: 0.051440801471471786\n",
      "Iteration 169, Batch: 10, Loss: 0.0763382539153099\n",
      "Iteration 169, Batch: 11, Loss: 0.039984386414289474\n",
      "Iteration 169, Batch: 12, Loss: 0.06433737277984619\n",
      "Iteration 169, Batch: 13, Loss: 0.04849432408809662\n",
      "Iteration 169, Batch: 14, Loss: 0.0364035964012146\n",
      "Iteration 169, Batch: 15, Loss: 0.062471892684698105\n",
      "Iteration 169, Batch: 16, Loss: 0.060935456305742264\n",
      "Iteration 169, Batch: 17, Loss: 0.06070399656891823\n",
      "Iteration 169, Batch: 18, Loss: 0.06574923545122147\n",
      "Iteration 169, Batch: 19, Loss: 0.05929533764719963\n",
      "Iteration 169, Batch: 20, Loss: 0.07017725706100464\n",
      "Iteration 169, Batch: 21, Loss: 0.0715438649058342\n",
      "Iteration 169, Batch: 22, Loss: 0.041957251727581024\n",
      "Iteration 169, Batch: 23, Loss: 0.06591854244470596\n",
      "Iteration 169, Batch: 24, Loss: 0.04581659287214279\n",
      "Iteration 169, Batch: 25, Loss: 0.06638511270284653\n",
      "Iteration 169, Batch: 26, Loss: 0.06430749595165253\n",
      "Iteration 169, Batch: 27, Loss: 0.07650058716535568\n",
      "Iteration 169, Batch: 28, Loss: 0.10007885098457336\n",
      "Iteration 169, Batch: 29, Loss: 0.06395051628351212\n",
      "Iteration 169, Batch: 30, Loss: 0.051499929279088974\n",
      "Iteration 169, Batch: 31, Loss: 0.05382074415683746\n",
      "Iteration 169, Batch: 32, Loss: 0.07419022172689438\n",
      "Iteration 169, Batch: 33, Loss: 0.04908161237835884\n",
      "Iteration 169, Batch: 34, Loss: 0.049382228404283524\n",
      "Iteration 169, Batch: 35, Loss: 0.0783296748995781\n",
      "Iteration 169, Batch: 36, Loss: 0.06459763646125793\n",
      "Iteration 169, Batch: 37, Loss: 0.04956881329417229\n",
      "Iteration 169, Batch: 38, Loss: 0.06572206318378448\n",
      "Iteration 169, Batch: 39, Loss: 0.06438679993152618\n",
      "Iteration 169, Batch: 40, Loss: 0.06773724406957626\n",
      "Iteration 169, Batch: 41, Loss: 0.06329214572906494\n",
      "Iteration 169, Batch: 42, Loss: 0.08564704656600952\n",
      "Iteration 169, Batch: 43, Loss: 0.09214048087596893\n",
      "Iteration 169, Batch: 44, Loss: 0.11189326643943787\n",
      "Iteration 169, Batch: 45, Loss: 0.07562477141618729\n",
      "Iteration 169, Batch: 46, Loss: 0.0752202719449997\n",
      "Iteration 169, Batch: 47, Loss: 0.07808630913496017\n",
      "Iteration 169, Batch: 48, Loss: 0.08020099252462387\n",
      "Iteration 169, Batch: 49, Loss: 0.06176973134279251\n",
      "Iteration 170, Batch: 0, Loss: 0.10068359225988388\n",
      "Iteration 170, Batch: 1, Loss: 0.05177035182714462\n",
      "Iteration 170, Batch: 2, Loss: 0.07774875313043594\n",
      "Iteration 170, Batch: 3, Loss: 0.06811065226793289\n",
      "Iteration 170, Batch: 4, Loss: 0.06682734936475754\n",
      "Iteration 170, Batch: 5, Loss: 0.07139749079942703\n",
      "Iteration 170, Batch: 6, Loss: 0.0685139074921608\n",
      "Iteration 170, Batch: 7, Loss: 0.05422789603471756\n",
      "Iteration 170, Batch: 8, Loss: 0.07461097836494446\n",
      "Iteration 170, Batch: 9, Loss: 0.06296036392450333\n",
      "Iteration 170, Batch: 10, Loss: 0.08312354981899261\n",
      "Iteration 170, Batch: 11, Loss: 0.08983033895492554\n",
      "Iteration 170, Batch: 12, Loss: 0.0588192455470562\n",
      "Iteration 170, Batch: 13, Loss: 0.0872068926692009\n",
      "Iteration 170, Batch: 14, Loss: 0.0667896419763565\n",
      "Iteration 170, Batch: 15, Loss: 0.07027796655893326\n",
      "Iteration 170, Batch: 16, Loss: 0.10284238308668137\n",
      "Iteration 170, Batch: 17, Loss: 0.0632125735282898\n",
      "Iteration 170, Batch: 18, Loss: 0.08362911641597748\n",
      "Iteration 170, Batch: 19, Loss: 0.06978416442871094\n",
      "Iteration 170, Batch: 20, Loss: 0.08166496455669403\n",
      "Iteration 170, Batch: 21, Loss: 0.0675065666437149\n",
      "Iteration 170, Batch: 22, Loss: 0.07558324187994003\n",
      "Iteration 170, Batch: 23, Loss: 0.06854147464036942\n",
      "Iteration 170, Batch: 24, Loss: 0.07155822962522507\n",
      "Iteration 170, Batch: 25, Loss: 0.06416940689086914\n",
      "Iteration 170, Batch: 26, Loss: 0.050963059067726135\n",
      "Iteration 170, Batch: 27, Loss: 0.0666424036026001\n",
      "Iteration 170, Batch: 28, Loss: 0.06158643588423729\n",
      "Iteration 170, Batch: 29, Loss: 0.07645700871944427\n",
      "Iteration 170, Batch: 30, Loss: 0.07653190195560455\n",
      "Iteration 170, Batch: 31, Loss: 0.05558178201317787\n",
      "Iteration 170, Batch: 32, Loss: 0.060079123824834824\n",
      "Iteration 170, Batch: 33, Loss: 0.06808740645647049\n",
      "Iteration 170, Batch: 34, Loss: 0.08973503112792969\n",
      "Iteration 170, Batch: 35, Loss: 0.06649447977542877\n",
      "Iteration 170, Batch: 36, Loss: 0.0779256522655487\n",
      "Iteration 170, Batch: 37, Loss: 0.05789430812001228\n",
      "Iteration 170, Batch: 38, Loss: 0.07390483468770981\n",
      "Iteration 170, Batch: 39, Loss: 0.09730853885412216\n",
      "Iteration 170, Batch: 40, Loss: 0.10140607506036758\n",
      "Iteration 170, Batch: 41, Loss: 0.09605283290147781\n",
      "Iteration 170, Batch: 42, Loss: 0.11054298281669617\n",
      "Iteration 170, Batch: 43, Loss: 0.10240047425031662\n",
      "Iteration 170, Batch: 44, Loss: 0.13007186353206635\n",
      "Iteration 170, Batch: 45, Loss: 0.13786230981349945\n",
      "Iteration 170, Batch: 46, Loss: 0.09913495928049088\n",
      "Iteration 170, Batch: 47, Loss: 0.12593647837638855\n",
      "Iteration 170, Batch: 48, Loss: 0.07611088454723358\n",
      "Iteration 170, Batch: 49, Loss: 0.10695430636405945\n",
      "Iteration 171, Batch: 0, Loss: 0.06072787195444107\n",
      "Iteration 171, Batch: 1, Loss: 0.07231291383504868\n",
      "Iteration 171, Batch: 2, Loss: 0.04806194826960564\n",
      "Iteration 171, Batch: 3, Loss: 0.07249126583337784\n",
      "Iteration 171, Batch: 4, Loss: 0.06484346091747284\n",
      "Iteration 171, Batch: 5, Loss: 0.07942430675029755\n",
      "Iteration 171, Batch: 6, Loss: 0.08009710162878036\n",
      "Iteration 171, Batch: 7, Loss: 0.05555352941155434\n",
      "Iteration 171, Batch: 8, Loss: 0.08330873399972916\n",
      "Iteration 171, Batch: 9, Loss: 0.10148847103118896\n",
      "Iteration 171, Batch: 10, Loss: 0.07453300058841705\n",
      "Iteration 171, Batch: 11, Loss: 0.061052355915308\n",
      "Iteration 171, Batch: 12, Loss: 0.053651370108127594\n",
      "Iteration 171, Batch: 13, Loss: 0.06369060277938843\n",
      "Iteration 171, Batch: 14, Loss: 0.07779330015182495\n",
      "Iteration 171, Batch: 15, Loss: 0.058899447321891785\n",
      "Iteration 171, Batch: 16, Loss: 0.07828879356384277\n",
      "Iteration 171, Batch: 17, Loss: 0.0873854011297226\n",
      "Iteration 171, Batch: 18, Loss: 0.06618793308734894\n",
      "Iteration 171, Batch: 19, Loss: 0.08410368859767914\n",
      "Iteration 171, Batch: 20, Loss: 0.08586375415325165\n",
      "Iteration 171, Batch: 21, Loss: 0.05769374594092369\n",
      "Iteration 171, Batch: 22, Loss: 0.07812555879354477\n",
      "Iteration 171, Batch: 23, Loss: 0.09663783758878708\n",
      "Iteration 171, Batch: 24, Loss: 0.04672740772366524\n",
      "Iteration 171, Batch: 25, Loss: 0.07013881951570511\n",
      "Iteration 171, Batch: 26, Loss: 0.05437244474887848\n",
      "Iteration 171, Batch: 27, Loss: 0.09972655028104782\n",
      "Iteration 171, Batch: 28, Loss: 0.06696218252182007\n",
      "Iteration 171, Batch: 29, Loss: 0.05205364525318146\n",
      "Iteration 171, Batch: 30, Loss: 0.09144202619791031\n",
      "Iteration 171, Batch: 31, Loss: 0.060833338648080826\n",
      "Iteration 171, Batch: 32, Loss: 0.06944882869720459\n",
      "Iteration 171, Batch: 33, Loss: 0.0673203244805336\n",
      "Iteration 171, Batch: 34, Loss: 0.06792981922626495\n",
      "Iteration 171, Batch: 35, Loss: 0.06674916297197342\n",
      "Iteration 171, Batch: 36, Loss: 0.08422563970088959\n",
      "Iteration 171, Batch: 37, Loss: 0.07226220518350601\n",
      "Iteration 171, Batch: 38, Loss: 0.05437250807881355\n",
      "Iteration 171, Batch: 39, Loss: 0.049608103930950165\n",
      "Iteration 171, Batch: 40, Loss: 0.06671224534511566\n",
      "Iteration 171, Batch: 41, Loss: 0.05248142033815384\n",
      "Iteration 171, Batch: 42, Loss: 0.07881651818752289\n",
      "Iteration 171, Batch: 43, Loss: 0.04521981254220009\n",
      "Iteration 171, Batch: 44, Loss: 0.046780142933130264\n",
      "Iteration 171, Batch: 45, Loss: 0.06586888432502747\n",
      "Iteration 171, Batch: 46, Loss: 0.05908701941370964\n",
      "Iteration 171, Batch: 47, Loss: 0.06424665451049805\n",
      "Iteration 171, Batch: 48, Loss: 0.060305945575237274\n",
      "Iteration 171, Batch: 49, Loss: 0.06994963437318802\n",
      "Iteration 172, Batch: 0, Loss: 0.04767097532749176\n",
      "Iteration 172, Batch: 1, Loss: 0.07182740420103073\n",
      "Iteration 172, Batch: 2, Loss: 0.06344334781169891\n",
      "Iteration 172, Batch: 3, Loss: 0.0488504096865654\n",
      "Iteration 172, Batch: 4, Loss: 0.09188009798526764\n",
      "Iteration 172, Batch: 5, Loss: 0.066836416721344\n",
      "Iteration 172, Batch: 6, Loss: 0.036461569368839264\n",
      "Iteration 172, Batch: 7, Loss: 0.04602871835231781\n",
      "Iteration 172, Batch: 8, Loss: 0.0663786455988884\n",
      "Iteration 172, Batch: 9, Loss: 0.07197734713554382\n",
      "Iteration 172, Batch: 10, Loss: 0.064279705286026\n",
      "Iteration 172, Batch: 11, Loss: 0.06511934101581573\n",
      "Iteration 172, Batch: 12, Loss: 0.0676475465297699\n",
      "Iteration 172, Batch: 13, Loss: 0.04818364977836609\n",
      "Iteration 172, Batch: 14, Loss: 0.06640677154064178\n",
      "Iteration 172, Batch: 15, Loss: 0.056617945432662964\n",
      "Iteration 172, Batch: 16, Loss: 0.054479215294122696\n",
      "Iteration 172, Batch: 17, Loss: 0.07665115594863892\n",
      "Iteration 172, Batch: 18, Loss: 0.06641578674316406\n",
      "Iteration 172, Batch: 19, Loss: 0.0767388716340065\n",
      "Iteration 172, Batch: 20, Loss: 0.06067757308483124\n",
      "Iteration 172, Batch: 21, Loss: 0.09739671647548676\n",
      "Iteration 172, Batch: 22, Loss: 0.08113573491573334\n",
      "Iteration 172, Batch: 23, Loss: 0.08553365617990494\n",
      "Iteration 172, Batch: 24, Loss: 0.0738147646188736\n",
      "Iteration 172, Batch: 25, Loss: 0.06129040569067001\n",
      "Iteration 172, Batch: 26, Loss: 0.06901969015598297\n",
      "Iteration 172, Batch: 27, Loss: 0.031554002314805984\n",
      "Iteration 172, Batch: 28, Loss: 0.032727744430303574\n",
      "Iteration 172, Batch: 29, Loss: 0.07190892845392227\n",
      "Iteration 172, Batch: 30, Loss: 0.0754173994064331\n",
      "Iteration 172, Batch: 31, Loss: 0.07163013517856598\n",
      "Iteration 172, Batch: 32, Loss: 0.09580285847187042\n",
      "Iteration 172, Batch: 33, Loss: 0.045693378895521164\n",
      "Iteration 172, Batch: 34, Loss: 0.05643640458583832\n",
      "Iteration 172, Batch: 35, Loss: 0.09462550282478333\n",
      "Iteration 172, Batch: 36, Loss: 0.06299042701721191\n",
      "Iteration 172, Batch: 37, Loss: 0.06482384353876114\n",
      "Iteration 172, Batch: 38, Loss: 0.06790056079626083\n",
      "Iteration 172, Batch: 39, Loss: 0.07705935090780258\n",
      "Iteration 172, Batch: 40, Loss: 0.05808400362730026\n",
      "Iteration 172, Batch: 41, Loss: 0.0809825137257576\n",
      "Iteration 172, Batch: 42, Loss: 0.0672263354063034\n",
      "Iteration 172, Batch: 43, Loss: 0.08341697603464127\n",
      "Iteration 172, Batch: 44, Loss: 0.07996705174446106\n",
      "Iteration 172, Batch: 45, Loss: 0.07227896898984909\n",
      "Iteration 172, Batch: 46, Loss: 0.031214281916618347\n",
      "Iteration 172, Batch: 47, Loss: 0.08963517844676971\n",
      "Iteration 172, Batch: 48, Loss: 0.04636681079864502\n",
      "Iteration 172, Batch: 49, Loss: 0.05856635794043541\n",
      "Iteration 173, Batch: 0, Loss: 0.06537608802318573\n",
      "Iteration 173, Batch: 1, Loss: 0.0792674720287323\n",
      "Iteration 173, Batch: 2, Loss: 0.06211978942155838\n",
      "Iteration 173, Batch: 3, Loss: 0.09980573505163193\n",
      "Iteration 173, Batch: 4, Loss: 0.05649708956480026\n",
      "Iteration 173, Batch: 5, Loss: 0.07264690846204758\n",
      "Iteration 173, Batch: 6, Loss: 0.07901264727115631\n",
      "Iteration 173, Batch: 7, Loss: 0.06422161310911179\n",
      "Iteration 173, Batch: 8, Loss: 0.06119837611913681\n",
      "Iteration 173, Batch: 9, Loss: 0.06742368638515472\n",
      "Iteration 173, Batch: 10, Loss: 0.06216413527727127\n",
      "Iteration 173, Batch: 11, Loss: 0.044765766710042953\n",
      "Iteration 173, Batch: 12, Loss: 0.06387399137020111\n",
      "Iteration 173, Batch: 13, Loss: 0.06631685048341751\n",
      "Iteration 173, Batch: 14, Loss: 0.04213913902640343\n",
      "Iteration 173, Batch: 15, Loss: 0.04166141524910927\n",
      "Iteration 173, Batch: 16, Loss: 0.08143802732229233\n",
      "Iteration 173, Batch: 17, Loss: 0.04540130868554115\n",
      "Iteration 173, Batch: 18, Loss: 0.07327131927013397\n",
      "Iteration 173, Batch: 19, Loss: 0.07165227830410004\n",
      "Iteration 173, Batch: 20, Loss: 0.04163135588169098\n",
      "Iteration 173, Batch: 21, Loss: 0.06647616624832153\n",
      "Iteration 173, Batch: 22, Loss: 0.04924122616648674\n",
      "Iteration 173, Batch: 23, Loss: 0.0583108514547348\n",
      "Iteration 173, Batch: 24, Loss: 0.07513606548309326\n",
      "Iteration 173, Batch: 25, Loss: 0.06687269359827042\n",
      "Iteration 173, Batch: 26, Loss: 0.05432875454425812\n",
      "Iteration 173, Batch: 27, Loss: 0.06174766272306442\n",
      "Iteration 173, Batch: 28, Loss: 0.062406327575445175\n",
      "Iteration 173, Batch: 29, Loss: 0.06035299226641655\n",
      "Iteration 173, Batch: 30, Loss: 0.047379035502672195\n",
      "Iteration 173, Batch: 31, Loss: 0.07603011280298233\n",
      "Iteration 173, Batch: 32, Loss: 0.03815219923853874\n",
      "Iteration 173, Batch: 33, Loss: 0.07938463240861893\n",
      "Iteration 173, Batch: 34, Loss: 0.07244771718978882\n",
      "Iteration 173, Batch: 35, Loss: 0.07856179028749466\n",
      "Iteration 173, Batch: 36, Loss: 0.09733997285366058\n",
      "Iteration 173, Batch: 37, Loss: 0.06362789124250412\n",
      "Iteration 173, Batch: 38, Loss: 0.06741676479578018\n",
      "Iteration 173, Batch: 39, Loss: 0.04794508218765259\n",
      "Iteration 173, Batch: 40, Loss: 0.054718054831027985\n",
      "Iteration 173, Batch: 41, Loss: 0.050884317606687546\n",
      "Iteration 173, Batch: 42, Loss: 0.09043274074792862\n",
      "Iteration 173, Batch: 43, Loss: 0.08925031125545502\n",
      "Iteration 173, Batch: 44, Loss: 0.05326688662171364\n",
      "Iteration 173, Batch: 45, Loss: 0.07343368232250214\n",
      "Iteration 173, Batch: 46, Loss: 0.060536548495292664\n",
      "Iteration 173, Batch: 47, Loss: 0.06055372953414917\n",
      "Iteration 173, Batch: 48, Loss: 0.06846301257610321\n",
      "Iteration 173, Batch: 49, Loss: 0.06972198933362961\n",
      "Iteration 174, Batch: 0, Loss: 0.07387199252843857\n",
      "Iteration 174, Batch: 1, Loss: 0.051686517894268036\n",
      "Iteration 174, Batch: 2, Loss: 0.07795146107673645\n",
      "Iteration 174, Batch: 3, Loss: 0.04991234093904495\n",
      "Iteration 174, Batch: 4, Loss: 0.06213632598519325\n",
      "Iteration 174, Batch: 5, Loss: 0.05522550642490387\n",
      "Iteration 174, Batch: 6, Loss: 0.05240713059902191\n",
      "Iteration 174, Batch: 7, Loss: 0.07449589669704437\n",
      "Iteration 174, Batch: 8, Loss: 0.08060676604509354\n",
      "Iteration 174, Batch: 9, Loss: 0.04251005873084068\n",
      "Iteration 174, Batch: 10, Loss: 0.06762993335723877\n",
      "Iteration 174, Batch: 11, Loss: 0.05824371054768562\n",
      "Iteration 174, Batch: 12, Loss: 0.09632962942123413\n",
      "Iteration 174, Batch: 13, Loss: 0.07972356677055359\n",
      "Iteration 174, Batch: 14, Loss: 0.05146683007478714\n",
      "Iteration 174, Batch: 15, Loss: 0.04819886013865471\n",
      "Iteration 174, Batch: 16, Loss: 0.09130832552909851\n",
      "Iteration 174, Batch: 17, Loss: 0.05198604613542557\n",
      "Iteration 174, Batch: 18, Loss: 0.06374437361955643\n",
      "Iteration 174, Batch: 19, Loss: 0.048323314636945724\n",
      "Iteration 174, Batch: 20, Loss: 0.04515396058559418\n",
      "Iteration 174, Batch: 21, Loss: 0.10264063626527786\n",
      "Iteration 174, Batch: 22, Loss: 0.06666245311498642\n",
      "Iteration 174, Batch: 23, Loss: 0.07453569024801254\n",
      "Iteration 174, Batch: 24, Loss: 0.07585559040307999\n",
      "Iteration 174, Batch: 25, Loss: 0.0589781329035759\n",
      "Iteration 174, Batch: 26, Loss: 0.08764727413654327\n",
      "Iteration 174, Batch: 27, Loss: 0.07778797298669815\n",
      "Iteration 174, Batch: 28, Loss: 0.07699351012706757\n",
      "Iteration 174, Batch: 29, Loss: 0.07424598932266235\n",
      "Iteration 174, Batch: 30, Loss: 0.09334083646535873\n",
      "Iteration 174, Batch: 31, Loss: 0.06993658095598221\n",
      "Iteration 174, Batch: 32, Loss: 0.06555033475160599\n",
      "Iteration 174, Batch: 33, Loss: 0.07177992910146713\n",
      "Iteration 174, Batch: 34, Loss: 0.04847414791584015\n",
      "Iteration 174, Batch: 35, Loss: 0.074506014585495\n",
      "Iteration 174, Batch: 36, Loss: 0.05170020833611488\n",
      "Iteration 174, Batch: 37, Loss: 0.04271286725997925\n",
      "Iteration 174, Batch: 38, Loss: 0.08018390834331512\n",
      "Iteration 174, Batch: 39, Loss: 0.0657842829823494\n",
      "Iteration 174, Batch: 40, Loss: 0.08343608677387238\n",
      "Iteration 174, Batch: 41, Loss: 0.09244905412197113\n",
      "Iteration 174, Batch: 42, Loss: 0.07424846291542053\n",
      "Iteration 174, Batch: 43, Loss: 0.05768929049372673\n",
      "Iteration 174, Batch: 44, Loss: 0.05720795318484306\n",
      "Iteration 174, Batch: 45, Loss: 0.05239483714103699\n",
      "Iteration 174, Batch: 46, Loss: 0.06597726047039032\n",
      "Iteration 174, Batch: 47, Loss: 0.06192364543676376\n",
      "Iteration 174, Batch: 48, Loss: 0.07867302000522614\n",
      "Iteration 174, Batch: 49, Loss: 0.05357066169381142\n",
      "Iteration 175, Batch: 0, Loss: 0.061433594673871994\n",
      "Iteration 175, Batch: 1, Loss: 0.06859558820724487\n",
      "Iteration 175, Batch: 2, Loss: 0.07714340835809708\n",
      "Iteration 175, Batch: 3, Loss: 0.03498862683773041\n",
      "Iteration 175, Batch: 4, Loss: 0.03528090938925743\n",
      "Iteration 175, Batch: 5, Loss: 0.06253328919410706\n",
      "Iteration 175, Batch: 6, Loss: 0.0744679644703865\n",
      "Iteration 175, Batch: 7, Loss: 0.04703867807984352\n",
      "Iteration 175, Batch: 8, Loss: 0.057818859815597534\n",
      "Iteration 175, Batch: 9, Loss: 0.05181832239031792\n",
      "Iteration 175, Batch: 10, Loss: 0.0779961347579956\n",
      "Iteration 175, Batch: 11, Loss: 0.0330730676651001\n",
      "Iteration 175, Batch: 12, Loss: 0.06261295825242996\n",
      "Iteration 175, Batch: 13, Loss: 0.0674002394080162\n",
      "Iteration 175, Batch: 14, Loss: 0.04045262187719345\n",
      "Iteration 175, Batch: 15, Loss: 0.058061741292476654\n",
      "Iteration 175, Batch: 16, Loss: 0.053552500903606415\n",
      "Iteration 175, Batch: 17, Loss: 0.06280854344367981\n",
      "Iteration 175, Batch: 18, Loss: 0.04341272637248039\n",
      "Iteration 175, Batch: 19, Loss: 0.10388248413801193\n",
      "Iteration 175, Batch: 20, Loss: 0.057538993656635284\n",
      "Iteration 175, Batch: 21, Loss: 0.06547117978334427\n",
      "Iteration 175, Batch: 22, Loss: 0.07591644674539566\n",
      "Iteration 175, Batch: 23, Loss: 0.0607612319290638\n",
      "Iteration 175, Batch: 24, Loss: 0.05396775156259537\n",
      "Iteration 175, Batch: 25, Loss: 0.08806342631578445\n",
      "Iteration 175, Batch: 26, Loss: 0.07537110894918442\n",
      "Iteration 175, Batch: 27, Loss: 0.08217604458332062\n",
      "Iteration 175, Batch: 28, Loss: 0.040179770439863205\n",
      "Iteration 175, Batch: 29, Loss: 0.04430641606450081\n",
      "Iteration 175, Batch: 30, Loss: 0.08509872108697891\n",
      "Iteration 175, Batch: 31, Loss: 0.06460434943437576\n",
      "Iteration 175, Batch: 32, Loss: 0.05505617335438728\n",
      "Iteration 175, Batch: 33, Loss: 0.08511462807655334\n",
      "Iteration 175, Batch: 34, Loss: 0.08482576906681061\n",
      "Iteration 175, Batch: 35, Loss: 0.06585656851530075\n",
      "Iteration 175, Batch: 36, Loss: 0.0621383897960186\n",
      "Iteration 175, Batch: 37, Loss: 0.05938667058944702\n",
      "Iteration 175, Batch: 38, Loss: 0.05175188556313515\n",
      "Iteration 175, Batch: 39, Loss: 0.05422442406415939\n",
      "Iteration 175, Batch: 40, Loss: 0.062316082417964935\n",
      "Iteration 175, Batch: 41, Loss: 0.08089593052864075\n",
      "Iteration 175, Batch: 42, Loss: 0.09464292228221893\n",
      "Iteration 175, Batch: 43, Loss: 0.03228893503546715\n",
      "Iteration 175, Batch: 44, Loss: 0.05305089056491852\n",
      "Iteration 175, Batch: 45, Loss: 0.07345432788133621\n",
      "Iteration 175, Batch: 46, Loss: 0.07444166392087936\n",
      "Iteration 175, Batch: 47, Loss: 0.06802085041999817\n",
      "Iteration 175, Batch: 48, Loss: 0.09626004099845886\n",
      "Iteration 175, Batch: 49, Loss: 0.06939317286014557\n",
      "Iteration 176, Batch: 0, Loss: 0.06164835765957832\n",
      "Iteration 176, Batch: 1, Loss: 0.07653457671403885\n",
      "Iteration 176, Batch: 2, Loss: 0.06171597167849541\n",
      "Iteration 176, Batch: 3, Loss: 0.0698670819401741\n",
      "Iteration 176, Batch: 4, Loss: 0.054601095616817474\n",
      "Iteration 176, Batch: 5, Loss: 0.051164641976356506\n",
      "Iteration 176, Batch: 6, Loss: 0.10191632807254791\n",
      "Iteration 176, Batch: 7, Loss: 0.07385727018117905\n",
      "Iteration 176, Batch: 8, Loss: 0.07602767646312714\n",
      "Iteration 176, Batch: 9, Loss: 0.08369410037994385\n",
      "Iteration 176, Batch: 10, Loss: 0.08531786501407623\n",
      "Iteration 176, Batch: 11, Loss: 0.0845983698964119\n",
      "Iteration 176, Batch: 12, Loss: 0.07081697136163712\n",
      "Iteration 176, Batch: 13, Loss: 0.06382936984300613\n",
      "Iteration 176, Batch: 14, Loss: 0.07693974673748016\n",
      "Iteration 176, Batch: 15, Loss: 0.07672373950481415\n",
      "Iteration 176, Batch: 16, Loss: 0.07145777344703674\n",
      "Iteration 176, Batch: 17, Loss: 0.06342722475528717\n",
      "Iteration 176, Batch: 18, Loss: 0.04970550537109375\n",
      "Iteration 176, Batch: 19, Loss: 0.07461724430322647\n",
      "Iteration 176, Batch: 20, Loss: 0.051875803619623184\n",
      "Iteration 176, Batch: 21, Loss: 0.04417959228157997\n",
      "Iteration 176, Batch: 22, Loss: 0.042024269700050354\n",
      "Iteration 176, Batch: 23, Loss: 0.0661604180932045\n",
      "Iteration 176, Batch: 24, Loss: 0.06486707180738449\n",
      "Iteration 176, Batch: 25, Loss: 0.0676497146487236\n",
      "Iteration 176, Batch: 26, Loss: 0.07695343345403671\n",
      "Iteration 176, Batch: 27, Loss: 0.06035828962922096\n",
      "Iteration 176, Batch: 28, Loss: 0.050120268017053604\n",
      "Iteration 176, Batch: 29, Loss: 0.09616008400917053\n",
      "Iteration 176, Batch: 30, Loss: 0.0797862559556961\n",
      "Iteration 176, Batch: 31, Loss: 0.06236143782734871\n",
      "Iteration 176, Batch: 32, Loss: 0.045496661216020584\n",
      "Iteration 176, Batch: 33, Loss: 0.07914422452449799\n",
      "Iteration 176, Batch: 34, Loss: 0.07202782481908798\n",
      "Iteration 176, Batch: 35, Loss: 0.07701650261878967\n",
      "Iteration 176, Batch: 36, Loss: 0.06238153949379921\n",
      "Iteration 176, Batch: 37, Loss: 0.05743878334760666\n",
      "Iteration 176, Batch: 38, Loss: 0.08864691853523254\n",
      "Iteration 176, Batch: 39, Loss: 0.06664174050092697\n",
      "Iteration 176, Batch: 40, Loss: 0.04564662277698517\n",
      "Iteration 176, Batch: 41, Loss: 0.06761051714420319\n",
      "Iteration 176, Batch: 42, Loss: 0.0549115315079689\n",
      "Iteration 176, Batch: 43, Loss: 0.08351451903581619\n",
      "Iteration 176, Batch: 44, Loss: 0.09008102118968964\n",
      "Iteration 176, Batch: 45, Loss: 0.06878839433193207\n",
      "Iteration 176, Batch: 46, Loss: 0.05803621560335159\n",
      "Iteration 176, Batch: 47, Loss: 0.04985988140106201\n",
      "Iteration 176, Batch: 48, Loss: 0.05622899904847145\n",
      "Iteration 176, Batch: 49, Loss: 0.04728930443525314\n",
      "Iteration 177, Batch: 0, Loss: 0.08641549199819565\n",
      "Iteration 177, Batch: 1, Loss: 0.0571327768266201\n",
      "Iteration 177, Batch: 2, Loss: 0.11238317936658859\n",
      "Iteration 177, Batch: 3, Loss: 0.045019738376140594\n",
      "Iteration 177, Batch: 4, Loss: 0.06604145467281342\n",
      "Iteration 177, Batch: 5, Loss: 0.07947570830583572\n",
      "Iteration 177, Batch: 6, Loss: 0.07498203217983246\n",
      "Iteration 177, Batch: 7, Loss: 0.06644457578659058\n",
      "Iteration 177, Batch: 8, Loss: 0.05032477155327797\n",
      "Iteration 177, Batch: 9, Loss: 0.059518106281757355\n",
      "Iteration 177, Batch: 10, Loss: 0.042679183185100555\n",
      "Iteration 177, Batch: 11, Loss: 0.05363159999251366\n",
      "Iteration 177, Batch: 12, Loss: 0.07518784701824188\n",
      "Iteration 177, Batch: 13, Loss: 0.0622498020529747\n",
      "Iteration 177, Batch: 14, Loss: 0.09847715497016907\n",
      "Iteration 177, Batch: 15, Loss: 0.07056538760662079\n",
      "Iteration 177, Batch: 16, Loss: 0.07839785516262054\n",
      "Iteration 177, Batch: 17, Loss: 0.10154159367084503\n",
      "Iteration 177, Batch: 18, Loss: 0.08498912304639816\n",
      "Iteration 177, Batch: 19, Loss: 0.0847126916050911\n",
      "Iteration 177, Batch: 20, Loss: 0.11972068250179291\n",
      "Iteration 177, Batch: 21, Loss: 0.07554995268583298\n",
      "Iteration 177, Batch: 22, Loss: 0.09151376783847809\n",
      "Iteration 177, Batch: 23, Loss: 0.07563143223524094\n",
      "Iteration 177, Batch: 24, Loss: 0.0791470929980278\n",
      "Iteration 177, Batch: 25, Loss: 0.05226222798228264\n",
      "Iteration 177, Batch: 26, Loss: 0.06380056589841843\n",
      "Iteration 177, Batch: 27, Loss: 0.06727507710456848\n",
      "Iteration 177, Batch: 28, Loss: 0.07765188068151474\n",
      "Iteration 177, Batch: 29, Loss: 0.058852918446063995\n",
      "Iteration 177, Batch: 30, Loss: 0.050134047865867615\n",
      "Iteration 177, Batch: 31, Loss: 0.08161812275648117\n",
      "Iteration 177, Batch: 32, Loss: 0.06177454814314842\n",
      "Iteration 177, Batch: 33, Loss: 0.10381780564785004\n",
      "Iteration 177, Batch: 34, Loss: 0.06949527561664581\n",
      "Iteration 177, Batch: 35, Loss: 0.06075400859117508\n",
      "Iteration 177, Batch: 36, Loss: 0.07862456887960434\n",
      "Iteration 177, Batch: 37, Loss: 0.07072017341852188\n",
      "Iteration 177, Batch: 38, Loss: 0.05578978732228279\n",
      "Iteration 177, Batch: 39, Loss: 0.10358985513448715\n",
      "Iteration 177, Batch: 40, Loss: 0.06818302720785141\n",
      "Iteration 177, Batch: 41, Loss: 0.08057378232479095\n",
      "Iteration 177, Batch: 42, Loss: 0.0777648538351059\n",
      "Iteration 177, Batch: 43, Loss: 0.10144682973623276\n",
      "Iteration 177, Batch: 44, Loss: 0.06789378076791763\n",
      "Iteration 177, Batch: 45, Loss: 0.052867721766233444\n",
      "Iteration 177, Batch: 46, Loss: 0.09305310249328613\n",
      "Iteration 177, Batch: 47, Loss: 0.07623617351055145\n",
      "Iteration 177, Batch: 48, Loss: 0.08311496675014496\n",
      "Iteration 177, Batch: 49, Loss: 0.07981639355421066\n",
      "Iteration 178, Batch: 0, Loss: 0.05646780878305435\n",
      "Iteration 178, Batch: 1, Loss: 0.11679527163505554\n",
      "Iteration 178, Batch: 2, Loss: 0.065719373524189\n",
      "Iteration 178, Batch: 3, Loss: 0.06904486566781998\n",
      "Iteration 178, Batch: 4, Loss: 0.054357461631298065\n",
      "Iteration 178, Batch: 5, Loss: 0.026519950479269028\n",
      "Iteration 178, Batch: 6, Loss: 0.05910027399659157\n",
      "Iteration 178, Batch: 7, Loss: 0.047683533281087875\n",
      "Iteration 178, Batch: 8, Loss: 0.058132629841566086\n",
      "Iteration 178, Batch: 9, Loss: 0.044318847358226776\n",
      "Iteration 178, Batch: 10, Loss: 0.05941302329301834\n",
      "Iteration 178, Batch: 11, Loss: 0.09404046833515167\n",
      "Iteration 178, Batch: 12, Loss: 0.05730408430099487\n",
      "Iteration 178, Batch: 13, Loss: 0.060348160564899445\n",
      "Iteration 178, Batch: 14, Loss: 0.04147201403975487\n",
      "Iteration 178, Batch: 15, Loss: 0.06983157992362976\n",
      "Iteration 178, Batch: 16, Loss: 0.08708955347537994\n",
      "Iteration 178, Batch: 17, Loss: 0.08121787756681442\n",
      "Iteration 178, Batch: 18, Loss: 0.05896756052970886\n",
      "Iteration 178, Batch: 19, Loss: 0.061698682606220245\n",
      "Iteration 178, Batch: 20, Loss: 0.08306865394115448\n",
      "Iteration 178, Batch: 21, Loss: 0.06428670138120651\n",
      "Iteration 178, Batch: 22, Loss: 0.07218237966299057\n",
      "Iteration 178, Batch: 23, Loss: 0.07221002131700516\n",
      "Iteration 178, Batch: 24, Loss: 0.06648567318916321\n",
      "Iteration 178, Batch: 25, Loss: 0.06318791955709457\n",
      "Iteration 178, Batch: 26, Loss: 0.07493136078119278\n",
      "Iteration 178, Batch: 27, Loss: 0.035661231726408005\n",
      "Iteration 178, Batch: 28, Loss: 0.07303614914417267\n",
      "Iteration 178, Batch: 29, Loss: 0.05773241072893143\n",
      "Iteration 178, Batch: 30, Loss: 0.08356686681509018\n",
      "Iteration 178, Batch: 31, Loss: 0.07268910855054855\n",
      "Iteration 178, Batch: 32, Loss: 0.03955342620611191\n",
      "Iteration 178, Batch: 33, Loss: 0.06117911636829376\n",
      "Iteration 178, Batch: 34, Loss: 0.07517273724079132\n",
      "Iteration 178, Batch: 35, Loss: 0.05158793181180954\n",
      "Iteration 178, Batch: 36, Loss: 0.043140850961208344\n",
      "Iteration 178, Batch: 37, Loss: 0.04837656766176224\n",
      "Iteration 178, Batch: 38, Loss: 0.05525164306163788\n",
      "Iteration 178, Batch: 39, Loss: 0.06609855592250824\n",
      "Iteration 178, Batch: 40, Loss: 0.082826629281044\n",
      "Iteration 178, Batch: 41, Loss: 0.07979423552751541\n",
      "Iteration 178, Batch: 42, Loss: 0.08258741348981857\n",
      "Iteration 178, Batch: 43, Loss: 0.06864365190267563\n",
      "Iteration 178, Batch: 44, Loss: 0.05722327530384064\n",
      "Iteration 178, Batch: 45, Loss: 0.08488410711288452\n",
      "Iteration 178, Batch: 46, Loss: 0.08602635562419891\n",
      "Iteration 178, Batch: 47, Loss: 0.05087963119149208\n",
      "Iteration 178, Batch: 48, Loss: 0.06385409086942673\n",
      "Iteration 178, Batch: 49, Loss: 0.043349459767341614\n",
      "Iteration 179, Batch: 0, Loss: 0.07145817577838898\n",
      "Iteration 179, Batch: 1, Loss: 0.07559823989868164\n",
      "Iteration 179, Batch: 2, Loss: 0.07106275111436844\n",
      "Iteration 179, Batch: 3, Loss: 0.05401133373379707\n",
      "Iteration 179, Batch: 4, Loss: 0.03448343276977539\n",
      "Iteration 179, Batch: 5, Loss: 0.07800060510635376\n",
      "Iteration 179, Batch: 6, Loss: 0.06645116955041885\n",
      "Iteration 179, Batch: 7, Loss: 0.05206996947526932\n",
      "Iteration 179, Batch: 8, Loss: 0.071697898209095\n",
      "Iteration 179, Batch: 9, Loss: 0.06049354374408722\n",
      "Iteration 179, Batch: 10, Loss: 0.06927348673343658\n",
      "Iteration 179, Batch: 11, Loss: 0.07701907306909561\n",
      "Iteration 179, Batch: 12, Loss: 0.05128513649106026\n",
      "Iteration 179, Batch: 13, Loss: 0.0868256464600563\n",
      "Iteration 179, Batch: 14, Loss: 0.08268805593252182\n",
      "Iteration 179, Batch: 15, Loss: 0.06831515580415726\n",
      "Iteration 179, Batch: 16, Loss: 0.07292607426643372\n",
      "Iteration 179, Batch: 17, Loss: 0.04348219186067581\n",
      "Iteration 179, Batch: 18, Loss: 0.06173263490200043\n",
      "Iteration 179, Batch: 19, Loss: 0.08060691505670547\n",
      "Iteration 179, Batch: 20, Loss: 0.060544319450855255\n",
      "Iteration 179, Batch: 21, Loss: 0.047706279903650284\n",
      "Iteration 179, Batch: 22, Loss: 0.07137787342071533\n",
      "Iteration 179, Batch: 23, Loss: 0.07254862040281296\n",
      "Iteration 179, Batch: 24, Loss: 0.07508459687232971\n",
      "Iteration 179, Batch: 25, Loss: 0.0795925185084343\n",
      "Iteration 179, Batch: 26, Loss: 0.04437434673309326\n",
      "Iteration 179, Batch: 27, Loss: 0.06750737130641937\n",
      "Iteration 179, Batch: 28, Loss: 0.07296296954154968\n",
      "Iteration 179, Batch: 29, Loss: 0.05460415780544281\n",
      "Iteration 179, Batch: 30, Loss: 0.056823018938302994\n",
      "Iteration 179, Batch: 31, Loss: 0.07526509463787079\n",
      "Iteration 179, Batch: 32, Loss: 0.08047544211149216\n",
      "Iteration 179, Batch: 33, Loss: 0.07866683602333069\n",
      "Iteration 179, Batch: 34, Loss: 0.032597657293081284\n",
      "Iteration 179, Batch: 35, Loss: 0.10096164792776108\n",
      "Iteration 179, Batch: 36, Loss: 0.07891552895307541\n",
      "Iteration 179, Batch: 37, Loss: 0.05176546052098274\n",
      "Iteration 179, Batch: 38, Loss: 0.049107082188129425\n",
      "Iteration 179, Batch: 39, Loss: 0.08551547676324844\n",
      "Iteration 179, Batch: 40, Loss: 0.079347625374794\n",
      "Iteration 179, Batch: 41, Loss: 0.0670163482427597\n",
      "Iteration 179, Batch: 42, Loss: 0.08433106541633606\n",
      "Iteration 179, Batch: 43, Loss: 0.06979759782552719\n",
      "Iteration 179, Batch: 44, Loss: 0.04777084290981293\n",
      "Iteration 179, Batch: 45, Loss: 0.0675615593791008\n",
      "Iteration 179, Batch: 46, Loss: 0.0729672834277153\n",
      "Iteration 179, Batch: 47, Loss: 0.08109209686517715\n",
      "Iteration 179, Batch: 48, Loss: 0.06381204724311829\n",
      "Iteration 179, Batch: 49, Loss: 0.05528317764401436\n",
      "Iteration 180, Batch: 0, Loss: 0.08763620257377625\n",
      "Iteration 180, Batch: 1, Loss: 0.03219619020819664\n",
      "Iteration 180, Batch: 2, Loss: 0.04522455483675003\n",
      "Iteration 180, Batch: 3, Loss: 0.05850209668278694\n",
      "Iteration 180, Batch: 4, Loss: 0.06661933660507202\n",
      "Iteration 180, Batch: 5, Loss: 0.06907565891742706\n",
      "Iteration 180, Batch: 6, Loss: 0.04026198759675026\n",
      "Iteration 180, Batch: 7, Loss: 0.053905729204416275\n",
      "Iteration 180, Batch: 8, Loss: 0.04085227847099304\n",
      "Iteration 180, Batch: 9, Loss: 0.05005054175853729\n",
      "Iteration 180, Batch: 10, Loss: 0.055837567895650864\n",
      "Iteration 180, Batch: 11, Loss: 0.06659554690122604\n",
      "Iteration 180, Batch: 12, Loss: 0.07369274646043777\n",
      "Iteration 180, Batch: 13, Loss: 0.051969509571790695\n",
      "Iteration 180, Batch: 14, Loss: 0.05095329135656357\n",
      "Iteration 180, Batch: 15, Loss: 0.07875854521989822\n",
      "Iteration 180, Batch: 16, Loss: 0.06218937784433365\n",
      "Iteration 180, Batch: 17, Loss: 0.06946056336164474\n",
      "Iteration 180, Batch: 18, Loss: 0.08498446643352509\n",
      "Iteration 180, Batch: 19, Loss: 0.04620860144495964\n",
      "Iteration 180, Batch: 20, Loss: 0.05350383743643761\n",
      "Iteration 180, Batch: 21, Loss: 0.09394485503435135\n",
      "Iteration 180, Batch: 22, Loss: 0.08567075431346893\n",
      "Iteration 180, Batch: 23, Loss: 0.09335527569055557\n",
      "Iteration 180, Batch: 24, Loss: 0.0886019840836525\n",
      "Iteration 180, Batch: 25, Loss: 0.07575196027755737\n",
      "Iteration 180, Batch: 26, Loss: 0.05306296795606613\n",
      "Iteration 180, Batch: 27, Loss: 0.05033967271447182\n",
      "Iteration 180, Batch: 28, Loss: 0.06048385053873062\n",
      "Iteration 180, Batch: 29, Loss: 0.06434892117977142\n",
      "Iteration 180, Batch: 30, Loss: 0.08771945536136627\n",
      "Iteration 180, Batch: 31, Loss: 0.02743348851799965\n",
      "Iteration 180, Batch: 32, Loss: 0.06890209764242172\n",
      "Iteration 180, Batch: 33, Loss: 0.2584145963191986\n",
      "Iteration 180, Batch: 34, Loss: 0.05826500430703163\n",
      "Iteration 180, Batch: 35, Loss: 0.030641650781035423\n",
      "Iteration 180, Batch: 36, Loss: 0.05919518321752548\n",
      "Iteration 180, Batch: 37, Loss: 0.06866714358329773\n",
      "Iteration 180, Batch: 38, Loss: 0.08166693150997162\n",
      "Iteration 180, Batch: 39, Loss: 0.05308492109179497\n",
      "Iteration 180, Batch: 40, Loss: 0.045785821974277496\n",
      "Iteration 180, Batch: 41, Loss: 0.04886936396360397\n",
      "Iteration 180, Batch: 42, Loss: 0.053362321108579636\n",
      "Iteration 180, Batch: 43, Loss: 0.06752648204565048\n",
      "Iteration 180, Batch: 44, Loss: 0.04980240762233734\n",
      "Iteration 180, Batch: 45, Loss: 0.07423041015863419\n",
      "Iteration 180, Batch: 46, Loss: 0.06408355385065079\n",
      "Iteration 180, Batch: 47, Loss: 0.3585125207901001\n",
      "Iteration 180, Batch: 48, Loss: 0.04468170925974846\n",
      "Iteration 180, Batch: 49, Loss: 0.04715491086244583\n",
      "Iteration 181, Batch: 0, Loss: 0.05537787824869156\n",
      "Iteration 181, Batch: 1, Loss: 0.05149301514029503\n",
      "Iteration 181, Batch: 2, Loss: 0.03535608574748039\n",
      "Iteration 181, Batch: 3, Loss: 0.06419169157743454\n",
      "Iteration 181, Batch: 4, Loss: 0.09399271011352539\n",
      "Iteration 181, Batch: 5, Loss: 0.05325812101364136\n",
      "Iteration 181, Batch: 6, Loss: 0.06535791605710983\n",
      "Iteration 181, Batch: 7, Loss: 0.064653679728508\n",
      "Iteration 181, Batch: 8, Loss: 0.06899319589138031\n",
      "Iteration 181, Batch: 9, Loss: 0.09741388261318207\n",
      "Iteration 181, Batch: 10, Loss: 0.07546208798885345\n",
      "Iteration 181, Batch: 11, Loss: 0.07843351364135742\n",
      "Iteration 181, Batch: 12, Loss: 0.08938651531934738\n",
      "Iteration 181, Batch: 13, Loss: 0.07809919863939285\n",
      "Iteration 181, Batch: 14, Loss: 0.0918194055557251\n",
      "Iteration 181, Batch: 15, Loss: 0.07001864165067673\n",
      "Iteration 181, Batch: 16, Loss: 0.10162930935621262\n",
      "Iteration 181, Batch: 17, Loss: 0.07844795286655426\n",
      "Iteration 181, Batch: 18, Loss: 0.08048084378242493\n",
      "Iteration 181, Batch: 19, Loss: 0.07651714980602264\n",
      "Iteration 181, Batch: 20, Loss: 0.09427659958600998\n",
      "Iteration 181, Batch: 21, Loss: 0.06845394521951675\n",
      "Iteration 181, Batch: 22, Loss: 0.06361553817987442\n",
      "Iteration 181, Batch: 23, Loss: 0.07336010783910751\n",
      "Iteration 181, Batch: 24, Loss: 0.08750516921281815\n",
      "Iteration 181, Batch: 25, Loss: 0.08496792614459991\n",
      "Iteration 181, Batch: 26, Loss: 0.08877822756767273\n",
      "Iteration 181, Batch: 27, Loss: 0.06665636599063873\n",
      "Iteration 181, Batch: 28, Loss: 0.050474490970373154\n",
      "Iteration 181, Batch: 29, Loss: 0.05902108922600746\n",
      "Iteration 181, Batch: 30, Loss: 0.06329967081546783\n",
      "Iteration 181, Batch: 31, Loss: 0.08101430535316467\n",
      "Iteration 181, Batch: 32, Loss: 0.043828826397657394\n",
      "Iteration 181, Batch: 33, Loss: 0.0729089006781578\n",
      "Iteration 181, Batch: 34, Loss: 0.09082843363285065\n",
      "Iteration 181, Batch: 35, Loss: 0.0418253093957901\n",
      "Iteration 181, Batch: 36, Loss: 0.053446751087903976\n",
      "Iteration 181, Batch: 37, Loss: 0.05919625982642174\n",
      "Iteration 181, Batch: 38, Loss: 0.04694619029760361\n",
      "Iteration 181, Batch: 39, Loss: 0.0644182413816452\n",
      "Iteration 181, Batch: 40, Loss: 0.07671284675598145\n",
      "Iteration 181, Batch: 41, Loss: 0.054500192403793335\n",
      "Iteration 181, Batch: 42, Loss: 0.10188207030296326\n",
      "Iteration 181, Batch: 43, Loss: 0.05669495090842247\n",
      "Iteration 181, Batch: 44, Loss: 0.07943395525217056\n",
      "Iteration 181, Batch: 45, Loss: 0.07641013711690903\n",
      "Iteration 181, Batch: 46, Loss: 0.0637516900897026\n",
      "Iteration 181, Batch: 47, Loss: 0.06271835416555405\n",
      "Iteration 181, Batch: 48, Loss: 0.05844583362340927\n",
      "Iteration 181, Batch: 49, Loss: 0.04453473165631294\n",
      "Iteration 182, Batch: 0, Loss: 0.08724256604909897\n",
      "Iteration 182, Batch: 1, Loss: 0.07611972838640213\n",
      "Iteration 182, Batch: 2, Loss: 0.08206361532211304\n",
      "Iteration 182, Batch: 3, Loss: 0.0627884492278099\n",
      "Iteration 182, Batch: 4, Loss: 0.0840151384472847\n",
      "Iteration 182, Batch: 5, Loss: 0.07653443515300751\n",
      "Iteration 182, Batch: 6, Loss: 0.08307857066392899\n",
      "Iteration 182, Batch: 7, Loss: 0.05817437916994095\n",
      "Iteration 182, Batch: 8, Loss: 0.046680111438035965\n",
      "Iteration 182, Batch: 9, Loss: 0.08212585002183914\n",
      "Iteration 182, Batch: 10, Loss: 0.07662666589021683\n",
      "Iteration 182, Batch: 11, Loss: 0.045858945697546005\n",
      "Iteration 182, Batch: 12, Loss: 0.041274022310972214\n",
      "Iteration 182, Batch: 13, Loss: 0.07527020573616028\n",
      "Iteration 182, Batch: 14, Loss: 0.050197675824165344\n",
      "Iteration 182, Batch: 15, Loss: 0.07704860717058182\n",
      "Iteration 182, Batch: 16, Loss: 0.05912776291370392\n",
      "Iteration 182, Batch: 17, Loss: 0.07154387980699539\n",
      "Iteration 182, Batch: 18, Loss: 0.08266784250736237\n",
      "Iteration 182, Batch: 19, Loss: 0.08331646770238876\n",
      "Iteration 182, Batch: 20, Loss: 0.10280252248048782\n",
      "Iteration 182, Batch: 21, Loss: 0.06702160090208054\n",
      "Iteration 182, Batch: 22, Loss: 0.05459411442279816\n",
      "Iteration 182, Batch: 23, Loss: 0.07118501514196396\n",
      "Iteration 182, Batch: 24, Loss: 0.09171037375926971\n",
      "Iteration 182, Batch: 25, Loss: 0.08304742723703384\n",
      "Iteration 182, Batch: 26, Loss: 0.07242836058139801\n",
      "Iteration 182, Batch: 27, Loss: 0.043538834899663925\n",
      "Iteration 182, Batch: 28, Loss: 0.059836965054273605\n",
      "Iteration 182, Batch: 29, Loss: 0.05603599548339844\n",
      "Iteration 182, Batch: 30, Loss: 0.03596147149801254\n",
      "Iteration 182, Batch: 31, Loss: 0.05369357392191887\n",
      "Iteration 182, Batch: 32, Loss: 0.06677236407995224\n",
      "Iteration 182, Batch: 33, Loss: 0.08749257028102875\n",
      "Iteration 182, Batch: 34, Loss: 0.08208680152893066\n",
      "Iteration 182, Batch: 35, Loss: 0.049543190747499466\n",
      "Iteration 182, Batch: 36, Loss: 0.05596442148089409\n",
      "Iteration 182, Batch: 37, Loss: 0.06626241654157639\n",
      "Iteration 182, Batch: 38, Loss: 0.060239408165216446\n",
      "Iteration 182, Batch: 39, Loss: 0.043093662708997726\n",
      "Iteration 182, Batch: 40, Loss: 0.09393861889839172\n",
      "Iteration 182, Batch: 41, Loss: 0.08999926596879959\n",
      "Iteration 182, Batch: 42, Loss: 0.0721367597579956\n",
      "Iteration 182, Batch: 43, Loss: 0.07239878922700882\n",
      "Iteration 182, Batch: 44, Loss: 0.04485121741890907\n",
      "Iteration 182, Batch: 45, Loss: 0.06921892613172531\n",
      "Iteration 182, Batch: 46, Loss: 0.058318980038166046\n",
      "Iteration 182, Batch: 47, Loss: 0.07208304852247238\n",
      "Iteration 182, Batch: 48, Loss: 0.06578109413385391\n",
      "Iteration 182, Batch: 49, Loss: 0.06561333686113358\n",
      "Iteration 183, Batch: 0, Loss: 0.10500404983758926\n",
      "Iteration 183, Batch: 1, Loss: 0.058173876255750656\n",
      "Iteration 183, Batch: 2, Loss: 0.06926125288009644\n",
      "Iteration 183, Batch: 3, Loss: 0.06738840788602829\n",
      "Iteration 183, Batch: 4, Loss: 0.08602935820817947\n",
      "Iteration 183, Batch: 5, Loss: 0.08595801144838333\n",
      "Iteration 183, Batch: 6, Loss: 0.07132274657487869\n",
      "Iteration 183, Batch: 7, Loss: 0.08536186069250107\n",
      "Iteration 183, Batch: 8, Loss: 0.06529813259840012\n",
      "Iteration 183, Batch: 9, Loss: 0.08295022696256638\n",
      "Iteration 183, Batch: 10, Loss: 0.06606926023960114\n",
      "Iteration 183, Batch: 11, Loss: 0.061710577458143234\n",
      "Iteration 183, Batch: 12, Loss: 0.11317327618598938\n",
      "Iteration 183, Batch: 13, Loss: 0.06299471110105515\n",
      "Iteration 183, Batch: 14, Loss: 0.06642916798591614\n",
      "Iteration 183, Batch: 15, Loss: 0.08729028701782227\n",
      "Iteration 183, Batch: 16, Loss: 0.0352015495300293\n",
      "Iteration 183, Batch: 17, Loss: 0.07553257793188095\n",
      "Iteration 183, Batch: 18, Loss: 0.06970202177762985\n",
      "Iteration 183, Batch: 19, Loss: 0.07709058374166489\n",
      "Iteration 183, Batch: 20, Loss: 0.07759895920753479\n",
      "Iteration 183, Batch: 21, Loss: 0.08646287024021149\n",
      "Iteration 183, Batch: 22, Loss: 0.0313536673784256\n",
      "Iteration 183, Batch: 23, Loss: 0.07245352864265442\n",
      "Iteration 183, Batch: 24, Loss: 0.053815655410289764\n",
      "Iteration 183, Batch: 25, Loss: 0.061930350959300995\n",
      "Iteration 183, Batch: 26, Loss: 0.045460376888513565\n",
      "Iteration 183, Batch: 27, Loss: 0.0517866350710392\n",
      "Iteration 183, Batch: 28, Loss: 0.04318650811910629\n",
      "Iteration 183, Batch: 29, Loss: 0.07737024873495102\n",
      "Iteration 183, Batch: 30, Loss: 0.06243060156702995\n",
      "Iteration 183, Batch: 31, Loss: 0.07310762256383896\n",
      "Iteration 183, Batch: 32, Loss: 0.07174315303564072\n",
      "Iteration 183, Batch: 33, Loss: 0.06135840341448784\n",
      "Iteration 183, Batch: 34, Loss: 0.03615092486143112\n",
      "Iteration 183, Batch: 35, Loss: 0.03652104735374451\n",
      "Iteration 183, Batch: 36, Loss: 0.058907024562358856\n",
      "Iteration 183, Batch: 37, Loss: 0.04583357274532318\n",
      "Iteration 183, Batch: 38, Loss: 0.07687525451183319\n",
      "Iteration 183, Batch: 39, Loss: 0.04506376013159752\n",
      "Iteration 183, Batch: 40, Loss: 0.0647168830037117\n",
      "Iteration 183, Batch: 41, Loss: 0.06847875565290451\n",
      "Iteration 183, Batch: 42, Loss: 0.06042907014489174\n",
      "Iteration 183, Batch: 43, Loss: 0.06722893565893173\n",
      "Iteration 183, Batch: 44, Loss: 0.058799274265766144\n",
      "Iteration 183, Batch: 45, Loss: 0.1107872948050499\n",
      "Iteration 183, Batch: 46, Loss: 0.06005023419857025\n",
      "Iteration 183, Batch: 47, Loss: 0.08035524934530258\n",
      "Iteration 183, Batch: 48, Loss: 0.053263742476701736\n",
      "Iteration 183, Batch: 49, Loss: 0.043282169848680496\n",
      "Iteration 184, Batch: 0, Loss: 0.05408480018377304\n",
      "Iteration 184, Batch: 1, Loss: 0.06825742870569229\n",
      "Iteration 184, Batch: 2, Loss: 0.0723743885755539\n",
      "Iteration 184, Batch: 3, Loss: 0.06905161589384079\n",
      "Iteration 184, Batch: 4, Loss: 0.06917133182287216\n",
      "Iteration 184, Batch: 5, Loss: 0.04975183308124542\n",
      "Iteration 184, Batch: 6, Loss: 0.07480594515800476\n",
      "Iteration 184, Batch: 7, Loss: 0.053637851029634476\n",
      "Iteration 184, Batch: 8, Loss: 0.06464610993862152\n",
      "Iteration 184, Batch: 9, Loss: 0.05595267191529274\n",
      "Iteration 184, Batch: 10, Loss: 0.08372388780117035\n",
      "Iteration 184, Batch: 11, Loss: 0.07191146910190582\n",
      "Iteration 184, Batch: 12, Loss: 0.05150585621595383\n",
      "Iteration 184, Batch: 13, Loss: 0.06574647128582001\n",
      "Iteration 184, Batch: 14, Loss: 0.06256643682718277\n",
      "Iteration 184, Batch: 15, Loss: 0.07715451717376709\n",
      "Iteration 184, Batch: 16, Loss: 0.10257371515035629\n",
      "Iteration 184, Batch: 17, Loss: 0.08768559247255325\n",
      "Iteration 184, Batch: 18, Loss: 0.06995068490505219\n",
      "Iteration 184, Batch: 19, Loss: 0.06565112620592117\n",
      "Iteration 184, Batch: 20, Loss: 0.1023118793964386\n",
      "Iteration 184, Batch: 21, Loss: 0.06531517207622528\n",
      "Iteration 184, Batch: 22, Loss: 0.05962873250246048\n",
      "Iteration 184, Batch: 23, Loss: 0.06467420607805252\n",
      "Iteration 184, Batch: 24, Loss: 0.052490461617708206\n",
      "Iteration 184, Batch: 25, Loss: 0.05446610227227211\n",
      "Iteration 184, Batch: 26, Loss: 0.07801205664873123\n",
      "Iteration 184, Batch: 27, Loss: 0.05706487596035004\n",
      "Iteration 184, Batch: 28, Loss: 0.05305939167737961\n",
      "Iteration 184, Batch: 29, Loss: 0.0649736300110817\n",
      "Iteration 184, Batch: 30, Loss: 0.06432726979255676\n",
      "Iteration 184, Batch: 31, Loss: 0.05658182129263878\n",
      "Iteration 184, Batch: 32, Loss: 0.05627620965242386\n",
      "Iteration 184, Batch: 33, Loss: 0.07805800437927246\n",
      "Iteration 184, Batch: 34, Loss: 0.06751281023025513\n",
      "Iteration 184, Batch: 35, Loss: 0.07253878563642502\n",
      "Iteration 184, Batch: 36, Loss: 0.08618468791246414\n",
      "Iteration 184, Batch: 37, Loss: 0.062361277639865875\n",
      "Iteration 184, Batch: 38, Loss: 0.07658758759498596\n",
      "Iteration 184, Batch: 39, Loss: 0.07419662177562714\n",
      "Iteration 184, Batch: 40, Loss: 0.06862273812294006\n",
      "Iteration 184, Batch: 41, Loss: 0.07653859257698059\n",
      "Iteration 184, Batch: 42, Loss: 0.10934367775917053\n",
      "Iteration 184, Batch: 43, Loss: 0.08980404585599899\n",
      "Iteration 184, Batch: 44, Loss: 0.035338569432497025\n",
      "Iteration 184, Batch: 45, Loss: 0.06801388412714005\n",
      "Iteration 184, Batch: 46, Loss: 0.07624468207359314\n",
      "Iteration 184, Batch: 47, Loss: 0.06599020957946777\n",
      "Iteration 184, Batch: 48, Loss: 0.06579329073429108\n",
      "Iteration 184, Batch: 49, Loss: 0.05709307640790939\n",
      "Iteration 185, Batch: 0, Loss: 0.09091312438249588\n",
      "Iteration 185, Batch: 1, Loss: 0.06808217614889145\n",
      "Iteration 185, Batch: 2, Loss: 0.054347965866327286\n",
      "Iteration 185, Batch: 3, Loss: 0.04503116011619568\n",
      "Iteration 185, Batch: 4, Loss: 0.06845884770154953\n",
      "Iteration 185, Batch: 5, Loss: 0.04047604650259018\n",
      "Iteration 185, Batch: 6, Loss: 0.10102816671133041\n",
      "Iteration 185, Batch: 7, Loss: 0.07483064383268356\n",
      "Iteration 185, Batch: 8, Loss: 0.06356579810380936\n",
      "Iteration 185, Batch: 9, Loss: 0.05891265720129013\n",
      "Iteration 185, Batch: 10, Loss: 0.06970646232366562\n",
      "Iteration 185, Batch: 11, Loss: 0.07768329232931137\n",
      "Iteration 185, Batch: 12, Loss: 0.06461827456951141\n",
      "Iteration 185, Batch: 13, Loss: 0.09394803643226624\n",
      "Iteration 185, Batch: 14, Loss: 0.05427098274230957\n",
      "Iteration 185, Batch: 15, Loss: 0.07215198874473572\n",
      "Iteration 185, Batch: 16, Loss: 0.05087057501077652\n",
      "Iteration 185, Batch: 17, Loss: 0.043638743460178375\n",
      "Iteration 185, Batch: 18, Loss: 0.05490783974528313\n",
      "Iteration 185, Batch: 19, Loss: 0.0665019303560257\n",
      "Iteration 185, Batch: 20, Loss: 0.06963883340358734\n",
      "Iteration 185, Batch: 21, Loss: 0.07596645504236221\n",
      "Iteration 185, Batch: 22, Loss: 0.06954570859670639\n",
      "Iteration 185, Batch: 23, Loss: 0.04275998845696449\n",
      "Iteration 185, Batch: 24, Loss: 0.052990589290857315\n",
      "Iteration 185, Batch: 25, Loss: 0.0697917491197586\n",
      "Iteration 185, Batch: 26, Loss: 0.06950531154870987\n",
      "Iteration 185, Batch: 27, Loss: 0.08474557101726532\n",
      "Iteration 185, Batch: 28, Loss: 0.08934825658798218\n",
      "Iteration 185, Batch: 29, Loss: 0.09876088052988052\n",
      "Iteration 185, Batch: 30, Loss: 0.09709041565656662\n",
      "Iteration 185, Batch: 31, Loss: 0.10483038425445557\n",
      "Iteration 185, Batch: 32, Loss: 0.11352447420358658\n",
      "Iteration 185, Batch: 33, Loss: 0.057389724999666214\n",
      "Iteration 185, Batch: 34, Loss: 0.06183047965168953\n",
      "Iteration 185, Batch: 35, Loss: 0.08953944593667984\n",
      "Iteration 185, Batch: 36, Loss: 0.08088058233261108\n",
      "Iteration 185, Batch: 37, Loss: 0.11456073820590973\n",
      "Iteration 185, Batch: 38, Loss: 0.0600280687212944\n",
      "Iteration 185, Batch: 39, Loss: 0.06102534011006355\n",
      "Iteration 185, Batch: 40, Loss: 0.061376724392175674\n",
      "Iteration 185, Batch: 41, Loss: 0.0735766589641571\n",
      "Iteration 185, Batch: 42, Loss: 0.06870400160551071\n",
      "Iteration 185, Batch: 43, Loss: 0.07714908570051193\n",
      "Iteration 185, Batch: 44, Loss: 0.06066286936402321\n",
      "Iteration 185, Batch: 45, Loss: 0.05791982635855675\n",
      "Iteration 185, Batch: 46, Loss: 0.07162254303693771\n",
      "Iteration 185, Batch: 47, Loss: 0.05858227610588074\n",
      "Iteration 185, Batch: 48, Loss: 0.1021328717470169\n",
      "Iteration 185, Batch: 49, Loss: 0.08434011042118073\n",
      "Iteration 186, Batch: 0, Loss: 0.048750534653663635\n",
      "Iteration 186, Batch: 1, Loss: 0.0768323689699173\n",
      "Iteration 186, Batch: 2, Loss: 0.07493523508310318\n",
      "Iteration 186, Batch: 3, Loss: 0.06356635689735413\n",
      "Iteration 186, Batch: 4, Loss: 0.05628877505660057\n",
      "Iteration 186, Batch: 5, Loss: 0.0639834925532341\n",
      "Iteration 186, Batch: 6, Loss: 0.056997306644916534\n",
      "Iteration 186, Batch: 7, Loss: 0.056922778487205505\n",
      "Iteration 186, Batch: 8, Loss: 0.053233079612255096\n",
      "Iteration 186, Batch: 9, Loss: 0.07594925910234451\n",
      "Iteration 186, Batch: 10, Loss: 0.07357668876647949\n",
      "Iteration 186, Batch: 11, Loss: 0.08196531236171722\n",
      "Iteration 186, Batch: 12, Loss: 0.0654011219739914\n",
      "Iteration 186, Batch: 13, Loss: 0.07271735370159149\n",
      "Iteration 186, Batch: 14, Loss: 0.07510390877723694\n",
      "Iteration 186, Batch: 15, Loss: 0.07032803446054459\n",
      "Iteration 186, Batch: 16, Loss: 0.06143413484096527\n",
      "Iteration 186, Batch: 17, Loss: 0.0507054328918457\n",
      "Iteration 186, Batch: 18, Loss: 0.04466188699007034\n",
      "Iteration 186, Batch: 19, Loss: 0.07858182489871979\n",
      "Iteration 186, Batch: 20, Loss: 0.07384321093559265\n",
      "Iteration 186, Batch: 21, Loss: 0.06700994074344635\n",
      "Iteration 186, Batch: 22, Loss: 0.0901288166642189\n",
      "Iteration 186, Batch: 23, Loss: 0.07991190999746323\n",
      "Iteration 186, Batch: 24, Loss: 0.0665743350982666\n",
      "Iteration 186, Batch: 25, Loss: 0.08977912366390228\n",
      "Iteration 186, Batch: 26, Loss: 0.05185016989707947\n",
      "Iteration 186, Batch: 27, Loss: 0.07566695660352707\n",
      "Iteration 186, Batch: 28, Loss: 0.0529978945851326\n",
      "Iteration 186, Batch: 29, Loss: 0.04100755974650383\n",
      "Iteration 186, Batch: 30, Loss: 0.09470582753419876\n",
      "Iteration 186, Batch: 31, Loss: 0.0671854093670845\n",
      "Iteration 186, Batch: 32, Loss: 0.06553330272436142\n",
      "Iteration 186, Batch: 33, Loss: 0.08231538534164429\n",
      "Iteration 186, Batch: 34, Loss: 0.04352656379342079\n",
      "Iteration 186, Batch: 35, Loss: 0.05300762131810188\n",
      "Iteration 186, Batch: 36, Loss: 0.054026324301958084\n",
      "Iteration 186, Batch: 37, Loss: 0.07087067514657974\n",
      "Iteration 186, Batch: 38, Loss: 0.06637661904096603\n",
      "Iteration 186, Batch: 39, Loss: 0.04879986494779587\n",
      "Iteration 186, Batch: 40, Loss: 0.0713709369301796\n",
      "Iteration 186, Batch: 41, Loss: 0.04618290811777115\n",
      "Iteration 186, Batch: 42, Loss: 0.06533391773700714\n",
      "Iteration 186, Batch: 43, Loss: 0.038597021251916885\n",
      "Iteration 186, Batch: 44, Loss: 0.05529995635151863\n",
      "Iteration 186, Batch: 45, Loss: 0.03700195997953415\n",
      "Iteration 186, Batch: 46, Loss: 0.07457089424133301\n",
      "Iteration 186, Batch: 47, Loss: 0.06531208753585815\n",
      "Iteration 186, Batch: 48, Loss: 0.0813274160027504\n",
      "Iteration 186, Batch: 49, Loss: 0.059805501252412796\n",
      "Iteration 187, Batch: 0, Loss: 0.09324594587087631\n",
      "Iteration 187, Batch: 1, Loss: 0.06909646838903427\n",
      "Iteration 187, Batch: 2, Loss: 0.06370841711759567\n",
      "Iteration 187, Batch: 3, Loss: 0.06092126667499542\n",
      "Iteration 187, Batch: 4, Loss: 0.056937236338853836\n",
      "Iteration 187, Batch: 5, Loss: 0.07374599575996399\n",
      "Iteration 187, Batch: 6, Loss: 0.07127711921930313\n",
      "Iteration 187, Batch: 7, Loss: 0.06601059436798096\n",
      "Iteration 187, Batch: 8, Loss: 0.07407762110233307\n",
      "Iteration 187, Batch: 9, Loss: 0.0576639249920845\n",
      "Iteration 187, Batch: 10, Loss: 0.08211851865053177\n",
      "Iteration 187, Batch: 11, Loss: 0.061658330261707306\n",
      "Iteration 187, Batch: 12, Loss: 0.06309962272644043\n",
      "Iteration 187, Batch: 13, Loss: 0.05473219230771065\n",
      "Iteration 187, Batch: 14, Loss: 0.03649119660258293\n",
      "Iteration 187, Batch: 15, Loss: 0.07124160975217819\n",
      "Iteration 187, Batch: 16, Loss: 0.08063272386789322\n",
      "Iteration 187, Batch: 17, Loss: 0.08009620755910873\n",
      "Iteration 187, Batch: 18, Loss: 0.11665914207696915\n",
      "Iteration 187, Batch: 19, Loss: 0.09652216732501984\n",
      "Iteration 187, Batch: 20, Loss: 0.054470133036375046\n",
      "Iteration 187, Batch: 21, Loss: 0.07424468547105789\n",
      "Iteration 187, Batch: 22, Loss: 0.04904927313327789\n",
      "Iteration 187, Batch: 23, Loss: 0.08162400871515274\n",
      "Iteration 187, Batch: 24, Loss: 0.07732099294662476\n",
      "Iteration 187, Batch: 25, Loss: 0.05626805126667023\n",
      "Iteration 187, Batch: 26, Loss: 0.06451532989740372\n",
      "Iteration 187, Batch: 27, Loss: 0.05726630985736847\n",
      "Iteration 187, Batch: 28, Loss: 0.06813590973615646\n",
      "Iteration 187, Batch: 29, Loss: 0.07299334555864334\n",
      "Iteration 187, Batch: 30, Loss: 0.05443234369158745\n",
      "Iteration 187, Batch: 31, Loss: 0.05993355065584183\n",
      "Iteration 187, Batch: 32, Loss: 0.06607083976268768\n",
      "Iteration 187, Batch: 33, Loss: 0.0743434876203537\n",
      "Iteration 187, Batch: 34, Loss: 0.07576523721218109\n",
      "Iteration 187, Batch: 35, Loss: 0.05581171438097954\n",
      "Iteration 187, Batch: 36, Loss: 0.04692947119474411\n",
      "Iteration 187, Batch: 37, Loss: 0.08224985003471375\n",
      "Iteration 187, Batch: 38, Loss: 0.03825530782341957\n",
      "Iteration 187, Batch: 39, Loss: 0.06659294664859772\n",
      "Iteration 187, Batch: 40, Loss: 0.05067715421319008\n",
      "Iteration 187, Batch: 41, Loss: 0.0410570465028286\n",
      "Iteration 187, Batch: 42, Loss: 0.05800371617078781\n",
      "Iteration 187, Batch: 43, Loss: 0.05558338016271591\n",
      "Iteration 187, Batch: 44, Loss: 0.0694415271282196\n",
      "Iteration 187, Batch: 45, Loss: 0.0787089541554451\n",
      "Iteration 187, Batch: 46, Loss: 0.054134514182806015\n",
      "Iteration 187, Batch: 47, Loss: 0.06374239176511765\n",
      "Iteration 187, Batch: 48, Loss: 0.056210603564977646\n",
      "Iteration 187, Batch: 49, Loss: 0.05757880210876465\n",
      "Iteration 188, Batch: 0, Loss: 0.057303983718156815\n",
      "Iteration 188, Batch: 1, Loss: 0.05858781561255455\n",
      "Iteration 188, Batch: 2, Loss: 0.04919697344303131\n",
      "Iteration 188, Batch: 3, Loss: 0.04985186830163002\n",
      "Iteration 188, Batch: 4, Loss: 0.06673578917980194\n",
      "Iteration 188, Batch: 5, Loss: 0.0747852772474289\n",
      "Iteration 188, Batch: 6, Loss: 0.07762394845485687\n",
      "Iteration 188, Batch: 7, Loss: 0.06352290511131287\n",
      "Iteration 188, Batch: 8, Loss: 0.04485442861914635\n",
      "Iteration 188, Batch: 9, Loss: 0.048140209168195724\n",
      "Iteration 188, Batch: 10, Loss: 0.07850532978773117\n",
      "Iteration 188, Batch: 11, Loss: 0.06852781027555466\n",
      "Iteration 188, Batch: 12, Loss: 0.06749921292066574\n",
      "Iteration 188, Batch: 13, Loss: 0.079989954829216\n",
      "Iteration 188, Batch: 14, Loss: 0.04378889128565788\n",
      "Iteration 188, Batch: 15, Loss: 0.08557572960853577\n",
      "Iteration 188, Batch: 16, Loss: 0.052600543946027756\n",
      "Iteration 188, Batch: 17, Loss: 0.030455583706498146\n",
      "Iteration 188, Batch: 18, Loss: 0.08777400851249695\n",
      "Iteration 188, Batch: 19, Loss: 0.04626017436385155\n",
      "Iteration 188, Batch: 20, Loss: 0.05098443478345871\n",
      "Iteration 188, Batch: 21, Loss: 0.10625622421503067\n",
      "Iteration 188, Batch: 22, Loss: 0.06577315926551819\n",
      "Iteration 188, Batch: 23, Loss: 0.048578593879938126\n",
      "Iteration 188, Batch: 24, Loss: 0.04956670105457306\n",
      "Iteration 188, Batch: 25, Loss: 0.06303931772708893\n",
      "Iteration 188, Batch: 26, Loss: 0.041451357305049896\n",
      "Iteration 188, Batch: 27, Loss: 0.05678759887814522\n",
      "Iteration 188, Batch: 28, Loss: 0.05651191249489784\n",
      "Iteration 188, Batch: 29, Loss: 0.060420818626880646\n",
      "Iteration 188, Batch: 30, Loss: 0.05681837722659111\n",
      "Iteration 188, Batch: 31, Loss: 0.044495828449726105\n",
      "Iteration 188, Batch: 32, Loss: 0.047960761934518814\n",
      "Iteration 188, Batch: 33, Loss: 0.04932788759469986\n",
      "Iteration 188, Batch: 34, Loss: 0.06188766285777092\n",
      "Iteration 188, Batch: 35, Loss: 0.07333356887102127\n",
      "Iteration 188, Batch: 36, Loss: 0.06912292540073395\n",
      "Iteration 188, Batch: 37, Loss: 0.0839020311832428\n",
      "Iteration 188, Batch: 38, Loss: 0.06696850806474686\n",
      "Iteration 188, Batch: 39, Loss: 0.08182157576084137\n",
      "Iteration 188, Batch: 40, Loss: 0.07915060222148895\n",
      "Iteration 188, Batch: 41, Loss: 0.05257384106516838\n",
      "Iteration 188, Batch: 42, Loss: 0.07040420919656754\n",
      "Iteration 188, Batch: 43, Loss: 0.07673775404691696\n",
      "Iteration 188, Batch: 44, Loss: 0.060224007815122604\n",
      "Iteration 188, Batch: 45, Loss: 0.07503529638051987\n",
      "Iteration 188, Batch: 46, Loss: 0.06593263149261475\n",
      "Iteration 188, Batch: 47, Loss: 0.07664096355438232\n",
      "Iteration 188, Batch: 48, Loss: 0.07205187529325485\n",
      "Iteration 188, Batch: 49, Loss: 0.03541041910648346\n",
      "Iteration 189, Batch: 0, Loss: 0.04591158777475357\n",
      "Iteration 189, Batch: 1, Loss: 0.0343347005546093\n",
      "Iteration 189, Batch: 2, Loss: 0.0780058428645134\n",
      "Iteration 189, Batch: 3, Loss: 0.04782024398446083\n",
      "Iteration 189, Batch: 4, Loss: 0.04592973738908768\n",
      "Iteration 189, Batch: 5, Loss: 0.08052822202444077\n",
      "Iteration 189, Batch: 6, Loss: 0.06483258306980133\n",
      "Iteration 189, Batch: 7, Loss: 0.06159655749797821\n",
      "Iteration 189, Batch: 8, Loss: 0.08380568027496338\n",
      "Iteration 189, Batch: 9, Loss: 0.05930066481232643\n",
      "Iteration 189, Batch: 10, Loss: 0.0708543211221695\n",
      "Iteration 189, Batch: 11, Loss: 0.07026634365320206\n",
      "Iteration 189, Batch: 12, Loss: 0.05217074230313301\n",
      "Iteration 189, Batch: 13, Loss: 0.037282731384038925\n",
      "Iteration 189, Batch: 14, Loss: 0.07047856599092484\n",
      "Iteration 189, Batch: 15, Loss: 0.08931777626276016\n",
      "Iteration 189, Batch: 16, Loss: 0.05947945639491081\n",
      "Iteration 189, Batch: 17, Loss: 0.057818420231342316\n",
      "Iteration 189, Batch: 18, Loss: 0.07165466248989105\n",
      "Iteration 189, Batch: 19, Loss: 0.06763654202222824\n",
      "Iteration 189, Batch: 20, Loss: 0.08551226556301117\n",
      "Iteration 189, Batch: 21, Loss: 0.06186757981777191\n",
      "Iteration 189, Batch: 22, Loss: 0.05870715156197548\n",
      "Iteration 189, Batch: 23, Loss: 0.06250086426734924\n",
      "Iteration 189, Batch: 24, Loss: 0.08905847370624542\n",
      "Iteration 189, Batch: 25, Loss: 0.06690455973148346\n",
      "Iteration 189, Batch: 26, Loss: 0.09310130029916763\n",
      "Iteration 189, Batch: 27, Loss: 0.0669993981719017\n",
      "Iteration 189, Batch: 28, Loss: 0.07959327101707458\n",
      "Iteration 189, Batch: 29, Loss: 0.06630781292915344\n",
      "Iteration 189, Batch: 30, Loss: 0.07163306325674057\n",
      "Iteration 189, Batch: 31, Loss: 0.07694847881793976\n",
      "Iteration 189, Batch: 32, Loss: 0.05523541942238808\n",
      "Iteration 189, Batch: 33, Loss: 0.08132055401802063\n",
      "Iteration 189, Batch: 34, Loss: 0.07706756144762039\n",
      "Iteration 189, Batch: 35, Loss: 0.08197713643312454\n",
      "Iteration 189, Batch: 36, Loss: 0.06637752801179886\n",
      "Iteration 189, Batch: 37, Loss: 0.0778278112411499\n",
      "Iteration 189, Batch: 38, Loss: 0.06583589315414429\n",
      "Iteration 189, Batch: 39, Loss: 0.06417763233184814\n",
      "Iteration 189, Batch: 40, Loss: 0.05019398406147957\n",
      "Iteration 189, Batch: 41, Loss: 0.059933003038167953\n",
      "Iteration 189, Batch: 42, Loss: 0.06835643947124481\n",
      "Iteration 189, Batch: 43, Loss: 0.06622625887393951\n",
      "Iteration 189, Batch: 44, Loss: 0.04949413239955902\n",
      "Iteration 189, Batch: 45, Loss: 0.07744640111923218\n",
      "Iteration 189, Batch: 46, Loss: 0.05626104399561882\n",
      "Iteration 189, Batch: 47, Loss: 0.054367635399103165\n",
      "Iteration 189, Batch: 48, Loss: 0.05260463431477547\n",
      "Iteration 189, Batch: 49, Loss: 0.07662881910800934\n",
      "Iteration 190, Batch: 0, Loss: 0.06512055546045303\n",
      "Iteration 190, Batch: 1, Loss: 0.06641954183578491\n",
      "Iteration 190, Batch: 2, Loss: 0.04254835471510887\n",
      "Iteration 190, Batch: 3, Loss: 0.0650782585144043\n",
      "Iteration 190, Batch: 4, Loss: 0.07167830318212509\n",
      "Iteration 190, Batch: 5, Loss: 0.07843874394893646\n",
      "Iteration 190, Batch: 6, Loss: 0.10097937285900116\n",
      "Iteration 190, Batch: 7, Loss: 0.04231972247362137\n",
      "Iteration 190, Batch: 8, Loss: 0.04071177914738655\n",
      "Iteration 190, Batch: 9, Loss: 0.05295839160680771\n",
      "Iteration 190, Batch: 10, Loss: 0.04310205206274986\n",
      "Iteration 190, Batch: 11, Loss: 0.05620236322283745\n",
      "Iteration 190, Batch: 12, Loss: 0.06671573221683502\n",
      "Iteration 190, Batch: 13, Loss: 0.054704245179891586\n",
      "Iteration 190, Batch: 14, Loss: 0.060179442167282104\n",
      "Iteration 190, Batch: 15, Loss: 0.058127544820308685\n",
      "Iteration 190, Batch: 16, Loss: 0.06792401522397995\n",
      "Iteration 190, Batch: 17, Loss: 0.07782386243343353\n",
      "Iteration 190, Batch: 18, Loss: 0.051922157406806946\n",
      "Iteration 190, Batch: 19, Loss: 0.05747068300843239\n",
      "Iteration 190, Batch: 20, Loss: 0.06410569697618484\n",
      "Iteration 190, Batch: 21, Loss: 0.04274944216012955\n",
      "Iteration 190, Batch: 22, Loss: 0.06578715890645981\n",
      "Iteration 190, Batch: 23, Loss: 0.07485611736774445\n",
      "Iteration 190, Batch: 24, Loss: 0.08500728756189346\n",
      "Iteration 190, Batch: 25, Loss: 0.0552712045609951\n",
      "Iteration 190, Batch: 26, Loss: 0.06268046051263809\n",
      "Iteration 190, Batch: 27, Loss: 0.05965987965464592\n",
      "Iteration 190, Batch: 28, Loss: 0.06624798476696014\n",
      "Iteration 190, Batch: 29, Loss: 0.05348353087902069\n",
      "Iteration 190, Batch: 30, Loss: 0.07214982807636261\n",
      "Iteration 190, Batch: 31, Loss: 0.07259935885667801\n",
      "Iteration 190, Batch: 32, Loss: 0.07424745708703995\n",
      "Iteration 190, Batch: 33, Loss: 0.05969909578561783\n",
      "Iteration 190, Batch: 34, Loss: 0.06287743151187897\n",
      "Iteration 190, Batch: 35, Loss: 0.05948129668831825\n",
      "Iteration 190, Batch: 36, Loss: 0.05649140104651451\n",
      "Iteration 190, Batch: 37, Loss: 0.06619495898485184\n",
      "Iteration 190, Batch: 38, Loss: 0.07041119784116745\n",
      "Iteration 190, Batch: 39, Loss: 0.07532483339309692\n",
      "Iteration 190, Batch: 40, Loss: 0.072843998670578\n",
      "Iteration 190, Batch: 41, Loss: 0.04896541312336922\n",
      "Iteration 190, Batch: 42, Loss: 0.04172034561634064\n",
      "Iteration 190, Batch: 43, Loss: 0.059865038841962814\n",
      "Iteration 190, Batch: 44, Loss: 0.07205401360988617\n",
      "Iteration 190, Batch: 45, Loss: 0.045970477163791656\n",
      "Iteration 190, Batch: 46, Loss: 0.04309042915701866\n",
      "Iteration 190, Batch: 47, Loss: 0.052981071174144745\n",
      "Iteration 190, Batch: 48, Loss: 0.04552755132317543\n",
      "Iteration 190, Batch: 49, Loss: 0.049622125923633575\n",
      "Iteration 191, Batch: 0, Loss: 0.07003451138734818\n",
      "Iteration 191, Batch: 1, Loss: 0.059980038553476334\n",
      "Iteration 191, Batch: 2, Loss: 0.04421325400471687\n",
      "Iteration 191, Batch: 3, Loss: 0.08825010061264038\n",
      "Iteration 191, Batch: 4, Loss: 0.04322902485728264\n",
      "Iteration 191, Batch: 5, Loss: 0.06798260658979416\n",
      "Iteration 191, Batch: 6, Loss: 0.07501514256000519\n",
      "Iteration 191, Batch: 7, Loss: 0.07739303261041641\n",
      "Iteration 191, Batch: 8, Loss: 0.06739059090614319\n",
      "Iteration 191, Batch: 9, Loss: 0.07636447995901108\n",
      "Iteration 191, Batch: 10, Loss: 0.06380916386842728\n",
      "Iteration 191, Batch: 11, Loss: 0.08776723593473434\n",
      "Iteration 191, Batch: 12, Loss: 0.05864263325929642\n",
      "Iteration 191, Batch: 13, Loss: 0.08696475625038147\n",
      "Iteration 191, Batch: 14, Loss: 0.05849305912852287\n",
      "Iteration 191, Batch: 15, Loss: 0.04489671066403389\n",
      "Iteration 191, Batch: 16, Loss: 0.07069067656993866\n",
      "Iteration 191, Batch: 17, Loss: 0.04417048767209053\n",
      "Iteration 191, Batch: 18, Loss: 0.028243202716112137\n",
      "Iteration 191, Batch: 19, Loss: 0.04682210832834244\n",
      "Iteration 191, Batch: 20, Loss: 0.05765136703848839\n",
      "Iteration 191, Batch: 21, Loss: 0.05330993980169296\n",
      "Iteration 191, Batch: 22, Loss: 0.0594518780708313\n",
      "Iteration 191, Batch: 23, Loss: 0.06465473771095276\n",
      "Iteration 191, Batch: 24, Loss: 0.07097557932138443\n",
      "Iteration 191, Batch: 25, Loss: 0.08412624150514603\n",
      "Iteration 191, Batch: 26, Loss: 0.05740184709429741\n",
      "Iteration 191, Batch: 27, Loss: 0.042192380875349045\n",
      "Iteration 191, Batch: 28, Loss: 0.05423952639102936\n",
      "Iteration 191, Batch: 29, Loss: 0.05574356019496918\n",
      "Iteration 191, Batch: 30, Loss: 0.05727630481123924\n",
      "Iteration 191, Batch: 31, Loss: 0.07174459099769592\n",
      "Iteration 191, Batch: 32, Loss: 0.06936534494161606\n",
      "Iteration 191, Batch: 33, Loss: 0.0702948272228241\n",
      "Iteration 191, Batch: 34, Loss: 0.08456476032733917\n",
      "Iteration 191, Batch: 35, Loss: 0.08242525160312653\n",
      "Iteration 191, Batch: 36, Loss: 0.05476544424891472\n",
      "Iteration 191, Batch: 37, Loss: 0.08660638332366943\n",
      "Iteration 191, Batch: 38, Loss: 0.049868058413267136\n",
      "Iteration 191, Batch: 39, Loss: 0.044671811163425446\n",
      "Iteration 191, Batch: 40, Loss: 0.040606994181871414\n",
      "Iteration 191, Batch: 41, Loss: 0.081679567694664\n",
      "Iteration 191, Batch: 42, Loss: 0.07009372860193253\n",
      "Iteration 191, Batch: 43, Loss: 0.04114976152777672\n",
      "Iteration 191, Batch: 44, Loss: 0.047521695494651794\n",
      "Iteration 191, Batch: 45, Loss: 0.03789877891540527\n",
      "Iteration 191, Batch: 46, Loss: 0.049080558121204376\n",
      "Iteration 191, Batch: 47, Loss: 0.051670193672180176\n",
      "Iteration 191, Batch: 48, Loss: 0.061803434044122696\n",
      "Iteration 191, Batch: 49, Loss: 0.07000326365232468\n",
      "Iteration 192, Batch: 0, Loss: 0.03436202183365822\n",
      "Iteration 192, Batch: 1, Loss: 0.05662059783935547\n",
      "Iteration 192, Batch: 2, Loss: 0.05900528281927109\n",
      "Iteration 192, Batch: 3, Loss: 0.05345447361469269\n",
      "Iteration 192, Batch: 4, Loss: 0.06789527833461761\n",
      "Iteration 192, Batch: 5, Loss: 0.05879707261919975\n",
      "Iteration 192, Batch: 6, Loss: 0.037516724318265915\n",
      "Iteration 192, Batch: 7, Loss: 0.046154603362083435\n",
      "Iteration 192, Batch: 8, Loss: 0.06258602440357208\n",
      "Iteration 192, Batch: 9, Loss: 0.07732655853033066\n",
      "Iteration 192, Batch: 10, Loss: 0.0668879896402359\n",
      "Iteration 192, Batch: 11, Loss: 0.0817793756723404\n",
      "Iteration 192, Batch: 12, Loss: 0.056539542973041534\n",
      "Iteration 192, Batch: 13, Loss: 0.06203724071383476\n",
      "Iteration 192, Batch: 14, Loss: 0.045086171478033066\n",
      "Iteration 192, Batch: 15, Loss: 0.05127818137407303\n",
      "Iteration 192, Batch: 16, Loss: 0.0589672215282917\n",
      "Iteration 192, Batch: 17, Loss: 0.07629777491092682\n",
      "Iteration 192, Batch: 18, Loss: 0.10018149763345718\n",
      "Iteration 192, Batch: 19, Loss: 0.06037670001387596\n",
      "Iteration 192, Batch: 20, Loss: 0.06925641745328903\n",
      "Iteration 192, Batch: 21, Loss: 0.04711001366376877\n",
      "Iteration 192, Batch: 22, Loss: 0.07021671533584595\n",
      "Iteration 192, Batch: 23, Loss: 0.05276375636458397\n",
      "Iteration 192, Batch: 24, Loss: 0.07740975171327591\n",
      "Iteration 192, Batch: 25, Loss: 0.11189121007919312\n",
      "Iteration 192, Batch: 26, Loss: 0.07348005473613739\n",
      "Iteration 192, Batch: 27, Loss: 0.05687136575579643\n",
      "Iteration 192, Batch: 28, Loss: 0.06385355442762375\n",
      "Iteration 192, Batch: 29, Loss: 0.05048699304461479\n",
      "Iteration 192, Batch: 30, Loss: 0.053379133343696594\n",
      "Iteration 192, Batch: 31, Loss: 0.07857119292020798\n",
      "Iteration 192, Batch: 32, Loss: 0.07004191726446152\n",
      "Iteration 192, Batch: 33, Loss: 0.08215257525444031\n",
      "Iteration 192, Batch: 34, Loss: 0.06862293183803558\n",
      "Iteration 192, Batch: 35, Loss: 0.052273787558078766\n",
      "Iteration 192, Batch: 36, Loss: 0.038811519742012024\n",
      "Iteration 192, Batch: 37, Loss: 0.053329262882471085\n",
      "Iteration 192, Batch: 38, Loss: 0.06895001232624054\n",
      "Iteration 192, Batch: 39, Loss: 0.05532251298427582\n",
      "Iteration 192, Batch: 40, Loss: 0.05320967733860016\n",
      "Iteration 192, Batch: 41, Loss: 0.08161157369613647\n",
      "Iteration 192, Batch: 42, Loss: 0.06631045043468475\n",
      "Iteration 192, Batch: 43, Loss: 0.0634770318865776\n",
      "Iteration 192, Batch: 44, Loss: 0.06953492015600204\n",
      "Iteration 192, Batch: 45, Loss: 0.07138226181268692\n",
      "Iteration 192, Batch: 46, Loss: 0.06399200111627579\n",
      "Iteration 192, Batch: 47, Loss: 0.05017312988638878\n",
      "Iteration 192, Batch: 48, Loss: 0.04757467657327652\n",
      "Iteration 192, Batch: 49, Loss: 0.08310382068157196\n",
      "Iteration 193, Batch: 0, Loss: 0.07628052681684494\n",
      "Iteration 193, Batch: 1, Loss: 0.08658945560455322\n",
      "Iteration 193, Batch: 2, Loss: 0.07304790616035461\n",
      "Iteration 193, Batch: 3, Loss: 0.07558630406856537\n",
      "Iteration 193, Batch: 4, Loss: 0.0836288332939148\n",
      "Iteration 193, Batch: 5, Loss: 0.06412076205015182\n",
      "Iteration 193, Batch: 6, Loss: 0.05171962454915047\n",
      "Iteration 193, Batch: 7, Loss: 0.07236561924219131\n",
      "Iteration 193, Batch: 8, Loss: 0.04524645581841469\n",
      "Iteration 193, Batch: 9, Loss: 0.056021638214588165\n",
      "Iteration 193, Batch: 10, Loss: 0.07386934757232666\n",
      "Iteration 193, Batch: 11, Loss: 0.07058016955852509\n",
      "Iteration 193, Batch: 12, Loss: 0.06159789860248566\n",
      "Iteration 193, Batch: 13, Loss: 0.05615482106804848\n",
      "Iteration 193, Batch: 14, Loss: 0.04622513800859451\n",
      "Iteration 193, Batch: 15, Loss: 0.06524743884801865\n",
      "Iteration 193, Batch: 16, Loss: 0.028598293662071228\n",
      "Iteration 193, Batch: 17, Loss: 0.04551287367939949\n",
      "Iteration 193, Batch: 18, Loss: 0.044337280094623566\n",
      "Iteration 193, Batch: 19, Loss: 0.05907168239355087\n",
      "Iteration 193, Batch: 20, Loss: 0.05664834752678871\n",
      "Iteration 193, Batch: 21, Loss: 0.09509636461734772\n",
      "Iteration 193, Batch: 22, Loss: 0.05383943393826485\n",
      "Iteration 193, Batch: 23, Loss: 0.056179217994213104\n",
      "Iteration 193, Batch: 24, Loss: 0.06476516276597977\n",
      "Iteration 193, Batch: 25, Loss: 0.06340214610099792\n",
      "Iteration 193, Batch: 26, Loss: 0.07853899151086807\n",
      "Iteration 193, Batch: 27, Loss: 0.06718581914901733\n",
      "Iteration 193, Batch: 28, Loss: 0.06473777443170547\n",
      "Iteration 193, Batch: 29, Loss: 0.06701089441776276\n",
      "Iteration 193, Batch: 30, Loss: 0.05449230968952179\n",
      "Iteration 193, Batch: 31, Loss: 0.07016431540250778\n",
      "Iteration 193, Batch: 32, Loss: 0.058593522757291794\n",
      "Iteration 193, Batch: 33, Loss: 0.05563458427786827\n",
      "Iteration 193, Batch: 34, Loss: 0.04756808280944824\n",
      "Iteration 193, Batch: 35, Loss: 0.06044696643948555\n",
      "Iteration 193, Batch: 36, Loss: 0.03822499141097069\n",
      "Iteration 193, Batch: 37, Loss: 0.06349276751279831\n",
      "Iteration 193, Batch: 38, Loss: 0.05109073966741562\n",
      "Iteration 193, Batch: 39, Loss: 0.06580287963151932\n",
      "Iteration 193, Batch: 40, Loss: 0.04347766190767288\n",
      "Iteration 193, Batch: 41, Loss: 0.08551563322544098\n",
      "Iteration 193, Batch: 42, Loss: 0.07568608969449997\n",
      "Iteration 193, Batch: 43, Loss: 0.05703409016132355\n",
      "Iteration 193, Batch: 44, Loss: 0.0722898468375206\n",
      "Iteration 193, Batch: 45, Loss: 0.09250298887491226\n",
      "Iteration 193, Batch: 46, Loss: 0.055160701274871826\n",
      "Iteration 193, Batch: 47, Loss: 0.05558319762349129\n",
      "Iteration 193, Batch: 48, Loss: 0.060730528086423874\n",
      "Iteration 193, Batch: 49, Loss: 0.07817310839891434\n",
      "Iteration 194, Batch: 0, Loss: 0.046437278389930725\n",
      "Iteration 194, Batch: 1, Loss: 0.03407489135861397\n",
      "Iteration 194, Batch: 2, Loss: 0.06364457309246063\n",
      "Iteration 194, Batch: 3, Loss: 0.06356199085712433\n",
      "Iteration 194, Batch: 4, Loss: 0.05404417961835861\n",
      "Iteration 194, Batch: 5, Loss: 0.0653679296374321\n",
      "Iteration 194, Batch: 6, Loss: 0.03720776364207268\n",
      "Iteration 194, Batch: 7, Loss: 0.06603049486875534\n",
      "Iteration 194, Batch: 8, Loss: 0.09176575392484665\n",
      "Iteration 194, Batch: 9, Loss: 0.064352847635746\n",
      "Iteration 194, Batch: 10, Loss: 0.06355103105306625\n",
      "Iteration 194, Batch: 11, Loss: 0.07115702331066132\n",
      "Iteration 194, Batch: 12, Loss: 0.09068462997674942\n",
      "Iteration 194, Batch: 13, Loss: 0.06412560492753983\n",
      "Iteration 194, Batch: 14, Loss: 0.08426982909440994\n",
      "Iteration 194, Batch: 15, Loss: 0.07515019178390503\n",
      "Iteration 194, Batch: 16, Loss: 0.04273846000432968\n",
      "Iteration 194, Batch: 17, Loss: 0.07626989483833313\n",
      "Iteration 194, Batch: 18, Loss: 0.048228148370981216\n",
      "Iteration 194, Batch: 19, Loss: 0.05678790435194969\n",
      "Iteration 194, Batch: 20, Loss: 0.08038831502199173\n",
      "Iteration 194, Batch: 21, Loss: 0.06867949664592743\n",
      "Iteration 194, Batch: 22, Loss: 0.0598808117210865\n",
      "Iteration 194, Batch: 23, Loss: 0.05179764702916145\n",
      "Iteration 194, Batch: 24, Loss: 0.05027708783745766\n",
      "Iteration 194, Batch: 25, Loss: 0.05968291684985161\n",
      "Iteration 194, Batch: 26, Loss: 0.060503046959638596\n",
      "Iteration 194, Batch: 27, Loss: 0.08608472347259521\n",
      "Iteration 194, Batch: 28, Loss: 0.07580019533634186\n",
      "Iteration 194, Batch: 29, Loss: 0.06894218176603317\n",
      "Iteration 194, Batch: 30, Loss: 0.08057794719934464\n",
      "Iteration 194, Batch: 31, Loss: 0.08691035956144333\n",
      "Iteration 194, Batch: 32, Loss: 0.06897080689668655\n",
      "Iteration 194, Batch: 33, Loss: 0.06982945650815964\n",
      "Iteration 194, Batch: 34, Loss: 0.09439979493618011\n",
      "Iteration 194, Batch: 35, Loss: 0.07380306720733643\n",
      "Iteration 194, Batch: 36, Loss: 0.07714486122131348\n",
      "Iteration 194, Batch: 37, Loss: 0.059633951634168625\n",
      "Iteration 194, Batch: 38, Loss: 0.0702681839466095\n",
      "Iteration 194, Batch: 39, Loss: 0.07022816687822342\n",
      "Iteration 194, Batch: 40, Loss: 0.07240202277898788\n",
      "Iteration 194, Batch: 41, Loss: 0.05422903969883919\n",
      "Iteration 194, Batch: 42, Loss: 0.07339417934417725\n",
      "Iteration 194, Batch: 43, Loss: 0.06019880250096321\n",
      "Iteration 194, Batch: 44, Loss: 0.06549221277236938\n",
      "Iteration 194, Batch: 45, Loss: 0.0627463310956955\n",
      "Iteration 194, Batch: 46, Loss: 0.047625720500946045\n",
      "Iteration 194, Batch: 47, Loss: 0.06229402497410774\n",
      "Iteration 194, Batch: 48, Loss: 0.08255588263273239\n",
      "Iteration 194, Batch: 49, Loss: 0.09330207854509354\n",
      "Iteration 195, Batch: 0, Loss: 0.06666958332061768\n",
      "Iteration 195, Batch: 1, Loss: 0.11390575021505356\n",
      "Iteration 195, Batch: 2, Loss: 0.060213785618543625\n",
      "Iteration 195, Batch: 3, Loss: 0.07207979261875153\n",
      "Iteration 195, Batch: 4, Loss: 0.07333801686763763\n",
      "Iteration 195, Batch: 5, Loss: 0.036818962544202805\n",
      "Iteration 195, Batch: 6, Loss: 0.0747470110654831\n",
      "Iteration 195, Batch: 7, Loss: 0.08498496562242508\n",
      "Iteration 195, Batch: 8, Loss: 0.029778575524687767\n",
      "Iteration 195, Batch: 9, Loss: 0.06381914764642715\n",
      "Iteration 195, Batch: 10, Loss: 0.06158558651804924\n",
      "Iteration 195, Batch: 11, Loss: 0.08227063715457916\n",
      "Iteration 195, Batch: 12, Loss: 0.053319644182920456\n",
      "Iteration 195, Batch: 13, Loss: 0.08104083687067032\n",
      "Iteration 195, Batch: 14, Loss: 0.05588073655962944\n",
      "Iteration 195, Batch: 15, Loss: 0.05577241629362106\n",
      "Iteration 195, Batch: 16, Loss: 0.08032914996147156\n",
      "Iteration 195, Batch: 17, Loss: 0.07569558918476105\n",
      "Iteration 195, Batch: 18, Loss: 0.06896669417619705\n",
      "Iteration 195, Batch: 19, Loss: 0.08540742099285126\n",
      "Iteration 195, Batch: 20, Loss: 0.05425630137324333\n",
      "Iteration 195, Batch: 21, Loss: 0.07038042694330215\n",
      "Iteration 195, Batch: 22, Loss: 0.07170005142688751\n",
      "Iteration 195, Batch: 23, Loss: 0.07046523690223694\n",
      "Iteration 195, Batch: 24, Loss: 0.07957570254802704\n",
      "Iteration 195, Batch: 25, Loss: 0.056559257209300995\n",
      "Iteration 195, Batch: 26, Loss: 0.0478176586329937\n",
      "Iteration 195, Batch: 27, Loss: 0.03713838383555412\n",
      "Iteration 195, Batch: 28, Loss: 0.06887807697057724\n",
      "Iteration 195, Batch: 29, Loss: 0.07052531838417053\n",
      "Iteration 195, Batch: 30, Loss: 0.04864409938454628\n",
      "Iteration 195, Batch: 31, Loss: 0.05831148847937584\n",
      "Iteration 195, Batch: 32, Loss: 0.0709618479013443\n",
      "Iteration 195, Batch: 33, Loss: 0.05356677994132042\n",
      "Iteration 195, Batch: 34, Loss: 0.07908062636852264\n",
      "Iteration 195, Batch: 35, Loss: 0.04068630188703537\n",
      "Iteration 195, Batch: 36, Loss: 0.06017037853598595\n",
      "Iteration 195, Batch: 37, Loss: 0.044986579567193985\n",
      "Iteration 195, Batch: 38, Loss: 0.04813116043806076\n",
      "Iteration 195, Batch: 39, Loss: 0.061110515147447586\n",
      "Iteration 195, Batch: 40, Loss: 0.08266650140285492\n",
      "Iteration 195, Batch: 41, Loss: 0.053447283804416656\n",
      "Iteration 195, Batch: 42, Loss: 0.037138666957616806\n",
      "Iteration 195, Batch: 43, Loss: 0.060318537056446075\n",
      "Iteration 195, Batch: 44, Loss: 0.09245257824659348\n",
      "Iteration 195, Batch: 45, Loss: 0.07365087419748306\n",
      "Iteration 195, Batch: 46, Loss: 0.0629744827747345\n",
      "Iteration 195, Batch: 47, Loss: 0.05724070593714714\n",
      "Iteration 195, Batch: 48, Loss: 0.06912430375814438\n",
      "Iteration 195, Batch: 49, Loss: 0.08495916426181793\n",
      "Iteration 196, Batch: 0, Loss: 0.048434991389513016\n",
      "Iteration 196, Batch: 1, Loss: 0.0638289526104927\n",
      "Iteration 196, Batch: 2, Loss: 0.0457434356212616\n",
      "Iteration 196, Batch: 3, Loss: 0.03822702914476395\n",
      "Iteration 196, Batch: 4, Loss: 0.056012172251939774\n",
      "Iteration 196, Batch: 5, Loss: 0.07164052128791809\n",
      "Iteration 196, Batch: 6, Loss: 0.04928634688258171\n",
      "Iteration 196, Batch: 7, Loss: 0.08553506433963776\n",
      "Iteration 196, Batch: 8, Loss: 0.04420967772603035\n",
      "Iteration 196, Batch: 9, Loss: 0.06798948347568512\n",
      "Iteration 196, Batch: 10, Loss: 0.06802572309970856\n",
      "Iteration 196, Batch: 11, Loss: 0.055775463581085205\n",
      "Iteration 196, Batch: 12, Loss: 0.07878749072551727\n",
      "Iteration 196, Batch: 13, Loss: 0.054648321121931076\n",
      "Iteration 196, Batch: 14, Loss: 0.058702800422906876\n",
      "Iteration 196, Batch: 15, Loss: 0.04187604784965515\n",
      "Iteration 196, Batch: 16, Loss: 0.07541479170322418\n",
      "Iteration 196, Batch: 17, Loss: 0.07817303389310837\n",
      "Iteration 196, Batch: 18, Loss: 0.05572830140590668\n",
      "Iteration 196, Batch: 19, Loss: 0.0747036412358284\n",
      "Iteration 196, Batch: 20, Loss: 0.061387863010168076\n",
      "Iteration 196, Batch: 21, Loss: 0.09029684960842133\n",
      "Iteration 196, Batch: 22, Loss: 0.05095136538147926\n",
      "Iteration 196, Batch: 23, Loss: 0.04006645083427429\n",
      "Iteration 196, Batch: 24, Loss: 0.07628142088651657\n",
      "Iteration 196, Batch: 25, Loss: 0.055780038237571716\n",
      "Iteration 196, Batch: 26, Loss: 0.09300018101930618\n",
      "Iteration 196, Batch: 27, Loss: 0.04396308958530426\n",
      "Iteration 196, Batch: 28, Loss: 0.07892455905675888\n",
      "Iteration 196, Batch: 29, Loss: 0.046450186520814896\n",
      "Iteration 196, Batch: 30, Loss: 0.06321479380130768\n",
      "Iteration 196, Batch: 31, Loss: 0.08453740924596786\n",
      "Iteration 196, Batch: 32, Loss: 0.07365420460700989\n",
      "Iteration 196, Batch: 33, Loss: 0.08892503380775452\n",
      "Iteration 196, Batch: 34, Loss: 0.08033549785614014\n",
      "Iteration 196, Batch: 35, Loss: 0.06612462550401688\n",
      "Iteration 196, Batch: 36, Loss: 0.0444418340921402\n",
      "Iteration 196, Batch: 37, Loss: 0.06962112337350845\n",
      "Iteration 196, Batch: 38, Loss: 0.057997483760118484\n",
      "Iteration 196, Batch: 39, Loss: 0.08018504083156586\n",
      "Iteration 196, Batch: 40, Loss: 0.052329692989587784\n",
      "Iteration 196, Batch: 41, Loss: 0.041840456426143646\n",
      "Iteration 196, Batch: 42, Loss: 0.07030396163463593\n",
      "Iteration 196, Batch: 43, Loss: 0.08421441912651062\n",
      "Iteration 196, Batch: 44, Loss: 0.05569552257657051\n",
      "Iteration 196, Batch: 45, Loss: 0.04934351518750191\n",
      "Iteration 196, Batch: 46, Loss: 0.07741765677928925\n",
      "Iteration 196, Batch: 47, Loss: 0.06537086516618729\n",
      "Iteration 196, Batch: 48, Loss: 0.10161925852298737\n",
      "Iteration 196, Batch: 49, Loss: 0.07555782049894333\n",
      "Iteration 197, Batch: 0, Loss: 0.07403057813644409\n",
      "Iteration 197, Batch: 1, Loss: 0.07650764286518097\n",
      "Iteration 197, Batch: 2, Loss: 0.04860405996441841\n",
      "Iteration 197, Batch: 3, Loss: 0.06984663754701614\n",
      "Iteration 197, Batch: 4, Loss: 0.05978332459926605\n",
      "Iteration 197, Batch: 5, Loss: 0.045817192643880844\n",
      "Iteration 197, Batch: 6, Loss: 0.050832316279411316\n",
      "Iteration 197, Batch: 7, Loss: 0.07207966595888138\n",
      "Iteration 197, Batch: 8, Loss: 0.07898349314928055\n",
      "Iteration 197, Batch: 9, Loss: 0.053281787782907486\n",
      "Iteration 197, Batch: 10, Loss: 0.07154709845781326\n",
      "Iteration 197, Batch: 11, Loss: 0.05460419878363609\n",
      "Iteration 197, Batch: 12, Loss: 0.0764719769358635\n",
      "Iteration 197, Batch: 13, Loss: 0.062467824667692184\n",
      "Iteration 197, Batch: 14, Loss: 0.05313338339328766\n",
      "Iteration 197, Batch: 15, Loss: 0.05872798711061478\n",
      "Iteration 197, Batch: 16, Loss: 0.10578306764364243\n",
      "Iteration 197, Batch: 17, Loss: 0.048183877021074295\n",
      "Iteration 197, Batch: 18, Loss: 0.06653248518705368\n",
      "Iteration 197, Batch: 19, Loss: 0.07739537209272385\n",
      "Iteration 197, Batch: 20, Loss: 0.06111745536327362\n",
      "Iteration 197, Batch: 21, Loss: 0.051163967698812485\n",
      "Iteration 197, Batch: 22, Loss: 0.06483068317174911\n",
      "Iteration 197, Batch: 23, Loss: 0.048292167484760284\n",
      "Iteration 197, Batch: 24, Loss: 0.051565106958150864\n",
      "Iteration 197, Batch: 25, Loss: 0.07832308113574982\n",
      "Iteration 197, Batch: 26, Loss: 0.05339856073260307\n",
      "Iteration 197, Batch: 27, Loss: 0.048376042395830154\n",
      "Iteration 197, Batch: 28, Loss: 0.044903162866830826\n",
      "Iteration 197, Batch: 29, Loss: 0.05843343585729599\n",
      "Iteration 197, Batch: 30, Loss: 0.056510474532842636\n",
      "Iteration 197, Batch: 31, Loss: 0.06322596222162247\n",
      "Iteration 197, Batch: 32, Loss: 0.048494938760995865\n",
      "Iteration 197, Batch: 33, Loss: 0.04714591056108475\n",
      "Iteration 197, Batch: 34, Loss: 0.10282167047262192\n",
      "Iteration 197, Batch: 35, Loss: 0.05839906632900238\n",
      "Iteration 197, Batch: 36, Loss: 0.03988708183169365\n",
      "Iteration 197, Batch: 37, Loss: 0.051687564700841904\n",
      "Iteration 197, Batch: 38, Loss: 0.06764145940542221\n",
      "Iteration 197, Batch: 39, Loss: 0.07436158508062363\n",
      "Iteration 197, Batch: 40, Loss: 0.0651460513472557\n",
      "Iteration 197, Batch: 41, Loss: 0.04146570712327957\n",
      "Iteration 197, Batch: 42, Loss: 0.08862461149692535\n",
      "Iteration 197, Batch: 43, Loss: 0.05550765246152878\n",
      "Iteration 197, Batch: 44, Loss: 0.08900779485702515\n",
      "Iteration 197, Batch: 45, Loss: 0.07402046024799347\n",
      "Iteration 197, Batch: 46, Loss: 0.06276432424783707\n",
      "Iteration 197, Batch: 47, Loss: 0.046619199216365814\n",
      "Iteration 197, Batch: 48, Loss: 0.07130437344312668\n",
      "Iteration 197, Batch: 49, Loss: 0.052441611886024475\n",
      "Iteration 198, Batch: 0, Loss: 0.06459043174982071\n",
      "Iteration 198, Batch: 1, Loss: 0.06627336144447327\n",
      "Iteration 198, Batch: 2, Loss: 0.0447726733982563\n",
      "Iteration 198, Batch: 3, Loss: 0.05540478974580765\n",
      "Iteration 198, Batch: 4, Loss: 0.08633996546268463\n",
      "Iteration 198, Batch: 5, Loss: 0.05869346484541893\n",
      "Iteration 198, Batch: 6, Loss: 0.06300156563520432\n",
      "Iteration 198, Batch: 7, Loss: 0.042536720633506775\n",
      "Iteration 198, Batch: 8, Loss: 0.05298186093568802\n",
      "Iteration 198, Batch: 9, Loss: 0.04350905492901802\n",
      "Iteration 198, Batch: 10, Loss: 0.06612494587898254\n",
      "Iteration 198, Batch: 11, Loss: 0.06326913833618164\n",
      "Iteration 198, Batch: 12, Loss: 0.07982531934976578\n",
      "Iteration 198, Batch: 13, Loss: 0.05541619658470154\n",
      "Iteration 198, Batch: 14, Loss: 0.0581648051738739\n",
      "Iteration 198, Batch: 15, Loss: 0.0688544437289238\n",
      "Iteration 198, Batch: 16, Loss: 0.07801114022731781\n",
      "Iteration 198, Batch: 17, Loss: 0.06536994874477386\n",
      "Iteration 198, Batch: 18, Loss: 0.06773912161588669\n",
      "Iteration 198, Batch: 19, Loss: 0.047413505613803864\n",
      "Iteration 198, Batch: 20, Loss: 0.07105529308319092\n",
      "Iteration 198, Batch: 21, Loss: 0.06993252038955688\n",
      "Iteration 198, Batch: 22, Loss: 0.06065675988793373\n",
      "Iteration 198, Batch: 23, Loss: 0.07462488114833832\n",
      "Iteration 198, Batch: 24, Loss: 0.0677989199757576\n",
      "Iteration 198, Batch: 25, Loss: 0.06815434247255325\n",
      "Iteration 198, Batch: 26, Loss: 0.07543092966079712\n",
      "Iteration 198, Batch: 27, Loss: 0.09795373678207397\n",
      "Iteration 198, Batch: 28, Loss: 0.03988519683480263\n",
      "Iteration 198, Batch: 29, Loss: 0.09865541011095047\n",
      "Iteration 198, Batch: 30, Loss: 0.03868373855948448\n",
      "Iteration 198, Batch: 31, Loss: 0.07943721860647202\n",
      "Iteration 198, Batch: 32, Loss: 0.05346008017659187\n",
      "Iteration 198, Batch: 33, Loss: 0.10153523087501526\n",
      "Iteration 198, Batch: 34, Loss: 0.08814998716115952\n",
      "Iteration 198, Batch: 35, Loss: 0.08070342987775803\n",
      "Iteration 198, Batch: 36, Loss: 0.05483418330550194\n",
      "Iteration 198, Batch: 37, Loss: 0.06929483264684677\n",
      "Iteration 198, Batch: 38, Loss: 0.06329328566789627\n",
      "Iteration 198, Batch: 39, Loss: 0.06009352579712868\n",
      "Iteration 198, Batch: 40, Loss: 0.056882936507463455\n",
      "Iteration 198, Batch: 41, Loss: 0.09165172278881073\n",
      "Iteration 198, Batch: 42, Loss: 0.07022035121917725\n",
      "Iteration 198, Batch: 43, Loss: 0.047337692230939865\n",
      "Iteration 198, Batch: 44, Loss: 0.07781093567609787\n",
      "Iteration 198, Batch: 45, Loss: 0.07382980734109879\n",
      "Iteration 198, Batch: 46, Loss: 0.08257440477609634\n",
      "Iteration 198, Batch: 47, Loss: 0.08448068797588348\n",
      "Iteration 198, Batch: 48, Loss: 0.05145898833870888\n",
      "Iteration 198, Batch: 49, Loss: 0.05701228976249695\n",
      "Iteration 199, Batch: 0, Loss: 0.08221770823001862\n",
      "Iteration 199, Batch: 1, Loss: 0.06210039183497429\n",
      "Iteration 199, Batch: 2, Loss: 0.05282262712717056\n",
      "Iteration 199, Batch: 3, Loss: 0.06151612102985382\n",
      "Iteration 199, Batch: 4, Loss: 0.06342536211013794\n",
      "Iteration 199, Batch: 5, Loss: 0.0922459289431572\n",
      "Iteration 199, Batch: 6, Loss: 0.058296747505664825\n",
      "Iteration 199, Batch: 7, Loss: 0.06674610078334808\n",
      "Iteration 199, Batch: 8, Loss: 0.10093802213668823\n",
      "Iteration 199, Batch: 9, Loss: 0.0756721943616867\n",
      "Iteration 199, Batch: 10, Loss: 0.06876850873231888\n",
      "Iteration 199, Batch: 11, Loss: 0.10597427189350128\n",
      "Iteration 199, Batch: 12, Loss: 0.08090326189994812\n",
      "Iteration 199, Batch: 13, Loss: 0.06849312037229538\n",
      "Iteration 199, Batch: 14, Loss: 0.08545169979333878\n",
      "Iteration 199, Batch: 15, Loss: 0.04496358707547188\n",
      "Iteration 199, Batch: 16, Loss: 0.08261033892631531\n",
      "Iteration 199, Batch: 17, Loss: 0.04890909418463707\n",
      "Iteration 199, Batch: 18, Loss: 0.06633031368255615\n",
      "Iteration 199, Batch: 19, Loss: 0.10031037777662277\n",
      "Iteration 199, Batch: 20, Loss: 0.08207954466342926\n",
      "Iteration 199, Batch: 21, Loss: 0.06700089573860168\n",
      "Iteration 199, Batch: 22, Loss: 0.053318336606025696\n",
      "Iteration 199, Batch: 23, Loss: 0.056353963911533356\n",
      "Iteration 199, Batch: 24, Loss: 0.07811713963747025\n",
      "Iteration 199, Batch: 25, Loss: 0.09363576024770737\n",
      "Iteration 199, Batch: 26, Loss: 0.0589577816426754\n",
      "Iteration 199, Batch: 27, Loss: 0.07870990037918091\n",
      "Iteration 199, Batch: 28, Loss: 0.07283344864845276\n",
      "Iteration 199, Batch: 29, Loss: 0.03140683099627495\n",
      "Iteration 199, Batch: 30, Loss: 0.049323730170726776\n",
      "Iteration 199, Batch: 31, Loss: 0.054924678057432175\n",
      "Iteration 199, Batch: 32, Loss: 0.05034324899315834\n",
      "Iteration 199, Batch: 33, Loss: 0.05797021836042404\n",
      "Iteration 199, Batch: 34, Loss: 0.06705222278833389\n",
      "Iteration 199, Batch: 35, Loss: 0.07050378620624542\n",
      "Iteration 199, Batch: 36, Loss: 0.07750153541564941\n",
      "Iteration 199, Batch: 37, Loss: 0.08054181188344955\n",
      "Iteration 199, Batch: 38, Loss: 0.07022298872470856\n",
      "Iteration 199, Batch: 39, Loss: 0.03814854472875595\n",
      "Iteration 199, Batch: 40, Loss: 0.05201178416609764\n",
      "Iteration 199, Batch: 41, Loss: 0.06427265703678131\n",
      "Iteration 199, Batch: 42, Loss: 0.07092723995447159\n",
      "Iteration 199, Batch: 43, Loss: 0.06184476241469383\n",
      "Iteration 199, Batch: 44, Loss: 0.05320343002676964\n",
      "Iteration 199, Batch: 45, Loss: 0.05945175886154175\n",
      "Iteration 199, Batch: 46, Loss: 0.1026601791381836\n",
      "Iteration 199, Batch: 47, Loss: 0.06741980463266373\n",
      "Iteration 199, Batch: 48, Loss: 0.05748916417360306\n",
      "Iteration 199, Batch: 49, Loss: 0.049475815147161484\n",
      "Iteration 200, Batch: 0, Loss: 0.04202756658196449\n",
      "Iteration 200, Batch: 1, Loss: 0.08018692582845688\n",
      "Iteration 200, Batch: 2, Loss: 0.05659336969256401\n",
      "Iteration 200, Batch: 3, Loss: 0.07086274027824402\n",
      "Iteration 200, Batch: 4, Loss: 0.06948571652173996\n",
      "Iteration 200, Batch: 5, Loss: 0.078447625041008\n",
      "Iteration 200, Batch: 6, Loss: 0.055264174938201904\n",
      "Iteration 200, Batch: 7, Loss: 0.07044634968042374\n",
      "Iteration 200, Batch: 8, Loss: 0.06971564143896103\n",
      "Iteration 200, Batch: 9, Loss: 0.053928256034851074\n",
      "Iteration 200, Batch: 10, Loss: 0.043043799698352814\n",
      "Iteration 200, Batch: 11, Loss: 0.03867713734507561\n",
      "Iteration 200, Batch: 12, Loss: 0.062119074165821075\n",
      "Iteration 200, Batch: 13, Loss: 0.10414336621761322\n",
      "Iteration 200, Batch: 14, Loss: 0.056548092514276505\n",
      "Iteration 200, Batch: 15, Loss: 0.04720652475953102\n",
      "Iteration 200, Batch: 16, Loss: 0.06514512002468109\n",
      "Iteration 200, Batch: 17, Loss: 0.08865755051374435\n",
      "Iteration 200, Batch: 18, Loss: 0.05999727547168732\n",
      "Iteration 200, Batch: 19, Loss: 0.08233659714460373\n",
      "Iteration 200, Batch: 20, Loss: 0.04418351873755455\n",
      "Iteration 200, Batch: 21, Loss: 0.07462722808122635\n",
      "Iteration 200, Batch: 22, Loss: 0.05241263285279274\n",
      "Iteration 200, Batch: 23, Loss: 0.0426756851375103\n",
      "Iteration 200, Batch: 24, Loss: 0.06848645210266113\n",
      "Iteration 200, Batch: 25, Loss: 0.06429999321699142\n",
      "Iteration 200, Batch: 26, Loss: 0.07877752929925919\n",
      "Iteration 200, Batch: 27, Loss: 0.046970631927251816\n",
      "Iteration 200, Batch: 28, Loss: 0.04685402661561966\n",
      "Iteration 200, Batch: 29, Loss: 0.07853160798549652\n",
      "Iteration 200, Batch: 30, Loss: 0.045805830508470535\n",
      "Iteration 200, Batch: 31, Loss: 0.05517474189400673\n",
      "Iteration 200, Batch: 32, Loss: 0.05824973061680794\n",
      "Iteration 200, Batch: 33, Loss: 0.0824621394276619\n",
      "Iteration 200, Batch: 34, Loss: 0.06777705997228622\n",
      "Iteration 200, Batch: 35, Loss: 0.05320168286561966\n",
      "Iteration 200, Batch: 36, Loss: 0.062074754387140274\n",
      "Iteration 200, Batch: 37, Loss: 0.05901473015546799\n",
      "Iteration 200, Batch: 38, Loss: 0.07737637311220169\n",
      "Iteration 200, Batch: 39, Loss: 0.05975620448589325\n",
      "Iteration 200, Batch: 40, Loss: 0.10709018260240555\n",
      "Iteration 200, Batch: 41, Loss: 0.06669406592845917\n",
      "Iteration 200, Batch: 42, Loss: 0.06413009017705917\n",
      "Iteration 200, Batch: 43, Loss: 0.05959763005375862\n",
      "Iteration 200, Batch: 44, Loss: 0.07753580063581467\n",
      "Iteration 200, Batch: 45, Loss: 0.06605403125286102\n",
      "Iteration 200, Batch: 46, Loss: 0.06410985440015793\n",
      "Iteration 200, Batch: 47, Loss: 0.05547461286187172\n",
      "Iteration 200, Batch: 48, Loss: 0.09163379669189453\n",
      "Iteration 200, Batch: 49, Loss: 0.053878121078014374\n",
      "Iteration 201, Batch: 0, Loss: 0.07734525203704834\n",
      "Iteration 201, Batch: 1, Loss: 0.06012101098895073\n",
      "Iteration 201, Batch: 2, Loss: 0.05771063640713692\n",
      "Iteration 201, Batch: 3, Loss: 0.08038608729839325\n",
      "Iteration 201, Batch: 4, Loss: 0.062355782836675644\n",
      "Iteration 201, Batch: 5, Loss: 0.0557408332824707\n",
      "Iteration 201, Batch: 6, Loss: 0.06576810777187347\n",
      "Iteration 201, Batch: 7, Loss: 0.06942804902791977\n",
      "Iteration 201, Batch: 8, Loss: 0.05509084463119507\n",
      "Iteration 201, Batch: 9, Loss: 0.0781233087182045\n",
      "Iteration 201, Batch: 10, Loss: 0.05237603932619095\n",
      "Iteration 201, Batch: 11, Loss: 0.04703127592802048\n",
      "Iteration 201, Batch: 12, Loss: 0.06272076815366745\n",
      "Iteration 201, Batch: 13, Loss: 0.06116141751408577\n",
      "Iteration 201, Batch: 14, Loss: 0.05191166698932648\n",
      "Iteration 201, Batch: 15, Loss: 0.060317713767290115\n",
      "Iteration 201, Batch: 16, Loss: 0.09060706198215485\n",
      "Iteration 201, Batch: 17, Loss: 0.07518527656793594\n",
      "Iteration 201, Batch: 18, Loss: 0.05675292760133743\n",
      "Iteration 201, Batch: 19, Loss: 0.0654856264591217\n",
      "Iteration 201, Batch: 20, Loss: 0.06182028725743294\n",
      "Iteration 201, Batch: 21, Loss: 0.05265726149082184\n",
      "Iteration 201, Batch: 22, Loss: 0.07460859417915344\n",
      "Iteration 201, Batch: 23, Loss: 0.052045777440071106\n",
      "Iteration 201, Batch: 24, Loss: 0.058359239250421524\n",
      "Iteration 201, Batch: 25, Loss: 0.04699022322893143\n",
      "Iteration 201, Batch: 26, Loss: 0.053841061890125275\n",
      "Iteration 201, Batch: 27, Loss: 0.07703658193349838\n",
      "Iteration 201, Batch: 28, Loss: 0.04528661072254181\n",
      "Iteration 201, Batch: 29, Loss: 0.05074802413582802\n",
      "Iteration 201, Batch: 30, Loss: 0.05786031484603882\n",
      "Iteration 201, Batch: 31, Loss: 0.05990298092365265\n",
      "Iteration 201, Batch: 32, Loss: 0.04403218254446983\n",
      "Iteration 201, Batch: 33, Loss: 0.061736900359392166\n",
      "Iteration 201, Batch: 34, Loss: 0.062125708907842636\n",
      "Iteration 201, Batch: 35, Loss: 0.07282446324825287\n",
      "Iteration 201, Batch: 36, Loss: 0.0694304034113884\n",
      "Iteration 201, Batch: 37, Loss: 0.05928489565849304\n",
      "Iteration 201, Batch: 38, Loss: 0.04811440408229828\n",
      "Iteration 201, Batch: 39, Loss: 0.08865462988615036\n",
      "Iteration 201, Batch: 40, Loss: 0.06678435951471329\n",
      "Iteration 201, Batch: 41, Loss: 0.06463343650102615\n",
      "Iteration 201, Batch: 42, Loss: 0.055709149688482285\n",
      "Iteration 201, Batch: 43, Loss: 0.07585838437080383\n",
      "Iteration 201, Batch: 44, Loss: 0.09463546425104141\n",
      "Iteration 201, Batch: 45, Loss: 0.06858213990926743\n",
      "Iteration 201, Batch: 46, Loss: 0.06661754101514816\n",
      "Iteration 201, Batch: 47, Loss: 0.07333856076002121\n",
      "Iteration 201, Batch: 48, Loss: 0.06973455846309662\n",
      "Iteration 201, Batch: 49, Loss: 0.07753986120223999\n",
      "Iteration 202, Batch: 0, Loss: 0.03452116250991821\n",
      "Iteration 202, Batch: 1, Loss: 0.050380952656269073\n",
      "Iteration 202, Batch: 2, Loss: 0.0691249743103981\n",
      "Iteration 202, Batch: 3, Loss: 0.08457592129707336\n",
      "Iteration 202, Batch: 4, Loss: 0.0640702024102211\n",
      "Iteration 202, Batch: 5, Loss: 0.07249460369348526\n",
      "Iteration 202, Batch: 6, Loss: 0.054283514618873596\n",
      "Iteration 202, Batch: 7, Loss: 0.059514306485652924\n",
      "Iteration 202, Batch: 8, Loss: 0.07060302048921585\n",
      "Iteration 202, Batch: 9, Loss: 0.08818252384662628\n",
      "Iteration 202, Batch: 10, Loss: 0.10675471276044846\n",
      "Iteration 202, Batch: 11, Loss: 0.06667287647724152\n",
      "Iteration 202, Batch: 12, Loss: 0.0759439766407013\n",
      "Iteration 202, Batch: 13, Loss: 0.07938486337661743\n",
      "Iteration 202, Batch: 14, Loss: 0.058712758123874664\n",
      "Iteration 202, Batch: 15, Loss: 0.04193677380681038\n",
      "Iteration 202, Batch: 16, Loss: 0.07969280332326889\n",
      "Iteration 202, Batch: 17, Loss: 0.07854454219341278\n",
      "Iteration 202, Batch: 18, Loss: 0.03615495190024376\n",
      "Iteration 202, Batch: 19, Loss: 0.07366486638784409\n",
      "Iteration 202, Batch: 20, Loss: 0.07334643602371216\n",
      "Iteration 202, Batch: 21, Loss: 0.059491559863090515\n",
      "Iteration 202, Batch: 22, Loss: 0.04729712754487991\n",
      "Iteration 202, Batch: 23, Loss: 0.049242742359638214\n",
      "Iteration 202, Batch: 24, Loss: 0.07754199951887131\n",
      "Iteration 202, Batch: 25, Loss: 0.07002668082714081\n",
      "Iteration 202, Batch: 26, Loss: 0.05946275591850281\n",
      "Iteration 202, Batch: 27, Loss: 0.06867879629135132\n",
      "Iteration 202, Batch: 28, Loss: 0.07450816035270691\n",
      "Iteration 202, Batch: 29, Loss: 0.04269465431571007\n",
      "Iteration 202, Batch: 30, Loss: 0.04485136270523071\n",
      "Iteration 202, Batch: 31, Loss: 0.07403984665870667\n",
      "Iteration 202, Batch: 32, Loss: 0.050787027925252914\n",
      "Iteration 202, Batch: 33, Loss: 0.04194358363747597\n",
      "Iteration 202, Batch: 34, Loss: 0.06159154698252678\n",
      "Iteration 202, Batch: 35, Loss: 0.04288366064429283\n",
      "Iteration 202, Batch: 36, Loss: 0.04125215485692024\n",
      "Iteration 202, Batch: 37, Loss: 0.058685168623924255\n",
      "Iteration 202, Batch: 38, Loss: 0.054699573665857315\n",
      "Iteration 202, Batch: 39, Loss: 0.0547848604619503\n",
      "Iteration 202, Batch: 40, Loss: 0.08632634580135345\n",
      "Iteration 202, Batch: 41, Loss: 0.08469312638044357\n",
      "Iteration 202, Batch: 42, Loss: 0.046905338764190674\n",
      "Iteration 202, Batch: 43, Loss: 0.07198335230350494\n",
      "Iteration 202, Batch: 44, Loss: 0.032549645751714706\n",
      "Iteration 202, Batch: 45, Loss: 0.07951663434505463\n",
      "Iteration 202, Batch: 46, Loss: 0.07445256412029266\n",
      "Iteration 202, Batch: 47, Loss: 0.08208809792995453\n",
      "Iteration 202, Batch: 48, Loss: 0.07605256140232086\n",
      "Iteration 202, Batch: 49, Loss: 0.06550772488117218\n",
      "Iteration 203, Batch: 0, Loss: 0.0646374523639679\n",
      "Iteration 203, Batch: 1, Loss: 0.0441095307469368\n",
      "Iteration 203, Batch: 2, Loss: 0.06411797553300858\n",
      "Iteration 203, Batch: 3, Loss: 0.06584468483924866\n",
      "Iteration 203, Batch: 4, Loss: 0.049133650958538055\n",
      "Iteration 203, Batch: 5, Loss: 0.08970683813095093\n",
      "Iteration 203, Batch: 6, Loss: 0.0646570548415184\n",
      "Iteration 203, Batch: 7, Loss: 0.0678955540060997\n",
      "Iteration 203, Batch: 8, Loss: 0.07761643081903458\n",
      "Iteration 203, Batch: 9, Loss: 0.08349914103746414\n",
      "Iteration 203, Batch: 10, Loss: 0.03546888008713722\n",
      "Iteration 203, Batch: 11, Loss: 0.07624144107103348\n",
      "Iteration 203, Batch: 12, Loss: 0.05533811077475548\n",
      "Iteration 203, Batch: 13, Loss: 0.05395287275314331\n",
      "Iteration 203, Batch: 14, Loss: 0.049690693616867065\n",
      "Iteration 203, Batch: 15, Loss: 0.0634329617023468\n",
      "Iteration 203, Batch: 16, Loss: 0.044656701385974884\n",
      "Iteration 203, Batch: 17, Loss: 0.06055278703570366\n",
      "Iteration 203, Batch: 18, Loss: 0.05583111196756363\n",
      "Iteration 203, Batch: 19, Loss: 0.06629132479429245\n",
      "Iteration 203, Batch: 20, Loss: 0.06992955505847931\n",
      "Iteration 203, Batch: 21, Loss: 0.05809878557920456\n",
      "Iteration 203, Batch: 22, Loss: 0.07141436636447906\n",
      "Iteration 203, Batch: 23, Loss: 0.04568108543753624\n",
      "Iteration 203, Batch: 24, Loss: 0.06333285570144653\n",
      "Iteration 203, Batch: 25, Loss: 0.06884925812482834\n",
      "Iteration 203, Batch: 26, Loss: 0.07286980748176575\n",
      "Iteration 203, Batch: 27, Loss: 0.06479508429765701\n",
      "Iteration 203, Batch: 28, Loss: 0.04620196297764778\n",
      "Iteration 203, Batch: 29, Loss: 0.05054387077689171\n",
      "Iteration 203, Batch: 30, Loss: 0.048428211361169815\n",
      "Iteration 203, Batch: 31, Loss: 0.027350077405571938\n",
      "Iteration 203, Batch: 32, Loss: 0.05091343820095062\n",
      "Iteration 203, Batch: 33, Loss: 0.0632336214184761\n",
      "Iteration 203, Batch: 34, Loss: 0.056848328560590744\n",
      "Iteration 203, Batch: 35, Loss: 0.06018328666687012\n",
      "Iteration 203, Batch: 36, Loss: 0.061336711049079895\n",
      "Iteration 203, Batch: 37, Loss: 0.0809708759188652\n",
      "Iteration 203, Batch: 38, Loss: 0.045107174664735794\n",
      "Iteration 203, Batch: 39, Loss: 0.07954476773738861\n",
      "Iteration 203, Batch: 40, Loss: 0.06510579586029053\n",
      "Iteration 203, Batch: 41, Loss: 0.04024866968393326\n",
      "Iteration 203, Batch: 42, Loss: 0.06787785142660141\n",
      "Iteration 203, Batch: 43, Loss: 0.039783213287591934\n",
      "Iteration 203, Batch: 44, Loss: 0.07629239559173584\n",
      "Iteration 203, Batch: 45, Loss: 0.06658133119344711\n",
      "Iteration 203, Batch: 46, Loss: 0.08091403543949127\n",
      "Iteration 203, Batch: 47, Loss: 0.04736490175127983\n",
      "Iteration 203, Batch: 48, Loss: 0.08549247682094574\n",
      "Iteration 203, Batch: 49, Loss: 0.06511365622282028\n",
      "Iteration 204, Batch: 0, Loss: 0.07366807758808136\n",
      "Iteration 204, Batch: 1, Loss: 0.06636237353086472\n",
      "Iteration 204, Batch: 2, Loss: 0.0762181282043457\n",
      "Iteration 204, Batch: 3, Loss: 0.05421524867415428\n",
      "Iteration 204, Batch: 4, Loss: 0.06358563899993896\n",
      "Iteration 204, Batch: 5, Loss: 0.07744409888982773\n",
      "Iteration 204, Batch: 6, Loss: 0.07358682155609131\n",
      "Iteration 204, Batch: 7, Loss: 0.05818191543221474\n",
      "Iteration 204, Batch: 8, Loss: 0.07133603096008301\n",
      "Iteration 204, Batch: 9, Loss: 0.07171445339918137\n",
      "Iteration 204, Batch: 10, Loss: 0.057234324514865875\n",
      "Iteration 204, Batch: 11, Loss: 0.05744020268321037\n",
      "Iteration 204, Batch: 12, Loss: 0.06984688341617584\n",
      "Iteration 204, Batch: 13, Loss: 0.04277757927775383\n",
      "Iteration 204, Batch: 14, Loss: 0.05266302824020386\n",
      "Iteration 204, Batch: 15, Loss: 0.02890843339264393\n",
      "Iteration 204, Batch: 16, Loss: 0.058916330337524414\n",
      "Iteration 204, Batch: 17, Loss: 0.07919254899024963\n",
      "Iteration 204, Batch: 18, Loss: 0.06003778427839279\n",
      "Iteration 204, Batch: 19, Loss: 0.07430952787399292\n",
      "Iteration 204, Batch: 20, Loss: 0.04359322786331177\n",
      "Iteration 204, Batch: 21, Loss: 0.08683402836322784\n",
      "Iteration 204, Batch: 22, Loss: 0.04976025968790054\n",
      "Iteration 204, Batch: 23, Loss: 0.04738875851035118\n",
      "Iteration 204, Batch: 24, Loss: 0.05606479570269585\n",
      "Iteration 204, Batch: 25, Loss: 0.07105773687362671\n",
      "Iteration 204, Batch: 26, Loss: 0.06673956662416458\n",
      "Iteration 204, Batch: 27, Loss: 0.06883673369884491\n",
      "Iteration 204, Batch: 28, Loss: 0.0848163589835167\n",
      "Iteration 204, Batch: 29, Loss: 0.056152403354644775\n",
      "Iteration 204, Batch: 30, Loss: 0.09098122268915176\n",
      "Iteration 204, Batch: 31, Loss: 0.08124221861362457\n",
      "Iteration 204, Batch: 32, Loss: 0.059025753289461136\n",
      "Iteration 204, Batch: 33, Loss: 0.07319764792919159\n",
      "Iteration 204, Batch: 34, Loss: 0.06561164557933807\n",
      "Iteration 204, Batch: 35, Loss: 0.0528995506465435\n",
      "Iteration 204, Batch: 36, Loss: 0.06210736930370331\n",
      "Iteration 204, Batch: 37, Loss: 0.06267915666103363\n",
      "Iteration 204, Batch: 38, Loss: 0.02906678058207035\n",
      "Iteration 204, Batch: 39, Loss: 0.07614272087812424\n",
      "Iteration 204, Batch: 40, Loss: 0.06453347206115723\n",
      "Iteration 204, Batch: 41, Loss: 0.10598026216030121\n",
      "Iteration 204, Batch: 42, Loss: 0.061162643134593964\n",
      "Iteration 204, Batch: 43, Loss: 0.07368363440036774\n",
      "Iteration 204, Batch: 44, Loss: 0.07475171983242035\n",
      "Iteration 204, Batch: 45, Loss: 0.06937107443809509\n",
      "Iteration 204, Batch: 46, Loss: 0.08508662134408951\n",
      "Iteration 204, Batch: 47, Loss: 0.0761604979634285\n",
      "Iteration 204, Batch: 48, Loss: 0.08807911723852158\n",
      "Iteration 204, Batch: 49, Loss: 0.10624739527702332\n",
      "Iteration 205, Batch: 0, Loss: 0.11599309742450714\n",
      "Iteration 205, Batch: 1, Loss: 0.11536125093698502\n",
      "Iteration 205, Batch: 2, Loss: 0.10769450664520264\n",
      "Iteration 205, Batch: 3, Loss: 0.0977688655257225\n",
      "Iteration 205, Batch: 4, Loss: 0.08512948453426361\n",
      "Iteration 205, Batch: 5, Loss: 0.06949756294488907\n",
      "Iteration 205, Batch: 6, Loss: 0.04207606613636017\n",
      "Iteration 205, Batch: 7, Loss: 0.05837100371718407\n",
      "Iteration 205, Batch: 8, Loss: 0.07292401045560837\n",
      "Iteration 205, Batch: 9, Loss: 0.0376933328807354\n",
      "Iteration 205, Batch: 10, Loss: 0.056144941598176956\n",
      "Iteration 205, Batch: 11, Loss: 0.05202534422278404\n",
      "Iteration 205, Batch: 12, Loss: 0.07019083201885223\n",
      "Iteration 205, Batch: 13, Loss: 0.046823278069496155\n",
      "Iteration 205, Batch: 14, Loss: 0.07184413075447083\n",
      "Iteration 205, Batch: 15, Loss: 0.07377465814352036\n",
      "Iteration 205, Batch: 16, Loss: 0.052304428070783615\n",
      "Iteration 205, Batch: 17, Loss: 0.08267321437597275\n",
      "Iteration 205, Batch: 18, Loss: 0.048803288489580154\n",
      "Iteration 205, Batch: 19, Loss: 0.04536563903093338\n",
      "Iteration 205, Batch: 20, Loss: 0.06814520061016083\n",
      "Iteration 205, Batch: 21, Loss: 0.03923625499010086\n",
      "Iteration 205, Batch: 22, Loss: 0.04913831502199173\n",
      "Iteration 205, Batch: 23, Loss: 0.053971461951732635\n",
      "Iteration 205, Batch: 24, Loss: 0.049784429371356964\n",
      "Iteration 205, Batch: 25, Loss: 0.04756684973835945\n",
      "Iteration 205, Batch: 26, Loss: 0.043333619832992554\n",
      "Iteration 205, Batch: 27, Loss: 0.07886777818202972\n",
      "Iteration 205, Batch: 28, Loss: 0.051901139318943024\n",
      "Iteration 205, Batch: 29, Loss: 0.08536773175001144\n",
      "Iteration 205, Batch: 30, Loss: 0.08935193717479706\n",
      "Iteration 205, Batch: 31, Loss: 0.044064395129680634\n",
      "Iteration 205, Batch: 32, Loss: 0.060876596719026566\n",
      "Iteration 205, Batch: 33, Loss: 0.08508816361427307\n",
      "Iteration 205, Batch: 34, Loss: 0.08936555683612823\n",
      "Iteration 205, Batch: 35, Loss: 0.0812423899769783\n",
      "Iteration 205, Batch: 36, Loss: 0.04909149929881096\n",
      "Iteration 205, Batch: 37, Loss: 0.07920564711093903\n",
      "Iteration 205, Batch: 38, Loss: 0.05370411276817322\n",
      "Iteration 205, Batch: 39, Loss: 0.06020616367459297\n",
      "Iteration 205, Batch: 40, Loss: 0.05573218688368797\n",
      "Iteration 205, Batch: 41, Loss: 0.06280234456062317\n",
      "Iteration 205, Batch: 42, Loss: 0.07383739948272705\n",
      "Iteration 205, Batch: 43, Loss: 0.05865085497498512\n",
      "Iteration 205, Batch: 44, Loss: 0.0738850086927414\n",
      "Iteration 205, Batch: 45, Loss: 0.06701847165822983\n",
      "Iteration 205, Batch: 46, Loss: 0.0787324383854866\n",
      "Iteration 205, Batch: 47, Loss: 0.07300439476966858\n",
      "Iteration 205, Batch: 48, Loss: 0.052549343556165695\n",
      "Iteration 205, Batch: 49, Loss: 0.06760706007480621\n",
      "Iteration 206, Batch: 0, Loss: 0.04352065920829773\n",
      "Iteration 206, Batch: 1, Loss: 0.10772625356912613\n",
      "Iteration 206, Batch: 2, Loss: 0.07242939621210098\n",
      "Iteration 206, Batch: 3, Loss: 0.05981665477156639\n",
      "Iteration 206, Batch: 4, Loss: 0.08563512563705444\n",
      "Iteration 206, Batch: 5, Loss: 0.06953462958335876\n",
      "Iteration 206, Batch: 6, Loss: 0.07032889127731323\n",
      "Iteration 206, Batch: 7, Loss: 0.06453833729028702\n",
      "Iteration 206, Batch: 8, Loss: 0.05282283574342728\n",
      "Iteration 206, Batch: 9, Loss: 0.05791763588786125\n",
      "Iteration 206, Batch: 10, Loss: 0.06029045954346657\n",
      "Iteration 206, Batch: 11, Loss: 0.06916067749261856\n",
      "Iteration 206, Batch: 12, Loss: 0.07618379592895508\n",
      "Iteration 206, Batch: 13, Loss: 0.0737028568983078\n",
      "Iteration 206, Batch: 14, Loss: 0.0773891806602478\n",
      "Iteration 206, Batch: 15, Loss: 0.08693954348564148\n",
      "Iteration 206, Batch: 16, Loss: 0.08042345196008682\n",
      "Iteration 206, Batch: 17, Loss: 0.08190206438302994\n",
      "Iteration 206, Batch: 18, Loss: 0.06000655144453049\n",
      "Iteration 206, Batch: 19, Loss: 0.05560482293367386\n",
      "Iteration 206, Batch: 20, Loss: 0.06669583171606064\n",
      "Iteration 206, Batch: 21, Loss: 0.05163675546646118\n",
      "Iteration 206, Batch: 22, Loss: 0.08189195394515991\n",
      "Iteration 206, Batch: 23, Loss: 0.06422598659992218\n",
      "Iteration 206, Batch: 24, Loss: 0.06323331594467163\n",
      "Iteration 206, Batch: 25, Loss: 0.06030835956335068\n",
      "Iteration 206, Batch: 26, Loss: 0.0831180140376091\n",
      "Iteration 206, Batch: 27, Loss: 0.059547003358602524\n",
      "Iteration 206, Batch: 28, Loss: 0.07933495938777924\n",
      "Iteration 206, Batch: 29, Loss: 0.11812255531549454\n",
      "Iteration 206, Batch: 30, Loss: 0.1002790704369545\n",
      "Iteration 206, Batch: 31, Loss: 0.06889181584119797\n",
      "Iteration 206, Batch: 32, Loss: 0.1798657923936844\n",
      "Iteration 206, Batch: 33, Loss: 0.06336436420679092\n",
      "Iteration 206, Batch: 34, Loss: 0.08828655630350113\n",
      "Iteration 206, Batch: 35, Loss: 0.0643467903137207\n",
      "Iteration 206, Batch: 36, Loss: 0.04609124734997749\n",
      "Iteration 206, Batch: 37, Loss: 0.026756571605801582\n",
      "Iteration 206, Batch: 38, Loss: 0.06965240091085434\n",
      "Iteration 206, Batch: 39, Loss: 0.08888374269008636\n",
      "Iteration 206, Batch: 40, Loss: 0.04166380688548088\n",
      "Iteration 206, Batch: 41, Loss: 0.06208331882953644\n",
      "Iteration 206, Batch: 42, Loss: 0.054210126399993896\n",
      "Iteration 206, Batch: 43, Loss: 0.07686962187290192\n",
      "Iteration 206, Batch: 44, Loss: 0.06366996467113495\n",
      "Iteration 206, Batch: 45, Loss: 0.054508283734321594\n",
      "Iteration 206, Batch: 46, Loss: 0.04805566743016243\n",
      "Iteration 206, Batch: 47, Loss: 0.04606863856315613\n",
      "Iteration 206, Batch: 48, Loss: 0.04285256192088127\n",
      "Iteration 206, Batch: 49, Loss: 0.08562198281288147\n",
      "Iteration 207, Batch: 0, Loss: 0.06938993185758591\n",
      "Iteration 207, Batch: 1, Loss: 0.08471634984016418\n",
      "Iteration 207, Batch: 2, Loss: 0.07542736828327179\n",
      "Iteration 207, Batch: 3, Loss: 0.05414116382598877\n",
      "Iteration 207, Batch: 4, Loss: 0.07923883944749832\n",
      "Iteration 207, Batch: 5, Loss: 0.06317716091871262\n",
      "Iteration 207, Batch: 6, Loss: 0.08917297422885895\n",
      "Iteration 207, Batch: 7, Loss: 0.11526242643594742\n",
      "Iteration 207, Batch: 8, Loss: 0.06590442359447479\n",
      "Iteration 207, Batch: 9, Loss: 0.08090568333864212\n",
      "Iteration 207, Batch: 10, Loss: 0.07292098551988602\n",
      "Iteration 207, Batch: 11, Loss: 0.046126771718263626\n",
      "Iteration 207, Batch: 12, Loss: 0.08275792747735977\n",
      "Iteration 207, Batch: 13, Loss: 0.04962260648608208\n",
      "Iteration 207, Batch: 14, Loss: 0.0856839194893837\n",
      "Iteration 207, Batch: 15, Loss: 0.09291606396436691\n",
      "Iteration 207, Batch: 16, Loss: 0.05656658113002777\n",
      "Iteration 207, Batch: 17, Loss: 0.0818709284067154\n",
      "Iteration 207, Batch: 18, Loss: 0.05135352537035942\n",
      "Iteration 207, Batch: 19, Loss: 0.06196434050798416\n",
      "Iteration 207, Batch: 20, Loss: 0.06191468611359596\n",
      "Iteration 207, Batch: 21, Loss: 0.05990085378289223\n",
      "Iteration 207, Batch: 22, Loss: 0.05443725362420082\n",
      "Iteration 207, Batch: 23, Loss: 0.053996991366147995\n",
      "Iteration 207, Batch: 24, Loss: 0.05119956284761429\n",
      "Iteration 207, Batch: 25, Loss: 0.05187974497675896\n",
      "Iteration 207, Batch: 26, Loss: 0.05587979778647423\n",
      "Iteration 207, Batch: 27, Loss: 0.06720752269029617\n",
      "Iteration 207, Batch: 28, Loss: 0.08090299367904663\n",
      "Iteration 207, Batch: 29, Loss: 0.05404234305024147\n",
      "Iteration 207, Batch: 30, Loss: 0.07637432217597961\n",
      "Iteration 207, Batch: 31, Loss: 0.08773338049650192\n",
      "Iteration 207, Batch: 32, Loss: 0.057976026087999344\n",
      "Iteration 207, Batch: 33, Loss: 0.051753394305706024\n",
      "Iteration 207, Batch: 34, Loss: 0.07122759521007538\n",
      "Iteration 207, Batch: 35, Loss: 0.0824478343129158\n",
      "Iteration 207, Batch: 36, Loss: 0.05622226744890213\n",
      "Iteration 207, Batch: 37, Loss: 0.08098649978637695\n",
      "Iteration 207, Batch: 38, Loss: 0.054730381816625595\n",
      "Iteration 207, Batch: 39, Loss: 0.09185048937797546\n",
      "Iteration 207, Batch: 40, Loss: 0.06207653880119324\n",
      "Iteration 207, Batch: 41, Loss: 0.05583232641220093\n",
      "Iteration 207, Batch: 42, Loss: 0.05568363890051842\n",
      "Iteration 207, Batch: 43, Loss: 0.06459756195545197\n",
      "Iteration 207, Batch: 44, Loss: 0.0604986734688282\n",
      "Iteration 207, Batch: 45, Loss: 0.09491239488124847\n",
      "Iteration 207, Batch: 46, Loss: 0.07049845904111862\n",
      "Iteration 207, Batch: 47, Loss: 0.07601478695869446\n",
      "Iteration 207, Batch: 48, Loss: 0.057029154151678085\n",
      "Iteration 207, Batch: 49, Loss: 0.05399882048368454\n",
      "Iteration 208, Batch: 0, Loss: 0.05903330817818642\n",
      "Iteration 208, Batch: 1, Loss: 0.049754925072193146\n",
      "Iteration 208, Batch: 2, Loss: 0.0632607564330101\n",
      "Iteration 208, Batch: 3, Loss: 0.05249979719519615\n",
      "Iteration 208, Batch: 4, Loss: 0.04017733410000801\n",
      "Iteration 208, Batch: 5, Loss: 0.053293026983737946\n",
      "Iteration 208, Batch: 6, Loss: 0.0650913342833519\n",
      "Iteration 208, Batch: 7, Loss: 0.05100909620523453\n",
      "Iteration 208, Batch: 8, Loss: 0.0562157966196537\n",
      "Iteration 208, Batch: 9, Loss: 0.09618924558162689\n",
      "Iteration 208, Batch: 10, Loss: 0.09200379997491837\n",
      "Iteration 208, Batch: 11, Loss: 0.07245705276727676\n",
      "Iteration 208, Batch: 12, Loss: 0.06565190851688385\n",
      "Iteration 208, Batch: 13, Loss: 0.06316941976547241\n",
      "Iteration 208, Batch: 14, Loss: 0.04570787772536278\n",
      "Iteration 208, Batch: 15, Loss: 0.06154617667198181\n",
      "Iteration 208, Batch: 16, Loss: 0.04119284451007843\n",
      "Iteration 208, Batch: 17, Loss: 0.07146419584751129\n",
      "Iteration 208, Batch: 18, Loss: 0.05073648318648338\n",
      "Iteration 208, Batch: 19, Loss: 0.04332241043448448\n",
      "Iteration 208, Batch: 20, Loss: 0.07393894344568253\n",
      "Iteration 208, Batch: 21, Loss: 0.09212981909513474\n",
      "Iteration 208, Batch: 22, Loss: 0.05845719203352928\n",
      "Iteration 208, Batch: 23, Loss: 0.05235688015818596\n",
      "Iteration 208, Batch: 24, Loss: 0.03817901387810707\n",
      "Iteration 208, Batch: 25, Loss: 0.039563968777656555\n",
      "Iteration 208, Batch: 26, Loss: 0.06364183872938156\n",
      "Iteration 208, Batch: 27, Loss: 0.0664232075214386\n",
      "Iteration 208, Batch: 28, Loss: 0.048481572419404984\n",
      "Iteration 208, Batch: 29, Loss: 0.09774362295866013\n",
      "Iteration 208, Batch: 30, Loss: 0.0704525038599968\n",
      "Iteration 208, Batch: 31, Loss: 0.06233898550271988\n",
      "Iteration 208, Batch: 32, Loss: 0.056973714381456375\n",
      "Iteration 208, Batch: 33, Loss: 0.05668877437710762\n",
      "Iteration 208, Batch: 34, Loss: 0.04766137897968292\n",
      "Iteration 208, Batch: 35, Loss: 0.08534339815378189\n",
      "Iteration 208, Batch: 36, Loss: 0.047823715955019\n",
      "Iteration 208, Batch: 37, Loss: 0.052947815507650375\n",
      "Iteration 208, Batch: 38, Loss: 0.06066666170954704\n",
      "Iteration 208, Batch: 39, Loss: 0.07867034524679184\n",
      "Iteration 208, Batch: 40, Loss: 0.06113680824637413\n",
      "Iteration 208, Batch: 41, Loss: 0.04734009504318237\n",
      "Iteration 208, Batch: 42, Loss: 0.0775468498468399\n",
      "Iteration 208, Batch: 43, Loss: 0.0683739110827446\n",
      "Iteration 208, Batch: 44, Loss: 0.05287482589483261\n",
      "Iteration 208, Batch: 45, Loss: 0.03831464797258377\n",
      "Iteration 208, Batch: 46, Loss: 0.0602131150662899\n",
      "Iteration 208, Batch: 47, Loss: 0.07425300031900406\n",
      "Iteration 208, Batch: 48, Loss: 0.07089166343212128\n",
      "Iteration 208, Batch: 49, Loss: 0.07261443883180618\n",
      "Iteration 209, Batch: 0, Loss: 0.056135620921850204\n",
      "Iteration 209, Batch: 1, Loss: 0.08789590746164322\n",
      "Iteration 209, Batch: 2, Loss: 0.05550488457083702\n",
      "Iteration 209, Batch: 3, Loss: 0.059478264302015305\n",
      "Iteration 209, Batch: 4, Loss: 0.0482405461370945\n",
      "Iteration 209, Batch: 5, Loss: 0.04576664790511131\n",
      "Iteration 209, Batch: 6, Loss: 0.059602998197078705\n",
      "Iteration 209, Batch: 7, Loss: 0.06960005313158035\n",
      "Iteration 209, Batch: 8, Loss: 0.0695284754037857\n",
      "Iteration 209, Batch: 9, Loss: 0.06519375741481781\n",
      "Iteration 209, Batch: 10, Loss: 0.06977182626724243\n",
      "Iteration 209, Batch: 11, Loss: 0.05616234615445137\n",
      "Iteration 209, Batch: 12, Loss: 0.05511854216456413\n",
      "Iteration 209, Batch: 13, Loss: 0.08044055104255676\n",
      "Iteration 209, Batch: 14, Loss: 0.06619144976139069\n",
      "Iteration 209, Batch: 15, Loss: 0.07132585346698761\n",
      "Iteration 209, Batch: 16, Loss: 0.0726643055677414\n",
      "Iteration 209, Batch: 17, Loss: 0.06716332584619522\n",
      "Iteration 209, Batch: 18, Loss: 0.07423989474773407\n",
      "Iteration 209, Batch: 19, Loss: 0.07325766235589981\n",
      "Iteration 209, Batch: 20, Loss: 0.06331576406955719\n",
      "Iteration 209, Batch: 21, Loss: 0.10202541947364807\n",
      "Iteration 209, Batch: 22, Loss: 0.07042009383440018\n",
      "Iteration 209, Batch: 23, Loss: 0.05430782213807106\n",
      "Iteration 209, Batch: 24, Loss: 0.07795111835002899\n",
      "Iteration 209, Batch: 25, Loss: 0.07516183704137802\n",
      "Iteration 209, Batch: 26, Loss: 0.07065887004137039\n",
      "Iteration 209, Batch: 27, Loss: 0.08025193959474564\n",
      "Iteration 209, Batch: 28, Loss: 0.06071849539875984\n",
      "Iteration 209, Batch: 29, Loss: 0.08527186512947083\n",
      "Iteration 209, Batch: 30, Loss: 0.04413524642586708\n",
      "Iteration 209, Batch: 31, Loss: 0.06698483973741531\n",
      "Iteration 209, Batch: 32, Loss: 0.054046619683504105\n",
      "Iteration 209, Batch: 33, Loss: 0.07732630521059036\n",
      "Iteration 209, Batch: 34, Loss: 0.07217006385326385\n",
      "Iteration 209, Batch: 35, Loss: 0.03252327814698219\n",
      "Iteration 209, Batch: 36, Loss: 0.08688154071569443\n",
      "Iteration 209, Batch: 37, Loss: 0.04760591313242912\n",
      "Iteration 209, Batch: 38, Loss: 0.08329007774591446\n",
      "Iteration 209, Batch: 39, Loss: 0.0433284267783165\n",
      "Iteration 209, Batch: 40, Loss: 0.07709363847970963\n",
      "Iteration 209, Batch: 41, Loss: 0.04913502559065819\n",
      "Iteration 209, Batch: 42, Loss: 0.07755880802869797\n",
      "Iteration 209, Batch: 43, Loss: 0.0808112621307373\n",
      "Iteration 209, Batch: 44, Loss: 0.07269212603569031\n",
      "Iteration 209, Batch: 45, Loss: 0.08249381184577942\n",
      "Iteration 209, Batch: 46, Loss: 0.08065768331289291\n",
      "Iteration 209, Batch: 47, Loss: 0.08541447669267654\n",
      "Iteration 209, Batch: 48, Loss: 0.09103269129991531\n",
      "Iteration 209, Batch: 49, Loss: 0.08554267883300781\n",
      "Iteration 210, Batch: 0, Loss: 0.06976392865180969\n",
      "Iteration 210, Batch: 1, Loss: 0.11851434409618378\n",
      "Iteration 210, Batch: 2, Loss: 0.08031146973371506\n",
      "Iteration 210, Batch: 3, Loss: 0.0731772631406784\n",
      "Iteration 210, Batch: 4, Loss: 0.07531199604272842\n",
      "Iteration 210, Batch: 5, Loss: 0.07159249484539032\n",
      "Iteration 210, Batch: 6, Loss: 0.03901538997888565\n",
      "Iteration 210, Batch: 7, Loss: 0.04152906313538551\n",
      "Iteration 210, Batch: 8, Loss: 0.10533681511878967\n",
      "Iteration 210, Batch: 9, Loss: 0.0518733412027359\n",
      "Iteration 210, Batch: 10, Loss: 0.07539524883031845\n",
      "Iteration 210, Batch: 11, Loss: 0.06553132832050323\n",
      "Iteration 210, Batch: 12, Loss: 0.046130288392305374\n",
      "Iteration 210, Batch: 13, Loss: 0.0667678639292717\n",
      "Iteration 210, Batch: 14, Loss: 0.08446233719587326\n",
      "Iteration 210, Batch: 15, Loss: 0.051790911704301834\n",
      "Iteration 210, Batch: 16, Loss: 0.03901337832212448\n",
      "Iteration 210, Batch: 17, Loss: 0.04387526586651802\n",
      "Iteration 210, Batch: 18, Loss: 0.060329753905534744\n",
      "Iteration 210, Batch: 19, Loss: 0.07652682065963745\n",
      "Iteration 210, Batch: 20, Loss: 0.07417172193527222\n",
      "Iteration 210, Batch: 21, Loss: 0.049898695200681686\n",
      "Iteration 210, Batch: 22, Loss: 0.054437413811683655\n",
      "Iteration 210, Batch: 23, Loss: 0.08337003737688065\n",
      "Iteration 210, Batch: 24, Loss: 0.07687143236398697\n",
      "Iteration 210, Batch: 25, Loss: 0.04333537444472313\n",
      "Iteration 210, Batch: 26, Loss: 0.06368003040552139\n",
      "Iteration 210, Batch: 27, Loss: 0.07782473415136337\n",
      "Iteration 210, Batch: 28, Loss: 0.046910591423511505\n",
      "Iteration 210, Batch: 29, Loss: 0.05482093617320061\n",
      "Iteration 210, Batch: 30, Loss: 0.07140295207500458\n",
      "Iteration 210, Batch: 31, Loss: 0.07203994691371918\n",
      "Iteration 210, Batch: 32, Loss: 0.09632720053195953\n",
      "Iteration 210, Batch: 33, Loss: 0.07297268509864807\n",
      "Iteration 210, Batch: 34, Loss: 0.07870431244373322\n",
      "Iteration 210, Batch: 35, Loss: 0.08863037079572678\n",
      "Iteration 210, Batch: 36, Loss: 0.06890609860420227\n",
      "Iteration 210, Batch: 37, Loss: 0.09383426606655121\n",
      "Iteration 210, Batch: 38, Loss: 0.09345542639493942\n",
      "Iteration 210, Batch: 39, Loss: 0.08929095417261124\n",
      "Iteration 210, Batch: 40, Loss: 0.10171700268983841\n",
      "Iteration 210, Batch: 41, Loss: 0.06207873672246933\n",
      "Iteration 210, Batch: 42, Loss: 0.07454788684844971\n",
      "Iteration 210, Batch: 43, Loss: 0.07395250350236893\n",
      "Iteration 210, Batch: 44, Loss: 0.07318463176488876\n",
      "Iteration 210, Batch: 45, Loss: 0.058799948543310165\n",
      "Iteration 210, Batch: 46, Loss: 0.08050872385501862\n",
      "Iteration 210, Batch: 47, Loss: 0.07676271349191666\n",
      "Iteration 210, Batch: 48, Loss: 0.04727238044142723\n",
      "Iteration 210, Batch: 49, Loss: 0.06482307612895966\n",
      "Iteration 211, Batch: 0, Loss: 0.043556373566389084\n",
      "Iteration 211, Batch: 1, Loss: 0.06894344091415405\n",
      "Iteration 211, Batch: 2, Loss: 0.06107901409268379\n",
      "Iteration 211, Batch: 3, Loss: 0.059925541281700134\n",
      "Iteration 211, Batch: 4, Loss: 0.08523032814264297\n",
      "Iteration 211, Batch: 5, Loss: 0.05783894285559654\n",
      "Iteration 211, Batch: 6, Loss: 0.07965636998414993\n",
      "Iteration 211, Batch: 7, Loss: 0.08131099492311478\n",
      "Iteration 211, Batch: 8, Loss: 0.05514360964298248\n",
      "Iteration 211, Batch: 9, Loss: 0.06594587862491608\n",
      "Iteration 211, Batch: 10, Loss: 0.07239393144845963\n",
      "Iteration 211, Batch: 11, Loss: 0.05662181228399277\n",
      "Iteration 211, Batch: 12, Loss: 0.05790717899799347\n",
      "Iteration 211, Batch: 13, Loss: 0.06215577572584152\n",
      "Iteration 211, Batch: 14, Loss: 0.036649540066719055\n",
      "Iteration 211, Batch: 15, Loss: 0.05761170759797096\n",
      "Iteration 211, Batch: 16, Loss: 0.0800580382347107\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m loss_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Get loss from model\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mc_loss_batch_simul(model, batch, time_array, n, k, time_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m325\u001b[39m, p_bad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     37\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     38\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/home/groups/montanar/vietvu01/SparseDiffusion/loss.py:81\u001b[0m, in \u001b[0;36mmc_loss_batch_simul\u001b[0;34m(model, batch, time_array, n, k, time_threshold, p_bad, device)\u001b[0m\n\u001b[1;32m     78\u001b[0m y_t_expand \u001b[38;5;241m=\u001b[39m y_t\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Get output from the denoiser\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m out \u001b[38;5;241m=\u001b[39m model(x_expand, y_t_expand, random_times)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# Get loss by rank-1 matrices\u001b[39;00m\n\u001b[1;32m     84\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((out \u001b[38;5;241m-\u001b[39m batch_outer) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 106\u001b[0m, in \u001b[0;36mTestMPNN_3.forward\u001b[0;34m(self, X, E, time_array, noise_free)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Get new output\u001b[39;00m\n\u001b[1;32m    105\u001b[0m upd_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([v, agg], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)    \u001b[38;5;66;03m# [B, N, 2]\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m v_new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupd_mlps[l](upd_in)            \u001b[38;5;66;03m# [B, N, 1]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# Update\u001b[39;00m\n\u001b[1;32m    109\u001b[0m alpha_l \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogit_alpha[l]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha_max\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/nn/modules/activation.py:828\u001b[0m, in \u001b[0;36mLeakyReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 828\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnegative_slope, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minplace)\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/nn/functional.py:1902\u001b[0m, in \u001b[0;36mleaky_relu\u001b[0;34m(input, negative_slope, inplace)\u001b[0m\n\u001b[1;32m   1900\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mleaky_relu_(\u001b[38;5;28minput\u001b[39m, negative_slope)\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1902\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mleaky_relu(\u001b[38;5;28minput\u001b[39m, negative_slope)\n\u001b[1;32m   1903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First fine-tuning round for n = 350. Do not touch for n = 150\n",
    "# Specify parameters\n",
    "N = 10000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Reload model\n",
    "model = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"test_2_intermediate_1.pth\", weights_only=False))\n",
    "\n",
    "# Create dataset for training\n",
    "train_data = sample_data(N, n, k, device=device)\n",
    "train_dataset = SubmatrixDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "# Specify parameters\n",
    "num_epochs = 300\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# When fine-tune, have to reset min_loss to take into account the new points\n",
    "min_loss = float(\"inf\")\n",
    "model_opt = deepcopy(model)\n",
    "\n",
    "for it in range(num_epochs):\n",
    "    counter = 0\n",
    "    loss_total = 0\n",
    "    for batch in train_dataloader:\n",
    "        # Get loss from model\n",
    "        loss = mc_loss_batch_simul(model, batch, time_array, n, k, time_threshold=325, p_bad=0.05, device=device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "        # Backpropagate\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Iteration {}, Batch: {}, Loss: {}\".format(it, counter, loss.item()))\n",
    "\n",
    "        # Update counter\n",
    "        counter += 1\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    # Update best model\n",
    "    with torch.no_grad():\n",
    "        if loss_total < min_loss:\n",
    "            min_loss = loss_total\n",
    "            model_opt = deepcopy(model)\n",
    "\n",
    "# Save optimal model to save time for generation\n",
    "torch.save(model_opt.state_dict(), \"test_2_intermediate_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb497c90-0e2c-41eb-92d8-f2602a5baa13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimal model to save time for generation\n",
    "torch.save(model_opt.state_dict(), \"test_2_intermediate_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a28555b3-fdea-4891-83af-7809bfde1f30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Batch: 0, Loss: 0.1428450047969818\n",
      "Iteration 0, Batch: 1, Loss: 0.12676885724067688\n",
      "Iteration 0, Batch: 2, Loss: 0.14825084805488586\n",
      "Iteration 0, Batch: 3, Loss: 0.1680430769920349\n",
      "Iteration 0, Batch: 4, Loss: 0.09954133629798889\n",
      "Iteration 0, Batch: 5, Loss: 0.15108445286750793\n",
      "Iteration 0, Batch: 6, Loss: 0.0937529131770134\n",
      "Iteration 0, Batch: 7, Loss: 0.16559863090515137\n",
      "Iteration 0, Batch: 8, Loss: 0.10987984389066696\n",
      "Iteration 0, Batch: 9, Loss: 0.12722806632518768\n",
      "Iteration 0, Batch: 10, Loss: 0.14999760687351227\n",
      "Iteration 0, Batch: 11, Loss: 0.11992042511701584\n",
      "Iteration 0, Batch: 12, Loss: 0.13086305558681488\n",
      "Iteration 0, Batch: 13, Loss: 0.11362026631832123\n",
      "Iteration 0, Batch: 14, Loss: 0.12464948743581772\n",
      "Iteration 0, Batch: 15, Loss: 0.15490560233592987\n",
      "Iteration 0, Batch: 16, Loss: 0.16568470001220703\n",
      "Iteration 0, Batch: 17, Loss: 0.13603676855564117\n",
      "Iteration 0, Batch: 18, Loss: 0.10905703902244568\n",
      "Iteration 0, Batch: 19, Loss: 0.1377224177122116\n",
      "Iteration 0, Batch: 20, Loss: 0.14016427099704742\n",
      "Iteration 0, Batch: 21, Loss: 0.13267777860164642\n",
      "Iteration 0, Batch: 22, Loss: 0.1312379390001297\n",
      "Iteration 0, Batch: 23, Loss: 0.13099633157253265\n",
      "Iteration 0, Batch: 24, Loss: 0.10468259453773499\n",
      "Iteration 0, Batch: 25, Loss: 0.120147705078125\n",
      "Iteration 0, Batch: 26, Loss: 0.1252128779888153\n",
      "Iteration 0, Batch: 27, Loss: 0.14315178990364075\n",
      "Iteration 0, Batch: 28, Loss: 0.08789331465959549\n",
      "Iteration 0, Batch: 29, Loss: 0.13707835972309113\n",
      "Iteration 0, Batch: 30, Loss: 0.10260309278964996\n",
      "Iteration 0, Batch: 31, Loss: 0.13315925002098083\n",
      "Iteration 0, Batch: 32, Loss: 0.13771609961986542\n",
      "Iteration 0, Batch: 33, Loss: 0.11112752556800842\n",
      "Iteration 0, Batch: 34, Loss: 0.12150567024946213\n",
      "Iteration 0, Batch: 35, Loss: 0.16114932298660278\n",
      "Iteration 0, Batch: 36, Loss: 0.14221738278865814\n",
      "Iteration 0, Batch: 37, Loss: 0.11374219506978989\n",
      "Iteration 0, Batch: 38, Loss: 0.10837217420339584\n",
      "Iteration 0, Batch: 39, Loss: 0.14595262706279755\n",
      "Iteration 0, Batch: 40, Loss: 0.13907118141651154\n",
      "Iteration 0, Batch: 41, Loss: 0.12189732491970062\n",
      "Iteration 0, Batch: 42, Loss: 0.11907102167606354\n",
      "Iteration 0, Batch: 43, Loss: 0.12853243947029114\n",
      "Iteration 0, Batch: 44, Loss: 0.10190784186124802\n",
      "Iteration 0, Batch: 45, Loss: 0.12255008518695831\n",
      "Iteration 0, Batch: 46, Loss: 0.11249247938394547\n",
      "Iteration 0, Batch: 47, Loss: 0.13456329703330994\n",
      "Iteration 0, Batch: 48, Loss: 0.11075133830308914\n",
      "Iteration 0, Batch: 49, Loss: 0.14776764810085297\n",
      "Iteration 1, Batch: 0, Loss: 0.0004836777807213366\n",
      "Iteration 1, Batch: 1, Loss: 0.004219341091811657\n",
      "Iteration 1, Batch: 2, Loss: 0.00014379224739968777\n",
      "Iteration 1, Batch: 3, Loss: 7.684768206672743e-05\n",
      "Iteration 1, Batch: 4, Loss: 5.190086449147202e-05\n",
      "Iteration 1, Batch: 5, Loss: 7.906493556220084e-05\n",
      "Iteration 1, Batch: 6, Loss: 0.00012151854753028601\n",
      "Iteration 1, Batch: 7, Loss: 0.000164856159244664\n",
      "Iteration 1, Batch: 8, Loss: 0.00020213528478052467\n",
      "Iteration 1, Batch: 9, Loss: 0.00020014989422634244\n",
      "Iteration 1, Batch: 10, Loss: 0.00015016818360891193\n",
      "Iteration 1, Batch: 11, Loss: 9.947050421033055e-05\n",
      "Iteration 1, Batch: 12, Loss: 6.456722621805966e-05\n",
      "Iteration 1, Batch: 13, Loss: 6.273789767874405e-05\n",
      "Iteration 1, Batch: 14, Loss: 7.418588211294264e-05\n",
      "Iteration 1, Batch: 15, Loss: 4.421028643264435e-05\n",
      "Iteration 1, Batch: 16, Loss: 3.24916654790286e-05\n",
      "Iteration 1, Batch: 17, Loss: 0.002057798206806183\n",
      "Iteration 1, Batch: 18, Loss: 7.43405826142407e-06\n",
      "Iteration 1, Batch: 19, Loss: 1.6699335901648737e-05\n",
      "Iteration 1, Batch: 20, Loss: 2.6417845219839364e-05\n",
      "Iteration 1, Batch: 21, Loss: 3.278688745922409e-05\n",
      "Iteration 1, Batch: 22, Loss: 1.9317703845445067e-05\n",
      "Iteration 1, Batch: 23, Loss: 3.7523128412431106e-05\n",
      "Iteration 1, Batch: 24, Loss: 0.00038246429176069796\n",
      "Iteration 1, Batch: 25, Loss: 1.1332575013511814e-05\n",
      "Iteration 1, Batch: 26, Loss: 1.2241871445439756e-05\n",
      "Iteration 1, Batch: 27, Loss: 1.69276991073275e-05\n",
      "Iteration 1, Batch: 28, Loss: 1.1070831533288583e-05\n",
      "Iteration 1, Batch: 29, Loss: 1.4303937859949656e-05\n",
      "Iteration 1, Batch: 30, Loss: 4.486661964619998e-06\n",
      "Iteration 1, Batch: 31, Loss: 9.561480510456022e-06\n",
      "Iteration 1, Batch: 32, Loss: 1.2544821402116213e-05\n",
      "Iteration 1, Batch: 33, Loss: 1.3712707186641637e-05\n",
      "Iteration 1, Batch: 34, Loss: 9.955643690773286e-06\n",
      "Iteration 1, Batch: 35, Loss: 9.985719771066215e-06\n",
      "Iteration 1, Batch: 36, Loss: 1.2855792192567606e-05\n",
      "Iteration 1, Batch: 37, Loss: 5.8723553593154065e-06\n",
      "Iteration 1, Batch: 38, Loss: 1.0943528650386725e-05\n",
      "Iteration 1, Batch: 39, Loss: 1.3189179298933595e-05\n",
      "Iteration 1, Batch: 40, Loss: 4.860276021645404e-06\n",
      "Iteration 1, Batch: 41, Loss: 5.124321887706174e-06\n",
      "Iteration 1, Batch: 42, Loss: 4.118984634260414e-06\n",
      "Iteration 1, Batch: 43, Loss: 4.22889570472762e-06\n",
      "Iteration 1, Batch: 44, Loss: 6.416822998289717e-06\n",
      "Iteration 1, Batch: 45, Loss: 5.386496468418045e-06\n",
      "Iteration 1, Batch: 46, Loss: 4.551955498754978e-06\n",
      "Iteration 1, Batch: 47, Loss: 5.663709998771083e-06\n",
      "Iteration 1, Batch: 48, Loss: 4.102729235455627e-06\n",
      "Iteration 1, Batch: 49, Loss: 6.293177648331039e-06\n",
      "Iteration 2, Batch: 0, Loss: 0.13431154191493988\n",
      "Iteration 2, Batch: 1, Loss: 0.13634082674980164\n",
      "Iteration 2, Batch: 2, Loss: 0.14447429776191711\n",
      "Iteration 2, Batch: 3, Loss: 0.12210258096456528\n",
      "Iteration 2, Batch: 4, Loss: 0.12553609907627106\n",
      "Iteration 2, Batch: 5, Loss: 0.1526118516921997\n",
      "Iteration 2, Batch: 6, Loss: 0.11408305168151855\n",
      "Iteration 2, Batch: 7, Loss: 0.1559203863143921\n",
      "Iteration 2, Batch: 8, Loss: 0.10833869874477386\n",
      "Iteration 2, Batch: 9, Loss: 0.1261702924966812\n",
      "Iteration 2, Batch: 10, Loss: 0.12395359575748444\n",
      "Iteration 2, Batch: 11, Loss: 0.15365692973136902\n",
      "Iteration 2, Batch: 12, Loss: 0.1394539475440979\n",
      "Iteration 2, Batch: 13, Loss: 0.13421382009983063\n",
      "Iteration 2, Batch: 14, Loss: 0.1408698707818985\n",
      "Iteration 2, Batch: 15, Loss: 0.10876074433326721\n",
      "Iteration 2, Batch: 16, Loss: 0.13755454123020172\n",
      "Iteration 2, Batch: 17, Loss: 0.11595355719327927\n",
      "Iteration 2, Batch: 18, Loss: 0.13293017446994781\n",
      "Iteration 2, Batch: 19, Loss: 0.1046397015452385\n",
      "Iteration 2, Batch: 20, Loss: 0.15940886735916138\n",
      "Iteration 2, Batch: 21, Loss: 0.12138155847787857\n",
      "Iteration 2, Batch: 22, Loss: 0.13167241215705872\n",
      "Iteration 2, Batch: 23, Loss: 0.13463537395000458\n",
      "Iteration 2, Batch: 24, Loss: 0.16090667247772217\n",
      "Iteration 2, Batch: 25, Loss: 0.1450585275888443\n",
      "Iteration 2, Batch: 26, Loss: 0.1213703379034996\n",
      "Iteration 2, Batch: 27, Loss: 0.13885077834129333\n",
      "Iteration 2, Batch: 28, Loss: 0.14166046679019928\n",
      "Iteration 2, Batch: 29, Loss: 0.1349414438009262\n",
      "Iteration 2, Batch: 30, Loss: 0.13016431033611298\n",
      "Iteration 2, Batch: 31, Loss: 0.0946134701371193\n",
      "Iteration 2, Batch: 32, Loss: 0.12110143154859543\n",
      "Iteration 2, Batch: 33, Loss: 0.12704730033874512\n",
      "Iteration 2, Batch: 34, Loss: 0.1669500321149826\n",
      "Iteration 2, Batch: 35, Loss: 0.15704430639743805\n",
      "Iteration 2, Batch: 36, Loss: 0.10267440229654312\n",
      "Iteration 2, Batch: 37, Loss: 0.1425158679485321\n",
      "Iteration 2, Batch: 38, Loss: 0.12673169374465942\n",
      "Iteration 2, Batch: 39, Loss: 0.13313084840774536\n",
      "Iteration 2, Batch: 40, Loss: 0.11082082241773605\n",
      "Iteration 2, Batch: 41, Loss: 0.14710445702075958\n",
      "Iteration 2, Batch: 42, Loss: 0.12467517703771591\n",
      "Iteration 2, Batch: 43, Loss: 0.1155669167637825\n",
      "Iteration 2, Batch: 44, Loss: 0.10141962021589279\n",
      "Iteration 2, Batch: 45, Loss: 0.10878024995326996\n",
      "Iteration 2, Batch: 46, Loss: 0.1033003181219101\n",
      "Iteration 2, Batch: 47, Loss: 0.1407407522201538\n",
      "Iteration 2, Batch: 48, Loss: 0.11257816106081009\n",
      "Iteration 2, Batch: 49, Loss: 0.10696851462125778\n",
      "Iteration 3, Batch: 0, Loss: 0.019856689497828484\n",
      "Iteration 3, Batch: 1, Loss: 0.0035592203494161367\n",
      "Iteration 3, Batch: 2, Loss: 8.701414481038228e-05\n",
      "Iteration 3, Batch: 3, Loss: 0.0016295795794576406\n",
      "Iteration 3, Batch: 4, Loss: 7.892845314927399e-05\n",
      "Iteration 3, Batch: 5, Loss: 2.77831859420985e-05\n",
      "Iteration 3, Batch: 6, Loss: 5.0851616833824664e-05\n",
      "Iteration 3, Batch: 7, Loss: 0.002446133876219392\n",
      "Iteration 3, Batch: 8, Loss: 0.00011873025505337864\n",
      "Iteration 3, Batch: 9, Loss: 0.00012038748536724597\n",
      "Iteration 3, Batch: 10, Loss: 0.00016353286628145725\n",
      "Iteration 3, Batch: 11, Loss: 0.0001959892688319087\n",
      "Iteration 3, Batch: 12, Loss: 0.00021845697483513504\n",
      "Iteration 3, Batch: 13, Loss: 0.00023539164976682514\n",
      "Iteration 3, Batch: 14, Loss: 0.00018466768960934132\n",
      "Iteration 3, Batch: 15, Loss: 0.0001509806897956878\n",
      "Iteration 3, Batch: 16, Loss: 0.0001116324492613785\n",
      "Iteration 3, Batch: 17, Loss: 9.933011460816488e-05\n",
      "Iteration 3, Batch: 18, Loss: 6.137784657767043e-05\n",
      "Iteration 3, Batch: 19, Loss: 3.670458318083547e-05\n",
      "Iteration 3, Batch: 20, Loss: 2.566577859397512e-05\n",
      "Iteration 3, Batch: 21, Loss: 0.00010004790965467691\n",
      "Iteration 3, Batch: 22, Loss: 3.137511885142885e-05\n",
      "Iteration 3, Batch: 23, Loss: 3.4200085792690516e-05\n",
      "Iteration 3, Batch: 24, Loss: 1.9285973394289613e-05\n",
      "Iteration 3, Batch: 25, Loss: 1.1979973351117224e-05\n",
      "Iteration 3, Batch: 26, Loss: 1.5694329704274423e-05\n",
      "Iteration 3, Batch: 27, Loss: 2.3951171897351742e-05\n",
      "Iteration 3, Batch: 28, Loss: 0.0037163232918828726\n",
      "Iteration 3, Batch: 29, Loss: 1.4663866750197485e-05\n",
      "Iteration 3, Batch: 30, Loss: 1.0290353202435654e-05\n",
      "Iteration 3, Batch: 31, Loss: 1.1063477359130047e-05\n",
      "Iteration 3, Batch: 32, Loss: 1.6539486750843935e-05\n",
      "Iteration 3, Batch: 33, Loss: 1.7238349755643867e-05\n",
      "Iteration 3, Batch: 34, Loss: 1.275372505915584e-05\n",
      "Iteration 3, Batch: 35, Loss: 1.1031428584828973e-05\n",
      "Iteration 3, Batch: 36, Loss: 1.2591844097187277e-05\n",
      "Iteration 3, Batch: 37, Loss: 1.2199648153909948e-05\n",
      "Iteration 3, Batch: 38, Loss: 1.1097698916273657e-05\n",
      "Iteration 3, Batch: 39, Loss: 8.951402378443163e-06\n",
      "Iteration 3, Batch: 40, Loss: 9.062620847544167e-06\n",
      "Iteration 3, Batch: 41, Loss: 7.724655915808398e-06\n",
      "Iteration 3, Batch: 42, Loss: 7.367920261458494e-06\n",
      "Iteration 3, Batch: 43, Loss: 7.080141585902311e-06\n",
      "Iteration 3, Batch: 44, Loss: 5.5325804169115145e-06\n",
      "Iteration 3, Batch: 45, Loss: 5.629809948004549e-06\n",
      "Iteration 3, Batch: 46, Loss: 5.325591246219119e-06\n",
      "Iteration 3, Batch: 47, Loss: 5.000923920306377e-06\n",
      "Iteration 3, Batch: 48, Loss: 4.207326583127724e-06\n",
      "Iteration 3, Batch: 49, Loss: 3.5049390589847462e-06\n",
      "Iteration 4, Batch: 0, Loss: 0.15005117654800415\n",
      "Iteration 4, Batch: 1, Loss: 0.15094301104545593\n",
      "Iteration 4, Batch: 2, Loss: 0.14813372492790222\n",
      "Iteration 4, Batch: 3, Loss: 0.1276981085538864\n",
      "Iteration 4, Batch: 4, Loss: 0.13741527497768402\n",
      "Iteration 4, Batch: 5, Loss: 0.11308074742555618\n",
      "Iteration 4, Batch: 6, Loss: 0.12252674996852875\n",
      "Iteration 4, Batch: 7, Loss: 0.11807125806808472\n",
      "Iteration 4, Batch: 8, Loss: 0.15147273242473602\n",
      "Iteration 4, Batch: 9, Loss: 0.15024228394031525\n",
      "Iteration 4, Batch: 10, Loss: 0.1225166767835617\n",
      "Iteration 4, Batch: 11, Loss: 0.12018787115812302\n",
      "Iteration 4, Batch: 12, Loss: 0.12461602687835693\n",
      "Iteration 4, Batch: 13, Loss: 0.11113839596509933\n",
      "Iteration 4, Batch: 14, Loss: 0.1683492213487625\n",
      "Iteration 4, Batch: 15, Loss: 0.13117963075637817\n",
      "Iteration 4, Batch: 16, Loss: 0.10684603452682495\n",
      "Iteration 4, Batch: 17, Loss: 0.0664447769522667\n",
      "Iteration 4, Batch: 18, Loss: 0.11343255639076233\n",
      "Iteration 4, Batch: 19, Loss: 0.11231456696987152\n",
      "Iteration 4, Batch: 20, Loss: 0.09937123954296112\n",
      "Iteration 4, Batch: 21, Loss: 0.128339484333992\n",
      "Iteration 4, Batch: 22, Loss: 0.15311843156814575\n",
      "Iteration 4, Batch: 23, Loss: 0.1519235074520111\n",
      "Iteration 4, Batch: 24, Loss: 0.1417340785264969\n",
      "Iteration 4, Batch: 25, Loss: 0.12157297134399414\n",
      "Iteration 4, Batch: 26, Loss: 0.1326214075088501\n",
      "Iteration 4, Batch: 27, Loss: 0.14225904643535614\n",
      "Iteration 4, Batch: 28, Loss: 0.09491771459579468\n",
      "Iteration 4, Batch: 29, Loss: 0.1054694727063179\n",
      "Iteration 4, Batch: 30, Loss: 0.10932227969169617\n",
      "Iteration 4, Batch: 31, Loss: 0.13245412707328796\n",
      "Iteration 4, Batch: 32, Loss: 0.12134862691164017\n",
      "Iteration 4, Batch: 33, Loss: 0.14127209782600403\n",
      "Iteration 4, Batch: 34, Loss: 0.10156583786010742\n",
      "Iteration 4, Batch: 35, Loss: 0.11677568405866623\n",
      "Iteration 4, Batch: 36, Loss: 0.10362613201141357\n",
      "Iteration 4, Batch: 37, Loss: 0.11883042752742767\n",
      "Iteration 4, Batch: 38, Loss: 0.12312586605548859\n",
      "Iteration 4, Batch: 39, Loss: 0.09309005737304688\n",
      "Iteration 4, Batch: 40, Loss: 0.1562560647726059\n",
      "Iteration 4, Batch: 41, Loss: 0.10288497805595398\n",
      "Iteration 4, Batch: 42, Loss: 0.12176695466041565\n",
      "Iteration 4, Batch: 43, Loss: 0.13399450480937958\n",
      "Iteration 4, Batch: 44, Loss: 0.0988616794347763\n",
      "Iteration 4, Batch: 45, Loss: 0.13010913133621216\n",
      "Iteration 4, Batch: 46, Loss: 0.11617512255907059\n",
      "Iteration 4, Batch: 47, Loss: 0.15060736238956451\n",
      "Iteration 4, Batch: 48, Loss: 0.07863879948854446\n",
      "Iteration 4, Batch: 49, Loss: 0.09826848655939102\n",
      "Iteration 5, Batch: 0, Loss: 6.746647704858333e-05\n",
      "Iteration 5, Batch: 1, Loss: 2.2754915335099213e-05\n",
      "Iteration 5, Batch: 2, Loss: 5.5870117648737505e-05\n",
      "Iteration 5, Batch: 3, Loss: 0.00011198792344657704\n",
      "Iteration 5, Batch: 4, Loss: 9.216166654368863e-05\n",
      "Iteration 5, Batch: 5, Loss: 4.8055928346002474e-05\n",
      "Iteration 5, Batch: 6, Loss: 2.0065976059413515e-05\n",
      "Iteration 5, Batch: 7, Loss: 3.724326234078035e-05\n",
      "Iteration 5, Batch: 8, Loss: 6.360686529660597e-05\n",
      "Iteration 5, Batch: 9, Loss: 6.211349682416767e-05\n",
      "Iteration 5, Batch: 10, Loss: 2.987809966725763e-05\n",
      "Iteration 5, Batch: 11, Loss: 7.721978363406379e-06\n",
      "Iteration 5, Batch: 12, Loss: 1.9162625903845765e-05\n",
      "Iteration 5, Batch: 13, Loss: 3.8610611227340996e-05\n",
      "Iteration 5, Batch: 14, Loss: 3.632260995800607e-05\n",
      "Iteration 5, Batch: 15, Loss: 1.8109611119143665e-05\n",
      "Iteration 5, Batch: 16, Loss: 4.349933078628965e-06\n",
      "Iteration 5, Batch: 17, Loss: 1.089554098143708e-05\n",
      "Iteration 5, Batch: 18, Loss: 1.733099816192407e-05\n",
      "Iteration 5, Batch: 19, Loss: 2.275773476867471e-05\n",
      "Iteration 5, Batch: 20, Loss: 1.141677512350725e-05\n",
      "Iteration 5, Batch: 21, Loss: 2.937870931418729e-06\n",
      "Iteration 5, Batch: 22, Loss: 4.9447835408500396e-06\n",
      "Iteration 5, Batch: 23, Loss: 1.257529675058322e-05\n",
      "Iteration 5, Batch: 24, Loss: 1.5788726159371436e-05\n",
      "Iteration 5, Batch: 25, Loss: 7.748428288323339e-06\n",
      "Iteration 5, Batch: 26, Loss: 2.6921945845970185e-06\n",
      "Iteration 5, Batch: 27, Loss: 2.463012378939311e-06\n",
      "Iteration 5, Batch: 28, Loss: 5.850086381542496e-06\n",
      "Iteration 5, Batch: 29, Loss: 8.150955181918107e-06\n",
      "Iteration 5, Batch: 30, Loss: 5.7858874242811e-06\n",
      "Iteration 5, Batch: 31, Loss: 2.124811771864188e-06\n",
      "Iteration 5, Batch: 32, Loss: 1.5430309758812655e-06\n",
      "Iteration 5, Batch: 33, Loss: 3.4436682199157076e-06\n",
      "Iteration 5, Batch: 34, Loss: 5.912498181714909e-06\n",
      "Iteration 5, Batch: 35, Loss: 4.7201133384078275e-06\n",
      "Iteration 5, Batch: 36, Loss: 1.9664278170239413e-06\n",
      "Iteration 5, Batch: 37, Loss: 1.327788027083443e-06\n",
      "Iteration 5, Batch: 38, Loss: 2.1974638002575375e-06\n",
      "Iteration 5, Batch: 39, Loss: 2.891092435675091e-06\n",
      "Iteration 5, Batch: 40, Loss: 3.245330617573927e-06\n",
      "Iteration 5, Batch: 41, Loss: 1.9455044366623042e-06\n",
      "Iteration 5, Batch: 42, Loss: 1.1522216709636268e-06\n",
      "Iteration 5, Batch: 43, Loss: 1.7472054878453491e-06\n",
      "Iteration 5, Batch: 44, Loss: 2.748743327174452e-06\n",
      "Iteration 5, Batch: 45, Loss: 2.2859085220261477e-06\n",
      "Iteration 5, Batch: 46, Loss: 1.22329424812051e-06\n",
      "Iteration 5, Batch: 47, Loss: 1.0724301091613597e-06\n",
      "Iteration 5, Batch: 48, Loss: 1.4654495998911443e-06\n",
      "Iteration 5, Batch: 49, Loss: 1.959046812771703e-06\n",
      "Iteration 6, Batch: 0, Loss: 0.12240121513605118\n",
      "Iteration 6, Batch: 1, Loss: 0.15941527485847473\n",
      "Iteration 6, Batch: 2, Loss: 0.14924627542495728\n",
      "Iteration 6, Batch: 3, Loss: 0.1307383030653\n",
      "Iteration 6, Batch: 4, Loss: 0.09419546276330948\n",
      "Iteration 6, Batch: 5, Loss: 0.10579618066549301\n",
      "Iteration 6, Batch: 6, Loss: 0.10747464746236801\n",
      "Iteration 6, Batch: 7, Loss: 0.12441889196634293\n",
      "Iteration 6, Batch: 8, Loss: 0.10679088532924652\n",
      "Iteration 6, Batch: 9, Loss: 0.08487406373023987\n",
      "Iteration 6, Batch: 10, Loss: 0.12087011337280273\n",
      "Iteration 6, Batch: 11, Loss: 0.12315642833709717\n",
      "Iteration 6, Batch: 12, Loss: 0.12842930853366852\n",
      "Iteration 6, Batch: 13, Loss: 0.1362747848033905\n",
      "Iteration 6, Batch: 14, Loss: 0.10964861512184143\n",
      "Iteration 6, Batch: 15, Loss: 0.11032183468341827\n",
      "Iteration 6, Batch: 16, Loss: 0.15582625567913055\n",
      "Iteration 6, Batch: 17, Loss: 0.12648946046829224\n",
      "Iteration 6, Batch: 18, Loss: 0.13945983350276947\n",
      "Iteration 6, Batch: 19, Loss: 0.1620941460132599\n",
      "Iteration 6, Batch: 20, Loss: 0.15012678503990173\n",
      "Iteration 6, Batch: 21, Loss: 0.1089252457022667\n",
      "Iteration 6, Batch: 22, Loss: 0.14767517149448395\n",
      "Iteration 6, Batch: 23, Loss: 0.15721410512924194\n",
      "Iteration 6, Batch: 24, Loss: 0.11569023132324219\n",
      "Iteration 6, Batch: 25, Loss: 0.1364646553993225\n",
      "Iteration 6, Batch: 26, Loss: 0.12714947760105133\n",
      "Iteration 6, Batch: 27, Loss: 0.10612954199314117\n",
      "Iteration 6, Batch: 28, Loss: 0.1513037383556366\n",
      "Iteration 6, Batch: 29, Loss: 0.08094171434640884\n",
      "Iteration 6, Batch: 30, Loss: 0.14416147768497467\n",
      "Iteration 6, Batch: 31, Loss: 0.14295151829719543\n",
      "Iteration 6, Batch: 32, Loss: 0.11790338158607483\n",
      "Iteration 6, Batch: 33, Loss: 0.15728729963302612\n",
      "Iteration 6, Batch: 34, Loss: 0.12319287657737732\n",
      "Iteration 6, Batch: 35, Loss: 0.09564144164323807\n",
      "Iteration 6, Batch: 36, Loss: 0.12917719781398773\n",
      "Iteration 6, Batch: 37, Loss: 0.13155950605869293\n",
      "Iteration 6, Batch: 38, Loss: 0.08571264892816544\n",
      "Iteration 6, Batch: 39, Loss: 0.14249339699745178\n",
      "Iteration 6, Batch: 40, Loss: 0.12100579589605331\n",
      "Iteration 6, Batch: 41, Loss: 0.12986846268177032\n",
      "Iteration 6, Batch: 42, Loss: 0.11750238388776779\n",
      "Iteration 6, Batch: 43, Loss: 0.16893120110034943\n",
      "Iteration 6, Batch: 44, Loss: 0.11181201040744781\n",
      "Iteration 6, Batch: 45, Loss: 0.14578992128372192\n",
      "Iteration 6, Batch: 46, Loss: 0.09248669445514679\n",
      "Iteration 6, Batch: 47, Loss: 0.13984757661819458\n",
      "Iteration 6, Batch: 48, Loss: 0.10892518609762192\n",
      "Iteration 6, Batch: 49, Loss: 0.10475223511457443\n",
      "Iteration 7, Batch: 0, Loss: 0.00018391941557638347\n",
      "Iteration 7, Batch: 1, Loss: 9.376148227602243e-05\n",
      "Iteration 7, Batch: 2, Loss: 2.8693619242403656e-05\n",
      "Iteration 7, Batch: 3, Loss: 4.1547089494997635e-05\n",
      "Iteration 7, Batch: 4, Loss: 8.83394677657634e-05\n",
      "Iteration 7, Batch: 5, Loss: 9.153578139375895e-05\n",
      "Iteration 7, Batch: 6, Loss: 7.146997813833877e-05\n",
      "Iteration 7, Batch: 7, Loss: 5.416725980467163e-05\n",
      "Iteration 7, Batch: 8, Loss: 2.8128206395194866e-05\n",
      "Iteration 7, Batch: 9, Loss: 2.7980475351796485e-05\n",
      "Iteration 7, Batch: 10, Loss: 3.840794670395553e-05\n",
      "Iteration 7, Batch: 11, Loss: 2.8046613806509413e-05\n",
      "Iteration 7, Batch: 12, Loss: 2.931062044808641e-05\n",
      "Iteration 7, Batch: 13, Loss: 3.352712155901827e-05\n",
      "Iteration 7, Batch: 14, Loss: 2.6781739506986924e-05\n",
      "Iteration 7, Batch: 15, Loss: 0.005006449297070503\n",
      "Iteration 7, Batch: 16, Loss: 1.350316233583726e-05\n",
      "Iteration 7, Batch: 17, Loss: 8.255901775555685e-06\n",
      "Iteration 7, Batch: 18, Loss: 1.499514837632887e-05\n",
      "Iteration 7, Batch: 19, Loss: 2.428409061394632e-05\n",
      "Iteration 7, Batch: 20, Loss: 2.3278567823581398e-05\n",
      "Iteration 7, Batch: 21, Loss: 1.3087335901218466e-05\n",
      "Iteration 7, Batch: 22, Loss: 2.6141728994844016e-06\n",
      "Iteration 7, Batch: 23, Loss: 2.702224264794495e-06\n",
      "Iteration 7, Batch: 24, Loss: 1.040511233441066e-05\n",
      "Iteration 7, Batch: 25, Loss: 1.652657192607876e-05\n",
      "Iteration 7, Batch: 26, Loss: 1.1213915058760904e-05\n",
      "Iteration 7, Batch: 27, Loss: 4.772953161591431e-06\n",
      "Iteration 7, Batch: 28, Loss: 1.5581075558657176e-06\n",
      "Iteration 7, Batch: 29, Loss: 2.616632855279022e-06\n",
      "Iteration 7, Batch: 30, Loss: 5.412529390014242e-06\n",
      "Iteration 7, Batch: 31, Loss: 7.204993380582891e-06\n",
      "Iteration 7, Batch: 32, Loss: 6.002140253258403e-06\n",
      "Iteration 7, Batch: 33, Loss: 3.7625582081091125e-06\n",
      "Iteration 7, Batch: 34, Loss: 2.1679566089005675e-06\n",
      "Iteration 7, Batch: 35, Loss: 2.804351879603928e-06\n",
      "Iteration 7, Batch: 36, Loss: 3.6978028674639063e-06\n",
      "Iteration 7, Batch: 37, Loss: 3.0349494863912696e-06\n",
      "Iteration 7, Batch: 38, Loss: 2.857778781617526e-06\n",
      "Iteration 7, Batch: 39, Loss: 2.7166934160050005e-06\n",
      "Iteration 7, Batch: 40, Loss: 2.2815197553427424e-06\n",
      "Iteration 7, Batch: 41, Loss: 1.6168970660146442e-06\n",
      "Iteration 7, Batch: 42, Loss: 1.1528028380780597e-06\n",
      "Iteration 7, Batch: 43, Loss: 1.409534547747171e-06\n",
      "Iteration 7, Batch: 44, Loss: 2.072424194921041e-06\n",
      "Iteration 7, Batch: 45, Loss: 2.1083301362523343e-06\n",
      "Iteration 7, Batch: 46, Loss: 1.6956720401140046e-06\n",
      "Iteration 7, Batch: 47, Loss: 1.3134168739270535e-06\n",
      "Iteration 7, Batch: 48, Loss: 7.547122322648647e-07\n",
      "Iteration 7, Batch: 49, Loss: 1.016891360450245e-06\n",
      "Iteration 8, Batch: 0, Loss: 0.11679190397262573\n",
      "Iteration 8, Batch: 1, Loss: 0.14778076112270355\n",
      "Iteration 8, Batch: 2, Loss: 0.1920432299375534\n",
      "Iteration 8, Batch: 3, Loss: 0.09769871830940247\n",
      "Iteration 8, Batch: 4, Loss: 0.11510884761810303\n",
      "Iteration 8, Batch: 5, Loss: 0.15311254560947418\n",
      "Iteration 8, Batch: 6, Loss: 0.11917292326688766\n",
      "Iteration 8, Batch: 7, Loss: 0.11252069473266602\n",
      "Iteration 8, Batch: 8, Loss: 0.11491108685731888\n",
      "Iteration 8, Batch: 9, Loss: 0.12565116584300995\n",
      "Iteration 8, Batch: 10, Loss: 0.1242252066731453\n",
      "Iteration 8, Batch: 11, Loss: 0.11462979763746262\n",
      "Iteration 8, Batch: 12, Loss: 0.10205937922000885\n",
      "Iteration 8, Batch: 13, Loss: 0.07948435842990875\n",
      "Iteration 8, Batch: 14, Loss: 0.12806329131126404\n",
      "Iteration 8, Batch: 15, Loss: 0.09842535108327866\n",
      "Iteration 8, Batch: 16, Loss: 0.14765474200248718\n",
      "Iteration 8, Batch: 17, Loss: 0.11842473596334457\n",
      "Iteration 8, Batch: 18, Loss: 0.12090671062469482\n",
      "Iteration 8, Batch: 19, Loss: 0.1111655980348587\n",
      "Iteration 8, Batch: 20, Loss: 0.13025358319282532\n",
      "Iteration 8, Batch: 21, Loss: 0.11665056645870209\n",
      "Iteration 8, Batch: 22, Loss: 0.14264266192913055\n",
      "Iteration 8, Batch: 23, Loss: 0.12392311543226242\n",
      "Iteration 8, Batch: 24, Loss: 0.13970667123794556\n",
      "Iteration 8, Batch: 25, Loss: 0.13936559855937958\n",
      "Iteration 8, Batch: 26, Loss: 0.12396065145730972\n",
      "Iteration 8, Batch: 27, Loss: 0.1382095068693161\n",
      "Iteration 8, Batch: 28, Loss: 0.10831347852945328\n",
      "Iteration 8, Batch: 29, Loss: 0.11984440684318542\n",
      "Iteration 8, Batch: 30, Loss: 0.1370345503091812\n",
      "Iteration 8, Batch: 31, Loss: 0.1249283030629158\n",
      "Iteration 8, Batch: 32, Loss: 0.11790820211172104\n",
      "Iteration 8, Batch: 33, Loss: 0.11417154222726822\n",
      "Iteration 8, Batch: 34, Loss: 0.09228246659040451\n",
      "Iteration 8, Batch: 35, Loss: 0.13443244993686676\n",
      "Iteration 8, Batch: 36, Loss: 0.09900832176208496\n",
      "Iteration 8, Batch: 37, Loss: 0.12286902219057083\n",
      "Iteration 8, Batch: 38, Loss: 0.13415183126926422\n",
      "Iteration 8, Batch: 39, Loss: 0.12197383493185043\n",
      "Iteration 8, Batch: 40, Loss: 0.15553490817546844\n",
      "Iteration 8, Batch: 41, Loss: 0.1462772935628891\n",
      "Iteration 8, Batch: 42, Loss: 0.09529075771570206\n",
      "Iteration 8, Batch: 43, Loss: 0.09525278210639954\n",
      "Iteration 8, Batch: 44, Loss: 0.15543107688426971\n",
      "Iteration 8, Batch: 45, Loss: 0.12274493277072906\n",
      "Iteration 8, Batch: 46, Loss: 0.09755352884531021\n",
      "Iteration 8, Batch: 47, Loss: 0.1208801418542862\n",
      "Iteration 8, Batch: 48, Loss: 0.11876948922872543\n",
      "Iteration 8, Batch: 49, Loss: 0.1013161912560463\n",
      "Iteration 9, Batch: 0, Loss: 0.00012708205031231046\n",
      "Iteration 9, Batch: 1, Loss: 8.562745642848313e-05\n",
      "Iteration 9, Batch: 2, Loss: 2.424771446385421e-05\n",
      "Iteration 9, Batch: 3, Loss: 5.101663191453554e-06\n",
      "Iteration 9, Batch: 4, Loss: 0.005048565566539764\n",
      "Iteration 9, Batch: 5, Loss: 6.792933709220961e-05\n",
      "Iteration 9, Batch: 6, Loss: 5.369978680391796e-05\n",
      "Iteration 9, Batch: 7, Loss: 2.7240261260885745e-05\n",
      "Iteration 9, Batch: 8, Loss: 5.564527782553341e-06\n",
      "Iteration 9, Batch: 9, Loss: 2.4783452317933552e-05\n",
      "Iteration 9, Batch: 10, Loss: 4.7310339141404256e-05\n",
      "Iteration 9, Batch: 11, Loss: 0.004893457051366568\n",
      "Iteration 9, Batch: 12, Loss: 3.766000736504793e-05\n",
      "Iteration 9, Batch: 13, Loss: 0.004986920394003391\n",
      "Iteration 9, Batch: 14, Loss: 7.72882776800543e-06\n",
      "Iteration 9, Batch: 15, Loss: 2.0622968804673292e-05\n",
      "Iteration 9, Batch: 16, Loss: 3.179659688612446e-05\n",
      "Iteration 9, Batch: 17, Loss: 1.8310451196157373e-05\n",
      "Iteration 9, Batch: 18, Loss: 7.107923011062667e-06\n",
      "Iteration 9, Batch: 19, Loss: 7.249851478263736e-06\n",
      "Iteration 9, Batch: 20, Loss: 1.5559671737719327e-05\n",
      "Iteration 9, Batch: 21, Loss: 1.9866241927957162e-05\n",
      "Iteration 9, Batch: 22, Loss: 1.2157905985077377e-05\n",
      "Iteration 9, Batch: 23, Loss: 5.192356184124947e-06\n",
      "Iteration 9, Batch: 24, Loss: 6.095553089835448e-06\n",
      "Iteration 9, Batch: 25, Loss: 1.2661897017096635e-05\n",
      "Iteration 9, Batch: 26, Loss: 1.5510049706790596e-05\n",
      "Iteration 9, Batch: 27, Loss: 1.0687920621421654e-05\n",
      "Iteration 9, Batch: 28, Loss: 5.124078597873449e-06\n",
      "Iteration 9, Batch: 29, Loss: 6.003799171594437e-06\n",
      "Iteration 9, Batch: 30, Loss: 1.2442644219845533e-05\n",
      "Iteration 9, Batch: 31, Loss: 1.2896258340333588e-05\n",
      "Iteration 9, Batch: 32, Loss: 8.699391401023604e-06\n",
      "Iteration 9, Batch: 33, Loss: 5.394931122282287e-06\n",
      "Iteration 9, Batch: 34, Loss: 6.800998107792111e-06\n",
      "Iteration 9, Batch: 35, Loss: 1.0152984032174572e-05\n",
      "Iteration 9, Batch: 36, Loss: 9.111508916248567e-06\n",
      "Iteration 9, Batch: 37, Loss: 7.191014901763992e-06\n",
      "Iteration 9, Batch: 38, Loss: 6.272734026424587e-06\n",
      "Iteration 9, Batch: 39, Loss: 5.077679816167802e-06\n",
      "Iteration 9, Batch: 40, Loss: 7.005058250797447e-06\n",
      "Iteration 9, Batch: 41, Loss: 6.390906492015347e-06\n",
      "Iteration 9, Batch: 42, Loss: 6.033139015926281e-06\n",
      "Iteration 9, Batch: 43, Loss: 4.217568402964389e-06\n",
      "Iteration 9, Batch: 44, Loss: 5.1015117605857085e-06\n",
      "Iteration 9, Batch: 45, Loss: 6.780781404813752e-06\n",
      "Iteration 9, Batch: 46, Loss: 5.6416329243802465e-06\n",
      "Iteration 9, Batch: 47, Loss: 4.663990239350824e-06\n",
      "Iteration 9, Batch: 48, Loss: 3.770359626287245e-06\n",
      "Iteration 9, Batch: 49, Loss: 4.771616659127176e-06\n",
      "Iteration 10, Batch: 0, Loss: 0.15721924602985382\n",
      "Iteration 10, Batch: 1, Loss: 0.1669158935546875\n",
      "Iteration 10, Batch: 2, Loss: 0.12460872530937195\n",
      "Iteration 10, Batch: 3, Loss: 0.1437634825706482\n",
      "Iteration 10, Batch: 4, Loss: 0.10607364773750305\n",
      "Iteration 10, Batch: 5, Loss: 0.12412552535533905\n",
      "Iteration 10, Batch: 6, Loss: 0.13041524589061737\n",
      "Iteration 10, Batch: 7, Loss: 0.1581602841615677\n",
      "Iteration 10, Batch: 8, Loss: 0.1117754876613617\n",
      "Iteration 10, Batch: 9, Loss: 0.14733363687992096\n",
      "Iteration 10, Batch: 10, Loss: 0.15091001987457275\n",
      "Iteration 10, Batch: 11, Loss: 0.12473063915967941\n",
      "Iteration 10, Batch: 12, Loss: 0.17575770616531372\n",
      "Iteration 10, Batch: 13, Loss: 0.09791099280118942\n",
      "Iteration 10, Batch: 14, Loss: 0.15291185677051544\n",
      "Iteration 10, Batch: 15, Loss: 0.11432640999555588\n",
      "Iteration 10, Batch: 16, Loss: 0.07710227370262146\n",
      "Iteration 10, Batch: 17, Loss: 0.16438499093055725\n",
      "Iteration 10, Batch: 18, Loss: 0.12068679183721542\n",
      "Iteration 10, Batch: 19, Loss: 0.16118423640727997\n",
      "Iteration 10, Batch: 20, Loss: 0.12951761484146118\n",
      "Iteration 10, Batch: 21, Loss: 0.08236909657716751\n",
      "Iteration 10, Batch: 22, Loss: 0.13561509549617767\n",
      "Iteration 10, Batch: 23, Loss: 0.12608115375041962\n",
      "Iteration 10, Batch: 24, Loss: 0.09222596138715744\n",
      "Iteration 10, Batch: 25, Loss: 0.11048757284879684\n",
      "Iteration 10, Batch: 26, Loss: 0.1364210695028305\n",
      "Iteration 10, Batch: 27, Loss: 0.0979679748415947\n",
      "Iteration 10, Batch: 28, Loss: 0.14332906901836395\n",
      "Iteration 10, Batch: 29, Loss: 0.06491375714540482\n",
      "Iteration 10, Batch: 30, Loss: 0.13129828870296478\n",
      "Iteration 10, Batch: 31, Loss: 0.11942116171121597\n",
      "Iteration 10, Batch: 32, Loss: 0.11094564199447632\n",
      "Iteration 10, Batch: 33, Loss: 0.08970941603183746\n",
      "Iteration 10, Batch: 34, Loss: 0.10154104232788086\n",
      "Iteration 10, Batch: 35, Loss: 0.09667972475290298\n",
      "Iteration 10, Batch: 36, Loss: 0.15188856422901154\n",
      "Iteration 10, Batch: 37, Loss: 0.16619259119033813\n",
      "Iteration 10, Batch: 38, Loss: 0.1262194663286209\n",
      "Iteration 10, Batch: 39, Loss: 0.06422009319067001\n",
      "Iteration 10, Batch: 40, Loss: 0.11529620736837387\n",
      "Iteration 10, Batch: 41, Loss: 0.12820109724998474\n",
      "Iteration 10, Batch: 42, Loss: 0.11775092780590057\n",
      "Iteration 10, Batch: 43, Loss: 0.08906848728656769\n",
      "Iteration 10, Batch: 44, Loss: 0.15899643301963806\n",
      "Iteration 10, Batch: 45, Loss: 0.13348469138145447\n",
      "Iteration 10, Batch: 46, Loss: 0.10920000076293945\n",
      "Iteration 10, Batch: 47, Loss: 0.09538546949625015\n",
      "Iteration 10, Batch: 48, Loss: 0.14057785272598267\n",
      "Iteration 10, Batch: 49, Loss: 0.1364677995443344\n",
      "Iteration 11, Batch: 0, Loss: 4.549084042082541e-05\n",
      "Iteration 11, Batch: 1, Loss: 1.826065818022471e-05\n",
      "Iteration 11, Batch: 2, Loss: 1.1207461284357123e-05\n",
      "Iteration 11, Batch: 3, Loss: 8.609958058514167e-06\n",
      "Iteration 11, Batch: 4, Loss: 1.4925240975571796e-05\n",
      "Iteration 11, Batch: 5, Loss: 2.038846469076816e-05\n",
      "Iteration 11, Batch: 6, Loss: 1.6517704352736473e-05\n",
      "Iteration 11, Batch: 7, Loss: 8.838060239213519e-06\n",
      "Iteration 11, Batch: 8, Loss: 7.836908480385318e-06\n",
      "Iteration 11, Batch: 9, Loss: 8.068402166827582e-06\n",
      "Iteration 11, Batch: 10, Loss: 7.103476036718348e-06\n",
      "Iteration 11, Batch: 11, Loss: 6.246544671739684e-06\n",
      "Iteration 11, Batch: 12, Loss: 7.903566256572958e-06\n",
      "Iteration 11, Batch: 13, Loss: 6.242717972781975e-06\n",
      "Iteration 11, Batch: 14, Loss: 7.121376711438643e-06\n",
      "Iteration 11, Batch: 15, Loss: 4.750912466988666e-06\n",
      "Iteration 11, Batch: 16, Loss: 2.460674750182079e-06\n",
      "Iteration 11, Batch: 17, Loss: 3.310709644210874e-06\n",
      "Iteration 11, Batch: 18, Loss: 4.753727353090653e-06\n",
      "Iteration 11, Batch: 19, Loss: 5.7735724112717435e-06\n",
      "Iteration 11, Batch: 20, Loss: 4.5873734961787704e-06\n",
      "Iteration 11, Batch: 21, Loss: 2.3474697172787273e-06\n",
      "Iteration 11, Batch: 22, Loss: 3.010727823493653e-06\n",
      "Iteration 11, Batch: 23, Loss: 4.807669029105455e-06\n",
      "Iteration 11, Batch: 24, Loss: 4.5293400035006925e-06\n",
      "Iteration 11, Batch: 25, Loss: 3.334842631375068e-06\n",
      "Iteration 11, Batch: 26, Loss: 3.7400059227366e-06\n",
      "Iteration 11, Batch: 27, Loss: 1.9286619590275222e-06\n",
      "Iteration 11, Batch: 28, Loss: 2.1795153770653997e-06\n",
      "Iteration 11, Batch: 29, Loss: 4.530450496531557e-06\n",
      "Iteration 11, Batch: 30, Loss: 3.034042265426251e-06\n",
      "Iteration 11, Batch: 31, Loss: 2.3949214664753526e-06\n",
      "Iteration 11, Batch: 32, Loss: 3.3675712529657176e-06\n",
      "Iteration 11, Batch: 33, Loss: 3.5655696137837367e-06\n",
      "Iteration 11, Batch: 34, Loss: 2.182633579650428e-06\n",
      "Iteration 11, Batch: 35, Loss: 2.5285605715907877e-06\n",
      "Iteration 11, Batch: 36, Loss: 1.5257397762979963e-06\n",
      "Iteration 11, Batch: 37, Loss: 3.374008429091191e-06\n",
      "Iteration 11, Batch: 38, Loss: 3.4226836760353763e-06\n",
      "Iteration 11, Batch: 39, Loss: 3.3020983210008126e-06\n",
      "Iteration 11, Batch: 40, Loss: 1.714907057248638e-06\n",
      "Iteration 11, Batch: 41, Loss: 2.6564182462607278e-06\n",
      "Iteration 11, Batch: 42, Loss: 2.0931395283696475e-06\n",
      "Iteration 11, Batch: 43, Loss: 1.6628904404569766e-06\n",
      "Iteration 11, Batch: 44, Loss: 2.885133426389075e-06\n",
      "Iteration 11, Batch: 45, Loss: 1.4934715864001191e-06\n",
      "Iteration 11, Batch: 46, Loss: 9.775926628208254e-07\n",
      "Iteration 11, Batch: 47, Loss: 4.185983016213868e-06\n",
      "Iteration 11, Batch: 48, Loss: 1.0652843229763675e-06\n",
      "Iteration 11, Batch: 49, Loss: 1.580915863996779e-06\n",
      "Iteration 12, Batch: 0, Loss: 0.15566465258598328\n",
      "Iteration 12, Batch: 1, Loss: 0.14131368696689606\n",
      "Iteration 12, Batch: 2, Loss: 0.09641483426094055\n",
      "Iteration 12, Batch: 3, Loss: 0.09040538221597672\n",
      "Iteration 12, Batch: 4, Loss: 0.1463003009557724\n",
      "Iteration 12, Batch: 5, Loss: 0.16438628733158112\n",
      "Iteration 12, Batch: 6, Loss: 0.17401552200317383\n",
      "Iteration 12, Batch: 7, Loss: 0.11575482785701752\n",
      "Iteration 12, Batch: 8, Loss: 0.1388014405965805\n",
      "Iteration 12, Batch: 9, Loss: 0.15860313177108765\n",
      "Iteration 12, Batch: 10, Loss: 0.14609400928020477\n",
      "Iteration 12, Batch: 11, Loss: 0.1613100916147232\n",
      "Iteration 12, Batch: 12, Loss: 0.11395343393087387\n",
      "Iteration 12, Batch: 13, Loss: 0.1196456104516983\n",
      "Iteration 12, Batch: 14, Loss: 0.12895028293132782\n",
      "Iteration 12, Batch: 15, Loss: 0.11182908713817596\n",
      "Iteration 12, Batch: 16, Loss: 0.09678927809000015\n",
      "Iteration 12, Batch: 17, Loss: 0.10013396292924881\n",
      "Iteration 12, Batch: 18, Loss: 0.08992447704076767\n",
      "Iteration 12, Batch: 19, Loss: 0.10087428987026215\n",
      "Iteration 12, Batch: 20, Loss: 0.1395731419324875\n",
      "Iteration 12, Batch: 21, Loss: 0.08239854872226715\n",
      "Iteration 12, Batch: 22, Loss: 0.12606599926948547\n",
      "Iteration 12, Batch: 23, Loss: 0.13831786811351776\n",
      "Iteration 12, Batch: 24, Loss: 0.11518316715955734\n",
      "Iteration 12, Batch: 25, Loss: 0.12343679368495941\n",
      "Iteration 12, Batch: 26, Loss: 0.10322156548500061\n",
      "Iteration 12, Batch: 27, Loss: 0.14710959792137146\n",
      "Iteration 12, Batch: 28, Loss: 0.1451432704925537\n",
      "Iteration 12, Batch: 29, Loss: 0.11381816864013672\n",
      "Iteration 12, Batch: 30, Loss: 0.15538692474365234\n",
      "Iteration 12, Batch: 31, Loss: 0.1318548321723938\n",
      "Iteration 12, Batch: 32, Loss: 0.10996077954769135\n",
      "Iteration 12, Batch: 33, Loss: 0.10916484892368317\n",
      "Iteration 12, Batch: 34, Loss: 0.13507045805454254\n",
      "Iteration 12, Batch: 35, Loss: 0.12992973625659943\n",
      "Iteration 12, Batch: 36, Loss: 0.10420028120279312\n",
      "Iteration 12, Batch: 37, Loss: 0.12361200898885727\n",
      "Iteration 12, Batch: 38, Loss: 0.11243992298841476\n",
      "Iteration 12, Batch: 39, Loss: 0.12063439190387726\n",
      "Iteration 12, Batch: 40, Loss: 0.09162786602973938\n",
      "Iteration 12, Batch: 41, Loss: 0.13004226982593536\n",
      "Iteration 12, Batch: 42, Loss: 0.11612195521593094\n",
      "Iteration 12, Batch: 43, Loss: 0.14759445190429688\n",
      "Iteration 12, Batch: 44, Loss: 0.13832895457744598\n",
      "Iteration 12, Batch: 45, Loss: 0.1372324526309967\n",
      "Iteration 12, Batch: 46, Loss: 0.14441806077957153\n",
      "Iteration 12, Batch: 47, Loss: 0.10281995683908463\n",
      "Iteration 12, Batch: 48, Loss: 0.12331598252058029\n",
      "Iteration 12, Batch: 49, Loss: 0.11507068574428558\n",
      "Iteration 13, Batch: 0, Loss: 0.00028716842643916607\n",
      "Iteration 13, Batch: 1, Loss: 0.00017196714179590344\n",
      "Iteration 13, Batch: 2, Loss: 5.4199816077016294e-05\n",
      "Iteration 13, Batch: 3, Loss: 1.9618544683908112e-05\n",
      "Iteration 13, Batch: 4, Loss: 6.014052632963285e-05\n",
      "Iteration 13, Batch: 5, Loss: 0.00012354421778582036\n",
      "Iteration 13, Batch: 6, Loss: 0.00013606691209133714\n",
      "Iteration 13, Batch: 7, Loss: 0.00012372668425086886\n",
      "Iteration 13, Batch: 8, Loss: 6.612518336623907e-05\n",
      "Iteration 13, Batch: 9, Loss: 1.0694544471334666e-05\n",
      "Iteration 13, Batch: 10, Loss: 2.0177423721179366e-05\n",
      "Iteration 13, Batch: 11, Loss: 5.644959310302511e-05\n",
      "Iteration 13, Batch: 12, Loss: 9.394576773047447e-05\n",
      "Iteration 13, Batch: 13, Loss: 8.096051897155121e-05\n",
      "Iteration 13, Batch: 14, Loss: 5.0179332902189344e-05\n",
      "Iteration 13, Batch: 15, Loss: 1.0424270840303507e-05\n",
      "Iteration 13, Batch: 16, Loss: 7.369832928816322e-06\n",
      "Iteration 13, Batch: 17, Loss: 2.9775741495541297e-05\n",
      "Iteration 13, Batch: 18, Loss: 5.324957601260394e-05\n",
      "Iteration 13, Batch: 19, Loss: 5.00690184708219e-05\n",
      "Iteration 13, Batch: 20, Loss: 2.8759690394508652e-05\n",
      "Iteration 13, Batch: 21, Loss: 9.46974159887759e-06\n",
      "Iteration 13, Batch: 22, Loss: 5.564758339460241e-06\n",
      "Iteration 13, Batch: 23, Loss: 1.206646720675053e-05\n",
      "Iteration 13, Batch: 24, Loss: 2.44170987571124e-05\n",
      "Iteration 13, Batch: 25, Loss: 2.652001967362594e-05\n",
      "Iteration 13, Batch: 26, Loss: 1.891069405246526e-05\n",
      "Iteration 13, Batch: 27, Loss: 8.941555279307067e-06\n",
      "Iteration 13, Batch: 28, Loss: 5.09104756929446e-06\n",
      "Iteration 13, Batch: 29, Loss: 5.64232732358505e-06\n",
      "Iteration 13, Batch: 30, Loss: 9.494489859207533e-06\n",
      "Iteration 13, Batch: 31, Loss: 1.4828360690444242e-05\n",
      "Iteration 13, Batch: 32, Loss: 1.2117711776227225e-05\n",
      "Iteration 13, Batch: 33, Loss: 9.179761036648415e-06\n",
      "Iteration 13, Batch: 34, Loss: 4.590393928083358e-06\n",
      "Iteration 13, Batch: 35, Loss: 3.957638455176493e-06\n",
      "Iteration 13, Batch: 36, Loss: 5.0274293244001456e-06\n",
      "Iteration 13, Batch: 37, Loss: 8.162774065567646e-06\n",
      "Iteration 13, Batch: 38, Loss: 7.0613946263620164e-06\n",
      "Iteration 13, Batch: 39, Loss: 4.825232281291392e-06\n",
      "Iteration 13, Batch: 40, Loss: 3.0800833883404266e-06\n",
      "Iteration 13, Batch: 41, Loss: 2.2321864889818244e-06\n",
      "Iteration 13, Batch: 42, Loss: 2.9082245873723878e-06\n",
      "Iteration 13, Batch: 43, Loss: 4.290127435524482e-06\n",
      "Iteration 13, Batch: 44, Loss: 5.060784133092966e-06\n",
      "Iteration 13, Batch: 45, Loss: 4.216664365230827e-06\n",
      "Iteration 13, Batch: 46, Loss: 2.940669219242409e-06\n",
      "Iteration 13, Batch: 47, Loss: 2.6608179268805543e-06\n",
      "Iteration 13, Batch: 48, Loss: 2.1076314169476973e-06\n",
      "Iteration 13, Batch: 49, Loss: 2.4112334813253256e-06\n",
      "Iteration 14, Batch: 0, Loss: 0.15911084413528442\n",
      "Iteration 14, Batch: 1, Loss: 0.109034962952137\n",
      "Iteration 14, Batch: 2, Loss: 0.1670876145362854\n",
      "Iteration 14, Batch: 3, Loss: 0.11468660086393356\n",
      "Iteration 14, Batch: 4, Loss: 0.144802525639534\n",
      "Iteration 14, Batch: 5, Loss: 0.09474533796310425\n",
      "Iteration 14, Batch: 6, Loss: 0.16676577925682068\n",
      "Iteration 14, Batch: 7, Loss: 0.13812056183815002\n",
      "Iteration 14, Batch: 8, Loss: 0.14230774343013763\n",
      "Iteration 14, Batch: 9, Loss: 0.13546131551265717\n",
      "Iteration 14, Batch: 10, Loss: 0.14779792726039886\n",
      "Iteration 14, Batch: 11, Loss: 0.09998534619808197\n",
      "Iteration 14, Batch: 12, Loss: 0.08553873747587204\n",
      "Iteration 14, Batch: 13, Loss: 0.13014402985572815\n",
      "Iteration 14, Batch: 14, Loss: 0.11903192102909088\n",
      "Iteration 14, Batch: 15, Loss: 0.11547534912824631\n",
      "Iteration 14, Batch: 16, Loss: 0.1438950151205063\n",
      "Iteration 14, Batch: 17, Loss: 0.10596641153097153\n",
      "Iteration 14, Batch: 18, Loss: 0.11378040164709091\n",
      "Iteration 14, Batch: 19, Loss: 0.07782255858182907\n",
      "Iteration 14, Batch: 20, Loss: 0.15892188251018524\n",
      "Iteration 14, Batch: 21, Loss: 0.09249705821275711\n",
      "Iteration 14, Batch: 22, Loss: 0.1509404182434082\n",
      "Iteration 14, Batch: 23, Loss: 0.10592174530029297\n",
      "Iteration 14, Batch: 24, Loss: 0.08930816501379013\n",
      "Iteration 14, Batch: 25, Loss: 0.14295780658721924\n",
      "Iteration 14, Batch: 26, Loss: 0.0828663632273674\n",
      "Iteration 14, Batch: 27, Loss: 0.11634080857038498\n",
      "Iteration 14, Batch: 28, Loss: 0.09347938001155853\n",
      "Iteration 14, Batch: 29, Loss: 0.07415785640478134\n",
      "Iteration 14, Batch: 30, Loss: 0.11277724802494049\n",
      "Iteration 14, Batch: 31, Loss: 0.14821574091911316\n",
      "Iteration 14, Batch: 32, Loss: 0.12240840494632721\n",
      "Iteration 14, Batch: 33, Loss: 0.10486241430044174\n",
      "Iteration 14, Batch: 34, Loss: 0.11357127875089645\n",
      "Iteration 14, Batch: 35, Loss: 0.12781381607055664\n",
      "Iteration 14, Batch: 36, Loss: 0.11468250304460526\n",
      "Iteration 14, Batch: 37, Loss: 0.07182680070400238\n",
      "Iteration 14, Batch: 38, Loss: 0.10768340528011322\n",
      "Iteration 14, Batch: 39, Loss: 0.14360225200653076\n",
      "Iteration 14, Batch: 40, Loss: 0.10867667943239212\n",
      "Iteration 14, Batch: 41, Loss: 0.10877105593681335\n",
      "Iteration 14, Batch: 42, Loss: 0.11085575073957443\n",
      "Iteration 14, Batch: 43, Loss: 0.12504835426807404\n",
      "Iteration 14, Batch: 44, Loss: 0.11394504457712173\n",
      "Iteration 14, Batch: 45, Loss: 0.13534662127494812\n",
      "Iteration 14, Batch: 46, Loss: 0.10855850577354431\n",
      "Iteration 14, Batch: 47, Loss: 0.11018379777669907\n",
      "Iteration 14, Batch: 48, Loss: 0.15836083889007568\n",
      "Iteration 14, Batch: 49, Loss: 0.13396191596984863\n",
      "Iteration 15, Batch: 0, Loss: 0.0001631957566132769\n",
      "Iteration 15, Batch: 1, Loss: 6.386928725987673e-05\n",
      "Iteration 15, Batch: 2, Loss: 1.2434925338311587e-05\n",
      "Iteration 15, Batch: 3, Loss: 3.4616863558767363e-05\n",
      "Iteration 15, Batch: 4, Loss: 5.884789425181225e-05\n",
      "Iteration 15, Batch: 5, Loss: 9.547651279717684e-05\n",
      "Iteration 15, Batch: 6, Loss: 5.5248256103368476e-05\n",
      "Iteration 15, Batch: 7, Loss: 3.7901652831351385e-05\n",
      "Iteration 15, Batch: 8, Loss: 9.224437235388905e-06\n",
      "Iteration 15, Batch: 9, Loss: 3.4706713449850213e-06\n",
      "Iteration 15, Batch: 10, Loss: 1.935502041305881e-05\n",
      "Iteration 15, Batch: 11, Loss: 3.438820567680523e-05\n",
      "Iteration 15, Batch: 12, Loss: 3.31066403305158e-05\n",
      "Iteration 15, Batch: 13, Loss: 1.6560308722546324e-05\n",
      "Iteration 15, Batch: 14, Loss: 2.8540225684992038e-06\n",
      "Iteration 15, Batch: 15, Loss: 1.3961191598355072e-06\n",
      "Iteration 15, Batch: 16, Loss: 8.423637154919561e-06\n",
      "Iteration 15, Batch: 17, Loss: 1.631198938412126e-05\n",
      "Iteration 15, Batch: 18, Loss: 1.881938260339666e-05\n",
      "Iteration 15, Batch: 19, Loss: 1.1780898603319656e-05\n",
      "Iteration 15, Batch: 20, Loss: 5.2945442803320475e-06\n",
      "Iteration 15, Batch: 21, Loss: 1.3224910162534798e-06\n",
      "Iteration 15, Batch: 22, Loss: 4.033111054013716e-06\n",
      "Iteration 15, Batch: 23, Loss: 9.465407856623642e-06\n",
      "Iteration 15, Batch: 24, Loss: 7.99456120148534e-06\n",
      "Iteration 15, Batch: 25, Loss: 8.978070582088549e-06\n",
      "Iteration 15, Batch: 26, Loss: 4.914406872558175e-06\n",
      "Iteration 15, Batch: 27, Loss: 1.4901696658853325e-06\n",
      "Iteration 15, Batch: 28, Loss: 1.99108399101533e-06\n",
      "Iteration 15, Batch: 29, Loss: 4.2780625335581135e-06\n",
      "Iteration 15, Batch: 30, Loss: 6.307237526925746e-06\n",
      "Iteration 15, Batch: 31, Loss: 5.7945121625380125e-06\n",
      "Iteration 15, Batch: 32, Loss: 3.0081939712545136e-06\n",
      "Iteration 15, Batch: 33, Loss: 1.4178831406752579e-06\n",
      "Iteration 15, Batch: 34, Loss: 1.2801057209799183e-06\n",
      "Iteration 15, Batch: 35, Loss: 1.835618036238884e-06\n",
      "Iteration 15, Batch: 36, Loss: 4.236044787830906e-06\n",
      "Iteration 15, Batch: 37, Loss: 3.350543693159125e-06\n",
      "Iteration 15, Batch: 38, Loss: 2.525849595258478e-06\n",
      "Iteration 15, Batch: 39, Loss: 3.026166723429924e-06\n",
      "Iteration 15, Batch: 40, Loss: 1.2855023214797257e-06\n",
      "Iteration 15, Batch: 41, Loss: 1.5623770650563529e-06\n",
      "Iteration 15, Batch: 42, Loss: 2.588948973425431e-06\n",
      "Iteration 15, Batch: 43, Loss: 2.9087286748108454e-06\n",
      "Iteration 15, Batch: 44, Loss: 2.3582663288834738e-06\n",
      "Iteration 15, Batch: 45, Loss: 1.5524936998190242e-06\n",
      "Iteration 15, Batch: 46, Loss: 1.0767222420327016e-06\n",
      "Iteration 15, Batch: 47, Loss: 1.8358992974754074e-06\n",
      "Iteration 15, Batch: 48, Loss: 1.272632061954937e-06\n",
      "Iteration 15, Batch: 49, Loss: 1.6262461031146813e-06\n",
      "Iteration 16, Batch: 0, Loss: 0.16283752024173737\n",
      "Iteration 16, Batch: 1, Loss: 0.11096721142530441\n",
      "Iteration 16, Batch: 2, Loss: 0.11597473174333572\n",
      "Iteration 16, Batch: 3, Loss: 0.12556248903274536\n",
      "Iteration 16, Batch: 4, Loss: 0.15796896815299988\n",
      "Iteration 16, Batch: 5, Loss: 0.12108458578586578\n",
      "Iteration 16, Batch: 6, Loss: 0.10020174086093903\n",
      "Iteration 16, Batch: 7, Loss: 0.11314839869737625\n",
      "Iteration 16, Batch: 8, Loss: 0.0881093367934227\n",
      "Iteration 16, Batch: 9, Loss: 0.08518499881029129\n",
      "Iteration 16, Batch: 10, Loss: 0.13303010165691376\n",
      "Iteration 16, Batch: 11, Loss: 0.12236003577709198\n",
      "Iteration 16, Batch: 12, Loss: 0.11778615415096283\n",
      "Iteration 16, Batch: 13, Loss: 0.1305563896894455\n",
      "Iteration 16, Batch: 14, Loss: 0.12494350224733353\n",
      "Iteration 16, Batch: 15, Loss: 0.10269585996866226\n",
      "Iteration 16, Batch: 16, Loss: 0.14276668429374695\n",
      "Iteration 16, Batch: 17, Loss: 0.1394987404346466\n",
      "Iteration 16, Batch: 18, Loss: 0.1513242870569229\n",
      "Iteration 16, Batch: 19, Loss: 0.12722735106945038\n",
      "Iteration 16, Batch: 20, Loss: 0.14309412240982056\n",
      "Iteration 16, Batch: 21, Loss: 0.09214908629655838\n",
      "Iteration 16, Batch: 22, Loss: 0.10533801466226578\n",
      "Iteration 16, Batch: 23, Loss: 0.14050601422786713\n",
      "Iteration 16, Batch: 24, Loss: 0.1074342131614685\n",
      "Iteration 16, Batch: 25, Loss: 0.12310251593589783\n",
      "Iteration 16, Batch: 26, Loss: 0.10822930932044983\n",
      "Iteration 16, Batch: 27, Loss: 0.11022312939167023\n",
      "Iteration 16, Batch: 28, Loss: 0.10870985686779022\n",
      "Iteration 16, Batch: 29, Loss: 0.12385211884975433\n",
      "Iteration 16, Batch: 30, Loss: 0.12083771079778671\n",
      "Iteration 16, Batch: 31, Loss: 0.10738355666399002\n",
      "Iteration 16, Batch: 32, Loss: 0.12237947434186935\n",
      "Iteration 16, Batch: 33, Loss: 0.10839078575372696\n",
      "Iteration 16, Batch: 34, Loss: 0.14546281099319458\n",
      "Iteration 16, Batch: 35, Loss: 0.10427488386631012\n",
      "Iteration 16, Batch: 36, Loss: 0.09815149754285812\n",
      "Iteration 16, Batch: 37, Loss: 0.11148105561733246\n",
      "Iteration 16, Batch: 38, Loss: 0.12014123797416687\n",
      "Iteration 16, Batch: 39, Loss: 0.09436842054128647\n",
      "Iteration 16, Batch: 40, Loss: 0.10610699653625488\n",
      "Iteration 16, Batch: 41, Loss: 0.1347147673368454\n",
      "Iteration 16, Batch: 42, Loss: 0.0718977078795433\n",
      "Iteration 16, Batch: 43, Loss: 0.11929360777139664\n",
      "Iteration 16, Batch: 44, Loss: 0.12761937081813812\n",
      "Iteration 16, Batch: 45, Loss: 0.13196898996829987\n",
      "Iteration 16, Batch: 46, Loss: 0.1183454692363739\n",
      "Iteration 16, Batch: 47, Loss: 0.11675390601158142\n",
      "Iteration 16, Batch: 48, Loss: 0.0785660371184349\n",
      "Iteration 16, Batch: 49, Loss: 0.10101327300071716\n",
      "Iteration 17, Batch: 0, Loss: 3.682480746647343e-05\n",
      "Iteration 17, Batch: 1, Loss: 1.6453259377158247e-05\n",
      "Iteration 17, Batch: 2, Loss: 1.4623624338128138e-05\n",
      "Iteration 17, Batch: 3, Loss: 2.382313323323615e-05\n",
      "Iteration 17, Batch: 4, Loss: 3.8959577068453655e-05\n",
      "Iteration 17, Batch: 5, Loss: 4.522299423115328e-05\n",
      "Iteration 17, Batch: 6, Loss: 3.848395135719329e-05\n",
      "Iteration 17, Batch: 7, Loss: 2.9377650207607076e-05\n",
      "Iteration 17, Batch: 8, Loss: 3.3405263820895925e-05\n",
      "Iteration 17, Batch: 9, Loss: 0.003144440008327365\n",
      "Iteration 17, Batch: 10, Loss: 4.4238502596272156e-05\n",
      "Iteration 17, Batch: 11, Loss: 3.4805158065864816e-05\n",
      "Iteration 17, Batch: 12, Loss: 2.337135811103508e-05\n",
      "Iteration 17, Batch: 13, Loss: 1.970593621081207e-05\n",
      "Iteration 17, Batch: 14, Loss: 2.0349598344182596e-05\n",
      "Iteration 17, Batch: 15, Loss: 0.004336116835474968\n",
      "Iteration 17, Batch: 16, Loss: 1.7744017895893194e-05\n",
      "Iteration 17, Batch: 17, Loss: 1.5115995665837545e-05\n",
      "Iteration 17, Batch: 18, Loss: 1.2987369700567797e-05\n",
      "Iteration 17, Batch: 19, Loss: 1.4324924450193066e-05\n",
      "Iteration 17, Batch: 20, Loss: 1.44919204103644e-05\n",
      "Iteration 17, Batch: 21, Loss: 1.8242177247884683e-05\n",
      "Iteration 17, Batch: 22, Loss: 1.2255655747139826e-05\n",
      "Iteration 17, Batch: 23, Loss: 1.4060327885090373e-05\n",
      "Iteration 17, Batch: 24, Loss: 1.5256749065883923e-05\n",
      "Iteration 17, Batch: 25, Loss: 1.3493359801941551e-05\n",
      "Iteration 17, Batch: 26, Loss: 1.1049319255107548e-05\n",
      "Iteration 17, Batch: 27, Loss: 1.2128850357839838e-05\n",
      "Iteration 17, Batch: 28, Loss: 1.3516798389900941e-05\n",
      "Iteration 17, Batch: 29, Loss: 1.207635705213761e-05\n",
      "Iteration 17, Batch: 30, Loss: 1.2231520486238878e-05\n",
      "Iteration 17, Batch: 31, Loss: 9.262453204428311e-06\n",
      "Iteration 17, Batch: 32, Loss: 1.169090683106333e-05\n",
      "Iteration 17, Batch: 33, Loss: 1.0039587323262822e-05\n",
      "Iteration 17, Batch: 34, Loss: 1.138748302764725e-05\n",
      "Iteration 17, Batch: 35, Loss: 1.1346004612278193e-05\n",
      "Iteration 17, Batch: 36, Loss: 9.687964848126285e-06\n",
      "Iteration 17, Batch: 37, Loss: 1.0941067557723727e-05\n",
      "Iteration 17, Batch: 38, Loss: 8.326372153533157e-06\n",
      "Iteration 17, Batch: 39, Loss: 7.285148512892192e-06\n",
      "Iteration 17, Batch: 40, Loss: 8.527913450961933e-06\n",
      "Iteration 17, Batch: 41, Loss: 9.168812539428473e-06\n",
      "Iteration 17, Batch: 42, Loss: 6.504425527964486e-06\n",
      "Iteration 17, Batch: 43, Loss: 4.857458861806663e-06\n",
      "Iteration 17, Batch: 44, Loss: 6.090662736824015e-06\n",
      "Iteration 17, Batch: 45, Loss: 7.281299076566938e-06\n",
      "Iteration 17, Batch: 46, Loss: 6.5196431933145504e-06\n",
      "Iteration 17, Batch: 47, Loss: 3.6489861940935953e-06\n",
      "Iteration 17, Batch: 48, Loss: 3.7770676044601714e-06\n",
      "Iteration 17, Batch: 49, Loss: 4.309526957513299e-06\n",
      "Iteration 18, Batch: 0, Loss: 0.124050073325634\n",
      "Iteration 18, Batch: 1, Loss: 0.10873167961835861\n",
      "Iteration 18, Batch: 2, Loss: 0.0844544991850853\n",
      "Iteration 18, Batch: 3, Loss: 0.15716978907585144\n",
      "Iteration 18, Batch: 4, Loss: 0.10283783078193665\n",
      "Iteration 18, Batch: 5, Loss: 0.15572084486484528\n",
      "Iteration 18, Batch: 6, Loss: 0.12691830098628998\n",
      "Iteration 18, Batch: 7, Loss: 0.07057712227106094\n",
      "Iteration 18, Batch: 8, Loss: 0.10649295896291733\n",
      "Iteration 18, Batch: 9, Loss: 0.10355167090892792\n",
      "Iteration 18, Batch: 10, Loss: 0.14565235376358032\n",
      "Iteration 18, Batch: 11, Loss: 0.09942565858364105\n",
      "Iteration 18, Batch: 12, Loss: 0.10086840391159058\n",
      "Iteration 18, Batch: 13, Loss: 0.08610021322965622\n",
      "Iteration 18, Batch: 14, Loss: 0.11215095221996307\n",
      "Iteration 18, Batch: 15, Loss: 0.144748717546463\n",
      "Iteration 18, Batch: 16, Loss: 0.1408763825893402\n",
      "Iteration 18, Batch: 17, Loss: 0.09533291310071945\n",
      "Iteration 18, Batch: 18, Loss: 0.08886141330003738\n",
      "Iteration 18, Batch: 19, Loss: 0.10888908058404922\n",
      "Iteration 18, Batch: 20, Loss: 0.10083164274692535\n",
      "Iteration 18, Batch: 21, Loss: 0.1154768317937851\n",
      "Iteration 18, Batch: 22, Loss: 0.1299661248922348\n",
      "Iteration 18, Batch: 23, Loss: 0.09516461193561554\n",
      "Iteration 18, Batch: 24, Loss: 0.14338716864585876\n",
      "Iteration 18, Batch: 25, Loss: 0.10645949840545654\n",
      "Iteration 18, Batch: 26, Loss: 0.0875195786356926\n",
      "Iteration 18, Batch: 27, Loss: 0.1073823869228363\n",
      "Iteration 18, Batch: 28, Loss: 0.07854218035936356\n",
      "Iteration 18, Batch: 29, Loss: 0.15503130853176117\n",
      "Iteration 18, Batch: 30, Loss: 0.1258872002363205\n",
      "Iteration 18, Batch: 31, Loss: 0.10096406936645508\n",
      "Iteration 18, Batch: 32, Loss: 0.1332566738128662\n",
      "Iteration 18, Batch: 33, Loss: 0.12763556838035583\n",
      "Iteration 18, Batch: 34, Loss: 0.1553192138671875\n",
      "Iteration 18, Batch: 35, Loss: 0.12228704988956451\n",
      "Iteration 18, Batch: 36, Loss: 0.11542751640081406\n",
      "Iteration 18, Batch: 37, Loss: 0.09962721914052963\n",
      "Iteration 18, Batch: 38, Loss: 0.1212860494852066\n",
      "Iteration 18, Batch: 39, Loss: 0.13589300215244293\n",
      "Iteration 18, Batch: 40, Loss: 0.08886010199785233\n",
      "Iteration 18, Batch: 41, Loss: 0.07224661856889725\n",
      "Iteration 18, Batch: 42, Loss: 0.10237205028533936\n",
      "Iteration 18, Batch: 43, Loss: 0.09808950126171112\n",
      "Iteration 18, Batch: 44, Loss: 0.13607361912727356\n",
      "Iteration 18, Batch: 45, Loss: 0.08778066188097\n",
      "Iteration 18, Batch: 46, Loss: 0.18737903237342834\n",
      "Iteration 18, Batch: 47, Loss: 0.12941816449165344\n",
      "Iteration 18, Batch: 48, Loss: 0.15205374360084534\n",
      "Iteration 18, Batch: 49, Loss: 0.13382911682128906\n",
      "Iteration 19, Batch: 0, Loss: 4.134101254749112e-05\n",
      "Iteration 19, Batch: 1, Loss: 2.1841127818333916e-05\n",
      "Iteration 19, Batch: 2, Loss: 9.398509973834734e-06\n",
      "Iteration 19, Batch: 3, Loss: 2.166272133763414e-05\n",
      "Iteration 19, Batch: 4, Loss: 4.304427420720458e-05\n",
      "Iteration 19, Batch: 5, Loss: 5.382805102271959e-05\n",
      "Iteration 19, Batch: 6, Loss: 1.6169760783668607e-05\n",
      "Iteration 19, Batch: 7, Loss: 1.848082320066169e-05\n",
      "Iteration 19, Batch: 8, Loss: 4.3566120439209044e-05\n",
      "Iteration 19, Batch: 9, Loss: 5.4595835536019877e-05\n",
      "Iteration 19, Batch: 10, Loss: 5.556972973863594e-05\n",
      "Iteration 19, Batch: 11, Loss: 2.3678596335230395e-05\n",
      "Iteration 19, Batch: 12, Loss: 2.33397167903604e-05\n",
      "Iteration 19, Batch: 13, Loss: 2.5189885491272435e-05\n",
      "Iteration 19, Batch: 14, Loss: 0.004678698256611824\n",
      "Iteration 19, Batch: 15, Loss: 2.1385229047155008e-05\n",
      "Iteration 19, Batch: 16, Loss: 2.2544067178387195e-05\n",
      "Iteration 19, Batch: 17, Loss: 9.247440175386146e-06\n",
      "Iteration 19, Batch: 18, Loss: 1.2133705240557902e-05\n",
      "Iteration 19, Batch: 19, Loss: 1.9571016309782863e-05\n",
      "Iteration 19, Batch: 20, Loss: 1.052789684763411e-05\n",
      "Iteration 19, Batch: 21, Loss: 5.3210392252367456e-06\n",
      "Iteration 19, Batch: 22, Loss: 6.467490493378136e-06\n",
      "Iteration 19, Batch: 23, Loss: 9.473271347815171e-06\n",
      "Iteration 19, Batch: 24, Loss: 1.0100486178998835e-05\n",
      "Iteration 19, Batch: 25, Loss: 5.1008310038014315e-06\n",
      "Iteration 19, Batch: 26, Loss: 3.8991956898826174e-06\n",
      "Iteration 19, Batch: 27, Loss: 5.290392437018454e-06\n",
      "Iteration 19, Batch: 28, Loss: 5.952510946372058e-06\n",
      "Iteration 19, Batch: 29, Loss: 9.919283911585808e-06\n",
      "Iteration 19, Batch: 30, Loss: 4.751702363137156e-06\n",
      "Iteration 19, Batch: 31, Loss: 3.23889412356948e-06\n",
      "Iteration 19, Batch: 32, Loss: 4.734511094284244e-06\n",
      "Iteration 19, Batch: 33, Loss: 8.117882316582836e-06\n",
      "Iteration 19, Batch: 34, Loss: 5.613344001176301e-06\n",
      "Iteration 19, Batch: 35, Loss: 3.0615462947025662e-06\n",
      "Iteration 19, Batch: 36, Loss: 3.2673026453267084e-06\n",
      "Iteration 19, Batch: 37, Loss: 7.528623882535612e-06\n",
      "Iteration 19, Batch: 38, Loss: 6.572682195837842e-06\n",
      "Iteration 19, Batch: 39, Loss: 4.695819825428771e-06\n",
      "Iteration 19, Batch: 40, Loss: 3.719689857462072e-06\n",
      "Iteration 19, Batch: 41, Loss: 5.578784112003632e-06\n",
      "Iteration 19, Batch: 42, Loss: 4.980072844773531e-06\n",
      "Iteration 19, Batch: 43, Loss: 3.863743131660158e-06\n",
      "Iteration 19, Batch: 44, Loss: 3.1964113986759912e-06\n",
      "Iteration 19, Batch: 45, Loss: 4.268445081834216e-06\n",
      "Iteration 19, Batch: 46, Loss: 3.960622962040361e-06\n",
      "Iteration 19, Batch: 47, Loss: 3.856047442241106e-06\n",
      "Iteration 19, Batch: 48, Loss: 4.823685230803676e-06\n",
      "Iteration 19, Batch: 49, Loss: 2.9130892471584957e-06\n",
      "Iteration 20, Batch: 0, Loss: 0.1835995763540268\n",
      "Iteration 20, Batch: 1, Loss: 0.16452249884605408\n",
      "Iteration 20, Batch: 2, Loss: 0.15575449168682098\n",
      "Iteration 20, Batch: 3, Loss: 0.1328275054693222\n",
      "Iteration 20, Batch: 4, Loss: 0.16341504454612732\n",
      "Iteration 20, Batch: 5, Loss: 0.10494968295097351\n",
      "Iteration 20, Batch: 6, Loss: 0.125352680683136\n",
      "Iteration 20, Batch: 7, Loss: 0.15190882980823517\n",
      "Iteration 20, Batch: 8, Loss: 0.14228929579257965\n",
      "Iteration 20, Batch: 9, Loss: 0.17644527554512024\n",
      "Iteration 20, Batch: 10, Loss: 0.13807129859924316\n",
      "Iteration 20, Batch: 11, Loss: 0.12998870015144348\n",
      "Iteration 20, Batch: 12, Loss: 0.13307638466358185\n",
      "Iteration 20, Batch: 13, Loss: 0.10699281096458435\n",
      "Iteration 20, Batch: 14, Loss: 0.1347319632768631\n",
      "Iteration 20, Batch: 15, Loss: 0.12225966155529022\n",
      "Iteration 20, Batch: 16, Loss: 0.11471976339817047\n",
      "Iteration 20, Batch: 17, Loss: 0.11903142929077148\n",
      "Iteration 20, Batch: 18, Loss: 0.1297045797109604\n",
      "Iteration 20, Batch: 19, Loss: 0.11955728381872177\n",
      "Iteration 20, Batch: 20, Loss: 0.12229491770267487\n",
      "Iteration 20, Batch: 21, Loss: 0.11763012409210205\n",
      "Iteration 20, Batch: 22, Loss: 0.08532198518514633\n",
      "Iteration 20, Batch: 23, Loss: 0.12117532640695572\n",
      "Iteration 20, Batch: 24, Loss: 0.12658585608005524\n",
      "Iteration 20, Batch: 25, Loss: 0.11558210104703903\n",
      "Iteration 20, Batch: 26, Loss: 0.10926622152328491\n",
      "Iteration 20, Batch: 27, Loss: 0.13004730641841888\n",
      "Iteration 20, Batch: 28, Loss: 0.09994187206029892\n",
      "Iteration 20, Batch: 29, Loss: 0.1469564139842987\n",
      "Iteration 20, Batch: 30, Loss: 0.1044531986117363\n",
      "Iteration 20, Batch: 31, Loss: 0.12931354343891144\n",
      "Iteration 20, Batch: 32, Loss: 0.10471192747354507\n",
      "Iteration 20, Batch: 33, Loss: 0.10009831935167313\n",
      "Iteration 20, Batch: 34, Loss: 0.10966632515192032\n",
      "Iteration 20, Batch: 35, Loss: 0.14012554287910461\n",
      "Iteration 20, Batch: 36, Loss: 0.09671098738908768\n",
      "Iteration 20, Batch: 37, Loss: 0.15061111748218536\n",
      "Iteration 20, Batch: 38, Loss: 0.10154981166124344\n",
      "Iteration 20, Batch: 39, Loss: 0.10319911688566208\n",
      "Iteration 20, Batch: 40, Loss: 0.10729239881038666\n",
      "Iteration 20, Batch: 41, Loss: 0.11100544780492783\n",
      "Iteration 20, Batch: 42, Loss: 0.14068284630775452\n",
      "Iteration 20, Batch: 43, Loss: 0.10507699102163315\n",
      "Iteration 20, Batch: 44, Loss: 0.1319175809621811\n",
      "Iteration 20, Batch: 45, Loss: 0.1262223869562149\n",
      "Iteration 20, Batch: 46, Loss: 0.13985736668109894\n",
      "Iteration 20, Batch: 47, Loss: 0.12407033890485764\n",
      "Iteration 20, Batch: 48, Loss: 0.09892751276493073\n",
      "Iteration 20, Batch: 49, Loss: 0.132864847779274\n",
      "Iteration 21, Batch: 0, Loss: 5.6475451856385916e-05\n",
      "Iteration 21, Batch: 1, Loss: 1.3801490240439307e-05\n",
      "Iteration 21, Batch: 2, Loss: 2.8887414373457432e-05\n",
      "Iteration 21, Batch: 3, Loss: 7.99416666268371e-05\n",
      "Iteration 21, Batch: 4, Loss: 9.632641013013199e-05\n",
      "Iteration 21, Batch: 5, Loss: 7.49788450775668e-05\n",
      "Iteration 21, Batch: 6, Loss: 2.101716017932631e-05\n",
      "Iteration 21, Batch: 7, Loss: 5.756765403930331e-06\n",
      "Iteration 21, Batch: 8, Loss: 3.562312849680893e-05\n",
      "Iteration 21, Batch: 9, Loss: 5.4051382903708145e-05\n",
      "Iteration 21, Batch: 10, Loss: 5.8195953897666186e-05\n",
      "Iteration 21, Batch: 11, Loss: 3.776017183554359e-05\n",
      "Iteration 21, Batch: 12, Loss: 1.6129311916301958e-05\n",
      "Iteration 21, Batch: 13, Loss: 1.7834834579844028e-05\n",
      "Iteration 21, Batch: 14, Loss: 3.2454543543281034e-05\n",
      "Iteration 21, Batch: 15, Loss: 2.994622082042042e-05\n",
      "Iteration 21, Batch: 16, Loss: 2.8126734832767397e-05\n",
      "Iteration 21, Batch: 17, Loss: 1.9599199731601402e-05\n",
      "Iteration 21, Batch: 18, Loss: 1.884445737232454e-05\n",
      "Iteration 21, Batch: 19, Loss: 1.9196346329408698e-05\n",
      "Iteration 21, Batch: 20, Loss: 1.5961531971697696e-05\n",
      "Iteration 21, Batch: 21, Loss: 1.1025222192984074e-05\n",
      "Iteration 21, Batch: 22, Loss: 8.17744239611784e-06\n",
      "Iteration 21, Batch: 23, Loss: 1.7480055248597637e-05\n",
      "Iteration 21, Batch: 24, Loss: 1.9158263967256062e-05\n",
      "Iteration 21, Batch: 25, Loss: 1.055065786204068e-05\n",
      "Iteration 21, Batch: 26, Loss: 2.7310136374580907e-06\n",
      "Iteration 21, Batch: 27, Loss: 3.5274820220365655e-06\n",
      "Iteration 21, Batch: 28, Loss: 1.0349282092647627e-05\n",
      "Iteration 21, Batch: 29, Loss: 1.1397137313906569e-05\n",
      "Iteration 21, Batch: 30, Loss: 8.019489541766234e-06\n",
      "Iteration 21, Batch: 31, Loss: 2.095681111313752e-06\n",
      "Iteration 21, Batch: 32, Loss: 2.4675618988112547e-06\n",
      "Iteration 21, Batch: 33, Loss: 4.447335868462687e-06\n",
      "Iteration 21, Batch: 34, Loss: 6.6286556830164045e-06\n",
      "Iteration 21, Batch: 35, Loss: 4.822321898245718e-06\n",
      "Iteration 21, Batch: 36, Loss: 3.403691152925603e-06\n",
      "Iteration 21, Batch: 37, Loss: 1.9956858068326255e-06\n",
      "Iteration 21, Batch: 38, Loss: 1.964119292097166e-06\n",
      "Iteration 21, Batch: 39, Loss: 2.8740864763676655e-06\n",
      "Iteration 21, Batch: 40, Loss: 2.857445451809326e-06\n",
      "Iteration 21, Batch: 41, Loss: 2.1518569610634586e-06\n",
      "Iteration 21, Batch: 42, Loss: 1.6576092320974567e-06\n",
      "Iteration 21, Batch: 43, Loss: 2.2524454834638163e-06\n",
      "Iteration 21, Batch: 44, Loss: 2.2052372514735907e-06\n",
      "Iteration 21, Batch: 45, Loss: 2.2548069864569698e-06\n",
      "Iteration 21, Batch: 46, Loss: 1.334517719442374e-06\n",
      "Iteration 21, Batch: 47, Loss: 1.2949760730407434e-06\n",
      "Iteration 21, Batch: 48, Loss: 1.8770705310089397e-06\n",
      "Iteration 21, Batch: 49, Loss: 1.6438306147392723e-06\n",
      "Iteration 22, Batch: 0, Loss: 0.12261258065700531\n",
      "Iteration 22, Batch: 1, Loss: 0.12347730249166489\n",
      "Iteration 22, Batch: 2, Loss: 0.14135155081748962\n",
      "Iteration 22, Batch: 3, Loss: 0.16610278189182281\n",
      "Iteration 22, Batch: 4, Loss: 0.12269245833158493\n",
      "Iteration 22, Batch: 5, Loss: 0.09430433064699173\n",
      "Iteration 22, Batch: 6, Loss: 0.14274176955223083\n",
      "Iteration 22, Batch: 7, Loss: 0.13624531030654907\n",
      "Iteration 22, Batch: 8, Loss: 0.11092633754014969\n",
      "Iteration 22, Batch: 9, Loss: 0.14414756000041962\n",
      "Iteration 22, Batch: 10, Loss: 0.11878246814012527\n",
      "Iteration 22, Batch: 11, Loss: 0.16192016005516052\n",
      "Iteration 22, Batch: 12, Loss: 0.14813220500946045\n",
      "Iteration 22, Batch: 13, Loss: 0.0911196693778038\n",
      "Iteration 22, Batch: 14, Loss: 0.12367112189531326\n",
      "Iteration 22, Batch: 15, Loss: 0.12802284955978394\n",
      "Iteration 22, Batch: 16, Loss: 0.11646828055381775\n",
      "Iteration 22, Batch: 17, Loss: 0.10489336401224136\n",
      "Iteration 22, Batch: 18, Loss: 0.13074606657028198\n",
      "Iteration 22, Batch: 19, Loss: 0.12445147335529327\n",
      "Iteration 22, Batch: 20, Loss: 0.08882690221071243\n",
      "Iteration 22, Batch: 21, Loss: 0.1398058980703354\n",
      "Iteration 22, Batch: 22, Loss: 0.10686090588569641\n",
      "Iteration 22, Batch: 23, Loss: 0.10188419371843338\n",
      "Iteration 22, Batch: 24, Loss: 0.12237102538347244\n",
      "Iteration 22, Batch: 25, Loss: 0.11482516676187515\n",
      "Iteration 22, Batch: 26, Loss: 0.1039210855960846\n",
      "Iteration 22, Batch: 27, Loss: 0.10788707435131073\n",
      "Iteration 22, Batch: 28, Loss: 0.10093747824430466\n",
      "Iteration 22, Batch: 29, Loss: 0.11004933714866638\n",
      "Iteration 22, Batch: 30, Loss: 0.12549422681331635\n",
      "Iteration 22, Batch: 31, Loss: 0.13667549192905426\n",
      "Iteration 22, Batch: 32, Loss: 0.11101517081260681\n",
      "Iteration 22, Batch: 33, Loss: 0.10411076992750168\n",
      "Iteration 22, Batch: 34, Loss: 0.15473288297653198\n",
      "Iteration 22, Batch: 35, Loss: 0.0978318601846695\n",
      "Iteration 22, Batch: 36, Loss: 0.13371361792087555\n",
      "Iteration 22, Batch: 37, Loss: 0.1124972403049469\n",
      "Iteration 22, Batch: 38, Loss: 0.15199173986911774\n",
      "Iteration 22, Batch: 39, Loss: 0.15941840410232544\n",
      "Iteration 22, Batch: 40, Loss: 0.12485236674547195\n",
      "Iteration 22, Batch: 41, Loss: 0.16328266263008118\n",
      "Iteration 22, Batch: 42, Loss: 0.1142696812748909\n",
      "Iteration 22, Batch: 43, Loss: 0.11352213472127914\n",
      "Iteration 22, Batch: 44, Loss: 0.12696371972560883\n",
      "Iteration 22, Batch: 45, Loss: 0.13677044212818146\n",
      "Iteration 22, Batch: 46, Loss: 0.12967030704021454\n",
      "Iteration 22, Batch: 47, Loss: 0.11352798342704773\n",
      "Iteration 22, Batch: 48, Loss: 0.09855731576681137\n",
      "Iteration 22, Batch: 49, Loss: 0.1418784260749817\n",
      "Iteration 23, Batch: 0, Loss: 9.591240086592734e-05\n",
      "Iteration 23, Batch: 1, Loss: 6.0634021792793646e-05\n",
      "Iteration 23, Batch: 2, Loss: 4.354659540695138e-05\n",
      "Iteration 23, Batch: 3, Loss: 4.207536039757542e-05\n",
      "Iteration 23, Batch: 4, Loss: 3.991082849097438e-05\n",
      "Iteration 23, Batch: 5, Loss: 4.1869734559440985e-05\n",
      "Iteration 23, Batch: 6, Loss: 1.7852573364507407e-05\n",
      "Iteration 23, Batch: 7, Loss: 7.901830940681975e-06\n",
      "Iteration 23, Batch: 8, Loss: 2.07893863262143e-05\n",
      "Iteration 23, Batch: 9, Loss: 3.38936188200023e-05\n",
      "Iteration 23, Batch: 10, Loss: 2.8794011086574756e-05\n",
      "Iteration 23, Batch: 11, Loss: 9.35423668124713e-06\n",
      "Iteration 23, Batch: 12, Loss: 1.1129503718620981e-06\n",
      "Iteration 23, Batch: 13, Loss: 8.244583113992121e-06\n",
      "Iteration 23, Batch: 14, Loss: 1.898683149192948e-05\n",
      "Iteration 23, Batch: 15, Loss: 0.004138148855417967\n",
      "Iteration 23, Batch: 16, Loss: 1.2169333786005154e-05\n",
      "Iteration 23, Batch: 17, Loss: 3.273931724834256e-06\n",
      "Iteration 23, Batch: 18, Loss: 2.451237151035457e-06\n",
      "Iteration 23, Batch: 19, Loss: 0.003240872174501419\n",
      "Iteration 23, Batch: 20, Loss: 0.005627098027616739\n",
      "Iteration 23, Batch: 21, Loss: 0.0940747782588005\n",
      "Iteration 23, Batch: 22, Loss: 0.20561735332012177\n",
      "Iteration 23, Batch: 23, Loss: 0.2969210743904114\n",
      "Iteration 23, Batch: 24, Loss: 0.9071940183639526\n",
      "Iteration 23, Batch: 25, Loss: 0.5050671100616455\n",
      "Iteration 23, Batch: 26, Loss: 0.33221763372421265\n",
      "Iteration 23, Batch: 27, Loss: 0.056873518973588943\n",
      "Iteration 23, Batch: 28, Loss: 0.046181101351976395\n",
      "Iteration 23, Batch: 29, Loss: 0.13106559216976166\n",
      "Iteration 23, Batch: 30, Loss: 0.16718490421772003\n",
      "Iteration 23, Batch: 31, Loss: 0.0275092925876379\n",
      "Iteration 23, Batch: 32, Loss: 0.08288286626338959\n",
      "Iteration 23, Batch: 33, Loss: 0.03717877343297005\n",
      "Iteration 23, Batch: 34, Loss: 0.31255286931991577\n",
      "Iteration 23, Batch: 35, Loss: 0.5241924524307251\n",
      "Iteration 23, Batch: 36, Loss: 0.14800691604614258\n",
      "Iteration 23, Batch: 37, Loss: 0.020979437977075577\n",
      "Iteration 23, Batch: 38, Loss: 0.061465658247470856\n",
      "Iteration 23, Batch: 39, Loss: 0.014428004622459412\n",
      "Iteration 23, Batch: 40, Loss: 4.04109523515217e-05\n",
      "Iteration 23, Batch: 41, Loss: 0.0042169890366494656\n",
      "Iteration 23, Batch: 42, Loss: 0.06721416860818863\n",
      "Iteration 23, Batch: 43, Loss: 0.019953254610300064\n",
      "Iteration 23, Batch: 44, Loss: 6.0125919844722375e-05\n",
      "Iteration 23, Batch: 45, Loss: 1.982225694519002e-05\n",
      "Iteration 23, Batch: 46, Loss: 1.9157831047778018e-05\n",
      "Iteration 23, Batch: 47, Loss: 0.000713539426214993\n",
      "Iteration 23, Batch: 48, Loss: 0.00712963193655014\n",
      "Iteration 23, Batch: 49, Loss: 0.013543601147830486\n",
      "Iteration 24, Batch: 0, Loss: 0.25431129336357117\n",
      "Iteration 24, Batch: 1, Loss: 0.23832419514656067\n",
      "Iteration 24, Batch: 2, Loss: 0.2038542479276657\n",
      "Iteration 24, Batch: 3, Loss: 0.1845337450504303\n",
      "Iteration 24, Batch: 4, Loss: 0.13601990044116974\n",
      "Iteration 24, Batch: 5, Loss: 0.10554345697164536\n",
      "Iteration 24, Batch: 6, Loss: 0.14805781841278076\n",
      "Iteration 24, Batch: 7, Loss: 0.1492166668176651\n",
      "Iteration 24, Batch: 8, Loss: 0.11028875410556793\n",
      "Iteration 24, Batch: 9, Loss: 0.10198651999235153\n",
      "Iteration 24, Batch: 10, Loss: 0.15496483445167542\n",
      "Iteration 24, Batch: 11, Loss: 0.08659970760345459\n",
      "Iteration 24, Batch: 12, Loss: 0.10033053159713745\n",
      "Iteration 24, Batch: 13, Loss: 0.12378569692373276\n",
      "Iteration 24, Batch: 14, Loss: 0.12829367816448212\n",
      "Iteration 24, Batch: 15, Loss: 0.1120317280292511\n",
      "Iteration 24, Batch: 16, Loss: 0.14403079450130463\n",
      "Iteration 24, Batch: 17, Loss: 0.15495391190052032\n",
      "Iteration 24, Batch: 18, Loss: 0.12289673835039139\n",
      "Iteration 24, Batch: 19, Loss: 0.13572905957698822\n",
      "Iteration 24, Batch: 20, Loss: 0.15039129555225372\n",
      "Iteration 24, Batch: 21, Loss: 0.178131103515625\n",
      "Iteration 24, Batch: 22, Loss: 0.16917924582958221\n",
      "Iteration 24, Batch: 23, Loss: 0.14963579177856445\n",
      "Iteration 24, Batch: 24, Loss: 0.1326804906129837\n",
      "Iteration 24, Batch: 25, Loss: 0.12857088446617126\n",
      "Iteration 24, Batch: 26, Loss: 0.1692250818014145\n",
      "Iteration 24, Batch: 27, Loss: 0.09363901615142822\n",
      "Iteration 24, Batch: 28, Loss: 0.10175535827875137\n",
      "Iteration 24, Batch: 29, Loss: 0.10458484292030334\n",
      "Iteration 24, Batch: 30, Loss: 0.0964813232421875\n",
      "Iteration 24, Batch: 31, Loss: 0.09728652983903885\n",
      "Iteration 24, Batch: 32, Loss: 0.14121584594249725\n",
      "Iteration 24, Batch: 33, Loss: 0.13486938178539276\n",
      "Iteration 24, Batch: 34, Loss: 0.12027379870414734\n",
      "Iteration 24, Batch: 35, Loss: 0.139311745762825\n",
      "Iteration 24, Batch: 36, Loss: 0.15702643990516663\n",
      "Iteration 24, Batch: 37, Loss: 0.13067148625850677\n",
      "Iteration 24, Batch: 38, Loss: 0.08650088310241699\n",
      "Iteration 24, Batch: 39, Loss: 0.15149588882923126\n",
      "Iteration 24, Batch: 40, Loss: 0.09122209250926971\n",
      "Iteration 24, Batch: 41, Loss: 0.16728980839252472\n",
      "Iteration 24, Batch: 42, Loss: 0.09930728375911713\n",
      "Iteration 24, Batch: 43, Loss: 0.18415123224258423\n",
      "Iteration 24, Batch: 44, Loss: 0.12296238541603088\n",
      "Iteration 24, Batch: 45, Loss: 0.12580673396587372\n",
      "Iteration 24, Batch: 46, Loss: 0.14349296689033508\n",
      "Iteration 24, Batch: 47, Loss: 0.13407011330127716\n",
      "Iteration 24, Batch: 48, Loss: 0.11688758432865143\n",
      "Iteration 24, Batch: 49, Loss: 0.1673484444618225\n",
      "Iteration 25, Batch: 0, Loss: 0.025086762383580208\n",
      "Iteration 25, Batch: 1, Loss: 0.005054783541709185\n",
      "Iteration 25, Batch: 2, Loss: 0.005039710085839033\n",
      "Iteration 25, Batch: 3, Loss: 0.019962191581726074\n",
      "Iteration 25, Batch: 4, Loss: 0.033989548683166504\n",
      "Iteration 25, Batch: 5, Loss: 0.03007357008755207\n",
      "Iteration 25, Batch: 6, Loss: 0.03504278138279915\n",
      "Iteration 25, Batch: 7, Loss: 0.05006725341081619\n",
      "Iteration 25, Batch: 8, Loss: 0.07504121959209442\n",
      "Iteration 25, Batch: 9, Loss: 0.0945170447230339\n",
      "Iteration 25, Batch: 10, Loss: 8.735567098483443e-05\n",
      "Iteration 25, Batch: 11, Loss: 2.6621644792612642e-05\n",
      "Iteration 25, Batch: 12, Loss: 0.00028433138504624367\n",
      "Iteration 25, Batch: 13, Loss: 4.359766899142414e-05\n",
      "Iteration 25, Batch: 14, Loss: 0.0482993982732296\n",
      "Iteration 25, Batch: 15, Loss: 7.921308133518323e-05\n",
      "Iteration 25, Batch: 16, Loss: 0.07585224509239197\n",
      "Iteration 25, Batch: 17, Loss: 0.0016007518861442804\n",
      "Iteration 25, Batch: 18, Loss: 0.07084107398986816\n",
      "Iteration 25, Batch: 19, Loss: 0.07133114337921143\n",
      "Iteration 25, Batch: 20, Loss: 0.09486614912748337\n",
      "Iteration 25, Batch: 21, Loss: 0.026378830894827843\n",
      "Iteration 25, Batch: 22, Loss: 0.015637889504432678\n",
      "Iteration 25, Batch: 23, Loss: 0.05851171910762787\n",
      "Iteration 25, Batch: 24, Loss: 0.20171113312244415\n",
      "Iteration 25, Batch: 25, Loss: 0.15485912561416626\n",
      "Iteration 25, Batch: 26, Loss: 0.3065597116947174\n",
      "Iteration 25, Batch: 27, Loss: 0.5259849429130554\n",
      "Iteration 25, Batch: 28, Loss: 0.47866252064704895\n",
      "Iteration 25, Batch: 29, Loss: 0.42451900243759155\n",
      "Iteration 25, Batch: 30, Loss: 0.511735737323761\n",
      "Iteration 25, Batch: 31, Loss: 0.42484161257743835\n",
      "Iteration 25, Batch: 32, Loss: 0.32539424300193787\n",
      "Iteration 25, Batch: 33, Loss: 0.40202924609184265\n",
      "Iteration 25, Batch: 34, Loss: 0.5160354375839233\n",
      "Iteration 25, Batch: 35, Loss: 0.4217589497566223\n",
      "Iteration 25, Batch: 36, Loss: 0.4293861985206604\n",
      "Iteration 25, Batch: 37, Loss: 0.511696994304657\n",
      "Iteration 25, Batch: 38, Loss: 0.4684407711029053\n",
      "Iteration 25, Batch: 39, Loss: 0.3454393446445465\n",
      "Iteration 25, Batch: 40, Loss: 0.353276789188385\n",
      "Iteration 25, Batch: 41, Loss: 0.6719651818275452\n",
      "Iteration 25, Batch: 42, Loss: 0.5999011993408203\n",
      "Iteration 25, Batch: 43, Loss: 1.0981522798538208\n",
      "Iteration 25, Batch: 44, Loss: 1.260719895362854\n",
      "Iteration 25, Batch: 45, Loss: 1.0802364349365234\n",
      "Iteration 25, Batch: 46, Loss: 0.5640589594841003\n",
      "Iteration 25, Batch: 47, Loss: 0.46884021162986755\n",
      "Iteration 25, Batch: 48, Loss: 0.6924850344657898\n",
      "Iteration 25, Batch: 49, Loss: 0.43428000807762146\n",
      "Iteration 26, Batch: 0, Loss: 0.7562726140022278\n",
      "Iteration 26, Batch: 1, Loss: 0.5801095366477966\n",
      "Iteration 26, Batch: 2, Loss: 0.48996779322624207\n",
      "Iteration 26, Batch: 3, Loss: 0.4087892472743988\n",
      "Iteration 26, Batch: 4, Loss: 0.29925471544265747\n",
      "Iteration 26, Batch: 5, Loss: 0.21020491421222687\n",
      "Iteration 26, Batch: 6, Loss: 0.19691763818264008\n",
      "Iteration 26, Batch: 7, Loss: 0.20400245487689972\n",
      "Iteration 26, Batch: 8, Loss: 0.22622764110565186\n",
      "Iteration 26, Batch: 9, Loss: 0.17698439955711365\n",
      "Iteration 26, Batch: 10, Loss: 0.24232959747314453\n",
      "Iteration 26, Batch: 11, Loss: 0.19437573850154877\n",
      "Iteration 26, Batch: 12, Loss: 0.1887199729681015\n",
      "Iteration 26, Batch: 13, Loss: 0.10958819836378098\n",
      "Iteration 26, Batch: 14, Loss: 0.16596285998821259\n",
      "Iteration 26, Batch: 15, Loss: 0.12665222585201263\n",
      "Iteration 26, Batch: 16, Loss: 0.16280129551887512\n",
      "Iteration 26, Batch: 17, Loss: 0.2082543671131134\n",
      "Iteration 26, Batch: 18, Loss: 0.1293259859085083\n",
      "Iteration 26, Batch: 19, Loss: 0.16299590468406677\n",
      "Iteration 26, Batch: 20, Loss: 0.1591043323278427\n",
      "Iteration 26, Batch: 21, Loss: 0.13768215477466583\n",
      "Iteration 26, Batch: 22, Loss: 0.12604008615016937\n",
      "Iteration 26, Batch: 23, Loss: 0.1429242342710495\n",
      "Iteration 26, Batch: 24, Loss: 0.10998410731554031\n",
      "Iteration 26, Batch: 25, Loss: 0.10671494156122208\n",
      "Iteration 26, Batch: 26, Loss: 0.08540433645248413\n",
      "Iteration 26, Batch: 27, Loss: 0.11950017511844635\n",
      "Iteration 26, Batch: 28, Loss: 0.13232313096523285\n",
      "Iteration 26, Batch: 29, Loss: 0.14015071094036102\n",
      "Iteration 26, Batch: 30, Loss: 0.1515129655599594\n",
      "Iteration 26, Batch: 31, Loss: 0.11727636307477951\n",
      "Iteration 26, Batch: 32, Loss: 0.11620774865150452\n",
      "Iteration 26, Batch: 33, Loss: 0.11786488443613052\n",
      "Iteration 26, Batch: 34, Loss: 0.1272089183330536\n",
      "Iteration 26, Batch: 35, Loss: 0.10831666737794876\n",
      "Iteration 26, Batch: 36, Loss: 0.12575370073318481\n",
      "Iteration 26, Batch: 37, Loss: 0.15125881135463715\n",
      "Iteration 26, Batch: 38, Loss: 0.11349795758724213\n",
      "Iteration 26, Batch: 39, Loss: 0.11474037170410156\n",
      "Iteration 26, Batch: 40, Loss: 0.14712531864643097\n",
      "Iteration 26, Batch: 41, Loss: 0.1069270595908165\n",
      "Iteration 26, Batch: 42, Loss: 0.12513118982315063\n",
      "Iteration 26, Batch: 43, Loss: 0.08345530182123184\n",
      "Iteration 26, Batch: 44, Loss: 0.10739583522081375\n",
      "Iteration 26, Batch: 45, Loss: 0.11210617423057556\n",
      "Iteration 26, Batch: 46, Loss: 0.12944132089614868\n",
      "Iteration 26, Batch: 47, Loss: 0.1415509432554245\n",
      "Iteration 26, Batch: 48, Loss: 0.10579902678728104\n",
      "Iteration 26, Batch: 49, Loss: 0.0848105251789093\n",
      "Iteration 27, Batch: 0, Loss: 0.07178536057472229\n",
      "Iteration 27, Batch: 1, Loss: 0.054730191826820374\n",
      "Iteration 27, Batch: 2, Loss: 0.024968478828668594\n",
      "Iteration 27, Batch: 3, Loss: 0.0226259995251894\n",
      "Iteration 27, Batch: 4, Loss: 0.0183277390897274\n",
      "Iteration 27, Batch: 5, Loss: 0.032461486756801605\n",
      "Iteration 27, Batch: 6, Loss: 0.08820050954818726\n",
      "Iteration 27, Batch: 7, Loss: 1.1277663361397572e-05\n",
      "Iteration 27, Batch: 8, Loss: 0.04689143970608711\n",
      "Iteration 27, Batch: 9, Loss: 1.2023350791423582e-05\n",
      "Iteration 27, Batch: 10, Loss: 7.275848020071862e-06\n",
      "Iteration 27, Batch: 11, Loss: 0.0035953563638031483\n",
      "Iteration 27, Batch: 12, Loss: 0.004941864870488644\n",
      "Iteration 27, Batch: 13, Loss: 2.1857215415366227e-06\n",
      "Iteration 27, Batch: 14, Loss: 0.00465203495696187\n",
      "Iteration 27, Batch: 15, Loss: 0.004508204758167267\n",
      "Iteration 27, Batch: 16, Loss: 7.719254426774569e-06\n",
      "Iteration 27, Batch: 17, Loss: 6.471347205661004e-06\n",
      "Iteration 27, Batch: 18, Loss: 5.727521966036875e-06\n",
      "Iteration 27, Batch: 19, Loss: 2.1433736492326716e-06\n",
      "Iteration 27, Batch: 20, Loss: 1.4192424941938953e-06\n",
      "Iteration 27, Batch: 21, Loss: 2.8384326924424386e-06\n",
      "Iteration 27, Batch: 22, Loss: 4.7905350584187545e-06\n",
      "Iteration 27, Batch: 23, Loss: 3.458526180111221e-06\n",
      "Iteration 27, Batch: 24, Loss: 4.240877842676127e-06\n",
      "Iteration 27, Batch: 25, Loss: 3.120995643257629e-06\n",
      "Iteration 27, Batch: 26, Loss: 1.9702113149833167e-06\n",
      "Iteration 27, Batch: 27, Loss: 1.4438677453654236e-06\n",
      "Iteration 27, Batch: 28, Loss: 1.6009018963814015e-06\n",
      "Iteration 27, Batch: 29, Loss: 1.9895346667908598e-06\n",
      "Iteration 27, Batch: 30, Loss: 2.3590453110955423e-06\n",
      "Iteration 27, Batch: 31, Loss: 2.0359702830319293e-06\n",
      "Iteration 27, Batch: 32, Loss: 2.275464794365689e-06\n",
      "Iteration 27, Batch: 33, Loss: 7.660349297111679e-07\n",
      "Iteration 27, Batch: 34, Loss: 5.800906137665152e-07\n",
      "Iteration 27, Batch: 35, Loss: 6.534893373100203e-07\n",
      "Iteration 27, Batch: 36, Loss: 1.4252919982027379e-06\n",
      "Iteration 27, Batch: 37, Loss: 1.699717699921166e-06\n",
      "Iteration 27, Batch: 38, Loss: 1.4293507319962373e-06\n",
      "Iteration 27, Batch: 39, Loss: 8.907704227567592e-07\n",
      "Iteration 27, Batch: 40, Loss: 4.252791256931232e-07\n",
      "Iteration 27, Batch: 41, Loss: 4.4333972937238286e-07\n",
      "Iteration 27, Batch: 42, Loss: 8.810964686745137e-07\n",
      "Iteration 27, Batch: 43, Loss: 8.817522711979109e-07\n",
      "Iteration 27, Batch: 44, Loss: 8.119516223814571e-07\n",
      "Iteration 27, Batch: 45, Loss: 6.214845029717253e-07\n",
      "Iteration 27, Batch: 46, Loss: 4.770247414853657e-07\n",
      "Iteration 27, Batch: 47, Loss: 3.488275979179889e-07\n",
      "Iteration 27, Batch: 48, Loss: 3.26637746184133e-07\n",
      "Iteration 27, Batch: 49, Loss: 7.854872592361062e-07\n",
      "Iteration 28, Batch: 0, Loss: 0.12021660804748535\n",
      "Iteration 28, Batch: 1, Loss: 0.1652567982673645\n",
      "Iteration 28, Batch: 2, Loss: 0.1305789202451706\n",
      "Iteration 28, Batch: 3, Loss: 0.1261758655309677\n",
      "Iteration 28, Batch: 4, Loss: 0.1492871791124344\n",
      "Iteration 28, Batch: 5, Loss: 0.16028982400894165\n",
      "Iteration 28, Batch: 6, Loss: 0.10088329017162323\n",
      "Iteration 28, Batch: 7, Loss: 0.1269618570804596\n",
      "Iteration 28, Batch: 8, Loss: 0.10295447707176208\n",
      "Iteration 28, Batch: 9, Loss: 0.14801880717277527\n",
      "Iteration 28, Batch: 10, Loss: 0.10627441108226776\n",
      "Iteration 28, Batch: 11, Loss: 0.12703697383403778\n",
      "Iteration 28, Batch: 12, Loss: 0.10003591328859329\n",
      "Iteration 28, Batch: 13, Loss: 0.10927196592092514\n",
      "Iteration 28, Batch: 14, Loss: 0.16400249302387238\n",
      "Iteration 28, Batch: 15, Loss: 0.1397794634103775\n",
      "Iteration 28, Batch: 16, Loss: 0.08274803310632706\n",
      "Iteration 28, Batch: 17, Loss: 0.13935160636901855\n",
      "Iteration 28, Batch: 18, Loss: 0.10929925739765167\n",
      "Iteration 28, Batch: 19, Loss: 0.10665073245763779\n",
      "Iteration 28, Batch: 20, Loss: 0.11135557293891907\n",
      "Iteration 28, Batch: 21, Loss: 0.10990721732378006\n",
      "Iteration 28, Batch: 22, Loss: 0.09502849727869034\n",
      "Iteration 28, Batch: 23, Loss: 0.10515115410089493\n",
      "Iteration 28, Batch: 24, Loss: 0.1262354701757431\n",
      "Iteration 28, Batch: 25, Loss: 0.09178335219621658\n",
      "Iteration 28, Batch: 26, Loss: 0.14648057520389557\n",
      "Iteration 28, Batch: 27, Loss: 0.11705183982849121\n",
      "Iteration 28, Batch: 28, Loss: 0.1246722936630249\n",
      "Iteration 28, Batch: 29, Loss: 0.14216145873069763\n",
      "Iteration 28, Batch: 30, Loss: 0.10063689947128296\n",
      "Iteration 28, Batch: 31, Loss: 0.09446315467357635\n",
      "Iteration 28, Batch: 32, Loss: 0.17177054286003113\n",
      "Iteration 28, Batch: 33, Loss: 0.10449011623859406\n",
      "Iteration 28, Batch: 34, Loss: 0.08661363273859024\n",
      "Iteration 28, Batch: 35, Loss: 0.09429637342691422\n",
      "Iteration 28, Batch: 36, Loss: 0.09774338454008102\n",
      "Iteration 28, Batch: 37, Loss: 0.15628665685653687\n",
      "Iteration 28, Batch: 38, Loss: 0.08923400938510895\n",
      "Iteration 28, Batch: 39, Loss: 0.11267101019620895\n",
      "Iteration 28, Batch: 40, Loss: 0.11500648409128189\n",
      "Iteration 28, Batch: 41, Loss: 0.09389444440603256\n",
      "Iteration 28, Batch: 42, Loss: 0.13701432943344116\n",
      "Iteration 28, Batch: 43, Loss: 0.10334949195384979\n",
      "Iteration 28, Batch: 44, Loss: 0.09138339757919312\n",
      "Iteration 28, Batch: 45, Loss: 0.11020927131175995\n",
      "Iteration 28, Batch: 46, Loss: 0.10966712981462479\n",
      "Iteration 28, Batch: 47, Loss: 0.13567133247852325\n",
      "Iteration 28, Batch: 48, Loss: 0.08430381864309311\n",
      "Iteration 28, Batch: 49, Loss: 0.10128911584615707\n",
      "Iteration 29, Batch: 0, Loss: 0.00036288684350438416\n",
      "Iteration 29, Batch: 1, Loss: 0.017769549041986465\n",
      "Iteration 29, Batch: 2, Loss: 0.004212193191051483\n",
      "Iteration 29, Batch: 3, Loss: 0.018659576773643494\n",
      "Iteration 29, Batch: 4, Loss: 0.0007045194506645203\n",
      "Iteration 29, Batch: 5, Loss: 0.004813345614820719\n",
      "Iteration 29, Batch: 6, Loss: 0.00018645562522578984\n",
      "Iteration 29, Batch: 7, Loss: 0.0001862639473984018\n",
      "Iteration 29, Batch: 8, Loss: 0.0015237057814374566\n",
      "Iteration 29, Batch: 9, Loss: 6.824168394814478e-06\n",
      "Iteration 29, Batch: 10, Loss: 0.002722092205658555\n",
      "Iteration 29, Batch: 11, Loss: 0.0003514146665111184\n",
      "Iteration 29, Batch: 12, Loss: 8.840401278575882e-05\n",
      "Iteration 29, Batch: 13, Loss: 0.021278882399201393\n",
      "Iteration 29, Batch: 14, Loss: 0.038312334567308426\n",
      "Iteration 29, Batch: 15, Loss: 0.015937790274620056\n",
      "Iteration 29, Batch: 16, Loss: 9.952846448868513e-05\n",
      "Iteration 29, Batch: 17, Loss: 4.694982635555789e-05\n",
      "Iteration 29, Batch: 18, Loss: 3.712259422172792e-05\n",
      "Iteration 29, Batch: 19, Loss: 0.003102385438978672\n",
      "Iteration 29, Batch: 20, Loss: 2.0523435523500666e-05\n",
      "Iteration 29, Batch: 21, Loss: 0.000569289899431169\n",
      "Iteration 29, Batch: 22, Loss: 3.517328514135443e-05\n",
      "Iteration 29, Batch: 23, Loss: 5.331718421075493e-05\n",
      "Iteration 29, Batch: 24, Loss: 3.183913213433698e-05\n",
      "Iteration 29, Batch: 25, Loss: 0.014215929433703423\n",
      "Iteration 29, Batch: 26, Loss: 0.004370471928268671\n",
      "Iteration 29, Batch: 27, Loss: 2.0083891286049038e-05\n",
      "Iteration 29, Batch: 28, Loss: 1.6258543837466277e-05\n",
      "Iteration 29, Batch: 29, Loss: 0.0016397179570049047\n",
      "Iteration 29, Batch: 30, Loss: 0.0007584306295029819\n",
      "Iteration 29, Batch: 31, Loss: 0.00036431688931770623\n",
      "Iteration 29, Batch: 32, Loss: 0.00030496306135319173\n",
      "Iteration 29, Batch: 33, Loss: 2.6750103643280454e-05\n",
      "Iteration 29, Batch: 34, Loss: 2.6282923499820754e-05\n",
      "Iteration 29, Batch: 35, Loss: 3.55633492290508e-05\n",
      "Iteration 29, Batch: 36, Loss: 3.9984362956602126e-05\n",
      "Iteration 29, Batch: 37, Loss: 4.780791641678661e-05\n",
      "Iteration 29, Batch: 38, Loss: 4.131973037146963e-05\n",
      "Iteration 29, Batch: 39, Loss: 3.739760722965002e-05\n",
      "Iteration 29, Batch: 40, Loss: 3.4016655263258144e-05\n",
      "Iteration 29, Batch: 41, Loss: 3.72978720406536e-05\n",
      "Iteration 29, Batch: 42, Loss: 3.5099419619655237e-05\n",
      "Iteration 29, Batch: 43, Loss: 3.4709777537500486e-05\n",
      "Iteration 29, Batch: 44, Loss: 3.2377134630223736e-05\n",
      "Iteration 29, Batch: 45, Loss: 2.808968747558538e-05\n",
      "Iteration 29, Batch: 46, Loss: 2.4874461814761162e-05\n",
      "Iteration 29, Batch: 47, Loss: 2.1545891286223195e-05\n",
      "Iteration 29, Batch: 48, Loss: 1.6973513993434608e-05\n",
      "Iteration 29, Batch: 49, Loss: 1.992273064388428e-05\n",
      "Iteration 30, Batch: 0, Loss: 0.11773261427879333\n",
      "Iteration 30, Batch: 1, Loss: 0.11788467317819595\n",
      "Iteration 30, Batch: 2, Loss: 0.1505187451839447\n",
      "Iteration 30, Batch: 3, Loss: 0.13124269247055054\n",
      "Iteration 30, Batch: 4, Loss: 0.10124222189188004\n",
      "Iteration 30, Batch: 5, Loss: 0.1535540521144867\n",
      "Iteration 30, Batch: 6, Loss: 0.11524484306573868\n",
      "Iteration 30, Batch: 7, Loss: 0.13107159733772278\n",
      "Iteration 30, Batch: 8, Loss: 0.12997642159461975\n",
      "Iteration 30, Batch: 9, Loss: 0.09655700623989105\n",
      "Iteration 30, Batch: 10, Loss: 0.10215495526790619\n",
      "Iteration 30, Batch: 11, Loss: 0.12832225859165192\n",
      "Iteration 30, Batch: 12, Loss: 0.10316259413957596\n",
      "Iteration 30, Batch: 13, Loss: 0.09624968469142914\n",
      "Iteration 30, Batch: 14, Loss: 0.1211598739027977\n",
      "Iteration 30, Batch: 15, Loss: 0.15995293855667114\n",
      "Iteration 30, Batch: 16, Loss: 0.08207781612873077\n",
      "Iteration 30, Batch: 17, Loss: 0.1380341500043869\n",
      "Iteration 30, Batch: 18, Loss: 0.10633166879415512\n",
      "Iteration 30, Batch: 19, Loss: 0.10990312695503235\n",
      "Iteration 30, Batch: 20, Loss: 0.07832290232181549\n",
      "Iteration 30, Batch: 21, Loss: 0.11236533522605896\n",
      "Iteration 30, Batch: 22, Loss: 0.0826302245259285\n",
      "Iteration 30, Batch: 23, Loss: 0.10886010527610779\n",
      "Iteration 30, Batch: 24, Loss: 0.08691016584634781\n",
      "Iteration 30, Batch: 25, Loss: 0.10004448890686035\n",
      "Iteration 30, Batch: 26, Loss: 0.10708516091108322\n",
      "Iteration 30, Batch: 27, Loss: 0.11352130770683289\n",
      "Iteration 30, Batch: 28, Loss: 0.11378142237663269\n",
      "Iteration 30, Batch: 29, Loss: 0.09424842894077301\n",
      "Iteration 30, Batch: 30, Loss: 0.11403003334999084\n",
      "Iteration 30, Batch: 31, Loss: 0.11449148505926132\n",
      "Iteration 30, Batch: 32, Loss: 0.13607071340084076\n",
      "Iteration 30, Batch: 33, Loss: 0.125716894865036\n",
      "Iteration 30, Batch: 34, Loss: 0.10298950225114822\n",
      "Iteration 30, Batch: 35, Loss: 0.08883447200059891\n",
      "Iteration 30, Batch: 36, Loss: 0.12383752316236496\n",
      "Iteration 30, Batch: 37, Loss: 0.171161949634552\n",
      "Iteration 30, Batch: 38, Loss: 0.10563131421804428\n",
      "Iteration 30, Batch: 39, Loss: 0.157126784324646\n",
      "Iteration 30, Batch: 40, Loss: 0.11870225518941879\n",
      "Iteration 30, Batch: 41, Loss: 0.10750743746757507\n",
      "Iteration 30, Batch: 42, Loss: 0.09003443270921707\n",
      "Iteration 30, Batch: 43, Loss: 0.10459727048873901\n",
      "Iteration 30, Batch: 44, Loss: 0.12196937948465347\n",
      "Iteration 30, Batch: 45, Loss: 0.14090591669082642\n",
      "Iteration 30, Batch: 46, Loss: 0.09949629753828049\n",
      "Iteration 30, Batch: 47, Loss: 0.07780859619379044\n",
      "Iteration 30, Batch: 48, Loss: 0.14134769141674042\n",
      "Iteration 30, Batch: 49, Loss: 0.12498161196708679\n",
      "Iteration 31, Batch: 0, Loss: 8.65588299348019e-05\n",
      "Iteration 31, Batch: 1, Loss: 3.7865040212636814e-05\n",
      "Iteration 31, Batch: 2, Loss: 1.3125875739206094e-05\n",
      "Iteration 31, Batch: 3, Loss: 2.8690717954305e-05\n",
      "Iteration 31, Batch: 4, Loss: 5.754219091613777e-05\n",
      "Iteration 31, Batch: 5, Loss: 6.271639722399414e-05\n",
      "Iteration 31, Batch: 6, Loss: 3.1376290280604735e-05\n",
      "Iteration 31, Batch: 7, Loss: 1.1302676284685731e-05\n",
      "Iteration 31, Batch: 8, Loss: 2.681672958715353e-05\n",
      "Iteration 31, Batch: 9, Loss: 4.478551272768527e-05\n",
      "Iteration 31, Batch: 10, Loss: 3.5405581002123654e-05\n",
      "Iteration 31, Batch: 11, Loss: 1.3959167517896276e-05\n",
      "Iteration 31, Batch: 12, Loss: 9.521648280497175e-06\n",
      "Iteration 31, Batch: 13, Loss: 1.7867398128146306e-05\n",
      "Iteration 31, Batch: 14, Loss: 2.5492272470728494e-05\n",
      "Iteration 31, Batch: 15, Loss: 1.8888737031375058e-05\n",
      "Iteration 31, Batch: 16, Loss: 8.292635357065592e-06\n",
      "Iteration 31, Batch: 17, Loss: 6.462797500716988e-06\n",
      "Iteration 31, Batch: 18, Loss: 9.81044377112994e-06\n",
      "Iteration 31, Batch: 19, Loss: 1.5575742509099655e-05\n",
      "Iteration 31, Batch: 20, Loss: 9.705304364615586e-06\n",
      "Iteration 31, Batch: 21, Loss: 5.10163545186515e-06\n",
      "Iteration 31, Batch: 22, Loss: 4.839545908907894e-06\n",
      "Iteration 31, Batch: 23, Loss: 8.29803138913121e-06\n",
      "Iteration 31, Batch: 24, Loss: 1.1000039194186684e-05\n",
      "Iteration 31, Batch: 25, Loss: 8.00116868049372e-06\n",
      "Iteration 31, Batch: 26, Loss: 4.179657025815686e-06\n",
      "Iteration 31, Batch: 27, Loss: 3.1432525702257408e-06\n",
      "Iteration 31, Batch: 28, Loss: 6.260516784095671e-06\n",
      "Iteration 31, Batch: 29, Loss: 8.045572030823678e-06\n",
      "Iteration 31, Batch: 30, Loss: 4.713410362455761e-06\n",
      "Iteration 31, Batch: 31, Loss: 2.875468226193334e-06\n",
      "Iteration 31, Batch: 32, Loss: 3.864591690216912e-06\n",
      "Iteration 31, Batch: 33, Loss: 4.321846972743515e-06\n",
      "Iteration 31, Batch: 34, Loss: 4.239317604515236e-06\n",
      "Iteration 31, Batch: 35, Loss: 3.1973524983186508e-06\n",
      "Iteration 31, Batch: 36, Loss: 2.735647058216273e-06\n",
      "Iteration 31, Batch: 37, Loss: 3.4053657600452425e-06\n",
      "Iteration 31, Batch: 38, Loss: 3.958208253607154e-06\n",
      "Iteration 31, Batch: 39, Loss: 4.578304469760042e-06\n",
      "Iteration 31, Batch: 40, Loss: 3.0018011329957517e-06\n",
      "Iteration 31, Batch: 41, Loss: 2.8898696200485574e-06\n",
      "Iteration 31, Batch: 42, Loss: 2.7193989353690995e-06\n",
      "Iteration 31, Batch: 43, Loss: 3.1143270007305546e-06\n",
      "Iteration 31, Batch: 44, Loss: 0.005002642050385475\n",
      "Iteration 31, Batch: 45, Loss: 2.2315500700642588e-06\n",
      "Iteration 31, Batch: 46, Loss: 0.005002680234611034\n",
      "Iteration 31, Batch: 47, Loss: 3.2939360607997514e-06\n",
      "Iteration 31, Batch: 48, Loss: 2.8259557893761666e-06\n",
      "Iteration 31, Batch: 49, Loss: 2.3950635750225047e-06\n",
      "Iteration 32, Batch: 0, Loss: 0.11552705615758896\n",
      "Iteration 32, Batch: 1, Loss: 0.14747129380702972\n",
      "Iteration 32, Batch: 2, Loss: 0.08360379934310913\n",
      "Iteration 32, Batch: 3, Loss: 0.13939546048641205\n",
      "Iteration 32, Batch: 4, Loss: 0.09794221818447113\n",
      "Iteration 32, Batch: 5, Loss: 0.13662903010845184\n",
      "Iteration 32, Batch: 6, Loss: 0.13506466150283813\n",
      "Iteration 32, Batch: 7, Loss: 0.10443150997161865\n",
      "Iteration 32, Batch: 8, Loss: 0.1339721530675888\n",
      "Iteration 32, Batch: 9, Loss: 0.1196916475892067\n",
      "Iteration 32, Batch: 10, Loss: 0.15017305314540863\n",
      "Iteration 32, Batch: 11, Loss: 0.09096194058656693\n",
      "Iteration 32, Batch: 12, Loss: 0.11025304347276688\n",
      "Iteration 32, Batch: 13, Loss: 0.14518627524375916\n",
      "Iteration 32, Batch: 14, Loss: 0.11370976269245148\n",
      "Iteration 32, Batch: 15, Loss: 0.09303581714630127\n",
      "Iteration 32, Batch: 16, Loss: 0.10466937720775604\n",
      "Iteration 32, Batch: 17, Loss: 0.07686017453670502\n",
      "Iteration 32, Batch: 18, Loss: 0.08820034563541412\n",
      "Iteration 32, Batch: 19, Loss: 0.11100734770298004\n",
      "Iteration 32, Batch: 20, Loss: 0.09505091607570648\n",
      "Iteration 32, Batch: 21, Loss: 0.09895110875368118\n",
      "Iteration 32, Batch: 22, Loss: 0.13958704471588135\n",
      "Iteration 32, Batch: 23, Loss: 0.10991735011339188\n",
      "Iteration 32, Batch: 24, Loss: 0.120573028922081\n",
      "Iteration 32, Batch: 25, Loss: 0.14157141745090485\n",
      "Iteration 32, Batch: 26, Loss: 0.11344697326421738\n",
      "Iteration 32, Batch: 27, Loss: 0.11828681081533432\n",
      "Iteration 32, Batch: 28, Loss: 0.11372093856334686\n",
      "Iteration 32, Batch: 29, Loss: 0.09234114736318588\n",
      "Iteration 32, Batch: 30, Loss: 0.09776415675878525\n",
      "Iteration 32, Batch: 31, Loss: 0.11142553389072418\n",
      "Iteration 32, Batch: 32, Loss: 0.10275401175022125\n",
      "Iteration 32, Batch: 33, Loss: 0.12162289023399353\n",
      "Iteration 32, Batch: 34, Loss: 0.13492153584957123\n",
      "Iteration 32, Batch: 35, Loss: 0.13118049502372742\n",
      "Iteration 32, Batch: 36, Loss: 0.12767283618450165\n",
      "Iteration 32, Batch: 37, Loss: 0.10039994865655899\n",
      "Iteration 32, Batch: 38, Loss: 0.1255081593990326\n",
      "Iteration 32, Batch: 39, Loss: 0.13781112432479858\n",
      "Iteration 32, Batch: 40, Loss: 0.0947132259607315\n",
      "Iteration 32, Batch: 41, Loss: 0.10538013279438019\n",
      "Iteration 32, Batch: 42, Loss: 0.09885407984256744\n",
      "Iteration 32, Batch: 43, Loss: 0.13338574767112732\n",
      "Iteration 32, Batch: 44, Loss: 0.1867688000202179\n",
      "Iteration 32, Batch: 45, Loss: 0.11546006798744202\n",
      "Iteration 32, Batch: 46, Loss: 0.12858735024929047\n",
      "Iteration 32, Batch: 47, Loss: 0.13175474107265472\n",
      "Iteration 32, Batch: 48, Loss: 0.09350047260522842\n",
      "Iteration 32, Batch: 49, Loss: 0.11243817955255508\n",
      "Iteration 33, Batch: 0, Loss: 0.00016458035679534078\n",
      "Iteration 33, Batch: 1, Loss: 9.71764384303242e-05\n",
      "Iteration 33, Batch: 2, Loss: 0.000263312307652086\n",
      "Iteration 33, Batch: 3, Loss: 0.0014617001870647073\n",
      "Iteration 33, Batch: 4, Loss: 0.005072186700999737\n",
      "Iteration 33, Batch: 5, Loss: 0.0012389904586598277\n",
      "Iteration 33, Batch: 6, Loss: 0.002407470950856805\n",
      "Iteration 33, Batch: 7, Loss: 0.001268467167392373\n",
      "Iteration 33, Batch: 8, Loss: 2.030873110925313e-05\n",
      "Iteration 33, Batch: 9, Loss: 3.0479384804493748e-05\n",
      "Iteration 33, Batch: 10, Loss: 6.774104986106977e-05\n",
      "Iteration 33, Batch: 11, Loss: 8.753418660489842e-05\n",
      "Iteration 33, Batch: 12, Loss: 4.7796016588108614e-05\n",
      "Iteration 33, Batch: 13, Loss: 1.8383949281997047e-05\n",
      "Iteration 33, Batch: 14, Loss: 3.9999289583647624e-05\n",
      "Iteration 33, Batch: 15, Loss: 6.095521530369297e-05\n",
      "Iteration 33, Batch: 16, Loss: 6.994299474172294e-05\n",
      "Iteration 33, Batch: 17, Loss: 2.8297718017711304e-05\n",
      "Iteration 33, Batch: 18, Loss: 1.93938485608669e-05\n",
      "Iteration 33, Batch: 19, Loss: 4.0120099583873525e-05\n",
      "Iteration 33, Batch: 20, Loss: 4.5632030378328636e-05\n",
      "Iteration 33, Batch: 21, Loss: 3.162096254527569e-05\n",
      "Iteration 33, Batch: 22, Loss: 1.536970739834942e-05\n",
      "Iteration 33, Batch: 23, Loss: 1.147245529864449e-05\n",
      "Iteration 33, Batch: 24, Loss: 1.951083140738774e-05\n",
      "Iteration 33, Batch: 25, Loss: 2.0731231415993534e-05\n",
      "Iteration 33, Batch: 26, Loss: 1.5310335584217682e-05\n",
      "Iteration 33, Batch: 27, Loss: 7.754000762361102e-06\n",
      "Iteration 33, Batch: 28, Loss: 7.220230145321693e-06\n",
      "Iteration 33, Batch: 29, Loss: 1.0972516065521631e-05\n",
      "Iteration 33, Batch: 30, Loss: 1.504389274487039e-05\n",
      "Iteration 33, Batch: 31, Loss: 6.539292371599004e-06\n",
      "Iteration 33, Batch: 32, Loss: 3.6980866298108594e-06\n",
      "Iteration 33, Batch: 33, Loss: 4.818984052690212e-06\n",
      "Iteration 33, Batch: 34, Loss: 7.595328952447744e-06\n",
      "Iteration 33, Batch: 35, Loss: 6.961262442928273e-06\n",
      "Iteration 33, Batch: 36, Loss: 6.747753104718868e-06\n",
      "Iteration 33, Batch: 37, Loss: 1.8913415260612965e-06\n",
      "Iteration 33, Batch: 38, Loss: 6.475916507042712e-06\n",
      "Iteration 33, Batch: 39, Loss: 6.944389042473631e-06\n",
      "Iteration 33, Batch: 40, Loss: 6.793611646571662e-06\n",
      "Iteration 33, Batch: 41, Loss: 2.095664285661769e-06\n",
      "Iteration 33, Batch: 42, Loss: 1.1856302535306895e-06\n",
      "Iteration 33, Batch: 43, Loss: 2.378146291448502e-06\n",
      "Iteration 33, Batch: 44, Loss: 3.2248051411443157e-06\n",
      "Iteration 33, Batch: 45, Loss: 3.962610207963735e-06\n",
      "Iteration 33, Batch: 46, Loss: 1.4678851130156545e-06\n",
      "Iteration 33, Batch: 47, Loss: 2.105099156324286e-06\n",
      "Iteration 33, Batch: 48, Loss: 2.019071189351962e-06\n",
      "Iteration 33, Batch: 49, Loss: 1.6503274764545495e-06\n",
      "Iteration 34, Batch: 0, Loss: 0.135872021317482\n",
      "Iteration 34, Batch: 1, Loss: 0.15696553885936737\n",
      "Iteration 34, Batch: 2, Loss: 0.11132100969552994\n",
      "Iteration 34, Batch: 3, Loss: 0.15608729422092438\n",
      "Iteration 34, Batch: 4, Loss: 0.14907726645469666\n",
      "Iteration 34, Batch: 5, Loss: 0.1357404738664627\n",
      "Iteration 34, Batch: 6, Loss: 0.17545849084854126\n",
      "Iteration 34, Batch: 7, Loss: 0.12614835798740387\n",
      "Iteration 34, Batch: 8, Loss: 0.14061212539672852\n",
      "Iteration 34, Batch: 9, Loss: 0.12515340745449066\n",
      "Iteration 34, Batch: 10, Loss: 0.11952626705169678\n",
      "Iteration 34, Batch: 11, Loss: 0.12847036123275757\n",
      "Iteration 34, Batch: 12, Loss: 0.10871970653533936\n",
      "Iteration 34, Batch: 13, Loss: 0.1312669813632965\n",
      "Iteration 34, Batch: 14, Loss: 0.16124312579631805\n",
      "Iteration 34, Batch: 15, Loss: 0.10486650466918945\n",
      "Iteration 34, Batch: 16, Loss: 0.10478643327951431\n",
      "Iteration 34, Batch: 17, Loss: 0.09310846775770187\n",
      "Iteration 34, Batch: 18, Loss: 0.13069649040699005\n",
      "Iteration 34, Batch: 19, Loss: 0.1133979931473732\n",
      "Iteration 34, Batch: 20, Loss: 0.1362542361021042\n",
      "Iteration 34, Batch: 21, Loss: 0.16937290132045746\n",
      "Iteration 34, Batch: 22, Loss: 0.07166467607021332\n",
      "Iteration 34, Batch: 23, Loss: 0.1028558537364006\n",
      "Iteration 34, Batch: 24, Loss: 0.11752471327781677\n",
      "Iteration 34, Batch: 25, Loss: 0.1351204812526703\n",
      "Iteration 34, Batch: 26, Loss: 0.14406666159629822\n",
      "Iteration 34, Batch: 27, Loss: 0.13755296170711517\n",
      "Iteration 34, Batch: 28, Loss: 0.08644277602434158\n",
      "Iteration 34, Batch: 29, Loss: 0.13022927939891815\n",
      "Iteration 34, Batch: 30, Loss: 0.11010845005512238\n",
      "Iteration 34, Batch: 31, Loss: 0.0933057889342308\n",
      "Iteration 34, Batch: 32, Loss: 0.09589999169111252\n",
      "Iteration 34, Batch: 33, Loss: 0.12011363357305527\n",
      "Iteration 34, Batch: 34, Loss: 0.13199222087860107\n",
      "Iteration 34, Batch: 35, Loss: 0.10915769636631012\n",
      "Iteration 34, Batch: 36, Loss: 0.12680315971374512\n",
      "Iteration 34, Batch: 37, Loss: 0.12162056565284729\n",
      "Iteration 34, Batch: 38, Loss: 0.12445942312479019\n",
      "Iteration 34, Batch: 39, Loss: 0.11693985015153885\n",
      "Iteration 34, Batch: 40, Loss: 0.15411309897899628\n",
      "Iteration 34, Batch: 41, Loss: 0.14887556433677673\n",
      "Iteration 34, Batch: 42, Loss: 0.12934473156929016\n",
      "Iteration 34, Batch: 43, Loss: 0.14530760049819946\n",
      "Iteration 34, Batch: 44, Loss: 0.11061631888151169\n",
      "Iteration 34, Batch: 45, Loss: 0.12153352797031403\n",
      "Iteration 34, Batch: 46, Loss: 0.139776811003685\n",
      "Iteration 34, Batch: 47, Loss: 0.11167901754379272\n",
      "Iteration 34, Batch: 48, Loss: 0.1062844842672348\n",
      "Iteration 34, Batch: 49, Loss: 0.0926675945520401\n",
      "Iteration 35, Batch: 0, Loss: 5.546504326048307e-05\n",
      "Iteration 35, Batch: 1, Loss: 6.330232281470671e-05\n",
      "Iteration 35, Batch: 2, Loss: 2.1203024516580626e-05\n",
      "Iteration 35, Batch: 3, Loss: 6.31984357823967e-06\n",
      "Iteration 35, Batch: 4, Loss: 3.059140726691112e-05\n",
      "Iteration 35, Batch: 5, Loss: 4.2360799852758646e-05\n",
      "Iteration 35, Batch: 6, Loss: 2.4553346520406194e-05\n",
      "Iteration 35, Batch: 7, Loss: 4.922481366520515e-06\n",
      "Iteration 35, Batch: 8, Loss: 3.6542553516483167e-06\n",
      "Iteration 35, Batch: 9, Loss: 1.8290671505383216e-05\n",
      "Iteration 35, Batch: 10, Loss: 2.304072040715255e-05\n",
      "Iteration 35, Batch: 11, Loss: 1.4918337001290638e-05\n",
      "Iteration 35, Batch: 12, Loss: 0.0021350355818867683\n",
      "Iteration 35, Batch: 13, Loss: 4.482741587708006e-06\n",
      "Iteration 35, Batch: 14, Loss: 1.199264261231292e-05\n",
      "Iteration 35, Batch: 15, Loss: 1.2724893167614937e-05\n",
      "Iteration 35, Batch: 16, Loss: 4.958263616572367e-06\n",
      "Iteration 35, Batch: 17, Loss: 8.490898153468152e-07\n",
      "Iteration 35, Batch: 18, Loss: 5.099113423057133e-06\n",
      "Iteration 35, Batch: 19, Loss: 9.885623512673192e-06\n",
      "Iteration 35, Batch: 20, Loss: 8.393915777560323e-06\n",
      "Iteration 35, Batch: 21, Loss: 2.461889380356297e-06\n",
      "Iteration 35, Batch: 22, Loss: 1.6131273241626332e-06\n",
      "Iteration 35, Batch: 23, Loss: 5.795294327981537e-06\n",
      "Iteration 35, Batch: 24, Loss: 6.227983249118552e-06\n",
      "Iteration 35, Batch: 25, Loss: 3.7680963487218833e-06\n",
      "Iteration 35, Batch: 26, Loss: 1.2529201285360614e-06\n",
      "Iteration 35, Batch: 27, Loss: 2.3287354906642577e-06\n",
      "Iteration 35, Batch: 28, Loss: 7.714680577919353e-06\n",
      "Iteration 35, Batch: 29, Loss: 3.826846750598634e-06\n",
      "Iteration 35, Batch: 30, Loss: 2.0563247744576074e-06\n",
      "Iteration 35, Batch: 31, Loss: 2.510694230295485e-06\n",
      "Iteration 35, Batch: 32, Loss: 2.7181579298485303e-06\n",
      "Iteration 35, Batch: 33, Loss: 3.494251814117888e-06\n",
      "Iteration 35, Batch: 34, Loss: 3.5912735256715678e-06\n",
      "Iteration 35, Batch: 35, Loss: 2.4030221084103687e-06\n",
      "Iteration 35, Batch: 36, Loss: 2.3219804461405147e-06\n",
      "Iteration 35, Batch: 37, Loss: 2.575774260549224e-06\n",
      "Iteration 35, Batch: 38, Loss: 4.713675480161328e-06\n",
      "Iteration 35, Batch: 39, Loss: 1.6116765664264676e-06\n",
      "Iteration 35, Batch: 40, Loss: 1.5371282415799215e-06\n",
      "Iteration 35, Batch: 41, Loss: 1.8916139197244775e-06\n",
      "Iteration 35, Batch: 42, Loss: 2.2662841274723178e-06\n",
      "Iteration 35, Batch: 43, Loss: 2.037967760770698e-06\n",
      "Iteration 35, Batch: 44, Loss: 1.9966646505054086e-06\n",
      "Iteration 35, Batch: 45, Loss: 2.1490529888978926e-06\n",
      "Iteration 35, Batch: 46, Loss: 1.9905851331714075e-06\n",
      "Iteration 35, Batch: 47, Loss: 1.9001777218363713e-06\n",
      "Iteration 35, Batch: 48, Loss: 1.6515504057679209e-06\n",
      "Iteration 35, Batch: 49, Loss: 8.517531000507006e-07\n",
      "Iteration 36, Batch: 0, Loss: 0.12541228532791138\n",
      "Iteration 36, Batch: 1, Loss: 0.1140272319316864\n",
      "Iteration 36, Batch: 2, Loss: 0.12732532620429993\n",
      "Iteration 36, Batch: 3, Loss: 0.1146128848195076\n",
      "Iteration 36, Batch: 4, Loss: 0.09854282438755035\n",
      "Iteration 36, Batch: 5, Loss: 0.11303462833166122\n",
      "Iteration 36, Batch: 6, Loss: 0.11268388479948044\n",
      "Iteration 36, Batch: 7, Loss: 0.16171996295452118\n",
      "Iteration 36, Batch: 8, Loss: 0.13594135642051697\n",
      "Iteration 36, Batch: 9, Loss: 0.11069807410240173\n",
      "Iteration 36, Batch: 10, Loss: 0.14460518956184387\n",
      "Iteration 36, Batch: 11, Loss: 0.0902053639292717\n",
      "Iteration 36, Batch: 12, Loss: 0.10836639255285263\n",
      "Iteration 36, Batch: 13, Loss: 0.1454959660768509\n",
      "Iteration 36, Batch: 14, Loss: 0.125693678855896\n",
      "Iteration 36, Batch: 15, Loss: 0.10898158699274063\n",
      "Iteration 36, Batch: 16, Loss: 0.1367182731628418\n",
      "Iteration 36, Batch: 17, Loss: 0.09753085672855377\n",
      "Iteration 36, Batch: 18, Loss: 0.09461869299411774\n",
      "Iteration 36, Batch: 19, Loss: 0.0914108082652092\n",
      "Iteration 36, Batch: 20, Loss: 0.13243265450000763\n",
      "Iteration 36, Batch: 21, Loss: 0.13243232667446136\n",
      "Iteration 36, Batch: 22, Loss: 0.11394505947828293\n",
      "Iteration 36, Batch: 23, Loss: 0.09831602871417999\n",
      "Iteration 36, Batch: 24, Loss: 0.11519059538841248\n",
      "Iteration 36, Batch: 25, Loss: 0.13217449188232422\n",
      "Iteration 36, Batch: 26, Loss: 0.10769810527563095\n",
      "Iteration 36, Batch: 27, Loss: 0.1351500153541565\n",
      "Iteration 36, Batch: 28, Loss: 0.08884239196777344\n",
      "Iteration 36, Batch: 29, Loss: 0.13083797693252563\n",
      "Iteration 36, Batch: 30, Loss: 0.15291298925876617\n",
      "Iteration 36, Batch: 31, Loss: 0.11327660083770752\n",
      "Iteration 36, Batch: 32, Loss: 0.15634414553642273\n",
      "Iteration 36, Batch: 33, Loss: 0.10420327633619308\n",
      "Iteration 36, Batch: 34, Loss: 0.14214137196540833\n",
      "Iteration 36, Batch: 35, Loss: 0.13038282096385956\n",
      "Iteration 36, Batch: 36, Loss: 0.10214973986148834\n",
      "Iteration 36, Batch: 37, Loss: 0.15549951791763306\n",
      "Iteration 36, Batch: 38, Loss: 0.1154303252696991\n",
      "Iteration 36, Batch: 39, Loss: 0.13984641432762146\n",
      "Iteration 36, Batch: 40, Loss: 0.11913920938968658\n",
      "Iteration 36, Batch: 41, Loss: 0.13087661564350128\n",
      "Iteration 36, Batch: 42, Loss: 0.12273769080638885\n",
      "Iteration 36, Batch: 43, Loss: 0.08272621780633926\n",
      "Iteration 36, Batch: 44, Loss: 0.08416025340557098\n",
      "Iteration 36, Batch: 45, Loss: 0.09689002484083176\n",
      "Iteration 36, Batch: 46, Loss: 0.12228286266326904\n",
      "Iteration 36, Batch: 47, Loss: 0.14292621612548828\n",
      "Iteration 36, Batch: 48, Loss: 0.14346136152744293\n",
      "Iteration 36, Batch: 49, Loss: 0.1229025274515152\n",
      "Iteration 37, Batch: 0, Loss: 0.12260618060827255\n",
      "Iteration 37, Batch: 1, Loss: 0.14953115582466125\n",
      "Iteration 37, Batch: 2, Loss: 0.35713475942611694\n",
      "Iteration 37, Batch: 3, Loss: 0.43857917189598083\n",
      "Iteration 37, Batch: 4, Loss: 0.4077214002609253\n",
      "Iteration 37, Batch: 5, Loss: 0.4145404100418091\n",
      "Iteration 37, Batch: 6, Loss: 0.35235390067100525\n",
      "Iteration 37, Batch: 7, Loss: 0.20698630809783936\n",
      "Iteration 37, Batch: 8, Loss: 0.07812227308750153\n",
      "Iteration 37, Batch: 9, Loss: 0.05631329119205475\n",
      "Iteration 37, Batch: 10, Loss: 0.041704196482896805\n",
      "Iteration 37, Batch: 11, Loss: 0.2600060999393463\n",
      "Iteration 37, Batch: 12, Loss: 0.04287084937095642\n",
      "Iteration 37, Batch: 13, Loss: 0.054008278995752335\n",
      "Iteration 37, Batch: 14, Loss: 0.10039692372083664\n",
      "Iteration 37, Batch: 15, Loss: 0.15743616223335266\n",
      "Iteration 37, Batch: 16, Loss: 0.061378881335258484\n",
      "Iteration 37, Batch: 17, Loss: 0.05785491690039635\n",
      "Iteration 37, Batch: 18, Loss: 0.019411243498325348\n",
      "Iteration 37, Batch: 19, Loss: 0.015670515596866608\n",
      "Iteration 37, Batch: 20, Loss: 0.008350669406354427\n",
      "Iteration 37, Batch: 21, Loss: 0.011537675745785236\n",
      "Iteration 37, Batch: 22, Loss: 0.005753287114202976\n",
      "Iteration 37, Batch: 23, Loss: 0.0041776676662266254\n",
      "Iteration 37, Batch: 24, Loss: 0.004067802801728249\n",
      "Iteration 37, Batch: 25, Loss: 0.0040471479296684265\n",
      "Iteration 37, Batch: 26, Loss: 0.00605354318395257\n",
      "Iteration 37, Batch: 27, Loss: 0.0011797490296885371\n",
      "Iteration 37, Batch: 28, Loss: 0.11465193331241608\n",
      "Iteration 37, Batch: 29, Loss: 0.007812654599547386\n",
      "Iteration 37, Batch: 30, Loss: 0.002355398843064904\n",
      "Iteration 37, Batch: 31, Loss: 0.0004392921691760421\n",
      "Iteration 37, Batch: 32, Loss: 0.00023208234051708132\n",
      "Iteration 37, Batch: 33, Loss: 8.536241512047127e-05\n",
      "Iteration 37, Batch: 34, Loss: 0.00502120703458786\n",
      "Iteration 37, Batch: 35, Loss: 0.0002919896214734763\n",
      "Iteration 37, Batch: 36, Loss: 0.0002708247338887304\n",
      "Iteration 37, Batch: 37, Loss: 0.00010761435987660661\n",
      "Iteration 37, Batch: 38, Loss: 0.000484166870592162\n",
      "Iteration 37, Batch: 39, Loss: 0.0003358064277563244\n",
      "Iteration 37, Batch: 40, Loss: 0.0001367733784718439\n",
      "Iteration 37, Batch: 41, Loss: 5.937638707109727e-05\n",
      "Iteration 37, Batch: 42, Loss: 8.200639422284439e-05\n",
      "Iteration 37, Batch: 43, Loss: 0.00013923158985562623\n",
      "Iteration 37, Batch: 44, Loss: 0.00013601718819700181\n",
      "Iteration 37, Batch: 45, Loss: 9.151652920991182e-05\n",
      "Iteration 37, Batch: 46, Loss: 8.961441926658154e-05\n",
      "Iteration 37, Batch: 47, Loss: 0.0001110703669837676\n",
      "Iteration 37, Batch: 48, Loss: 0.00011599303252296522\n",
      "Iteration 37, Batch: 49, Loss: 6.891319208079949e-05\n",
      "Iteration 38, Batch: 0, Loss: 0.2572551667690277\n",
      "Iteration 38, Batch: 1, Loss: 0.2542870342731476\n",
      "Iteration 38, Batch: 2, Loss: 0.24500581622123718\n",
      "Iteration 38, Batch: 3, Loss: 0.22420069575309753\n",
      "Iteration 38, Batch: 4, Loss: 0.18114440143108368\n",
      "Iteration 38, Batch: 5, Loss: 0.20989990234375\n",
      "Iteration 38, Batch: 6, Loss: 0.17457717657089233\n",
      "Iteration 38, Batch: 7, Loss: 0.19721899926662445\n",
      "Iteration 38, Batch: 8, Loss: 0.19592967629432678\n",
      "Iteration 38, Batch: 9, Loss: 0.15710248053073883\n",
      "Iteration 38, Batch: 10, Loss: 0.18663658201694489\n",
      "Iteration 38, Batch: 11, Loss: 0.15555976331233978\n",
      "Iteration 38, Batch: 12, Loss: 0.21159537136554718\n",
      "Iteration 38, Batch: 13, Loss: 0.1727377325296402\n",
      "Iteration 38, Batch: 14, Loss: 0.15903741121292114\n",
      "Iteration 38, Batch: 15, Loss: 0.1671082079410553\n",
      "Iteration 38, Batch: 16, Loss: 0.1905699521303177\n",
      "Iteration 38, Batch: 17, Loss: 0.1428399235010147\n",
      "Iteration 38, Batch: 18, Loss: 0.1590920090675354\n",
      "Iteration 38, Batch: 19, Loss: 0.153811514377594\n",
      "Iteration 38, Batch: 20, Loss: 0.1145385131239891\n",
      "Iteration 38, Batch: 21, Loss: 0.13346132636070251\n",
      "Iteration 38, Batch: 22, Loss: 0.1345445066690445\n",
      "Iteration 38, Batch: 23, Loss: 0.10836037993431091\n",
      "Iteration 38, Batch: 24, Loss: 0.10486454516649246\n",
      "Iteration 38, Batch: 25, Loss: 0.12233667075634003\n",
      "Iteration 38, Batch: 26, Loss: 0.13467933237552643\n",
      "Iteration 38, Batch: 27, Loss: 0.1336510181427002\n",
      "Iteration 38, Batch: 28, Loss: 0.1658986508846283\n",
      "Iteration 38, Batch: 29, Loss: 0.1586877554655075\n",
      "Iteration 38, Batch: 30, Loss: 0.16577167809009552\n",
      "Iteration 38, Batch: 31, Loss: 0.14117431640625\n",
      "Iteration 38, Batch: 32, Loss: 0.09381284564733505\n",
      "Iteration 38, Batch: 33, Loss: 0.12210869789123535\n",
      "Iteration 38, Batch: 34, Loss: 0.1238870620727539\n",
      "Iteration 38, Batch: 35, Loss: 0.10738685727119446\n",
      "Iteration 38, Batch: 36, Loss: 0.1169276237487793\n",
      "Iteration 38, Batch: 37, Loss: 0.12057669460773468\n",
      "Iteration 38, Batch: 38, Loss: 0.13683220744132996\n",
      "Iteration 38, Batch: 39, Loss: 0.13760757446289062\n",
      "Iteration 38, Batch: 40, Loss: 0.12052226811647415\n",
      "Iteration 38, Batch: 41, Loss: 0.14696790277957916\n",
      "Iteration 38, Batch: 42, Loss: 0.13984814286231995\n",
      "Iteration 38, Batch: 43, Loss: 0.10917408019304276\n",
      "Iteration 38, Batch: 44, Loss: 0.12240584939718246\n",
      "Iteration 38, Batch: 45, Loss: 0.14060179889202118\n",
      "Iteration 38, Batch: 46, Loss: 0.13063879311084747\n",
      "Iteration 38, Batch: 47, Loss: 0.14809809625148773\n",
      "Iteration 38, Batch: 48, Loss: 0.12001635879278183\n",
      "Iteration 38, Batch: 49, Loss: 0.1462351232767105\n",
      "Iteration 39, Batch: 0, Loss: 0.15360726416110992\n",
      "Iteration 39, Batch: 1, Loss: 0.2543928623199463\n",
      "Iteration 39, Batch: 2, Loss: 0.16632571816444397\n",
      "Iteration 39, Batch: 3, Loss: 0.10191968828439713\n",
      "Iteration 39, Batch: 4, Loss: 0.0362955741584301\n",
      "Iteration 39, Batch: 5, Loss: 0.07731249928474426\n",
      "Iteration 39, Batch: 6, Loss: 0.05847194418311119\n",
      "Iteration 39, Batch: 7, Loss: 0.08464455604553223\n",
      "Iteration 39, Batch: 8, Loss: 0.049209002405405045\n",
      "Iteration 39, Batch: 9, Loss: 0.004694616422057152\n",
      "Iteration 39, Batch: 10, Loss: 0.0033566057682037354\n",
      "Iteration 39, Batch: 11, Loss: 0.001003348850645125\n",
      "Iteration 39, Batch: 12, Loss: 0.00028772748191840947\n",
      "Iteration 39, Batch: 13, Loss: 7.510782597819343e-05\n",
      "Iteration 39, Batch: 14, Loss: 0.00011315110168652609\n",
      "Iteration 39, Batch: 15, Loss: 0.0052123176865279675\n",
      "Iteration 39, Batch: 16, Loss: 0.00027092936215922236\n",
      "Iteration 39, Batch: 17, Loss: 0.0031557423062622547\n",
      "Iteration 39, Batch: 18, Loss: 0.00026668247301131487\n",
      "Iteration 39, Batch: 19, Loss: 0.0002702397177927196\n",
      "Iteration 39, Batch: 20, Loss: 0.0003309627063572407\n",
      "Iteration 39, Batch: 21, Loss: 0.0003348356403876096\n",
      "Iteration 39, Batch: 22, Loss: 0.0003475210687611252\n",
      "Iteration 39, Batch: 23, Loss: 0.0003681770758703351\n",
      "Iteration 39, Batch: 24, Loss: 0.0003351166669744998\n",
      "Iteration 39, Batch: 25, Loss: 0.00023223400057759136\n",
      "Iteration 39, Batch: 26, Loss: 0.00014380511129274964\n",
      "Iteration 39, Batch: 27, Loss: 3.9014230424072593e-05\n",
      "Iteration 39, Batch: 28, Loss: 3.586831735447049e-05\n",
      "Iteration 39, Batch: 29, Loss: 7.75444641476497e-05\n",
      "Iteration 39, Batch: 30, Loss: 9.73213100223802e-05\n",
      "Iteration 39, Batch: 31, Loss: 0.00010759052383946255\n",
      "Iteration 39, Batch: 32, Loss: 8.164543396560475e-05\n",
      "Iteration 39, Batch: 33, Loss: 3.089485107921064e-05\n",
      "Iteration 39, Batch: 34, Loss: 2.589301766420249e-05\n",
      "Iteration 39, Batch: 35, Loss: 5.3428157116286457e-05\n",
      "Iteration 39, Batch: 36, Loss: 4.692246511694975e-05\n",
      "Iteration 39, Batch: 37, Loss: 6.246118573471904e-05\n",
      "Iteration 39, Batch: 38, Loss: 4.936342520522885e-05\n",
      "Iteration 39, Batch: 39, Loss: 4.407326559885405e-05\n",
      "Iteration 39, Batch: 40, Loss: 3.1529038096778095e-05\n",
      "Iteration 39, Batch: 41, Loss: 2.7305739422445185e-05\n",
      "Iteration 39, Batch: 42, Loss: 2.097017022606451e-05\n",
      "Iteration 39, Batch: 43, Loss: 3.749253664864227e-05\n",
      "Iteration 39, Batch: 44, Loss: 3.6942910810466856e-05\n",
      "Iteration 39, Batch: 45, Loss: 4.839743269258179e-05\n",
      "Iteration 39, Batch: 46, Loss: 4.621038169716485e-05\n",
      "Iteration 39, Batch: 47, Loss: 2.5575591280357912e-05\n",
      "Iteration 39, Batch: 48, Loss: 2.49081331276102e-05\n",
      "Iteration 39, Batch: 49, Loss: 3.25232686009258e-05\n",
      "Iteration 40, Batch: 0, Loss: 0.19138243794441223\n",
      "Iteration 40, Batch: 1, Loss: 0.1745825707912445\n",
      "Iteration 40, Batch: 2, Loss: 0.18028174340724945\n",
      "Iteration 40, Batch: 3, Loss: 0.17213250696659088\n",
      "Iteration 40, Batch: 4, Loss: 0.13891802728176117\n",
      "Iteration 40, Batch: 5, Loss: 0.12790751457214355\n",
      "Iteration 40, Batch: 6, Loss: 0.1040550023317337\n",
      "Iteration 40, Batch: 7, Loss: 0.10802062600851059\n",
      "Iteration 40, Batch: 8, Loss: 0.15435895323753357\n",
      "Iteration 40, Batch: 9, Loss: 0.11860582232475281\n",
      "Iteration 40, Batch: 10, Loss: 0.15866965055465698\n",
      "Iteration 40, Batch: 11, Loss: 0.15324042737483978\n",
      "Iteration 40, Batch: 12, Loss: 0.1381566822528839\n",
      "Iteration 40, Batch: 13, Loss: 0.12067630141973495\n",
      "Iteration 40, Batch: 14, Loss: 0.12985347211360931\n",
      "Iteration 40, Batch: 15, Loss: 0.14568141102790833\n",
      "Iteration 40, Batch: 16, Loss: 0.16918987035751343\n",
      "Iteration 40, Batch: 17, Loss: 0.14522166550159454\n",
      "Iteration 40, Batch: 18, Loss: 0.08276097476482391\n",
      "Iteration 40, Batch: 19, Loss: 0.12439533323049545\n",
      "Iteration 40, Batch: 20, Loss: 0.11384760588407516\n",
      "Iteration 40, Batch: 21, Loss: 0.10969030112028122\n",
      "Iteration 40, Batch: 22, Loss: 0.09221860021352768\n",
      "Iteration 40, Batch: 23, Loss: 0.09444750845432281\n",
      "Iteration 40, Batch: 24, Loss: 0.20498980581760406\n",
      "Iteration 40, Batch: 25, Loss: 0.12572039663791656\n",
      "Iteration 40, Batch: 26, Loss: 0.15855978429317474\n",
      "Iteration 40, Batch: 27, Loss: 0.10312388837337494\n",
      "Iteration 40, Batch: 28, Loss: 0.09194920212030411\n",
      "Iteration 40, Batch: 29, Loss: 0.12632417678833008\n",
      "Iteration 40, Batch: 30, Loss: 0.14807359874248505\n",
      "Iteration 40, Batch: 31, Loss: 0.09709657728672028\n",
      "Iteration 40, Batch: 32, Loss: 0.06025904044508934\n",
      "Iteration 40, Batch: 33, Loss: 0.11444427073001862\n",
      "Iteration 40, Batch: 34, Loss: 0.11388494074344635\n",
      "Iteration 40, Batch: 35, Loss: 0.10329492390155792\n",
      "Iteration 40, Batch: 36, Loss: 0.12931646406650543\n",
      "Iteration 40, Batch: 37, Loss: 0.1159210205078125\n",
      "Iteration 40, Batch: 38, Loss: 0.10773307830095291\n",
      "Iteration 40, Batch: 39, Loss: 0.11071491986513138\n",
      "Iteration 40, Batch: 40, Loss: 0.14252202212810516\n",
      "Iteration 40, Batch: 41, Loss: 0.14006711542606354\n",
      "Iteration 40, Batch: 42, Loss: 0.08395152539014816\n",
      "Iteration 40, Batch: 43, Loss: 0.0869094654917717\n",
      "Iteration 40, Batch: 44, Loss: 0.10022523999214172\n",
      "Iteration 40, Batch: 45, Loss: 0.1287538856267929\n",
      "Iteration 40, Batch: 46, Loss: 0.11514588445425034\n",
      "Iteration 40, Batch: 47, Loss: 0.10976222902536392\n",
      "Iteration 40, Batch: 48, Loss: 0.05679633095860481\n",
      "Iteration 40, Batch: 49, Loss: 0.06902992725372314\n",
      "Iteration 41, Batch: 0, Loss: 0.0003003579331561923\n",
      "Iteration 41, Batch: 1, Loss: 0.00013575219782069325\n",
      "Iteration 41, Batch: 2, Loss: 1.838878233684227e-05\n",
      "Iteration 41, Batch: 3, Loss: 5.9970872825942934e-05\n",
      "Iteration 41, Batch: 4, Loss: 0.0045903343707323074\n",
      "Iteration 41, Batch: 5, Loss: 0.00033505840110592544\n",
      "Iteration 41, Batch: 6, Loss: 0.00025793875101953745\n",
      "Iteration 41, Batch: 7, Loss: 9.069917723536491e-05\n",
      "Iteration 41, Batch: 8, Loss: 3.507687142700888e-05\n",
      "Iteration 41, Batch: 9, Loss: 0.005067011807113886\n",
      "Iteration 41, Batch: 10, Loss: 0.00018647886463440955\n",
      "Iteration 41, Batch: 11, Loss: 0.00017264224879909307\n",
      "Iteration 41, Batch: 12, Loss: 7.772834214847535e-05\n",
      "Iteration 41, Batch: 13, Loss: 2.1826414013048634e-05\n",
      "Iteration 41, Batch: 14, Loss: 4.860680928686634e-05\n",
      "Iteration 41, Batch: 15, Loss: 0.000106186191260349\n",
      "Iteration 41, Batch: 16, Loss: 9.746223804540932e-05\n",
      "Iteration 41, Batch: 17, Loss: 5.300975317368284e-05\n",
      "Iteration 41, Batch: 18, Loss: 1.3265718735055998e-05\n",
      "Iteration 41, Batch: 19, Loss: 1.754454569891095e-05\n",
      "Iteration 41, Batch: 20, Loss: 5.1508126489352435e-05\n",
      "Iteration 41, Batch: 21, Loss: 5.304867590893991e-05\n",
      "Iteration 41, Batch: 22, Loss: 3.562971323844977e-05\n",
      "Iteration 41, Batch: 23, Loss: 6.159275926620467e-06\n",
      "Iteration 41, Batch: 24, Loss: 9.555492397339549e-06\n",
      "Iteration 41, Batch: 25, Loss: 2.797775414364878e-05\n",
      "Iteration 41, Batch: 26, Loss: 3.590495543903671e-05\n",
      "Iteration 41, Batch: 27, Loss: 1.7130381820607e-05\n",
      "Iteration 41, Batch: 28, Loss: 4.319759682402946e-06\n",
      "Iteration 41, Batch: 29, Loss: 5.452420737128705e-06\n",
      "Iteration 41, Batch: 30, Loss: 1.3934406524640508e-05\n",
      "Iteration 41, Batch: 31, Loss: 1.7922970073414035e-05\n",
      "Iteration 41, Batch: 32, Loss: 1.5044919564388692e-05\n",
      "Iteration 41, Batch: 33, Loss: 4.445412287168438e-06\n",
      "Iteration 41, Batch: 34, Loss: 3.0455601063295035e-06\n",
      "Iteration 41, Batch: 35, Loss: 0.0006626814720220864\n",
      "Iteration 41, Batch: 36, Loss: 1.6083977243397385e-05\n",
      "Iteration 41, Batch: 37, Loss: 1.4323461073217914e-05\n",
      "Iteration 41, Batch: 38, Loss: 6.525200660689734e-06\n",
      "Iteration 41, Batch: 39, Loss: 2.1272835510899313e-06\n",
      "Iteration 41, Batch: 40, Loss: 6.1131754591770004e-06\n",
      "Iteration 41, Batch: 41, Loss: 1.047057503456017e-05\n",
      "Iteration 41, Batch: 42, Loss: 9.526140274829231e-06\n",
      "Iteration 41, Batch: 43, Loss: 5.766180947830435e-06\n",
      "Iteration 41, Batch: 44, Loss: 1.2013958894385723e-06\n",
      "Iteration 41, Batch: 45, Loss: 3.819287940132199e-06\n",
      "Iteration 41, Batch: 46, Loss: 6.443721758842003e-06\n",
      "Iteration 41, Batch: 47, Loss: 6.182329798321007e-06\n",
      "Iteration 41, Batch: 48, Loss: 3.427235242270399e-06\n",
      "Iteration 41, Batch: 49, Loss: 1.8044086118607083e-06\n",
      "Iteration 42, Batch: 0, Loss: 0.1017545685172081\n",
      "Iteration 42, Batch: 1, Loss: 0.10125528275966644\n",
      "Iteration 42, Batch: 2, Loss: 0.11413228511810303\n",
      "Iteration 42, Batch: 3, Loss: 0.1282494068145752\n",
      "Iteration 42, Batch: 4, Loss: 0.10971670597791672\n",
      "Iteration 42, Batch: 5, Loss: 0.14521951973438263\n",
      "Iteration 42, Batch: 6, Loss: 0.11659207195043564\n",
      "Iteration 42, Batch: 7, Loss: 0.08962308615446091\n",
      "Iteration 42, Batch: 8, Loss: 0.10290072858333588\n",
      "Iteration 42, Batch: 9, Loss: 0.12353383749723434\n",
      "Iteration 42, Batch: 10, Loss: 0.12894755601882935\n",
      "Iteration 42, Batch: 11, Loss: 0.10711033642292023\n",
      "Iteration 42, Batch: 12, Loss: 0.10180743783712387\n",
      "Iteration 42, Batch: 13, Loss: 0.12778782844543457\n",
      "Iteration 42, Batch: 14, Loss: 0.10369665175676346\n",
      "Iteration 42, Batch: 15, Loss: 0.14573927223682404\n",
      "Iteration 42, Batch: 16, Loss: 0.1162271723151207\n",
      "Iteration 42, Batch: 17, Loss: 0.1428154557943344\n",
      "Iteration 42, Batch: 18, Loss: 0.09519904851913452\n",
      "Iteration 42, Batch: 19, Loss: 0.12280198931694031\n",
      "Iteration 42, Batch: 20, Loss: 0.08259236067533493\n",
      "Iteration 42, Batch: 21, Loss: 0.09467867016792297\n",
      "Iteration 42, Batch: 22, Loss: 0.15083573758602142\n",
      "Iteration 42, Batch: 23, Loss: 0.1423182636499405\n",
      "Iteration 42, Batch: 24, Loss: 0.09983506798744202\n",
      "Iteration 42, Batch: 25, Loss: 0.13031645119190216\n",
      "Iteration 42, Batch: 26, Loss: 0.14198315143585205\n",
      "Iteration 42, Batch: 27, Loss: 0.12804271280765533\n",
      "Iteration 42, Batch: 28, Loss: 0.12300935387611389\n",
      "Iteration 42, Batch: 29, Loss: 0.08184117823839188\n",
      "Iteration 42, Batch: 30, Loss: 0.07913585752248764\n",
      "Iteration 42, Batch: 31, Loss: 0.14334630966186523\n",
      "Iteration 42, Batch: 32, Loss: 0.13072223961353302\n",
      "Iteration 42, Batch: 33, Loss: 0.11053437739610672\n",
      "Iteration 42, Batch: 34, Loss: 0.11417944729328156\n",
      "Iteration 42, Batch: 35, Loss: 0.11730831116437912\n",
      "Iteration 42, Batch: 36, Loss: 0.18061362206935883\n",
      "Iteration 42, Batch: 37, Loss: 0.10983850061893463\n",
      "Iteration 42, Batch: 38, Loss: 0.13914842903614044\n",
      "Iteration 42, Batch: 39, Loss: 0.15057216584682465\n",
      "Iteration 42, Batch: 40, Loss: 0.10541605949401855\n",
      "Iteration 42, Batch: 41, Loss: 0.11460559815168381\n",
      "Iteration 42, Batch: 42, Loss: 0.12421395629644394\n",
      "Iteration 42, Batch: 43, Loss: 0.09868288040161133\n",
      "Iteration 42, Batch: 44, Loss: 0.0886080339550972\n",
      "Iteration 42, Batch: 45, Loss: 0.0854809358716011\n",
      "Iteration 42, Batch: 46, Loss: 0.10512196272611618\n",
      "Iteration 42, Batch: 47, Loss: 0.10685870051383972\n",
      "Iteration 42, Batch: 48, Loss: 0.09948278218507767\n",
      "Iteration 42, Batch: 49, Loss: 0.1831621527671814\n",
      "Iteration 43, Batch: 0, Loss: 0.0001231757487403229\n",
      "Iteration 43, Batch: 1, Loss: 5.1779057685052976e-05\n",
      "Iteration 43, Batch: 2, Loss: 1.0963932254526298e-05\n",
      "Iteration 43, Batch: 3, Loss: 4.1378654714208096e-05\n",
      "Iteration 43, Batch: 4, Loss: 8.050560427363962e-05\n",
      "Iteration 43, Batch: 5, Loss: 6.777435191906989e-05\n",
      "Iteration 43, Batch: 6, Loss: 3.128498428850435e-05\n",
      "Iteration 43, Batch: 7, Loss: 8.478119525534566e-06\n",
      "Iteration 43, Batch: 8, Loss: 3.0088931453065015e-05\n",
      "Iteration 43, Batch: 9, Loss: 4.699456985690631e-05\n",
      "Iteration 43, Batch: 10, Loss: 4.590458775055595e-05\n",
      "Iteration 43, Batch: 11, Loss: 1.5422734577441588e-05\n",
      "Iteration 43, Batch: 12, Loss: 4.855308361584321e-06\n",
      "Iteration 43, Batch: 13, Loss: 1.5804165741428733e-05\n",
      "Iteration 43, Batch: 14, Loss: 3.1979037885321304e-05\n",
      "Iteration 43, Batch: 15, Loss: 2.8950395062565804e-05\n",
      "Iteration 43, Batch: 16, Loss: 1.1292922863503918e-05\n",
      "Iteration 43, Batch: 17, Loss: 3.1023023439047392e-06\n",
      "Iteration 43, Batch: 18, Loss: 1.2120557585149072e-05\n",
      "Iteration 43, Batch: 19, Loss: 1.6683312423992902e-05\n",
      "Iteration 43, Batch: 20, Loss: 1.6149526345543563e-05\n",
      "Iteration 43, Batch: 21, Loss: 5.836619493493345e-06\n",
      "Iteration 43, Batch: 22, Loss: 1.836431124502269e-06\n",
      "Iteration 43, Batch: 23, Loss: 6.785841378587065e-06\n",
      "Iteration 43, Batch: 24, Loss: 8.224486919061746e-06\n",
      "Iteration 43, Batch: 25, Loss: 1.0338853826397099e-05\n",
      "Iteration 43, Batch: 26, Loss: 4.534936579148052e-06\n",
      "Iteration 43, Batch: 27, Loss: 1.9144022189721e-06\n",
      "Iteration 43, Batch: 28, Loss: 3.2918496799538843e-06\n",
      "Iteration 43, Batch: 29, Loss: 4.625490419130074e-06\n",
      "Iteration 43, Batch: 30, Loss: 6.107553872425342e-06\n",
      "Iteration 43, Batch: 31, Loss: 4.378577614261303e-06\n",
      "Iteration 43, Batch: 32, Loss: 1.8066214124701219e-06\n",
      "Iteration 43, Batch: 33, Loss: 2.416966935925302e-06\n",
      "Iteration 43, Batch: 34, Loss: 4.194092525722226e-06\n",
      "Iteration 43, Batch: 35, Loss: 4.310053554945625e-06\n",
      "Iteration 43, Batch: 36, Loss: 2.2268072825681884e-06\n",
      "Iteration 43, Batch: 37, Loss: 1.2281122963031521e-06\n",
      "Iteration 43, Batch: 38, Loss: 2.233768782389234e-06\n",
      "Iteration 43, Batch: 39, Loss: 2.4065441266429843e-06\n",
      "Iteration 43, Batch: 40, Loss: 2.468338834660244e-06\n",
      "Iteration 43, Batch: 41, Loss: 2.0132101781200618e-06\n",
      "Iteration 43, Batch: 42, Loss: 1.3908123719374998e-06\n",
      "Iteration 43, Batch: 43, Loss: 1.320040610153228e-06\n",
      "Iteration 43, Batch: 44, Loss: 1.8891415720645455e-06\n",
      "Iteration 43, Batch: 45, Loss: 2.378716317252838e-06\n",
      "Iteration 43, Batch: 46, Loss: 1.470808115300315e-06\n",
      "Iteration 43, Batch: 47, Loss: 1.2180931889815838e-06\n",
      "Iteration 43, Batch: 48, Loss: 1.5299400502044591e-06\n",
      "Iteration 43, Batch: 49, Loss: 1.852173454608419e-06\n",
      "Iteration 44, Batch: 0, Loss: 0.14685684442520142\n",
      "Iteration 44, Batch: 1, Loss: 0.15014149248600006\n",
      "Iteration 44, Batch: 2, Loss: 0.13014762103557587\n",
      "Iteration 44, Batch: 3, Loss: 0.12791870534420013\n",
      "Iteration 44, Batch: 4, Loss: 0.10144872218370438\n",
      "Iteration 44, Batch: 5, Loss: 0.1334114372730255\n",
      "Iteration 44, Batch: 6, Loss: 0.131705641746521\n",
      "Iteration 44, Batch: 7, Loss: 0.12393278628587723\n",
      "Iteration 44, Batch: 8, Loss: 0.09802494198083878\n",
      "Iteration 44, Batch: 9, Loss: 0.09079110622406006\n",
      "Iteration 44, Batch: 10, Loss: 0.09143958985805511\n",
      "Iteration 44, Batch: 11, Loss: 0.14234203100204468\n",
      "Iteration 44, Batch: 12, Loss: 0.0993456244468689\n",
      "Iteration 44, Batch: 13, Loss: 0.10641313344240189\n",
      "Iteration 44, Batch: 14, Loss: 0.09723464399576187\n",
      "Iteration 44, Batch: 15, Loss: 0.09618101269006729\n",
      "Iteration 44, Batch: 16, Loss: 0.10018449276685715\n",
      "Iteration 44, Batch: 17, Loss: 0.09914607554674149\n",
      "Iteration 44, Batch: 18, Loss: 0.10119835287332535\n",
      "Iteration 44, Batch: 19, Loss: 0.0844273567199707\n",
      "Iteration 44, Batch: 20, Loss: 0.13160568475723267\n",
      "Iteration 44, Batch: 21, Loss: 0.12555533647537231\n",
      "Iteration 44, Batch: 22, Loss: 0.07546503096818924\n",
      "Iteration 44, Batch: 23, Loss: 0.08811788260936737\n",
      "Iteration 44, Batch: 24, Loss: 0.11034612357616425\n",
      "Iteration 44, Batch: 25, Loss: 0.13060228526592255\n",
      "Iteration 44, Batch: 26, Loss: 0.11540383100509644\n",
      "Iteration 44, Batch: 27, Loss: 0.11597780138254166\n",
      "Iteration 44, Batch: 28, Loss: 0.10405166447162628\n",
      "Iteration 44, Batch: 29, Loss: 0.10938073694705963\n",
      "Iteration 44, Batch: 30, Loss: 0.12261077761650085\n",
      "Iteration 44, Batch: 31, Loss: 0.12236084043979645\n",
      "Iteration 44, Batch: 32, Loss: 0.11791564524173737\n",
      "Iteration 44, Batch: 33, Loss: 0.12901368737220764\n",
      "Iteration 44, Batch: 34, Loss: 0.0894635021686554\n",
      "Iteration 44, Batch: 35, Loss: 0.14655108749866486\n",
      "Iteration 44, Batch: 36, Loss: 0.10977476090192795\n",
      "Iteration 44, Batch: 37, Loss: 0.09733699262142181\n",
      "Iteration 44, Batch: 38, Loss: 0.0963345542550087\n",
      "Iteration 44, Batch: 39, Loss: 0.1113331988453865\n",
      "Iteration 44, Batch: 40, Loss: 0.11654871702194214\n",
      "Iteration 44, Batch: 41, Loss: 0.10457572340965271\n",
      "Iteration 44, Batch: 42, Loss: 0.10493074357509613\n",
      "Iteration 44, Batch: 43, Loss: 0.08403748273849487\n",
      "Iteration 44, Batch: 44, Loss: 0.10686425864696503\n",
      "Iteration 44, Batch: 45, Loss: 0.13243445754051208\n",
      "Iteration 44, Batch: 46, Loss: 0.1523537039756775\n",
      "Iteration 44, Batch: 47, Loss: 0.12918078899383545\n",
      "Iteration 44, Batch: 48, Loss: 0.12692557275295258\n",
      "Iteration 44, Batch: 49, Loss: 0.13530853390693665\n",
      "Iteration 45, Batch: 0, Loss: 0.00014137163816485554\n",
      "Iteration 45, Batch: 1, Loss: 9.507669892627746e-05\n",
      "Iteration 45, Batch: 2, Loss: 2.159805444534868e-05\n",
      "Iteration 45, Batch: 3, Loss: 2.280337685078848e-05\n",
      "Iteration 45, Batch: 4, Loss: 8.018797961995006e-05\n",
      "Iteration 45, Batch: 5, Loss: 9.439339919481426e-05\n",
      "Iteration 45, Batch: 6, Loss: 6.82821701047942e-05\n",
      "Iteration 45, Batch: 7, Loss: 2.3206732294056565e-05\n",
      "Iteration 45, Batch: 8, Loss: 2.007474540732801e-05\n",
      "Iteration 45, Batch: 9, Loss: 4.727584746433422e-05\n",
      "Iteration 45, Batch: 10, Loss: 6.42541199340485e-05\n",
      "Iteration 45, Batch: 11, Loss: 5.203450928092934e-05\n",
      "Iteration 45, Batch: 12, Loss: 2.1230971469776705e-05\n",
      "Iteration 45, Batch: 13, Loss: 1.3641576515510678e-05\n",
      "Iteration 45, Batch: 14, Loss: 2.93801767838886e-05\n",
      "Iteration 45, Batch: 15, Loss: 4.193946733721532e-05\n",
      "Iteration 45, Batch: 16, Loss: 3.501737955957651e-05\n",
      "Iteration 45, Batch: 17, Loss: 1.4438382095249835e-05\n",
      "Iteration 45, Batch: 18, Loss: 6.756415132258553e-06\n",
      "Iteration 45, Batch: 19, Loss: 1.675183193583507e-05\n",
      "Iteration 45, Batch: 20, Loss: 2.0742103515658528e-05\n",
      "Iteration 45, Batch: 21, Loss: 2.224359559477307e-05\n",
      "Iteration 45, Batch: 22, Loss: 8.157352567650378e-06\n",
      "Iteration 45, Batch: 23, Loss: 3.413774948057835e-06\n",
      "Iteration 45, Batch: 24, Loss: 9.880589459498879e-06\n",
      "Iteration 45, Batch: 25, Loss: 1.5144427379709668e-05\n",
      "Iteration 45, Batch: 26, Loss: 1.0883483810175676e-05\n",
      "Iteration 45, Batch: 27, Loss: 3.6040385111846263e-06\n",
      "Iteration 45, Batch: 28, Loss: 2.196214836658328e-06\n",
      "Iteration 45, Batch: 29, Loss: 6.104077783675166e-06\n",
      "Iteration 45, Batch: 30, Loss: 9.2650125225191e-06\n",
      "Iteration 45, Batch: 31, Loss: 5.743509063904639e-06\n",
      "Iteration 45, Batch: 32, Loss: 1.590962483533076e-06\n",
      "Iteration 45, Batch: 33, Loss: 1.5248933777911589e-06\n",
      "Iteration 45, Batch: 34, Loss: 3.6148510389466537e-06\n",
      "Iteration 45, Batch: 35, Loss: 4.338457074481994e-06\n",
      "Iteration 45, Batch: 36, Loss: 3.857899173453916e-06\n",
      "Iteration 45, Batch: 37, Loss: 1.8049348682325217e-06\n",
      "Iteration 45, Batch: 38, Loss: 8.974980119091924e-07\n",
      "Iteration 45, Batch: 39, Loss: 1.8405768287266255e-06\n",
      "Iteration 45, Batch: 40, Loss: 2.8246076908544637e-06\n",
      "Iteration 45, Batch: 41, Loss: 3.6781984817935154e-06\n",
      "Iteration 45, Batch: 42, Loss: 1.4058601891520084e-06\n",
      "Iteration 45, Batch: 43, Loss: 7.319993073906517e-07\n",
      "Iteration 45, Batch: 44, Loss: 1.507114006926713e-06\n",
      "Iteration 45, Batch: 45, Loss: 2.3580580545967678e-06\n",
      "Iteration 45, Batch: 46, Loss: 2.403556891295011e-06\n",
      "Iteration 45, Batch: 47, Loss: 1.0732601367635652e-06\n",
      "Iteration 45, Batch: 48, Loss: 6.869603339509922e-07\n",
      "Iteration 45, Batch: 49, Loss: 1.508906393610232e-06\n",
      "Iteration 46, Batch: 0, Loss: 0.0989857092499733\n",
      "Iteration 46, Batch: 1, Loss: 0.12690486013889313\n",
      "Iteration 46, Batch: 2, Loss: 0.09800304472446442\n",
      "Iteration 46, Batch: 3, Loss: 0.10405752062797546\n",
      "Iteration 46, Batch: 4, Loss: 0.09876316040754318\n",
      "Iteration 46, Batch: 5, Loss: 0.10766248404979706\n",
      "Iteration 46, Batch: 6, Loss: 0.12615369260311127\n",
      "Iteration 46, Batch: 7, Loss: 0.09451962262392044\n",
      "Iteration 46, Batch: 8, Loss: 0.11835497617721558\n",
      "Iteration 46, Batch: 9, Loss: 0.12608115375041962\n",
      "Iteration 46, Batch: 10, Loss: 0.1127486377954483\n",
      "Iteration 46, Batch: 11, Loss: 0.1022438108921051\n",
      "Iteration 46, Batch: 12, Loss: 0.12480706721544266\n",
      "Iteration 46, Batch: 13, Loss: 0.0927516371011734\n",
      "Iteration 46, Batch: 14, Loss: 0.14172610640525818\n",
      "Iteration 46, Batch: 15, Loss: 0.11570907384157181\n",
      "Iteration 46, Batch: 16, Loss: 0.08270914107561111\n",
      "Iteration 46, Batch: 17, Loss: 0.09114808589220047\n",
      "Iteration 46, Batch: 18, Loss: 0.10812992602586746\n",
      "Iteration 46, Batch: 19, Loss: 0.12238328903913498\n",
      "Iteration 46, Batch: 20, Loss: 0.0986722856760025\n",
      "Iteration 46, Batch: 21, Loss: 0.1417239010334015\n",
      "Iteration 46, Batch: 22, Loss: 0.14478501677513123\n",
      "Iteration 46, Batch: 23, Loss: 0.10891612619161606\n",
      "Iteration 46, Batch: 24, Loss: 0.11033391952514648\n",
      "Iteration 46, Batch: 25, Loss: 0.12527477741241455\n",
      "Iteration 46, Batch: 26, Loss: 0.10387340933084488\n",
      "Iteration 46, Batch: 27, Loss: 0.12499034404754639\n",
      "Iteration 46, Batch: 28, Loss: 0.11876485496759415\n",
      "Iteration 46, Batch: 29, Loss: 0.13695478439331055\n",
      "Iteration 46, Batch: 30, Loss: 0.09850878268480301\n",
      "Iteration 46, Batch: 31, Loss: 0.0950329378247261\n",
      "Iteration 46, Batch: 32, Loss: 0.1132408082485199\n",
      "Iteration 46, Batch: 33, Loss: 0.1070672944188118\n",
      "Iteration 46, Batch: 34, Loss: 0.09541390091180801\n",
      "Iteration 46, Batch: 35, Loss: 0.12496968358755112\n",
      "Iteration 46, Batch: 36, Loss: 0.08591026812791824\n",
      "Iteration 46, Batch: 37, Loss: 0.09999671578407288\n",
      "Iteration 46, Batch: 38, Loss: 0.10804396122694016\n",
      "Iteration 46, Batch: 39, Loss: 0.12502405047416687\n",
      "Iteration 46, Batch: 40, Loss: 0.11029601842164993\n",
      "Iteration 46, Batch: 41, Loss: 0.09780078381299973\n",
      "Iteration 46, Batch: 42, Loss: 0.08397343754768372\n",
      "Iteration 46, Batch: 43, Loss: 0.1248471587896347\n",
      "Iteration 46, Batch: 44, Loss: 0.09891864657402039\n",
      "Iteration 46, Batch: 45, Loss: 0.10635822266340256\n",
      "Iteration 46, Batch: 46, Loss: 0.13496538996696472\n",
      "Iteration 46, Batch: 47, Loss: 0.1162748858332634\n",
      "Iteration 46, Batch: 48, Loss: 0.09667997062206268\n",
      "Iteration 46, Batch: 49, Loss: 0.08815132826566696\n",
      "Iteration 47, Batch: 0, Loss: 4.54188120784238e-05\n",
      "Iteration 47, Batch: 1, Loss: 3.123330679954961e-05\n",
      "Iteration 47, Batch: 2, Loss: 8.756645001994912e-06\n",
      "Iteration 47, Batch: 3, Loss: 9.466099982091691e-06\n",
      "Iteration 47, Batch: 4, Loss: 2.3856604457250796e-05\n",
      "Iteration 47, Batch: 5, Loss: 3.2977932278299704e-05\n",
      "Iteration 47, Batch: 6, Loss: 1.92608822544571e-05\n",
      "Iteration 47, Batch: 7, Loss: 5.2071200116188265e-06\n",
      "Iteration 47, Batch: 8, Loss: 7.329315394599689e-06\n",
      "Iteration 47, Batch: 9, Loss: 1.4668818948848639e-05\n",
      "Iteration 47, Batch: 10, Loss: 1.7133175788330846e-05\n",
      "Iteration 47, Batch: 11, Loss: 1.1750222256523557e-05\n",
      "Iteration 47, Batch: 12, Loss: 3.581039436539868e-06\n",
      "Iteration 47, Batch: 13, Loss: 4.808237463294063e-06\n",
      "Iteration 47, Batch: 14, Loss: 8.674887794768438e-06\n",
      "Iteration 47, Batch: 15, Loss: 1.0808330443978775e-05\n",
      "Iteration 47, Batch: 16, Loss: 8.270590114989318e-06\n",
      "Iteration 47, Batch: 17, Loss: 2.30248951993417e-06\n",
      "Iteration 47, Batch: 18, Loss: 2.45062574322219e-06\n",
      "Iteration 47, Batch: 19, Loss: 6.214293989614816e-06\n",
      "Iteration 47, Batch: 20, Loss: 7.643673598067835e-06\n",
      "Iteration 47, Batch: 21, Loss: 3.3612086554057896e-06\n",
      "Iteration 47, Batch: 22, Loss: 1.5900884591246722e-06\n",
      "Iteration 47, Batch: 23, Loss: 1.5560269730485743e-06\n",
      "Iteration 47, Batch: 24, Loss: 5.092206265544519e-06\n",
      "Iteration 47, Batch: 25, Loss: 3.7775887449242873e-06\n",
      "Iteration 47, Batch: 26, Loss: 2.5249373720726e-06\n",
      "Iteration 47, Batch: 27, Loss: 1.6633749737593462e-06\n",
      "Iteration 47, Batch: 28, Loss: 1.4610949392590555e-06\n",
      "Iteration 47, Batch: 29, Loss: 2.13801718018658e-06\n",
      "Iteration 47, Batch: 30, Loss: 2.4198459414037643e-06\n",
      "Iteration 47, Batch: 31, Loss: 2.118694510500063e-06\n",
      "Iteration 47, Batch: 32, Loss: 1.310279117205937e-06\n",
      "Iteration 47, Batch: 33, Loss: 1.317426608693495e-06\n",
      "Iteration 47, Batch: 34, Loss: 2.073128143820213e-06\n",
      "Iteration 47, Batch: 35, Loss: 2.670560434125946e-06\n",
      "Iteration 47, Batch: 36, Loss: 1.2157189530626056e-06\n",
      "Iteration 47, Batch: 37, Loss: 1.1397560228942893e-06\n",
      "Iteration 47, Batch: 38, Loss: 1.5090523675098666e-06\n",
      "Iteration 47, Batch: 39, Loss: 1.907913429022301e-06\n",
      "Iteration 47, Batch: 40, Loss: 1.2465545751183527e-06\n",
      "Iteration 47, Batch: 41, Loss: 1.408502157573821e-06\n",
      "Iteration 47, Batch: 42, Loss: 1.8553323570813518e-06\n",
      "Iteration 47, Batch: 43, Loss: 1.4225967106540338e-06\n",
      "Iteration 47, Batch: 44, Loss: 1.5240672155414359e-06\n",
      "Iteration 47, Batch: 45, Loss: 1.6451990632049274e-06\n",
      "Iteration 47, Batch: 46, Loss: 1.277038109037676e-06\n",
      "Iteration 47, Batch: 47, Loss: 1.3432778587230132e-06\n",
      "Iteration 47, Batch: 48, Loss: 9.593086360837333e-07\n",
      "Iteration 47, Batch: 49, Loss: 1.6864422605067375e-06\n",
      "Iteration 48, Batch: 0, Loss: 0.14654803276062012\n",
      "Iteration 48, Batch: 1, Loss: 0.11139994114637375\n",
      "Iteration 48, Batch: 2, Loss: 0.09536498785018921\n",
      "Iteration 48, Batch: 3, Loss: 0.12901872396469116\n",
      "Iteration 48, Batch: 4, Loss: 0.14293448626995087\n",
      "Iteration 48, Batch: 5, Loss: 0.14596353471279144\n",
      "Iteration 48, Batch: 6, Loss: 0.12576107680797577\n",
      "Iteration 48, Batch: 7, Loss: 0.10782333463430405\n",
      "Iteration 48, Batch: 8, Loss: 0.12810686230659485\n",
      "Iteration 48, Batch: 9, Loss: 0.12700213491916656\n",
      "Iteration 48, Batch: 10, Loss: 0.1249673143029213\n",
      "Iteration 48, Batch: 11, Loss: 0.09743175655603409\n",
      "Iteration 48, Batch: 12, Loss: 0.10494484007358551\n",
      "Iteration 48, Batch: 13, Loss: 0.1429029107093811\n",
      "Iteration 48, Batch: 14, Loss: 0.11472059041261673\n",
      "Iteration 48, Batch: 15, Loss: 0.11729873716831207\n",
      "Iteration 48, Batch: 16, Loss: 0.08786266297101974\n",
      "Iteration 48, Batch: 17, Loss: 0.13860531151294708\n",
      "Iteration 48, Batch: 18, Loss: 0.1396971493959427\n",
      "Iteration 48, Batch: 19, Loss: 0.11749612540006638\n",
      "Iteration 48, Batch: 20, Loss: 0.10318534821271896\n",
      "Iteration 48, Batch: 21, Loss: 0.0995609238743782\n",
      "Iteration 48, Batch: 22, Loss: 0.12519900500774384\n",
      "Iteration 48, Batch: 23, Loss: 0.10432656854391098\n",
      "Iteration 48, Batch: 24, Loss: 0.13354186713695526\n",
      "Iteration 48, Batch: 25, Loss: 0.13416841626167297\n",
      "Iteration 48, Batch: 26, Loss: 0.10171719640493393\n",
      "Iteration 48, Batch: 27, Loss: 0.07182115316390991\n",
      "Iteration 48, Batch: 28, Loss: 0.1346035599708557\n",
      "Iteration 48, Batch: 29, Loss: 0.11805190145969391\n",
      "Iteration 48, Batch: 30, Loss: 0.12769943475723267\n",
      "Iteration 48, Batch: 31, Loss: 0.1456887423992157\n",
      "Iteration 48, Batch: 32, Loss: 0.09084378927946091\n",
      "Iteration 48, Batch: 33, Loss: 0.09550528973340988\n",
      "Iteration 48, Batch: 34, Loss: 0.09901925921440125\n",
      "Iteration 48, Batch: 35, Loss: 0.0834554061293602\n",
      "Iteration 48, Batch: 36, Loss: 0.14985598623752594\n",
      "Iteration 48, Batch: 37, Loss: 0.11366476863622665\n",
      "Iteration 48, Batch: 38, Loss: 0.11641710251569748\n",
      "Iteration 48, Batch: 39, Loss: 0.09708266705274582\n",
      "Iteration 48, Batch: 40, Loss: 0.0990612581372261\n",
      "Iteration 48, Batch: 41, Loss: 0.09908172488212585\n",
      "Iteration 48, Batch: 42, Loss: 0.10900314152240753\n",
      "Iteration 48, Batch: 43, Loss: 0.1502508670091629\n",
      "Iteration 48, Batch: 44, Loss: 0.14134979248046875\n",
      "Iteration 48, Batch: 45, Loss: 0.10474387556314468\n",
      "Iteration 48, Batch: 46, Loss: 0.11223375797271729\n",
      "Iteration 48, Batch: 47, Loss: 0.10699918866157532\n",
      "Iteration 48, Batch: 48, Loss: 0.1299896538257599\n",
      "Iteration 48, Batch: 49, Loss: 0.09422346949577332\n",
      "Iteration 49, Batch: 0, Loss: 0.00013118411879986525\n",
      "Iteration 49, Batch: 1, Loss: 6.974544521654025e-05\n",
      "Iteration 49, Batch: 2, Loss: 3.0814611818641424e-05\n",
      "Iteration 49, Batch: 3, Loss: 3.932764957426116e-05\n",
      "Iteration 49, Batch: 4, Loss: 7.525864930357784e-05\n",
      "Iteration 49, Batch: 5, Loss: 8.447591244475916e-05\n",
      "Iteration 49, Batch: 6, Loss: 5.2123919886071235e-05\n",
      "Iteration 49, Batch: 7, Loss: 2.335413773835171e-05\n",
      "Iteration 49, Batch: 8, Loss: 3.00384454021696e-05\n",
      "Iteration 49, Batch: 9, Loss: 5.4646625358145684e-05\n",
      "Iteration 49, Batch: 10, Loss: 5.039581810706295e-05\n",
      "Iteration 49, Batch: 11, Loss: 2.782524097710848e-05\n",
      "Iteration 49, Batch: 12, Loss: 9.764633432496339e-06\n",
      "Iteration 49, Batch: 13, Loss: 1.9500819689710625e-05\n",
      "Iteration 49, Batch: 14, Loss: 3.367342287674546e-05\n",
      "Iteration 49, Batch: 15, Loss: 2.774120912363287e-05\n",
      "Iteration 49, Batch: 16, Loss: 1.230710495292442e-05\n",
      "Iteration 49, Batch: 17, Loss: 3.341940328027704e-06\n",
      "Iteration 49, Batch: 18, Loss: 1.1989627637376543e-05\n",
      "Iteration 49, Batch: 19, Loss: 1.7533173377159983e-05\n",
      "Iteration 49, Batch: 20, Loss: 1.5069426808622666e-05\n",
      "Iteration 49, Batch: 21, Loss: 6.679479611193528e-06\n",
      "Iteration 49, Batch: 22, Loss: 1.8924429241451435e-06\n",
      "Iteration 49, Batch: 23, Loss: 6.751082764822058e-06\n",
      "Iteration 49, Batch: 24, Loss: 1.1007905413862318e-05\n",
      "Iteration 49, Batch: 25, Loss: 7.995356099854689e-06\n",
      "Iteration 49, Batch: 26, Loss: 3.975571871706052e-06\n",
      "Iteration 49, Batch: 27, Loss: 2.013362745856284e-06\n",
      "Iteration 49, Batch: 28, Loss: 4.124428414797876e-06\n",
      "Iteration 49, Batch: 29, Loss: 6.017835403326899e-06\n",
      "Iteration 49, Batch: 30, Loss: 5.304554633767111e-06\n",
      "Iteration 49, Batch: 31, Loss: 1.9109377262793714e-06\n",
      "Iteration 49, Batch: 32, Loss: 1.6121715589179075e-06\n",
      "Iteration 49, Batch: 33, Loss: 2.6664527013053885e-06\n",
      "Iteration 49, Batch: 34, Loss: 4.615609668690013e-06\n",
      "Iteration 49, Batch: 35, Loss: 2.9617251584568294e-06\n",
      "Iteration 49, Batch: 36, Loss: 1.3092447943563457e-06\n",
      "Iteration 49, Batch: 37, Loss: 1.571704842717736e-06\n",
      "Iteration 49, Batch: 38, Loss: 2.3007855816103984e-06\n",
      "Iteration 49, Batch: 39, Loss: 2.69094357463473e-06\n",
      "Iteration 49, Batch: 40, Loss: 2.0780582872248488e-06\n",
      "Iteration 49, Batch: 41, Loss: 1.0162209491682006e-06\n",
      "Iteration 49, Batch: 42, Loss: 1.364913146062463e-06\n",
      "Iteration 49, Batch: 43, Loss: 1.4415886653296184e-06\n",
      "Iteration 49, Batch: 44, Loss: 2.6415066258778097e-06\n",
      "Iteration 49, Batch: 45, Loss: 1.6494200281158555e-06\n",
      "Iteration 49, Batch: 46, Loss: 6.451000444940291e-07\n",
      "Iteration 49, Batch: 47, Loss: 1.3232079254521523e-06\n",
      "Iteration 49, Batch: 48, Loss: 1.5012722087703878e-06\n",
      "Iteration 49, Batch: 49, Loss: 1.5358669998022378e-06\n",
      "Iteration 50, Batch: 0, Loss: 0.08812405169010162\n",
      "Iteration 50, Batch: 1, Loss: 0.10592813044786453\n",
      "Iteration 50, Batch: 2, Loss: 0.16101177036762238\n",
      "Iteration 50, Batch: 3, Loss: 0.13871558010578156\n",
      "Iteration 50, Batch: 4, Loss: 0.16542991995811462\n",
      "Iteration 50, Batch: 5, Loss: 0.14233195781707764\n",
      "Iteration 50, Batch: 6, Loss: 0.12334371358156204\n",
      "Iteration 50, Batch: 7, Loss: 0.14486655592918396\n",
      "Iteration 50, Batch: 8, Loss: 0.12239129841327667\n",
      "Iteration 50, Batch: 9, Loss: 0.1151803731918335\n",
      "Iteration 50, Batch: 10, Loss: 0.15885873138904572\n",
      "Iteration 50, Batch: 11, Loss: 0.10102276504039764\n",
      "Iteration 50, Batch: 12, Loss: 0.12281377613544464\n",
      "Iteration 50, Batch: 13, Loss: 0.12219984084367752\n",
      "Iteration 50, Batch: 14, Loss: 0.1367160528898239\n",
      "Iteration 50, Batch: 15, Loss: 0.12097877264022827\n",
      "Iteration 50, Batch: 16, Loss: 0.09984977543354034\n",
      "Iteration 50, Batch: 17, Loss: 0.08135942369699478\n",
      "Iteration 50, Batch: 18, Loss: 0.09809064865112305\n",
      "Iteration 50, Batch: 19, Loss: 0.13130930066108704\n",
      "Iteration 50, Batch: 20, Loss: 0.08127614110708237\n",
      "Iteration 50, Batch: 21, Loss: 0.11757209151983261\n",
      "Iteration 50, Batch: 22, Loss: 0.10502412915229797\n",
      "Iteration 50, Batch: 23, Loss: 0.10005292296409607\n",
      "Iteration 50, Batch: 24, Loss: 0.13034385442733765\n",
      "Iteration 50, Batch: 25, Loss: 0.10165160149335861\n",
      "Iteration 50, Batch: 26, Loss: 0.09927894175052643\n",
      "Iteration 50, Batch: 27, Loss: 0.09149278700351715\n",
      "Iteration 50, Batch: 28, Loss: 0.12405119836330414\n",
      "Iteration 50, Batch: 29, Loss: 0.0956292673945427\n",
      "Iteration 50, Batch: 30, Loss: 0.11141305416822433\n",
      "Iteration 50, Batch: 31, Loss: 0.12075326591730118\n",
      "Iteration 50, Batch: 32, Loss: 0.10081946104764938\n",
      "Iteration 50, Batch: 33, Loss: 0.09791628271341324\n",
      "Iteration 50, Batch: 34, Loss: 0.10265368968248367\n",
      "Iteration 50, Batch: 35, Loss: 0.12744653224945068\n",
      "Iteration 50, Batch: 36, Loss: 0.10307751595973969\n",
      "Iteration 50, Batch: 37, Loss: 0.10774951428174973\n",
      "Iteration 50, Batch: 38, Loss: 0.13439080119132996\n",
      "Iteration 50, Batch: 39, Loss: 0.1193925067782402\n",
      "Iteration 50, Batch: 40, Loss: 0.15095017850399017\n",
      "Iteration 50, Batch: 41, Loss: 0.11391427367925644\n",
      "Iteration 50, Batch: 42, Loss: 0.10006613284349442\n",
      "Iteration 50, Batch: 43, Loss: 0.13481223583221436\n",
      "Iteration 50, Batch: 44, Loss: 0.12111430615186691\n",
      "Iteration 50, Batch: 45, Loss: 0.11218293756246567\n",
      "Iteration 50, Batch: 46, Loss: 0.15121932327747345\n",
      "Iteration 50, Batch: 47, Loss: 0.11895494908094406\n",
      "Iteration 50, Batch: 48, Loss: 0.11596490442752838\n",
      "Iteration 50, Batch: 49, Loss: 0.13177819550037384\n",
      "Iteration 51, Batch: 0, Loss: 0.00035933151957578957\n",
      "Iteration 51, Batch: 1, Loss: 0.00020278348529245704\n",
      "Iteration 51, Batch: 2, Loss: 3.7169364077271894e-05\n",
      "Iteration 51, Batch: 3, Loss: 3.2403528166469187e-05\n",
      "Iteration 51, Batch: 4, Loss: 0.00016458520258311182\n",
      "Iteration 51, Batch: 5, Loss: 0.00021902313164900988\n",
      "Iteration 51, Batch: 6, Loss: 0.00012234880705364048\n",
      "Iteration 51, Batch: 7, Loss: 3.324837234686129e-05\n",
      "Iteration 51, Batch: 8, Loss: 4.591854303725995e-05\n",
      "Iteration 51, Batch: 9, Loss: 9.414829401066527e-05\n",
      "Iteration 51, Batch: 10, Loss: 0.00010026113886851817\n",
      "Iteration 51, Batch: 11, Loss: 3.7314781366148964e-05\n",
      "Iteration 51, Batch: 12, Loss: 2.712889909162186e-05\n",
      "Iteration 51, Batch: 13, Loss: 4.727925988845527e-05\n",
      "Iteration 51, Batch: 14, Loss: 7.825783541193232e-05\n",
      "Iteration 51, Batch: 15, Loss: 3.6202916817273945e-05\n",
      "Iteration 51, Batch: 16, Loss: 8.791923391981982e-06\n",
      "Iteration 51, Batch: 17, Loss: 1.4447896319325082e-05\n",
      "Iteration 51, Batch: 18, Loss: 3.747123264474794e-05\n",
      "Iteration 51, Batch: 19, Loss: 5.2174029406160116e-05\n",
      "Iteration 51, Batch: 20, Loss: 2.4076784029603004e-05\n",
      "Iteration 51, Batch: 21, Loss: 0.004120710771530867\n",
      "Iteration 51, Batch: 22, Loss: 3.4949744076584466e-06\n",
      "Iteration 51, Batch: 23, Loss: 0.011597514152526855\n",
      "Iteration 51, Batch: 24, Loss: 3.6875851947115734e-05\n",
      "Iteration 51, Batch: 25, Loss: 2.44920465775067e-05\n",
      "Iteration 51, Batch: 26, Loss: 1.3553923963627312e-05\n",
      "Iteration 51, Batch: 27, Loss: 8.036319741222542e-06\n",
      "Iteration 51, Batch: 28, Loss: 1.128018084273208e-05\n",
      "Iteration 51, Batch: 29, Loss: 1.1546327186806593e-05\n",
      "Iteration 51, Batch: 30, Loss: 1.566459468449466e-05\n",
      "Iteration 51, Batch: 31, Loss: 1.2419279300956987e-05\n",
      "Iteration 51, Batch: 32, Loss: 1.164787681773305e-05\n",
      "Iteration 51, Batch: 33, Loss: 1.2446383152564522e-05\n",
      "Iteration 51, Batch: 34, Loss: 1.1420185728638899e-05\n",
      "Iteration 51, Batch: 35, Loss: 6.487394784926437e-06\n",
      "Iteration 51, Batch: 36, Loss: 9.642291843192652e-06\n",
      "Iteration 51, Batch: 37, Loss: 1.3427956218947656e-05\n",
      "Iteration 51, Batch: 38, Loss: 1.0616928193485364e-05\n",
      "Iteration 51, Batch: 39, Loss: 9.135022992268205e-06\n",
      "Iteration 51, Batch: 40, Loss: 7.276912128872937e-06\n",
      "Iteration 51, Batch: 41, Loss: 7.061419637466315e-06\n",
      "Iteration 51, Batch: 42, Loss: 8.317289939441252e-06\n",
      "Iteration 51, Batch: 43, Loss: 7.274511517607607e-06\n",
      "Iteration 51, Batch: 44, Loss: 8.836577762849629e-06\n",
      "Iteration 51, Batch: 45, Loss: 6.024351932865102e-06\n",
      "Iteration 51, Batch: 46, Loss: 5.3045732784084976e-06\n",
      "Iteration 51, Batch: 47, Loss: 5.175856585992733e-06\n",
      "Iteration 51, Batch: 48, Loss: 7.262683993758401e-06\n",
      "Iteration 51, Batch: 49, Loss: 5.633132786897477e-06\n",
      "Iteration 52, Batch: 0, Loss: 0.17982710897922516\n",
      "Iteration 52, Batch: 1, Loss: 0.1741715520620346\n",
      "Iteration 52, Batch: 2, Loss: 0.15862281620502472\n",
      "Iteration 52, Batch: 3, Loss: 0.15794746577739716\n",
      "Iteration 52, Batch: 4, Loss: 0.14390508830547333\n",
      "Iteration 52, Batch: 5, Loss: 0.15391533076763153\n",
      "Iteration 52, Batch: 6, Loss: 0.12224231660366058\n",
      "Iteration 52, Batch: 7, Loss: 0.1522214114665985\n",
      "Iteration 52, Batch: 8, Loss: 0.12301477789878845\n",
      "Iteration 52, Batch: 9, Loss: 0.12581364810466766\n",
      "Iteration 52, Batch: 10, Loss: 0.0840814933180809\n",
      "Iteration 52, Batch: 11, Loss: 0.13626216351985931\n",
      "Iteration 52, Batch: 12, Loss: 0.14042997360229492\n",
      "Iteration 52, Batch: 13, Loss: 0.09954501688480377\n",
      "Iteration 52, Batch: 14, Loss: 0.10601668804883957\n",
      "Iteration 52, Batch: 15, Loss: 0.15783929824829102\n",
      "Iteration 52, Batch: 16, Loss: 0.15180905163288116\n",
      "Iteration 52, Batch: 17, Loss: 0.14887206256389618\n",
      "Iteration 52, Batch: 18, Loss: 0.10354103147983551\n",
      "Iteration 52, Batch: 19, Loss: 0.10170907527208328\n",
      "Iteration 52, Batch: 20, Loss: 0.11394514888525009\n",
      "Iteration 52, Batch: 21, Loss: 0.08349613845348358\n",
      "Iteration 52, Batch: 22, Loss: 0.13707496225833893\n",
      "Iteration 52, Batch: 23, Loss: 0.1409556120634079\n",
      "Iteration 52, Batch: 24, Loss: 0.1269667148590088\n",
      "Iteration 52, Batch: 25, Loss: 0.13629373908042908\n",
      "Iteration 52, Batch: 26, Loss: 0.11154589056968689\n",
      "Iteration 52, Batch: 27, Loss: 0.11573325842618942\n",
      "Iteration 52, Batch: 28, Loss: 0.13842445611953735\n",
      "Iteration 52, Batch: 29, Loss: 0.10442154854536057\n",
      "Iteration 52, Batch: 30, Loss: 0.12524403631687164\n",
      "Iteration 52, Batch: 31, Loss: 0.15407149493694305\n",
      "Iteration 52, Batch: 32, Loss: 0.11630234122276306\n",
      "Iteration 52, Batch: 33, Loss: 0.1224992424249649\n",
      "Iteration 52, Batch: 34, Loss: 0.0790366604924202\n",
      "Iteration 52, Batch: 35, Loss: 0.09638132154941559\n",
      "Iteration 52, Batch: 36, Loss: 0.13964012265205383\n",
      "Iteration 52, Batch: 37, Loss: 0.11474361270666122\n",
      "Iteration 52, Batch: 38, Loss: 0.12782375514507294\n",
      "Iteration 52, Batch: 39, Loss: 0.12911193072795868\n",
      "Iteration 52, Batch: 40, Loss: 0.10144218802452087\n",
      "Iteration 52, Batch: 41, Loss: 0.11756426841020584\n",
      "Iteration 52, Batch: 42, Loss: 0.12706667184829712\n",
      "Iteration 52, Batch: 43, Loss: 0.11211742460727692\n",
      "Iteration 52, Batch: 44, Loss: 0.11896876990795135\n",
      "Iteration 52, Batch: 45, Loss: 0.08335389196872711\n",
      "Iteration 52, Batch: 46, Loss: 0.10669900476932526\n",
      "Iteration 52, Batch: 47, Loss: 0.0949549451470375\n",
      "Iteration 52, Batch: 48, Loss: 0.10587605834007263\n",
      "Iteration 52, Batch: 49, Loss: 0.13687177002429962\n",
      "Iteration 53, Batch: 0, Loss: 8.986825559986755e-05\n",
      "Iteration 53, Batch: 1, Loss: 5.971983773633838e-05\n",
      "Iteration 53, Batch: 2, Loss: 2.3685259293415584e-05\n",
      "Iteration 53, Batch: 3, Loss: 2.3690543457632884e-05\n",
      "Iteration 53, Batch: 4, Loss: 4.870336488238536e-05\n",
      "Iteration 53, Batch: 5, Loss: 6.267002027016133e-05\n",
      "Iteration 53, Batch: 6, Loss: 4.023759174742736e-05\n",
      "Iteration 53, Batch: 7, Loss: 1.215787142427871e-05\n",
      "Iteration 53, Batch: 8, Loss: 7.5197790465608705e-06\n",
      "Iteration 53, Batch: 9, Loss: 2.2746124159311876e-05\n",
      "Iteration 53, Batch: 10, Loss: 3.498830847092904e-05\n",
      "Iteration 53, Batch: 11, Loss: 2.0570121705532074e-05\n",
      "Iteration 53, Batch: 12, Loss: 9.110752216656692e-06\n",
      "Iteration 53, Batch: 13, Loss: 6.058569397282554e-06\n",
      "Iteration 53, Batch: 14, Loss: 1.1978323527728207e-05\n",
      "Iteration 53, Batch: 15, Loss: 1.3557324564317241e-05\n",
      "Iteration 53, Batch: 16, Loss: 9.855683856585529e-06\n",
      "Iteration 53, Batch: 17, Loss: 4.849652668781346e-06\n",
      "Iteration 53, Batch: 18, Loss: 6.807085810578428e-06\n",
      "Iteration 53, Batch: 19, Loss: 9.084228622668888e-06\n",
      "Iteration 53, Batch: 20, Loss: 7.421554073516745e-06\n",
      "Iteration 53, Batch: 21, Loss: 3.7632516978192143e-06\n",
      "Iteration 53, Batch: 22, Loss: 1.7631459741096478e-06\n",
      "Iteration 53, Batch: 23, Loss: 0.0050116912461817265\n",
      "Iteration 53, Batch: 24, Loss: 7.2449702201993205e-06\n",
      "Iteration 53, Batch: 25, Loss: 6.714443316013785e-06\n",
      "Iteration 53, Batch: 26, Loss: 2.351966941205319e-06\n",
      "Iteration 53, Batch: 27, Loss: 3.6714030215989624e-07\n",
      "Iteration 53, Batch: 28, Loss: 1.1125566743430682e-06\n",
      "Iteration 53, Batch: 29, Loss: 3.5981629480374977e-06\n",
      "Iteration 53, Batch: 30, Loss: 4.087678462383337e-06\n",
      "Iteration 53, Batch: 31, Loss: 2.5253841613448458e-06\n",
      "Iteration 53, Batch: 32, Loss: 6.776726308999059e-07\n",
      "Iteration 53, Batch: 33, Loss: 8.263620543402794e-07\n",
      "Iteration 53, Batch: 34, Loss: 1.9001955706698936e-06\n",
      "Iteration 53, Batch: 35, Loss: 2.130156190105481e-06\n",
      "Iteration 53, Batch: 36, Loss: 1.5445829149030033e-06\n",
      "Iteration 53, Batch: 37, Loss: 7.250483804455143e-07\n",
      "Iteration 53, Batch: 38, Loss: 7.355339448622544e-07\n",
      "Iteration 53, Batch: 39, Loss: 1.1486044968478382e-06\n",
      "Iteration 53, Batch: 40, Loss: 1.3697551821678644e-06\n",
      "Iteration 53, Batch: 41, Loss: 7.34751665731892e-07\n",
      "Iteration 53, Batch: 42, Loss: 3.4161965345447243e-07\n",
      "Iteration 53, Batch: 43, Loss: 6.430461212403316e-07\n",
      "Iteration 53, Batch: 44, Loss: 8.94590925781813e-07\n",
      "Iteration 53, Batch: 45, Loss: 0.002479104558005929\n",
      "Iteration 53, Batch: 46, Loss: 8.01999419763888e-07\n",
      "Iteration 53, Batch: 47, Loss: 3.7538094943556644e-07\n",
      "Iteration 53, Batch: 48, Loss: 4.4254178988012427e-07\n",
      "Iteration 53, Batch: 49, Loss: 7.82627864737151e-07\n",
      "Iteration 54, Batch: 0, Loss: 0.12092222273349762\n",
      "Iteration 54, Batch: 1, Loss: 0.11974690854549408\n",
      "Iteration 54, Batch: 2, Loss: 0.11140535026788712\n",
      "Iteration 54, Batch: 3, Loss: 0.11372673511505127\n",
      "Iteration 54, Batch: 4, Loss: 0.16158322989940643\n",
      "Iteration 54, Batch: 5, Loss: 0.10558322817087173\n",
      "Iteration 54, Batch: 6, Loss: 0.13344840705394745\n",
      "Iteration 54, Batch: 7, Loss: 0.11529052257537842\n",
      "Iteration 54, Batch: 8, Loss: 0.08117351680994034\n",
      "Iteration 54, Batch: 9, Loss: 0.10474655032157898\n",
      "Iteration 54, Batch: 10, Loss: 0.1217360645532608\n",
      "Iteration 54, Batch: 11, Loss: 0.12160810828208923\n",
      "Iteration 54, Batch: 12, Loss: 0.12274710088968277\n",
      "Iteration 54, Batch: 13, Loss: 0.11588186025619507\n",
      "Iteration 54, Batch: 14, Loss: 0.12552107870578766\n",
      "Iteration 54, Batch: 15, Loss: 0.08908763527870178\n",
      "Iteration 54, Batch: 16, Loss: 0.10190164297819138\n",
      "Iteration 54, Batch: 17, Loss: 0.09190496057271957\n",
      "Iteration 54, Batch: 18, Loss: 0.09168616682291031\n",
      "Iteration 54, Batch: 19, Loss: 0.09393896907567978\n",
      "Iteration 54, Batch: 20, Loss: 0.10224957019090652\n",
      "Iteration 54, Batch: 21, Loss: 0.1043129488825798\n",
      "Iteration 54, Batch: 22, Loss: 0.08264749497175217\n",
      "Iteration 54, Batch: 23, Loss: 0.06720998138189316\n",
      "Iteration 54, Batch: 24, Loss: 0.09726269543170929\n",
      "Iteration 54, Batch: 25, Loss: 0.10073148459196091\n",
      "Iteration 54, Batch: 26, Loss: 0.10875623673200607\n",
      "Iteration 54, Batch: 27, Loss: 0.1378585249185562\n",
      "Iteration 54, Batch: 28, Loss: 0.08125000447034836\n",
      "Iteration 54, Batch: 29, Loss: 0.13120736181735992\n",
      "Iteration 54, Batch: 30, Loss: 0.11495038866996765\n",
      "Iteration 54, Batch: 31, Loss: 0.12004218995571136\n",
      "Iteration 54, Batch: 32, Loss: 0.11300652474164963\n",
      "Iteration 54, Batch: 33, Loss: 0.12280049920082092\n",
      "Iteration 54, Batch: 34, Loss: 0.09819824248552322\n",
      "Iteration 54, Batch: 35, Loss: 0.1108187809586525\n",
      "Iteration 54, Batch: 36, Loss: 0.1313466876745224\n",
      "Iteration 54, Batch: 37, Loss: 0.0865466445684433\n",
      "Iteration 54, Batch: 38, Loss: 0.12597741186618805\n",
      "Iteration 54, Batch: 39, Loss: 0.06654185056686401\n",
      "Iteration 54, Batch: 40, Loss: 0.1196836605668068\n",
      "Iteration 54, Batch: 41, Loss: 0.10627863556146622\n",
      "Iteration 54, Batch: 42, Loss: 0.09622808545827866\n",
      "Iteration 54, Batch: 43, Loss: 0.10345346480607986\n",
      "Iteration 54, Batch: 44, Loss: 0.13009223341941833\n",
      "Iteration 54, Batch: 45, Loss: 0.0952458381652832\n",
      "Iteration 54, Batch: 46, Loss: 0.1474730670452118\n",
      "Iteration 54, Batch: 47, Loss: 0.12886296212673187\n",
      "Iteration 54, Batch: 48, Loss: 0.09811579436063766\n",
      "Iteration 54, Batch: 49, Loss: 0.09843970835208893\n",
      "Iteration 55, Batch: 0, Loss: 1.0163450497202575e-05\n",
      "Iteration 55, Batch: 1, Loss: 5.609570052911295e-06\n",
      "Iteration 55, Batch: 2, Loss: 2.0457924620131962e-05\n",
      "Iteration 55, Batch: 3, Loss: 2.987810512422584e-05\n",
      "Iteration 55, Batch: 4, Loss: 2.1129435481270775e-05\n",
      "Iteration 55, Batch: 5, Loss: 6.449038664868567e-06\n",
      "Iteration 55, Batch: 6, Loss: 4.812301995116286e-06\n",
      "Iteration 55, Batch: 7, Loss: 1.4920256035111379e-05\n",
      "Iteration 55, Batch: 8, Loss: 1.7921072867466137e-05\n",
      "Iteration 55, Batch: 9, Loss: 1.3677195966010913e-05\n",
      "Iteration 55, Batch: 10, Loss: 5.642325049848296e-06\n",
      "Iteration 55, Batch: 11, Loss: 3.748260951397242e-06\n",
      "Iteration 55, Batch: 12, Loss: 8.650735253468156e-06\n",
      "Iteration 55, Batch: 13, Loss: 1.153880475612823e-05\n",
      "Iteration 55, Batch: 14, Loss: 8.330825039593037e-06\n",
      "Iteration 55, Batch: 15, Loss: 0.0017572494689375162\n",
      "Iteration 55, Batch: 16, Loss: 2.562332383604371e-06\n",
      "Iteration 55, Batch: 17, Loss: 3.87144928026828e-06\n",
      "Iteration 55, Batch: 18, Loss: 5.056113877799362e-06\n",
      "Iteration 55, Batch: 19, Loss: 4.5694414438912645e-06\n",
      "Iteration 55, Batch: 20, Loss: 2.8072186069039162e-06\n",
      "Iteration 55, Batch: 21, Loss: 1.9046104853259749e-06\n",
      "Iteration 55, Batch: 22, Loss: 2.4848700377333444e-06\n",
      "Iteration 55, Batch: 23, Loss: 3.2446275781694567e-06\n",
      "Iteration 55, Batch: 24, Loss: 2.439704758216976e-06\n",
      "Iteration 55, Batch: 25, Loss: 1.5964370732035604e-06\n",
      "Iteration 55, Batch: 26, Loss: 1.1142657285745372e-06\n",
      "Iteration 55, Batch: 27, Loss: 1.4423433185584145e-06\n",
      "Iteration 55, Batch: 28, Loss: 1.6180789543795981e-06\n",
      "Iteration 55, Batch: 29, Loss: 1.5787786651344504e-06\n",
      "Iteration 55, Batch: 30, Loss: 9.26748100482655e-07\n",
      "Iteration 55, Batch: 31, Loss: 8.308408041557414e-07\n",
      "Iteration 55, Batch: 32, Loss: 1.109941877075471e-06\n",
      "Iteration 55, Batch: 33, Loss: 1.1436907243478345e-06\n",
      "Iteration 55, Batch: 34, Loss: 1.0275238082613214e-06\n",
      "Iteration 55, Batch: 35, Loss: 5.550743935600622e-07\n",
      "Iteration 55, Batch: 36, Loss: 6.0611967001023e-07\n",
      "Iteration 55, Batch: 37, Loss: 7.69128803312924e-07\n",
      "Iteration 55, Batch: 38, Loss: 1.0402930001873756e-06\n",
      "Iteration 55, Batch: 39, Loss: 6.218477324182459e-07\n",
      "Iteration 55, Batch: 40, Loss: 4.7455188223466394e-07\n",
      "Iteration 55, Batch: 41, Loss: 5.427905875876604e-07\n",
      "Iteration 55, Batch: 42, Loss: 5.93756737998774e-07\n",
      "Iteration 55, Batch: 43, Loss: 6.080135790398344e-07\n",
      "Iteration 55, Batch: 44, Loss: 5.849573199157021e-07\n",
      "Iteration 55, Batch: 45, Loss: 3.6410287407306896e-07\n",
      "Iteration 55, Batch: 46, Loss: 5.991670946059457e-07\n",
      "Iteration 55, Batch: 47, Loss: 4.367163057850121e-07\n",
      "Iteration 55, Batch: 48, Loss: 5.962310751783662e-07\n",
      "Iteration 55, Batch: 49, Loss: 3.0628072522631555e-07\n",
      "Iteration 56, Batch: 0, Loss: 0.10804013907909393\n",
      "Iteration 56, Batch: 1, Loss: 0.13073916733264923\n",
      "Iteration 56, Batch: 2, Loss: 0.1230413094162941\n",
      "Iteration 56, Batch: 3, Loss: 0.11903930455446243\n",
      "Iteration 56, Batch: 4, Loss: 0.10645592212677002\n",
      "Iteration 56, Batch: 5, Loss: 0.13016603887081146\n",
      "Iteration 56, Batch: 6, Loss: 0.1181446984410286\n",
      "Iteration 56, Batch: 7, Loss: 0.12209272384643555\n",
      "Iteration 56, Batch: 8, Loss: 0.10022147744894028\n",
      "Iteration 56, Batch: 9, Loss: 0.118550606071949\n",
      "Iteration 56, Batch: 10, Loss: 0.1412707418203354\n",
      "Iteration 56, Batch: 11, Loss: 0.08546622842550278\n",
      "Iteration 56, Batch: 12, Loss: 0.13174591958522797\n",
      "Iteration 56, Batch: 13, Loss: 0.1627717912197113\n",
      "Iteration 56, Batch: 14, Loss: 0.09642881155014038\n",
      "Iteration 56, Batch: 15, Loss: 0.09687299281358719\n",
      "Iteration 56, Batch: 16, Loss: 0.10908747464418411\n",
      "Iteration 56, Batch: 17, Loss: 0.13869307935237885\n",
      "Iteration 56, Batch: 18, Loss: 0.14923988282680511\n",
      "Iteration 56, Batch: 19, Loss: 0.082419253885746\n",
      "Iteration 56, Batch: 20, Loss: 0.15272793173789978\n",
      "Iteration 56, Batch: 21, Loss: 0.11864888668060303\n",
      "Iteration 56, Batch: 22, Loss: 0.10077222436666489\n",
      "Iteration 56, Batch: 23, Loss: 0.0974993035197258\n",
      "Iteration 56, Batch: 24, Loss: 0.12282940000295639\n",
      "Iteration 56, Batch: 25, Loss: 0.07333508133888245\n",
      "Iteration 56, Batch: 26, Loss: 0.14733156561851501\n",
      "Iteration 56, Batch: 27, Loss: 0.14581012725830078\n",
      "Iteration 56, Batch: 28, Loss: 0.10260028392076492\n",
      "Iteration 56, Batch: 29, Loss: 0.11816556751728058\n",
      "Iteration 56, Batch: 30, Loss: 0.1325376033782959\n",
      "Iteration 56, Batch: 31, Loss: 0.08522426337003708\n",
      "Iteration 56, Batch: 32, Loss: 0.10521253198385239\n",
      "Iteration 56, Batch: 33, Loss: 0.11035025119781494\n",
      "Iteration 56, Batch: 34, Loss: 0.09741930663585663\n",
      "Iteration 56, Batch: 35, Loss: 0.12917619943618774\n",
      "Iteration 56, Batch: 36, Loss: 0.11778363585472107\n",
      "Iteration 56, Batch: 37, Loss: 0.13669149577617645\n",
      "Iteration 56, Batch: 38, Loss: 0.0976143628358841\n",
      "Iteration 56, Batch: 39, Loss: 0.09769327193498611\n",
      "Iteration 56, Batch: 40, Loss: 0.10732770711183548\n",
      "Iteration 56, Batch: 41, Loss: 0.08619233220815659\n",
      "Iteration 56, Batch: 42, Loss: 0.10163778811693192\n",
      "Iteration 56, Batch: 43, Loss: 0.1721106320619583\n",
      "Iteration 56, Batch: 44, Loss: 0.1056995764374733\n",
      "Iteration 56, Batch: 45, Loss: 0.08697129040956497\n",
      "Iteration 56, Batch: 46, Loss: 0.12041404098272324\n",
      "Iteration 56, Batch: 47, Loss: 0.11870355159044266\n",
      "Iteration 56, Batch: 48, Loss: 0.15861469507217407\n",
      "Iteration 56, Batch: 49, Loss: 0.10059592127799988\n",
      "Iteration 57, Batch: 0, Loss: 5.079462061985396e-05\n",
      "Iteration 57, Batch: 1, Loss: 2.634220254549291e-05\n",
      "Iteration 57, Batch: 2, Loss: 1.5826748494873755e-05\n",
      "Iteration 57, Batch: 3, Loss: 3.134234430035576e-05\n",
      "Iteration 57, Batch: 4, Loss: 4.840554902330041e-05\n",
      "Iteration 57, Batch: 5, Loss: 4.554029874270782e-05\n",
      "Iteration 57, Batch: 6, Loss: 3.0508743293466978e-05\n",
      "Iteration 57, Batch: 7, Loss: 2.931886956503149e-05\n",
      "Iteration 57, Batch: 8, Loss: 4.519638605415821e-05\n",
      "Iteration 57, Batch: 9, Loss: 4.983700637239963e-05\n",
      "Iteration 57, Batch: 10, Loss: 0.0005966306780464947\n",
      "Iteration 57, Batch: 11, Loss: 3.352551721036434e-05\n",
      "Iteration 57, Batch: 12, Loss: 2.782921183097642e-05\n",
      "Iteration 57, Batch: 13, Loss: 2.5643947083153762e-05\n",
      "Iteration 57, Batch: 14, Loss: 2.4867265892680734e-05\n",
      "Iteration 57, Batch: 15, Loss: 2.544258131820243e-05\n",
      "Iteration 57, Batch: 16, Loss: 1.9145774786011316e-05\n",
      "Iteration 57, Batch: 17, Loss: 1.2453729141270742e-05\n",
      "Iteration 57, Batch: 18, Loss: 1.1014291885658167e-05\n",
      "Iteration 57, Batch: 19, Loss: 1.2236968359502498e-05\n",
      "Iteration 57, Batch: 20, Loss: 1.0165283129026648e-05\n",
      "Iteration 57, Batch: 21, Loss: 6.7340115492697805e-06\n",
      "Iteration 57, Batch: 22, Loss: 5.059130216977792e-06\n",
      "Iteration 57, Batch: 23, Loss: 4.891636763204588e-06\n",
      "Iteration 57, Batch: 24, Loss: 5.1467759476508945e-06\n",
      "Iteration 57, Batch: 25, Loss: 4.135446488362504e-06\n",
      "Iteration 57, Batch: 26, Loss: 2.487533720341162e-06\n",
      "Iteration 57, Batch: 27, Loss: 1.6512517504452262e-06\n",
      "Iteration 57, Batch: 28, Loss: 1.924321395563311e-06\n",
      "Iteration 57, Batch: 29, Loss: 2.115350753228995e-06\n",
      "Iteration 57, Batch: 30, Loss: 1.4742088296770817e-06\n",
      "Iteration 57, Batch: 31, Loss: 8.025587590054784e-07\n",
      "Iteration 57, Batch: 32, Loss: 7.217732331810112e-07\n",
      "Iteration 57, Batch: 33, Loss: 9.489044714428019e-07\n",
      "Iteration 57, Batch: 34, Loss: 9.891364243230782e-07\n",
      "Iteration 57, Batch: 35, Loss: 7.247155053846654e-07\n",
      "Iteration 57, Batch: 36, Loss: 3.5611665794021974e-07\n",
      "Iteration 57, Batch: 37, Loss: 2.753947399014578e-07\n",
      "Iteration 57, Batch: 38, Loss: 4.824968868888391e-07\n",
      "Iteration 57, Batch: 39, Loss: 5.588326530414633e-07\n",
      "Iteration 57, Batch: 40, Loss: 3.739664578006341e-07\n",
      "Iteration 57, Batch: 41, Loss: 1.5926336516258743e-07\n",
      "Iteration 57, Batch: 42, Loss: 1.751313902786933e-07\n",
      "Iteration 57, Batch: 43, Loss: 2.91915512207197e-07\n",
      "Iteration 57, Batch: 44, Loss: 0.005000266246497631\n",
      "Iteration 57, Batch: 45, Loss: 1.6657378409945522e-07\n",
      "Iteration 57, Batch: 46, Loss: 9.082339147425955e-08\n",
      "Iteration 57, Batch: 47, Loss: 1.382004768402112e-07\n",
      "Iteration 57, Batch: 48, Loss: 1.867446144387941e-07\n",
      "Iteration 57, Batch: 49, Loss: 1.3604555704205268e-07\n",
      "Iteration 58, Batch: 0, Loss: 0.15861868858337402\n",
      "Iteration 58, Batch: 1, Loss: 0.0774892047047615\n",
      "Iteration 58, Batch: 2, Loss: 0.15713734924793243\n",
      "Iteration 58, Batch: 3, Loss: 0.1374891698360443\n",
      "Iteration 58, Batch: 4, Loss: 0.13225573301315308\n",
      "Iteration 58, Batch: 5, Loss: 0.1266145557165146\n",
      "Iteration 58, Batch: 6, Loss: 0.1262052208185196\n",
      "Iteration 58, Batch: 7, Loss: 0.1300671100616455\n",
      "Iteration 58, Batch: 8, Loss: 0.11597516387701035\n",
      "Iteration 58, Batch: 9, Loss: 0.10938334465026855\n",
      "Iteration 58, Batch: 10, Loss: 0.09526018053293228\n",
      "Iteration 58, Batch: 11, Loss: 0.14087119698524475\n",
      "Iteration 58, Batch: 12, Loss: 0.09448068588972092\n",
      "Iteration 58, Batch: 13, Loss: 0.06978615373373032\n",
      "Iteration 58, Batch: 14, Loss: 0.1031469777226448\n",
      "Iteration 58, Batch: 15, Loss: 0.134559765458107\n",
      "Iteration 58, Batch: 16, Loss: 0.07377340644598007\n",
      "Iteration 58, Batch: 17, Loss: 0.07596058398485184\n",
      "Iteration 58, Batch: 18, Loss: 0.07700375467538834\n",
      "Iteration 58, Batch: 19, Loss: 0.09631427377462387\n",
      "Iteration 58, Batch: 20, Loss: 0.12990231812000275\n",
      "Iteration 58, Batch: 21, Loss: 0.10327751934528351\n",
      "Iteration 58, Batch: 22, Loss: 0.11098890006542206\n",
      "Iteration 58, Batch: 23, Loss: 0.1270100623369217\n",
      "Iteration 58, Batch: 24, Loss: 0.12463745474815369\n",
      "Iteration 58, Batch: 25, Loss: 0.09118065983057022\n",
      "Iteration 58, Batch: 26, Loss: 0.12934763729572296\n",
      "Iteration 58, Batch: 27, Loss: 0.12381026893854141\n",
      "Iteration 58, Batch: 28, Loss: 0.09932389855384827\n",
      "Iteration 58, Batch: 29, Loss: 0.08048604428768158\n",
      "Iteration 58, Batch: 30, Loss: 0.14444494247436523\n",
      "Iteration 58, Batch: 31, Loss: 0.06210591271519661\n",
      "Iteration 58, Batch: 32, Loss: 0.10588111728429794\n",
      "Iteration 58, Batch: 33, Loss: 0.10884737968444824\n",
      "Iteration 58, Batch: 34, Loss: 0.13695627450942993\n",
      "Iteration 58, Batch: 35, Loss: 0.12537133693695068\n",
      "Iteration 58, Batch: 36, Loss: 0.13087061047554016\n",
      "Iteration 58, Batch: 37, Loss: 0.12531575560569763\n",
      "Iteration 58, Batch: 38, Loss: 0.10475367307662964\n",
      "Iteration 58, Batch: 39, Loss: 0.11221083253622055\n",
      "Iteration 58, Batch: 40, Loss: 0.09649205952882767\n",
      "Iteration 58, Batch: 41, Loss: 0.1353432685136795\n",
      "Iteration 58, Batch: 42, Loss: 0.1183297336101532\n",
      "Iteration 58, Batch: 43, Loss: 0.10826167464256287\n",
      "Iteration 58, Batch: 44, Loss: 0.1016261950135231\n",
      "Iteration 58, Batch: 45, Loss: 0.09484933316707611\n",
      "Iteration 58, Batch: 46, Loss: 0.14421169459819794\n",
      "Iteration 58, Batch: 47, Loss: 0.13029122352600098\n",
      "Iteration 58, Batch: 48, Loss: 0.12030690908432007\n",
      "Iteration 58, Batch: 49, Loss: 0.10446422547101974\n",
      "Iteration 59, Batch: 0, Loss: 0.000173262320458889\n",
      "Iteration 59, Batch: 1, Loss: 7.575039489893243e-05\n",
      "Iteration 59, Batch: 2, Loss: 1.0990905138896778e-05\n",
      "Iteration 59, Batch: 3, Loss: 0.0020231979433447123\n",
      "Iteration 59, Batch: 4, Loss: 0.00023559037072118372\n",
      "Iteration 59, Batch: 5, Loss: 0.00018826792074833065\n",
      "Iteration 59, Batch: 6, Loss: 6.363879947457463e-05\n",
      "Iteration 59, Batch: 7, Loss: 6.128600489319069e-06\n",
      "Iteration 59, Batch: 8, Loss: 8.38442865642719e-05\n",
      "Iteration 59, Batch: 9, Loss: 0.00012227539264131337\n",
      "Iteration 59, Batch: 10, Loss: 7.841843762435019e-05\n",
      "Iteration 59, Batch: 11, Loss: 1.5560055544483475e-05\n",
      "Iteration 59, Batch: 12, Loss: 9.091530046134721e-06\n",
      "Iteration 59, Batch: 13, Loss: 5.823519313707948e-05\n",
      "Iteration 59, Batch: 14, Loss: 5.8405592426424846e-05\n",
      "Iteration 59, Batch: 15, Loss: 4.282081499695778e-05\n",
      "Iteration 59, Batch: 16, Loss: 2.4683522497070953e-06\n",
      "Iteration 59, Batch: 17, Loss: 1.7270935131818987e-05\n",
      "Iteration 59, Batch: 18, Loss: 4.5105152821633965e-05\n",
      "Iteration 59, Batch: 19, Loss: 3.617977927206084e-05\n",
      "Iteration 59, Batch: 20, Loss: 1.0063530680781696e-05\n",
      "Iteration 59, Batch: 21, Loss: 2.116958512488054e-06\n",
      "Iteration 59, Batch: 22, Loss: 1.795354182831943e-05\n",
      "Iteration 59, Batch: 23, Loss: 2.545670031395275e-05\n",
      "Iteration 59, Batch: 24, Loss: 1.5952662579366006e-05\n",
      "Iteration 59, Batch: 25, Loss: 1.7780752159524127e-06\n",
      "Iteration 59, Batch: 26, Loss: 3.969222689192975e-06\n",
      "Iteration 59, Batch: 27, Loss: 1.5023505511635449e-05\n",
      "Iteration 59, Batch: 28, Loss: 1.280305878026411e-05\n",
      "Iteration 59, Batch: 29, Loss: 5.910590971325291e-06\n",
      "Iteration 59, Batch: 30, Loss: 3.7139636788197095e-07\n",
      "Iteration 59, Batch: 31, Loss: 4.429780346981715e-06\n",
      "Iteration 59, Batch: 32, Loss: 9.818363650992978e-06\n",
      "Iteration 59, Batch: 33, Loss: 6.8884569373039994e-06\n",
      "Iteration 59, Batch: 34, Loss: 1.6060868119893712e-06\n",
      "Iteration 59, Batch: 35, Loss: 1.0069854852190474e-06\n",
      "Iteration 59, Batch: 36, Loss: 4.486862508201739e-06\n",
      "Iteration 59, Batch: 37, Loss: 6.1283153627300635e-06\n",
      "Iteration 59, Batch: 38, Loss: 2.5071826712519396e-06\n",
      "Iteration 59, Batch: 39, Loss: 3.8159981841090485e-07\n",
      "Iteration 59, Batch: 40, Loss: 1.4125085954219685e-06\n",
      "Iteration 59, Batch: 41, Loss: 3.734025995072443e-06\n",
      "Iteration 59, Batch: 42, Loss: 3.5217340155213606e-06\n",
      "Iteration 59, Batch: 43, Loss: 1.139052073995117e-06\n",
      "Iteration 59, Batch: 44, Loss: 3.8855083062117046e-07\n",
      "Iteration 59, Batch: 45, Loss: 1.8455726831234642e-06\n",
      "Iteration 59, Batch: 46, Loss: 2.5685849323053844e-06\n",
      "Iteration 59, Batch: 47, Loss: 1.4035831554792821e-06\n",
      "Iteration 59, Batch: 48, Loss: 3.667859687084274e-07\n",
      "Iteration 59, Batch: 49, Loss: 7.693133170505462e-07\n",
      "Iteration 60, Batch: 0, Loss: 0.13546819984912872\n",
      "Iteration 60, Batch: 1, Loss: 0.1443972885608673\n",
      "Iteration 60, Batch: 2, Loss: 0.10126087069511414\n",
      "Iteration 60, Batch: 3, Loss: 0.13022083044052124\n",
      "Iteration 60, Batch: 4, Loss: 0.12066648155450821\n",
      "Iteration 60, Batch: 5, Loss: 0.0931941494345665\n",
      "Iteration 60, Batch: 6, Loss: 0.09510062634944916\n",
      "Iteration 60, Batch: 7, Loss: 0.131766214966774\n",
      "Iteration 60, Batch: 8, Loss: 0.1287388801574707\n",
      "Iteration 60, Batch: 9, Loss: 0.11044073104858398\n",
      "Iteration 60, Batch: 10, Loss: 0.1338621824979782\n",
      "Iteration 60, Batch: 11, Loss: 0.13267557322978973\n",
      "Iteration 60, Batch: 12, Loss: 0.14017727971076965\n",
      "Iteration 60, Batch: 13, Loss: 0.08269798010587692\n",
      "Iteration 60, Batch: 14, Loss: 0.10615552961826324\n",
      "Iteration 60, Batch: 15, Loss: 0.11152268946170807\n",
      "Iteration 60, Batch: 16, Loss: 0.11504103243350983\n",
      "Iteration 60, Batch: 17, Loss: 0.08676715940237045\n",
      "Iteration 60, Batch: 18, Loss: 0.10187600553035736\n",
      "Iteration 60, Batch: 19, Loss: 0.14881978929042816\n",
      "Iteration 60, Batch: 20, Loss: 0.10043544322252274\n",
      "Iteration 60, Batch: 21, Loss: 0.1027112677693367\n",
      "Iteration 60, Batch: 22, Loss: 0.11903991550207138\n",
      "Iteration 60, Batch: 23, Loss: 0.1222556084394455\n",
      "Iteration 60, Batch: 24, Loss: 0.08336256444454193\n",
      "Iteration 60, Batch: 25, Loss: 0.14260055124759674\n",
      "Iteration 60, Batch: 26, Loss: 0.12333811819553375\n",
      "Iteration 60, Batch: 27, Loss: 0.10639505088329315\n",
      "Iteration 60, Batch: 28, Loss: 0.13469569385051727\n",
      "Iteration 60, Batch: 29, Loss: 0.12524345517158508\n",
      "Iteration 60, Batch: 30, Loss: 0.11461561173200607\n",
      "Iteration 60, Batch: 31, Loss: 0.1337447464466095\n",
      "Iteration 60, Batch: 32, Loss: 0.09323310852050781\n",
      "Iteration 60, Batch: 33, Loss: 0.13015051186084747\n",
      "Iteration 60, Batch: 34, Loss: 0.10858302563428879\n",
      "Iteration 60, Batch: 35, Loss: 0.13694976270198822\n",
      "Iteration 60, Batch: 36, Loss: 0.10714368522167206\n",
      "Iteration 60, Batch: 37, Loss: 0.11149539798498154\n",
      "Iteration 60, Batch: 38, Loss: 0.1383269876241684\n",
      "Iteration 60, Batch: 39, Loss: 0.13593563437461853\n",
      "Iteration 60, Batch: 40, Loss: 0.10615306347608566\n",
      "Iteration 60, Batch: 41, Loss: 0.10411934554576874\n",
      "Iteration 60, Batch: 42, Loss: 0.10391595959663391\n",
      "Iteration 60, Batch: 43, Loss: 0.1510818898677826\n",
      "Iteration 60, Batch: 44, Loss: 0.0772201344370842\n",
      "Iteration 60, Batch: 45, Loss: 0.09802278131246567\n",
      "Iteration 60, Batch: 46, Loss: 0.0860719308257103\n",
      "Iteration 60, Batch: 47, Loss: 0.10345581918954849\n",
      "Iteration 60, Batch: 48, Loss: 0.11357370018959045\n",
      "Iteration 60, Batch: 49, Loss: 0.11523434519767761\n",
      "Iteration 61, Batch: 0, Loss: 4.355125565780327e-05\n",
      "Iteration 61, Batch: 1, Loss: 1.2702987078228034e-05\n",
      "Iteration 61, Batch: 2, Loss: 1.9179493392584845e-05\n",
      "Iteration 61, Batch: 3, Loss: 4.686516695073806e-05\n",
      "Iteration 61, Batch: 4, Loss: 4.3000949517590925e-05\n",
      "Iteration 61, Batch: 5, Loss: 2.0184865206829272e-05\n",
      "Iteration 61, Batch: 6, Loss: 7.787066351738758e-06\n",
      "Iteration 61, Batch: 7, Loss: 1.8761786122922786e-05\n",
      "Iteration 61, Batch: 8, Loss: 2.5477356757619418e-05\n",
      "Iteration 61, Batch: 9, Loss: 1.7602274965611286e-05\n",
      "Iteration 61, Batch: 10, Loss: 4.937352969136555e-06\n",
      "Iteration 61, Batch: 11, Loss: 3.349435064592399e-05\n",
      "Iteration 61, Batch: 12, Loss: 1.3290667084220331e-05\n",
      "Iteration 61, Batch: 13, Loss: 1.472458643547725e-05\n",
      "Iteration 61, Batch: 14, Loss: 7.945782272145152e-06\n",
      "Iteration 61, Batch: 15, Loss: 1.2798279840353644e-06\n",
      "Iteration 61, Batch: 16, Loss: 4.213090960547561e-06\n",
      "Iteration 61, Batch: 17, Loss: 9.897102245304268e-06\n",
      "Iteration 61, Batch: 18, Loss: 8.866486496117432e-06\n",
      "Iteration 61, Batch: 19, Loss: 3.1022482289699838e-06\n",
      "Iteration 61, Batch: 20, Loss: 3.0781191640016914e-07\n",
      "Iteration 61, Batch: 21, Loss: 3.06525203086494e-06\n",
      "Iteration 61, Batch: 22, Loss: 6.971685706957942e-06\n",
      "Iteration 61, Batch: 23, Loss: 0.0012082976754754782\n",
      "Iteration 61, Batch: 24, Loss: 3.493823442113353e-06\n",
      "Iteration 61, Batch: 25, Loss: 1.3031126400164794e-06\n",
      "Iteration 61, Batch: 26, Loss: 7.733353299954615e-07\n",
      "Iteration 61, Batch: 27, Loss: 1.7996038650380797e-06\n",
      "Iteration 61, Batch: 28, Loss: 1.928735628098366e-06\n",
      "Iteration 61, Batch: 29, Loss: 1.6598446563875768e-06\n",
      "Iteration 61, Batch: 30, Loss: 1.0956073310808279e-06\n",
      "Iteration 61, Batch: 31, Loss: 1.2908232065456104e-06\n",
      "Iteration 61, Batch: 32, Loss: 1.4756203654542333e-06\n",
      "Iteration 61, Batch: 33, Loss: 1.162046032732178e-06\n",
      "Iteration 61, Batch: 34, Loss: 5.370540065996465e-07\n",
      "Iteration 61, Batch: 35, Loss: 7.75152841470117e-07\n",
      "Iteration 61, Batch: 36, Loss: 1.268572759727249e-06\n",
      "Iteration 61, Batch: 37, Loss: 1.0977533975164988e-06\n",
      "Iteration 61, Batch: 38, Loss: 6.946926873752091e-07\n",
      "Iteration 61, Batch: 39, Loss: 2.8681515118478274e-07\n",
      "Iteration 61, Batch: 40, Loss: 5.831886369378481e-07\n",
      "Iteration 61, Batch: 41, Loss: 9.086217573894828e-07\n",
      "Iteration 61, Batch: 42, Loss: 9.035856010086718e-07\n",
      "Iteration 61, Batch: 43, Loss: 5.033757588535082e-07\n",
      "Iteration 61, Batch: 44, Loss: 3.1303130754167796e-07\n",
      "Iteration 61, Batch: 45, Loss: 3.6879390563626657e-07\n",
      "Iteration 61, Batch: 46, Loss: 7.303959250748449e-07\n",
      "Iteration 61, Batch: 47, Loss: 4.7565450245201646e-07\n",
      "Iteration 61, Batch: 48, Loss: 4.6098651296233584e-07\n",
      "Iteration 61, Batch: 49, Loss: 3.867068869567447e-07\n",
      "Iteration 62, Batch: 0, Loss: 0.08276000618934631\n",
      "Iteration 62, Batch: 1, Loss: 0.11200722306966782\n",
      "Iteration 62, Batch: 2, Loss: 0.11124709248542786\n",
      "Iteration 62, Batch: 3, Loss: 0.09748967736959457\n",
      "Iteration 62, Batch: 4, Loss: 0.10106026381254196\n",
      "Iteration 62, Batch: 5, Loss: 0.09920809417963028\n",
      "Iteration 62, Batch: 6, Loss: 0.12641099095344543\n",
      "Iteration 62, Batch: 7, Loss: 0.10805366933345795\n",
      "Iteration 62, Batch: 8, Loss: 0.09462233632802963\n",
      "Iteration 62, Batch: 9, Loss: 0.11088696867227554\n",
      "Iteration 62, Batch: 10, Loss: 0.10534109175205231\n",
      "Iteration 62, Batch: 11, Loss: 0.1464219093322754\n",
      "Iteration 62, Batch: 12, Loss: 0.1506022959947586\n",
      "Iteration 62, Batch: 13, Loss: 0.11970756202936172\n",
      "Iteration 62, Batch: 14, Loss: 0.12274828553199768\n",
      "Iteration 62, Batch: 15, Loss: 0.07448342442512512\n",
      "Iteration 62, Batch: 16, Loss: 0.09472566097974777\n",
      "Iteration 62, Batch: 17, Loss: 0.12200187146663666\n",
      "Iteration 62, Batch: 18, Loss: 0.09101923555135727\n",
      "Iteration 62, Batch: 19, Loss: 0.11781062930822372\n",
      "Iteration 62, Batch: 20, Loss: 0.08907520771026611\n",
      "Iteration 62, Batch: 21, Loss: 0.11367898434400558\n",
      "Iteration 62, Batch: 22, Loss: 0.14731815457344055\n",
      "Iteration 62, Batch: 23, Loss: 0.13407692313194275\n",
      "Iteration 62, Batch: 24, Loss: 0.14149446785449982\n",
      "Iteration 62, Batch: 25, Loss: 0.11830423027276993\n",
      "Iteration 62, Batch: 26, Loss: 0.0990729108452797\n",
      "Iteration 62, Batch: 27, Loss: 0.07619835436344147\n",
      "Iteration 62, Batch: 28, Loss: 0.07280557602643967\n",
      "Iteration 62, Batch: 29, Loss: 0.10254383832216263\n",
      "Iteration 62, Batch: 30, Loss: 0.12031150609254837\n",
      "Iteration 62, Batch: 31, Loss: 0.1403883397579193\n",
      "Iteration 62, Batch: 32, Loss: 0.10090802609920502\n",
      "Iteration 62, Batch: 33, Loss: 0.08883468061685562\n",
      "Iteration 62, Batch: 34, Loss: 0.12279727309942245\n",
      "Iteration 62, Batch: 35, Loss: 0.09712913632392883\n",
      "Iteration 62, Batch: 36, Loss: 0.11280244588851929\n",
      "Iteration 62, Batch: 37, Loss: 0.09697698056697845\n",
      "Iteration 62, Batch: 38, Loss: 0.1108841523528099\n",
      "Iteration 62, Batch: 39, Loss: 0.0795493796467781\n",
      "Iteration 62, Batch: 40, Loss: 0.0844949409365654\n",
      "Iteration 62, Batch: 41, Loss: 0.11917886137962341\n",
      "Iteration 62, Batch: 42, Loss: 0.11874629557132721\n",
      "Iteration 62, Batch: 43, Loss: 0.08937039226293564\n",
      "Iteration 62, Batch: 44, Loss: 0.12419761717319489\n",
      "Iteration 62, Batch: 45, Loss: 0.1297406107187271\n",
      "Iteration 62, Batch: 46, Loss: 0.11879554390907288\n",
      "Iteration 62, Batch: 47, Loss: 0.10386744886636734\n",
      "Iteration 62, Batch: 48, Loss: 0.08054706454277039\n",
      "Iteration 62, Batch: 49, Loss: 0.10555005073547363\n",
      "Iteration 63, Batch: 0, Loss: 5.945256634731777e-05\n",
      "Iteration 63, Batch: 1, Loss: 5.6332450185436755e-05\n",
      "Iteration 63, Batch: 2, Loss: 2.1768277292721905e-05\n",
      "Iteration 63, Batch: 3, Loss: 2.119129385391716e-05\n",
      "Iteration 63, Batch: 4, Loss: 4.1673472878756e-05\n",
      "Iteration 63, Batch: 5, Loss: 4.33483837696258e-05\n",
      "Iteration 63, Batch: 6, Loss: 2.9040378649369814e-05\n",
      "Iteration 63, Batch: 7, Loss: 1.4857711903459858e-05\n",
      "Iteration 63, Batch: 8, Loss: 2.0201183360768482e-05\n",
      "Iteration 63, Batch: 9, Loss: 2.915966251748614e-05\n",
      "Iteration 63, Batch: 10, Loss: 2.3623391825822182e-05\n",
      "Iteration 63, Batch: 11, Loss: 1.3701485841011163e-05\n",
      "Iteration 63, Batch: 12, Loss: 9.440622307010926e-06\n",
      "Iteration 63, Batch: 13, Loss: 1.2663605048146565e-05\n",
      "Iteration 63, Batch: 14, Loss: 1.5162012459768448e-05\n",
      "Iteration 63, Batch: 15, Loss: 1.3980498806631658e-05\n",
      "Iteration 63, Batch: 16, Loss: 6.992515864112647e-06\n",
      "Iteration 63, Batch: 17, Loss: 4.492062998906476e-06\n",
      "Iteration 63, Batch: 18, Loss: 8.064547728281468e-06\n",
      "Iteration 63, Batch: 19, Loss: 1.2072860045009293e-05\n",
      "Iteration 63, Batch: 20, Loss: 9.273927389585879e-06\n",
      "Iteration 63, Batch: 21, Loss: 3.844737420877209e-06\n",
      "Iteration 63, Batch: 22, Loss: 2.8880358513561077e-06\n",
      "Iteration 63, Batch: 23, Loss: 4.914597411698196e-06\n",
      "Iteration 63, Batch: 24, Loss: 9.266985216527246e-06\n",
      "Iteration 63, Batch: 25, Loss: 4.784933480550535e-06\n",
      "Iteration 63, Batch: 26, Loss: 2.425045977361151e-06\n",
      "Iteration 63, Batch: 27, Loss: 2.883818751797662e-06\n",
      "Iteration 63, Batch: 28, Loss: 4.051423729833914e-06\n",
      "Iteration 63, Batch: 29, Loss: 5.1485185394994915e-06\n",
      "Iteration 63, Batch: 30, Loss: 3.0504638743877877e-06\n",
      "Iteration 63, Batch: 31, Loss: 2.1002706489525735e-06\n",
      "Iteration 63, Batch: 32, Loss: 3.755418219952844e-06\n",
      "Iteration 63, Batch: 33, Loss: 4.162524419371039e-06\n",
      "Iteration 63, Batch: 34, Loss: 2.286384642502526e-06\n",
      "Iteration 63, Batch: 35, Loss: 1.0417410294394358e-06\n",
      "Iteration 63, Batch: 36, Loss: 1.7053678220690927e-06\n",
      "Iteration 63, Batch: 37, Loss: 2.55805616689031e-06\n",
      "Iteration 63, Batch: 38, Loss: 2.4007695174077526e-06\n",
      "Iteration 63, Batch: 39, Loss: 1.2418394135238486e-06\n",
      "Iteration 63, Batch: 40, Loss: 1.2223745216033421e-06\n",
      "Iteration 63, Batch: 41, Loss: 1.6032663552323356e-06\n",
      "Iteration 63, Batch: 42, Loss: 1.8855836287912098e-06\n",
      "Iteration 63, Batch: 43, Loss: 1.9188896658306476e-06\n",
      "Iteration 63, Batch: 44, Loss: 8.968461315816967e-07\n",
      "Iteration 63, Batch: 45, Loss: 1.6010985746106599e-06\n",
      "Iteration 63, Batch: 46, Loss: 1.3654927215611679e-06\n",
      "Iteration 63, Batch: 47, Loss: 1.2624452665477293e-06\n",
      "Iteration 63, Batch: 48, Loss: 1.2682705801125849e-06\n",
      "Iteration 63, Batch: 49, Loss: 1.0103073009304353e-06\n",
      "Iteration 64, Batch: 0, Loss: 0.1279379278421402\n",
      "Iteration 64, Batch: 1, Loss: 0.11508697271347046\n",
      "Iteration 64, Batch: 2, Loss: 0.10383966565132141\n",
      "Iteration 64, Batch: 3, Loss: 0.10931546241044998\n",
      "Iteration 64, Batch: 4, Loss: 0.14044569432735443\n",
      "Iteration 64, Batch: 5, Loss: 0.07333870977163315\n",
      "Iteration 64, Batch: 6, Loss: 0.12439268082380295\n",
      "Iteration 64, Batch: 7, Loss: 0.1787651926279068\n",
      "Iteration 64, Batch: 8, Loss: 0.08992329984903336\n",
      "Iteration 64, Batch: 9, Loss: 0.07591262459754944\n",
      "Iteration 64, Batch: 10, Loss: 0.09336758404970169\n",
      "Iteration 64, Batch: 11, Loss: 0.09419353306293488\n",
      "Iteration 64, Batch: 12, Loss: 0.08429831266403198\n",
      "Iteration 64, Batch: 13, Loss: 0.10455652326345444\n",
      "Iteration 64, Batch: 14, Loss: 0.06617815792560577\n",
      "Iteration 64, Batch: 15, Loss: 0.10774719715118408\n",
      "Iteration 64, Batch: 16, Loss: 0.14003987610340118\n",
      "Iteration 64, Batch: 17, Loss: 0.09573408216238022\n",
      "Iteration 64, Batch: 18, Loss: 0.11330527812242508\n",
      "Iteration 64, Batch: 19, Loss: 0.13075625896453857\n",
      "Iteration 64, Batch: 20, Loss: 0.13632740080356598\n",
      "Iteration 64, Batch: 21, Loss: 0.13778257369995117\n",
      "Iteration 64, Batch: 22, Loss: 0.11939080059528351\n",
      "Iteration 64, Batch: 23, Loss: 0.11900302767753601\n",
      "Iteration 64, Batch: 24, Loss: 0.09783238917589188\n",
      "Iteration 64, Batch: 25, Loss: 0.08874833583831787\n",
      "Iteration 64, Batch: 26, Loss: 0.11966633796691895\n",
      "Iteration 64, Batch: 27, Loss: 0.10405834019184113\n",
      "Iteration 64, Batch: 28, Loss: 0.13308429718017578\n",
      "Iteration 64, Batch: 29, Loss: 0.10457684099674225\n",
      "Iteration 64, Batch: 30, Loss: 0.11696294695138931\n",
      "Iteration 64, Batch: 31, Loss: 0.0952368900179863\n",
      "Iteration 64, Batch: 32, Loss: 0.11161414533853531\n",
      "Iteration 64, Batch: 33, Loss: 0.08433876931667328\n",
      "Iteration 64, Batch: 34, Loss: 0.16185128688812256\n",
      "Iteration 64, Batch: 35, Loss: 0.09920649975538254\n",
      "Iteration 64, Batch: 36, Loss: 0.10675270110368729\n",
      "Iteration 64, Batch: 37, Loss: 0.12253203243017197\n",
      "Iteration 64, Batch: 38, Loss: 0.10192535072565079\n",
      "Iteration 64, Batch: 39, Loss: 0.14777834713459015\n",
      "Iteration 64, Batch: 40, Loss: 0.12089797109365463\n",
      "Iteration 64, Batch: 41, Loss: 0.13503769040107727\n",
      "Iteration 64, Batch: 42, Loss: 0.13996657729148865\n",
      "Iteration 64, Batch: 43, Loss: 0.10035335272550583\n",
      "Iteration 64, Batch: 44, Loss: 0.10532741993665695\n",
      "Iteration 64, Batch: 45, Loss: 0.1290547102689743\n",
      "Iteration 64, Batch: 46, Loss: 0.11111675202846527\n",
      "Iteration 64, Batch: 47, Loss: 0.13426293432712555\n",
      "Iteration 64, Batch: 48, Loss: 0.06842891871929169\n",
      "Iteration 64, Batch: 49, Loss: 0.1412367969751358\n",
      "Iteration 65, Batch: 0, Loss: 0.00020251446403563023\n",
      "Iteration 65, Batch: 1, Loss: 4.909184644930065e-05\n",
      "Iteration 65, Batch: 2, Loss: 2.4596547518740408e-05\n",
      "Iteration 65, Batch: 3, Loss: 0.00010563986143097281\n",
      "Iteration 65, Batch: 4, Loss: 0.00021107170323375612\n",
      "Iteration 65, Batch: 5, Loss: 0.00015520871966145933\n",
      "Iteration 65, Batch: 6, Loss: 4.214958971715532e-05\n",
      "Iteration 65, Batch: 7, Loss: 3.426574403420091e-05\n",
      "Iteration 65, Batch: 8, Loss: 0.00010250359628116712\n",
      "Iteration 65, Batch: 9, Loss: 0.00010071555152535439\n",
      "Iteration 65, Batch: 10, Loss: 6.564008799614385e-05\n",
      "Iteration 65, Batch: 11, Loss: 3.1366143957711756e-05\n",
      "Iteration 65, Batch: 12, Loss: 4.521922528510913e-05\n",
      "Iteration 65, Batch: 13, Loss: 6.595992454094812e-05\n",
      "Iteration 65, Batch: 14, Loss: 6.494110130006447e-05\n",
      "Iteration 65, Batch: 15, Loss: 2.35171100939624e-05\n",
      "Iteration 65, Batch: 16, Loss: 1.2311935279285535e-05\n",
      "Iteration 65, Batch: 17, Loss: 3.918041329598054e-05\n",
      "Iteration 65, Batch: 18, Loss: 7.129609730327502e-05\n",
      "Iteration 65, Batch: 19, Loss: 3.879807627527043e-05\n",
      "Iteration 65, Batch: 20, Loss: 3.917045432899613e-06\n",
      "Iteration 65, Batch: 21, Loss: 8.716519914742094e-06\n",
      "Iteration 65, Batch: 22, Loss: 3.796111195697449e-05\n",
      "Iteration 65, Batch: 23, Loss: 4.021219865535386e-05\n",
      "Iteration 65, Batch: 24, Loss: 1.9050290575250983e-05\n",
      "Iteration 65, Batch: 25, Loss: 2.535080284360447e-06\n",
      "Iteration 65, Batch: 26, Loss: 7.302316134882858e-06\n",
      "Iteration 65, Batch: 27, Loss: 2.062459861917887e-05\n",
      "Iteration 65, Batch: 28, Loss: 1.86247743840795e-05\n",
      "Iteration 65, Batch: 29, Loss: 1.0499001291464083e-05\n",
      "Iteration 65, Batch: 30, Loss: 4.387488388601923e-06\n",
      "Iteration 65, Batch: 31, Loss: 5.202174634177936e-06\n",
      "Iteration 65, Batch: 32, Loss: 8.9854838734027e-06\n",
      "Iteration 65, Batch: 33, Loss: 9.202167348121293e-06\n",
      "Iteration 65, Batch: 34, Loss: 5.711615358450217e-06\n",
      "Iteration 65, Batch: 35, Loss: 2.8699287213385105e-06\n",
      "Iteration 65, Batch: 36, Loss: 5.6719172789598815e-06\n",
      "Iteration 65, Batch: 37, Loss: 6.547779776155949e-06\n",
      "Iteration 65, Batch: 38, Loss: 4.933286163577577e-06\n",
      "Iteration 65, Batch: 39, Loss: 2.0832619611610426e-06\n",
      "Iteration 65, Batch: 40, Loss: 1.5055095445859479e-06\n",
      "Iteration 65, Batch: 41, Loss: 4.4422763494367246e-06\n",
      "Iteration 65, Batch: 42, Loss: 5.221068931859918e-06\n",
      "Iteration 65, Batch: 43, Loss: 3.2992020351230167e-06\n",
      "Iteration 65, Batch: 44, Loss: 8.454859994344588e-07\n",
      "Iteration 65, Batch: 45, Loss: 7.82919414632488e-07\n",
      "Iteration 65, Batch: 46, Loss: 2.2500555587612325e-06\n",
      "Iteration 65, Batch: 47, Loss: 3.390867959751631e-06\n",
      "Iteration 65, Batch: 48, Loss: 2.2155570604809327e-06\n",
      "Iteration 65, Batch: 49, Loss: 8.793515462457435e-07\n",
      "Iteration 66, Batch: 0, Loss: 0.12224916368722916\n",
      "Iteration 66, Batch: 1, Loss: 0.07880997657775879\n",
      "Iteration 66, Batch: 2, Loss: 0.08061515539884567\n",
      "Iteration 66, Batch: 3, Loss: 0.13907401263713837\n",
      "Iteration 66, Batch: 4, Loss: 0.08350750803947449\n",
      "Iteration 66, Batch: 5, Loss: 0.13583046197891235\n",
      "Iteration 66, Batch: 6, Loss: 0.11980035156011581\n",
      "Iteration 66, Batch: 7, Loss: 0.09485726803541183\n",
      "Iteration 66, Batch: 8, Loss: 0.10123781114816666\n",
      "Iteration 66, Batch: 9, Loss: 0.11287656426429749\n",
      "Iteration 66, Batch: 10, Loss: 0.08756913989782333\n",
      "Iteration 66, Batch: 11, Loss: 0.10206703096628189\n",
      "Iteration 66, Batch: 12, Loss: 0.11483938246965408\n",
      "Iteration 66, Batch: 13, Loss: 0.09360142797231674\n",
      "Iteration 66, Batch: 14, Loss: 0.10502366721630096\n",
      "Iteration 66, Batch: 15, Loss: 0.13481448590755463\n",
      "Iteration 66, Batch: 16, Loss: 0.13959254324436188\n",
      "Iteration 66, Batch: 17, Loss: 0.133387491106987\n",
      "Iteration 66, Batch: 18, Loss: 0.11004960536956787\n",
      "Iteration 66, Batch: 19, Loss: 0.10898206382989883\n",
      "Iteration 66, Batch: 20, Loss: 0.13566315174102783\n",
      "Iteration 66, Batch: 21, Loss: 0.11430778354406357\n",
      "Iteration 66, Batch: 22, Loss: 0.13336803019046783\n",
      "Iteration 66, Batch: 23, Loss: 0.08519773185253143\n",
      "Iteration 66, Batch: 24, Loss: 0.05965208262205124\n",
      "Iteration 66, Batch: 25, Loss: 0.1466379016637802\n",
      "Iteration 66, Batch: 26, Loss: 0.1568763703107834\n",
      "Iteration 66, Batch: 27, Loss: 0.09990807622671127\n",
      "Iteration 66, Batch: 28, Loss: 0.09796170890331268\n",
      "Iteration 66, Batch: 29, Loss: 0.14077427983283997\n",
      "Iteration 66, Batch: 30, Loss: 0.09811358898878098\n",
      "Iteration 66, Batch: 31, Loss: 0.12514027953147888\n",
      "Iteration 66, Batch: 32, Loss: 0.15969136357307434\n",
      "Iteration 66, Batch: 33, Loss: 0.11584919691085815\n",
      "Iteration 66, Batch: 34, Loss: 0.15143786370754242\n",
      "Iteration 66, Batch: 35, Loss: 0.148927241563797\n",
      "Iteration 66, Batch: 36, Loss: 0.09449630230665207\n",
      "Iteration 66, Batch: 37, Loss: 0.11585525423288345\n",
      "Iteration 66, Batch: 38, Loss: 0.10815099626779556\n",
      "Iteration 66, Batch: 39, Loss: 0.0972813218832016\n",
      "Iteration 66, Batch: 40, Loss: 0.09753523021936417\n",
      "Iteration 66, Batch: 41, Loss: 0.13838575780391693\n",
      "Iteration 66, Batch: 42, Loss: 0.11462277919054031\n",
      "Iteration 66, Batch: 43, Loss: 0.10747034102678299\n",
      "Iteration 66, Batch: 44, Loss: 0.12776707112789154\n",
      "Iteration 66, Batch: 45, Loss: 0.08875942975282669\n",
      "Iteration 66, Batch: 46, Loss: 0.1117381826043129\n",
      "Iteration 66, Batch: 47, Loss: 0.1509280502796173\n",
      "Iteration 66, Batch: 48, Loss: 0.15009024739265442\n",
      "Iteration 66, Batch: 49, Loss: 0.12426126003265381\n",
      "Iteration 67, Batch: 0, Loss: 5.227823567111045e-05\n",
      "Iteration 67, Batch: 1, Loss: 3.6041848943568766e-05\n",
      "Iteration 67, Batch: 2, Loss: 1.169978168036323e-05\n",
      "Iteration 67, Batch: 3, Loss: 1.2417383913998492e-05\n",
      "Iteration 67, Batch: 4, Loss: 2.337736077606678e-05\n",
      "Iteration 67, Batch: 5, Loss: 2.6852570954361e-05\n",
      "Iteration 67, Batch: 6, Loss: 1.529103610664606e-05\n",
      "Iteration 67, Batch: 7, Loss: 8.235857421823312e-06\n",
      "Iteration 67, Batch: 8, Loss: 1.1193869795533828e-05\n",
      "Iteration 67, Batch: 9, Loss: 1.939854337251745e-05\n",
      "Iteration 67, Batch: 10, Loss: 1.5237051229632925e-05\n",
      "Iteration 67, Batch: 11, Loss: 6.081560513848672e-06\n",
      "Iteration 67, Batch: 12, Loss: 5.46431465409114e-06\n",
      "Iteration 67, Batch: 13, Loss: 0.001991341356188059\n",
      "Iteration 67, Batch: 14, Loss: 1.7438240320188925e-05\n",
      "Iteration 67, Batch: 15, Loss: 1.7268945157411508e-05\n",
      "Iteration 67, Batch: 16, Loss: 6.64874187350506e-06\n",
      "Iteration 67, Batch: 17, Loss: 3.224488978048612e-07\n",
      "Iteration 67, Batch: 18, Loss: 5.211292773310561e-06\n",
      "Iteration 67, Batch: 19, Loss: 1.2192301255709026e-05\n",
      "Iteration 67, Batch: 20, Loss: 6.967343779251678e-06\n",
      "Iteration 67, Batch: 21, Loss: 2.8543759071908426e-06\n",
      "Iteration 67, Batch: 22, Loss: 1.0107866046382696e-06\n",
      "Iteration 67, Batch: 23, Loss: 2.412641833871021e-06\n",
      "Iteration 67, Batch: 24, Loss: 4.696672021964332e-06\n",
      "Iteration 67, Batch: 25, Loss: 3.8056284665799467e-06\n",
      "Iteration 67, Batch: 26, Loss: 2.022207581831026e-06\n",
      "Iteration 67, Batch: 27, Loss: 1.0796486549224937e-06\n",
      "Iteration 67, Batch: 28, Loss: 1.8751177321973955e-06\n",
      "Iteration 67, Batch: 29, Loss: 3.850858320220141e-06\n",
      "Iteration 67, Batch: 30, Loss: 1.489774945184763e-06\n",
      "Iteration 67, Batch: 31, Loss: 8.761453500483185e-07\n",
      "Iteration 67, Batch: 32, Loss: 1.1870049547724193e-06\n",
      "Iteration 67, Batch: 33, Loss: 1.7652124597589136e-06\n",
      "Iteration 67, Batch: 34, Loss: 2.2775932393415133e-06\n",
      "Iteration 67, Batch: 35, Loss: 1.6520100416528294e-06\n",
      "Iteration 67, Batch: 36, Loss: 6.114647703725495e-07\n",
      "Iteration 67, Batch: 37, Loss: 6.619913506256125e-07\n",
      "Iteration 67, Batch: 38, Loss: 1.437059495401627e-06\n",
      "Iteration 67, Batch: 39, Loss: 1.89889840385149e-06\n",
      "Iteration 67, Batch: 40, Loss: 8.723513360564539e-07\n",
      "Iteration 67, Batch: 41, Loss: 9.640654070608434e-07\n",
      "Iteration 67, Batch: 42, Loss: 6.160259431453596e-07\n",
      "Iteration 67, Batch: 43, Loss: 1.5528371477557812e-06\n",
      "Iteration 67, Batch: 44, Loss: 1.6780270470917458e-06\n",
      "Iteration 67, Batch: 45, Loss: 1.4226561688701622e-06\n",
      "Iteration 67, Batch: 46, Loss: 7.920979783193616e-07\n",
      "Iteration 67, Batch: 47, Loss: 5.117532850817952e-07\n",
      "Iteration 67, Batch: 48, Loss: 1.1532599728525383e-06\n",
      "Iteration 67, Batch: 49, Loss: 1.1580302725633373e-06\n",
      "Iteration 68, Batch: 0, Loss: 0.15936443209648132\n",
      "Iteration 68, Batch: 1, Loss: 0.10753574967384338\n",
      "Iteration 68, Batch: 2, Loss: 0.1485186517238617\n",
      "Iteration 68, Batch: 3, Loss: 0.15912839770317078\n",
      "Iteration 68, Batch: 4, Loss: 0.12916554510593414\n",
      "Iteration 68, Batch: 5, Loss: 0.1206178292632103\n",
      "Iteration 68, Batch: 6, Loss: 0.09838802367448807\n",
      "Iteration 68, Batch: 7, Loss: 0.11008460819721222\n",
      "Iteration 68, Batch: 8, Loss: 0.16826263070106506\n",
      "Iteration 68, Batch: 9, Loss: 0.13952936232089996\n",
      "Iteration 68, Batch: 10, Loss: 0.10772763937711716\n",
      "Iteration 68, Batch: 11, Loss: 0.11303102225065231\n",
      "Iteration 68, Batch: 12, Loss: 0.134532630443573\n",
      "Iteration 68, Batch: 13, Loss: 0.09938404709100723\n",
      "Iteration 68, Batch: 14, Loss: 0.10874072462320328\n",
      "Iteration 68, Batch: 15, Loss: 0.12060607969760895\n",
      "Iteration 68, Batch: 16, Loss: 0.14758072793483734\n",
      "Iteration 68, Batch: 17, Loss: 0.10221230238676071\n",
      "Iteration 68, Batch: 18, Loss: 0.11386045068502426\n",
      "Iteration 68, Batch: 19, Loss: 0.11460079997777939\n",
      "Iteration 68, Batch: 20, Loss: 0.12189456075429916\n",
      "Iteration 68, Batch: 21, Loss: 0.10248811542987823\n",
      "Iteration 68, Batch: 22, Loss: 0.1047426387667656\n",
      "Iteration 68, Batch: 23, Loss: 0.10951295495033264\n",
      "Iteration 68, Batch: 24, Loss: 0.14132992923259735\n",
      "Iteration 68, Batch: 25, Loss: 0.13475683331489563\n",
      "Iteration 68, Batch: 26, Loss: 0.09816792607307434\n",
      "Iteration 68, Batch: 27, Loss: 0.12371446192264557\n",
      "Iteration 68, Batch: 28, Loss: 0.07323848456144333\n",
      "Iteration 68, Batch: 29, Loss: 0.16349467635154724\n",
      "Iteration 68, Batch: 30, Loss: 0.0979614108800888\n",
      "Iteration 68, Batch: 31, Loss: 0.10115305334329605\n",
      "Iteration 68, Batch: 32, Loss: 0.0939212292432785\n",
      "Iteration 68, Batch: 33, Loss: 0.09975328296422958\n",
      "Iteration 68, Batch: 34, Loss: 0.13437485694885254\n",
      "Iteration 68, Batch: 35, Loss: 0.0922040343284607\n",
      "Iteration 68, Batch: 36, Loss: 0.11459892243146896\n",
      "Iteration 68, Batch: 37, Loss: 0.08458089828491211\n",
      "Iteration 68, Batch: 38, Loss: 0.14824466407299042\n",
      "Iteration 68, Batch: 39, Loss: 0.14692245423793793\n",
      "Iteration 68, Batch: 40, Loss: 0.10824231803417206\n",
      "Iteration 68, Batch: 41, Loss: 0.11283061653375626\n",
      "Iteration 68, Batch: 42, Loss: 0.10943322628736496\n",
      "Iteration 68, Batch: 43, Loss: 0.09774505347013474\n",
      "Iteration 68, Batch: 44, Loss: 0.07500126957893372\n",
      "Iteration 68, Batch: 45, Loss: 0.09795628488063812\n",
      "Iteration 68, Batch: 46, Loss: 0.12544777989387512\n",
      "Iteration 68, Batch: 47, Loss: 0.08469483256340027\n",
      "Iteration 68, Batch: 48, Loss: 0.14908844232559204\n",
      "Iteration 68, Batch: 49, Loss: 0.10587083548307419\n",
      "Iteration 69, Batch: 0, Loss: 0.000222347371163778\n",
      "Iteration 69, Batch: 1, Loss: 8.557654655305669e-05\n",
      "Iteration 69, Batch: 2, Loss: 8.15722523839213e-06\n",
      "Iteration 69, Batch: 3, Loss: 7.911516149761155e-05\n",
      "Iteration 69, Batch: 4, Loss: 0.00017785441013984382\n",
      "Iteration 69, Batch: 5, Loss: 0.00013513804879039526\n",
      "Iteration 69, Batch: 6, Loss: 3.01556119666202e-05\n",
      "Iteration 69, Batch: 7, Loss: 1.3663213394465856e-05\n",
      "Iteration 69, Batch: 8, Loss: 5.888528176001273e-05\n",
      "Iteration 69, Batch: 9, Loss: 0.00010356282291468233\n",
      "Iteration 69, Batch: 10, Loss: 8.050043106777593e-05\n",
      "Iteration 69, Batch: 11, Loss: 2.2134818209451623e-05\n",
      "Iteration 69, Batch: 12, Loss: 4.752334916702239e-06\n",
      "Iteration 69, Batch: 13, Loss: 4.8951045755529776e-05\n",
      "Iteration 69, Batch: 14, Loss: 7.455130253219977e-05\n",
      "Iteration 69, Batch: 15, Loss: 5.3648142056772485e-05\n",
      "Iteration 69, Batch: 16, Loss: 1.1002167411788832e-05\n",
      "Iteration 69, Batch: 17, Loss: 1.0403159649285953e-05\n",
      "Iteration 69, Batch: 18, Loss: 3.6590532545233145e-05\n",
      "Iteration 69, Batch: 19, Loss: 4.931729563395493e-05\n",
      "Iteration 69, Batch: 20, Loss: 5.113204679219052e-05\n",
      "Iteration 69, Batch: 21, Loss: 9.323999620391987e-06\n",
      "Iteration 69, Batch: 22, Loss: 4.794751475856174e-06\n",
      "Iteration 69, Batch: 23, Loss: 1.585271820658818e-05\n",
      "Iteration 69, Batch: 24, Loss: 2.9708220608881675e-05\n",
      "Iteration 69, Batch: 25, Loss: 1.6076211977633648e-05\n",
      "Iteration 69, Batch: 26, Loss: 6.492570264526876e-06\n",
      "Iteration 69, Batch: 27, Loss: 5.814980795548763e-06\n",
      "Iteration 69, Batch: 28, Loss: 1.4595398170058616e-05\n",
      "Iteration 69, Batch: 29, Loss: 1.8156260921387002e-05\n",
      "Iteration 69, Batch: 30, Loss: 1.3026431588514242e-05\n",
      "Iteration 69, Batch: 31, Loss: 6.2944482124294154e-06\n",
      "Iteration 69, Batch: 32, Loss: 6.04685055805021e-06\n",
      "Iteration 69, Batch: 33, Loss: 1.0468922482687049e-05\n",
      "Iteration 69, Batch: 34, Loss: 1.2883488125225995e-05\n",
      "Iteration 69, Batch: 35, Loss: 8.781644282862544e-06\n",
      "Iteration 69, Batch: 36, Loss: 4.835040726902662e-06\n",
      "Iteration 69, Batch: 37, Loss: 5.506649358721916e-06\n",
      "Iteration 69, Batch: 38, Loss: 8.413123396167066e-06\n",
      "Iteration 69, Batch: 39, Loss: 8.326319402840454e-06\n",
      "Iteration 69, Batch: 40, Loss: 5.519057140190853e-06\n",
      "Iteration 69, Batch: 41, Loss: 3.3465682918176753e-06\n",
      "Iteration 69, Batch: 42, Loss: 5.0918229135277215e-06\n",
      "Iteration 69, Batch: 43, Loss: 6.844633389846422e-06\n",
      "Iteration 69, Batch: 44, Loss: 6.144866347312927e-06\n",
      "Iteration 69, Batch: 45, Loss: 2.8908834792673588e-06\n",
      "Iteration 69, Batch: 46, Loss: 2.2969895780988736e-06\n",
      "Iteration 69, Batch: 47, Loss: 3.4328240872127935e-06\n",
      "Iteration 69, Batch: 48, Loss: 4.417554009705782e-06\n",
      "Iteration 69, Batch: 49, Loss: 3.0312564831547206e-06\n",
      "Iteration 70, Batch: 0, Loss: 0.1294725239276886\n",
      "Iteration 70, Batch: 1, Loss: 0.14432859420776367\n",
      "Iteration 70, Batch: 2, Loss: 0.09106561541557312\n",
      "Iteration 70, Batch: 3, Loss: 0.10229595005512238\n",
      "Iteration 70, Batch: 4, Loss: 0.09312068670988083\n",
      "Iteration 70, Batch: 5, Loss: 0.11068938672542572\n",
      "Iteration 70, Batch: 6, Loss: 0.16131047904491425\n",
      "Iteration 70, Batch: 7, Loss: 0.1216295138001442\n",
      "Iteration 70, Batch: 8, Loss: 0.110837422311306\n",
      "Iteration 70, Batch: 9, Loss: 0.11553546786308289\n",
      "Iteration 70, Batch: 10, Loss: 0.10049602389335632\n",
      "Iteration 70, Batch: 11, Loss: 0.13466517627239227\n",
      "Iteration 70, Batch: 12, Loss: 0.110780730843544\n",
      "Iteration 70, Batch: 13, Loss: 0.10978955775499344\n",
      "Iteration 70, Batch: 14, Loss: 0.10546256601810455\n",
      "Iteration 70, Batch: 15, Loss: 0.14415627717971802\n",
      "Iteration 70, Batch: 16, Loss: 0.07464631646871567\n",
      "Iteration 70, Batch: 17, Loss: 0.09837526082992554\n",
      "Iteration 70, Batch: 18, Loss: 0.1161830946803093\n",
      "Iteration 70, Batch: 19, Loss: 0.1047324612736702\n",
      "Iteration 70, Batch: 20, Loss: 0.13249771296977997\n",
      "Iteration 70, Batch: 21, Loss: 0.11952009797096252\n",
      "Iteration 70, Batch: 22, Loss: 0.12883488833904266\n",
      "Iteration 70, Batch: 23, Loss: 0.10958246886730194\n",
      "Iteration 70, Batch: 24, Loss: 0.11909154802560806\n",
      "Iteration 70, Batch: 25, Loss: 0.10977538675069809\n",
      "Iteration 70, Batch: 26, Loss: 0.0734018087387085\n",
      "Iteration 70, Batch: 27, Loss: 0.1279013454914093\n",
      "Iteration 70, Batch: 28, Loss: 0.1308434009552002\n",
      "Iteration 70, Batch: 29, Loss: 0.10589063167572021\n",
      "Iteration 70, Batch: 30, Loss: 0.1414651870727539\n",
      "Iteration 70, Batch: 31, Loss: 0.09684666246175766\n",
      "Iteration 70, Batch: 32, Loss: 0.07916494458913803\n",
      "Iteration 70, Batch: 33, Loss: 0.09361694008111954\n",
      "Iteration 70, Batch: 34, Loss: 0.10429252684116364\n",
      "Iteration 70, Batch: 35, Loss: 0.11504876613616943\n",
      "Iteration 70, Batch: 36, Loss: 0.12509073317050934\n",
      "Iteration 70, Batch: 37, Loss: 0.10736024379730225\n",
      "Iteration 70, Batch: 38, Loss: 0.09218939393758774\n",
      "Iteration 70, Batch: 39, Loss: 0.1286044716835022\n",
      "Iteration 70, Batch: 40, Loss: 0.1017552986741066\n",
      "Iteration 70, Batch: 41, Loss: 0.11288118362426758\n",
      "Iteration 70, Batch: 42, Loss: 0.11690022051334381\n",
      "Iteration 70, Batch: 43, Loss: 0.07130102068185806\n",
      "Iteration 70, Batch: 44, Loss: 0.09544143825769424\n",
      "Iteration 70, Batch: 45, Loss: 0.1588057428598404\n",
      "Iteration 70, Batch: 46, Loss: 0.1338280886411667\n",
      "Iteration 70, Batch: 47, Loss: 0.11330192536115646\n",
      "Iteration 70, Batch: 48, Loss: 0.1069013774394989\n",
      "Iteration 70, Batch: 49, Loss: 0.1304192990064621\n",
      "Iteration 71, Batch: 0, Loss: 4.382224506116472e-05\n",
      "Iteration 71, Batch: 1, Loss: 1.0131576345884241e-05\n",
      "Iteration 71, Batch: 2, Loss: 2.7070070700574433e-06\n",
      "Iteration 71, Batch: 3, Loss: 2.1007655959692784e-05\n",
      "Iteration 71, Batch: 4, Loss: 3.290294989710674e-05\n",
      "Iteration 71, Batch: 5, Loss: 2.2528221961692907e-05\n",
      "Iteration 71, Batch: 6, Loss: 7.607677616761066e-06\n",
      "Iteration 71, Batch: 7, Loss: 1.6312956176989246e-06\n",
      "Iteration 71, Batch: 8, Loss: 1.2235473150212783e-05\n",
      "Iteration 71, Batch: 9, Loss: 2.657987533893902e-05\n",
      "Iteration 71, Batch: 10, Loss: 1.1057905794586986e-05\n",
      "Iteration 71, Batch: 11, Loss: 3.144157062706654e-06\n",
      "Iteration 71, Batch: 12, Loss: 2.125616219927906e-06\n",
      "Iteration 71, Batch: 13, Loss: 8.668175723869354e-06\n",
      "Iteration 71, Batch: 14, Loss: 1.0554339496593457e-05\n",
      "Iteration 71, Batch: 15, Loss: 7.473307505279081e-06\n",
      "Iteration 71, Batch: 16, Loss: 2.4990845304273535e-06\n",
      "Iteration 71, Batch: 17, Loss: 2.0152249362581642e-06\n",
      "Iteration 71, Batch: 18, Loss: 5.185686859476846e-06\n",
      "Iteration 71, Batch: 19, Loss: 7.4399172262928914e-06\n",
      "Iteration 71, Batch: 20, Loss: 4.454438112588832e-06\n",
      "Iteration 71, Batch: 21, Loss: 9.234931894752663e-07\n",
      "Iteration 71, Batch: 22, Loss: 1.6134531506395433e-06\n",
      "Iteration 71, Batch: 23, Loss: 4.249775429343572e-06\n",
      "Iteration 71, Batch: 24, Loss: 4.924672794004437e-06\n",
      "Iteration 71, Batch: 25, Loss: 2.9998789159435546e-06\n",
      "Iteration 71, Batch: 26, Loss: 8.607772201685293e-07\n",
      "Iteration 71, Batch: 27, Loss: 1.1877477845700923e-06\n",
      "Iteration 71, Batch: 28, Loss: 2.7165945084561827e-06\n",
      "Iteration 71, Batch: 29, Loss: 3.3745720884326147e-06\n",
      "Iteration 71, Batch: 30, Loss: 2.0633024178096093e-06\n",
      "Iteration 71, Batch: 31, Loss: 5.033620595895627e-07\n",
      "Iteration 71, Batch: 32, Loss: 1.4450721437242464e-06\n",
      "Iteration 71, Batch: 33, Loss: 1.974538463400677e-06\n",
      "Iteration 71, Batch: 34, Loss: 2.04218554245017e-06\n",
      "Iteration 71, Batch: 35, Loss: 1.213943960465258e-06\n",
      "Iteration 71, Batch: 36, Loss: 7.501092795791919e-07\n",
      "Iteration 71, Batch: 37, Loss: 8.781981932770577e-07\n",
      "Iteration 71, Batch: 38, Loss: 1.3000599210499786e-06\n",
      "Iteration 71, Batch: 39, Loss: 1.6987202116069966e-06\n",
      "Iteration 71, Batch: 40, Loss: 9.499024145043222e-07\n",
      "Iteration 71, Batch: 41, Loss: 5.868552079846268e-07\n",
      "Iteration 71, Batch: 42, Loss: 1.0549891840128112e-06\n",
      "Iteration 71, Batch: 43, Loss: 9.765152526597376e-07\n",
      "Iteration 71, Batch: 44, Loss: 1.0540595667407615e-06\n",
      "Iteration 71, Batch: 45, Loss: 8.569397209612362e-07\n",
      "Iteration 71, Batch: 46, Loss: 6.412810762412846e-07\n",
      "Iteration 71, Batch: 47, Loss: 7.608794589941681e-07\n",
      "Iteration 71, Batch: 48, Loss: 9.74448312263121e-07\n",
      "Iteration 71, Batch: 49, Loss: 7.276433393599291e-07\n",
      "Iteration 72, Batch: 0, Loss: 0.12391538172960281\n",
      "Iteration 72, Batch: 1, Loss: 0.1334727257490158\n",
      "Iteration 72, Batch: 2, Loss: 0.12453801929950714\n",
      "Iteration 72, Batch: 3, Loss: 0.09204023331403732\n",
      "Iteration 72, Batch: 4, Loss: 0.14268596470355988\n",
      "Iteration 72, Batch: 5, Loss: 0.11066894978284836\n",
      "Iteration 72, Batch: 6, Loss: 0.12353681027889252\n",
      "Iteration 72, Batch: 7, Loss: 0.16228456795215607\n",
      "Iteration 72, Batch: 8, Loss: 0.12623898684978485\n",
      "Iteration 72, Batch: 9, Loss: 0.13520114123821259\n",
      "Iteration 72, Batch: 10, Loss: 0.1281880885362625\n",
      "Iteration 72, Batch: 11, Loss: 0.06579127907752991\n",
      "Iteration 72, Batch: 12, Loss: 0.11495090276002884\n",
      "Iteration 72, Batch: 13, Loss: 0.13294945657253265\n",
      "Iteration 72, Batch: 14, Loss: 0.07016151398420334\n",
      "Iteration 72, Batch: 15, Loss: 0.10301107913255692\n",
      "Iteration 72, Batch: 16, Loss: 0.08634727448225021\n",
      "Iteration 72, Batch: 17, Loss: 0.12767857313156128\n",
      "Iteration 72, Batch: 18, Loss: 0.08850312978029251\n",
      "Iteration 72, Batch: 19, Loss: 0.1070244163274765\n",
      "Iteration 72, Batch: 20, Loss: 0.05134710296988487\n",
      "Iteration 72, Batch: 21, Loss: 0.1329469233751297\n",
      "Iteration 72, Batch: 22, Loss: 0.11942370980978012\n",
      "Iteration 72, Batch: 23, Loss: 0.10613077878952026\n",
      "Iteration 72, Batch: 24, Loss: 0.12733390927314758\n",
      "Iteration 72, Batch: 25, Loss: 0.08223329484462738\n",
      "Iteration 72, Batch: 26, Loss: 0.10368862003087997\n",
      "Iteration 72, Batch: 27, Loss: 0.14491666853427887\n",
      "Iteration 72, Batch: 28, Loss: 0.12010196596384048\n",
      "Iteration 72, Batch: 29, Loss: 0.10854455828666687\n",
      "Iteration 72, Batch: 30, Loss: 0.11113788187503815\n",
      "Iteration 72, Batch: 31, Loss: 0.12590625882148743\n",
      "Iteration 72, Batch: 32, Loss: 0.14055874943733215\n",
      "Iteration 72, Batch: 33, Loss: 0.08105802536010742\n",
      "Iteration 72, Batch: 34, Loss: 0.1382169872522354\n",
      "Iteration 72, Batch: 35, Loss: 0.11449714750051498\n",
      "Iteration 72, Batch: 36, Loss: 0.12185271829366684\n",
      "Iteration 72, Batch: 37, Loss: 0.09670142084360123\n",
      "Iteration 72, Batch: 38, Loss: 0.11877988278865814\n",
      "Iteration 72, Batch: 39, Loss: 0.10618962347507477\n",
      "Iteration 72, Batch: 40, Loss: 0.11665652692317963\n",
      "Iteration 72, Batch: 41, Loss: 0.12371160089969635\n",
      "Iteration 72, Batch: 42, Loss: 0.15471643209457397\n",
      "Iteration 72, Batch: 43, Loss: 0.10340980440378189\n",
      "Iteration 72, Batch: 44, Loss: 0.08856696635484695\n",
      "Iteration 72, Batch: 45, Loss: 0.09548118710517883\n",
      "Iteration 72, Batch: 46, Loss: 0.14743095636367798\n",
      "Iteration 72, Batch: 47, Loss: 0.1264267861843109\n",
      "Iteration 72, Batch: 48, Loss: 0.09835989773273468\n",
      "Iteration 72, Batch: 49, Loss: 0.1468448042869568\n",
      "Iteration 73, Batch: 0, Loss: 7.302741141756997e-05\n",
      "Iteration 73, Batch: 1, Loss: 2.6052181056002155e-05\n",
      "Iteration 73, Batch: 2, Loss: 1.5561116015305743e-05\n",
      "Iteration 73, Batch: 3, Loss: 4.352740870672278e-05\n",
      "Iteration 73, Batch: 4, Loss: 5.4955209634499624e-05\n",
      "Iteration 73, Batch: 5, Loss: 3.595254383981228e-05\n",
      "Iteration 73, Batch: 6, Loss: 1.1960693882429041e-05\n",
      "Iteration 73, Batch: 7, Loss: 6.968899924686411e-06\n",
      "Iteration 73, Batch: 8, Loss: 1.9395774870645255e-05\n",
      "Iteration 73, Batch: 9, Loss: 3.069339072681032e-05\n",
      "Iteration 73, Batch: 10, Loss: 2.252826016047038e-05\n",
      "Iteration 73, Batch: 11, Loss: 7.343992820096901e-06\n",
      "Iteration 73, Batch: 12, Loss: 5.034523837821325e-06\n",
      "Iteration 73, Batch: 13, Loss: 1.2573254025483038e-05\n",
      "Iteration 73, Batch: 14, Loss: 1.719211468298454e-05\n",
      "Iteration 73, Batch: 15, Loss: 1.3470088561007287e-05\n",
      "Iteration 73, Batch: 16, Loss: 4.001593424618477e-06\n",
      "Iteration 73, Batch: 17, Loss: 4.6561035560444e-06\n",
      "Iteration 73, Batch: 18, Loss: 1.2767638509103563e-05\n",
      "Iteration 73, Batch: 19, Loss: 1.0829008715518285e-05\n",
      "Iteration 73, Batch: 20, Loss: 6.042148470442044e-06\n",
      "Iteration 73, Batch: 21, Loss: 1.955474317583139e-06\n",
      "Iteration 73, Batch: 22, Loss: 5.255434643913759e-06\n",
      "Iteration 73, Batch: 23, Loss: 8.08745880931383e-06\n",
      "Iteration 73, Batch: 24, Loss: 6.648474936810089e-06\n",
      "Iteration 73, Batch: 25, Loss: 2.988169171658228e-06\n",
      "Iteration 73, Batch: 26, Loss: 7.979748488651239e-07\n",
      "Iteration 73, Batch: 27, Loss: 3.6395217648532707e-06\n",
      "Iteration 73, Batch: 28, Loss: 6.064648914616555e-06\n",
      "Iteration 73, Batch: 29, Loss: 3.943624051316874e-06\n",
      "Iteration 73, Batch: 30, Loss: 1.092728211915528e-06\n",
      "Iteration 73, Batch: 31, Loss: 8.959375463746255e-07\n",
      "Iteration 73, Batch: 32, Loss: 2.8222436867508804e-06\n",
      "Iteration 73, Batch: 33, Loss: 3.6248682135919807e-06\n",
      "Iteration 73, Batch: 34, Loss: 2.2335427729558432e-06\n",
      "Iteration 73, Batch: 35, Loss: 7.450188377333689e-07\n",
      "Iteration 73, Batch: 36, Loss: 1.1219897260161815e-06\n",
      "Iteration 73, Batch: 37, Loss: 2.074845497190836e-06\n",
      "Iteration 73, Batch: 38, Loss: 1.9670126221171813e-06\n",
      "Iteration 73, Batch: 39, Loss: 1.1845797871501418e-06\n",
      "Iteration 73, Batch: 40, Loss: 7.546636311417387e-07\n",
      "Iteration 73, Batch: 41, Loss: 8.382950795748911e-07\n",
      "Iteration 73, Batch: 42, Loss: 1.6839384215927566e-06\n",
      "Iteration 73, Batch: 43, Loss: 1.299238874707953e-06\n",
      "Iteration 73, Batch: 44, Loss: 8.29258453904913e-07\n",
      "Iteration 73, Batch: 45, Loss: 5.630060968542239e-07\n",
      "Iteration 73, Batch: 46, Loss: 7.987142112142465e-07\n",
      "Iteration 73, Batch: 47, Loss: 1.2129640936109354e-06\n",
      "Iteration 73, Batch: 48, Loss: 8.772082651375968e-07\n",
      "Iteration 73, Batch: 49, Loss: 6.539108312608732e-07\n",
      "Iteration 74, Batch: 0, Loss: 0.09065591543912888\n",
      "Iteration 74, Batch: 1, Loss: 0.06842423975467682\n",
      "Iteration 74, Batch: 2, Loss: 0.08930554986000061\n",
      "Iteration 74, Batch: 3, Loss: 0.11144101619720459\n",
      "Iteration 74, Batch: 4, Loss: 0.122158944606781\n",
      "Iteration 74, Batch: 5, Loss: 0.09638987481594086\n",
      "Iteration 74, Batch: 6, Loss: 0.14543473720550537\n",
      "Iteration 74, Batch: 7, Loss: 0.11896339803934097\n",
      "Iteration 74, Batch: 8, Loss: 0.10401634126901627\n",
      "Iteration 74, Batch: 9, Loss: 0.10426194965839386\n",
      "Iteration 74, Batch: 10, Loss: 0.11923286318778992\n",
      "Iteration 74, Batch: 11, Loss: 0.1382945477962494\n",
      "Iteration 74, Batch: 12, Loss: 0.11912122368812561\n",
      "Iteration 74, Batch: 13, Loss: 0.12069901078939438\n",
      "Iteration 74, Batch: 14, Loss: 0.13440768420696259\n",
      "Iteration 74, Batch: 15, Loss: 0.11671512573957443\n",
      "Iteration 74, Batch: 16, Loss: 0.10835155099630356\n",
      "Iteration 74, Batch: 17, Loss: 0.10643335431814194\n",
      "Iteration 74, Batch: 18, Loss: 0.15395893156528473\n",
      "Iteration 74, Batch: 19, Loss: 0.0943077802658081\n",
      "Iteration 74, Batch: 20, Loss: 0.10425738990306854\n",
      "Iteration 74, Batch: 21, Loss: 0.10038518905639648\n",
      "Iteration 74, Batch: 22, Loss: 0.10948018729686737\n",
      "Iteration 74, Batch: 23, Loss: 0.09424139559268951\n",
      "Iteration 74, Batch: 24, Loss: 0.08634988963603973\n",
      "Iteration 74, Batch: 25, Loss: 0.09844789654016495\n",
      "Iteration 74, Batch: 26, Loss: 0.10964855551719666\n",
      "Iteration 74, Batch: 27, Loss: 0.12060536444187164\n",
      "Iteration 74, Batch: 28, Loss: 0.10699138045310974\n",
      "Iteration 74, Batch: 29, Loss: 0.08899397403001785\n",
      "Iteration 74, Batch: 30, Loss: 0.10489838570356369\n",
      "Iteration 74, Batch: 31, Loss: 0.12619440257549286\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# Get loss from model\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m alt_flag:\n\u001b[0;32m---> 39\u001b[0m         loss \u001b[38;5;241m=\u001b[39m mc_loss_batch_simul(model, batch, time_array, n, k, time_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m205\u001b[39m, p_bad\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         loss \u001b[38;5;241m=\u001b[39m mc_loss_batch_noise_free(model, batch, n, k, predict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m/home/groups/montanar/vietvu01/SparseDiffusion/loss.py:87\u001b[0m, in \u001b[0;36mmc_loss_batch_simul\u001b[0;34m(model, batch, time_array, n, k, time_threshold, p_bad, device)\u001b[0m\n\u001b[1;32m     84\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum((out \u001b[38;5;241m-\u001b[39m batch_outer) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Return memory\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss \u001b[38;5;241m/\u001b[39m batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/diffusions/lib/python3.12/site-packages/torch/cuda/memory.py:192\u001b[0m, in \u001b[0;36mempty_cache\u001b[0;34m()\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 192\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_emptyCache()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# First fine-tuning round for n = 350. Do not touch for n = 150\n",
    "# Specify parameters\n",
    "N = 10000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Reload model\n",
    "model = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"test_2_intermediate_2.pth\", weights_only=False))\n",
    "\n",
    "# Create dataset for training\n",
    "train_data = sample_data(N, n, k, device=device)\n",
    "train_dataset = SubmatrixDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "# Specify parameters\n",
    "num_epochs = 300\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=7e-4)\n",
    "\n",
    "# When fine-tune, have to reset min_loss to take into account the new points\n",
    "min_loss = float(\"inf\")\n",
    "model_opt = deepcopy(model)\n",
    "\n",
    "# Alternating between noise and noise-free training\n",
    "alt_flag = 1\n",
    "\n",
    "for it in range(num_epochs):\n",
    "    counter = 0\n",
    "    loss_total = 0\n",
    "    for batch in train_dataloader:\n",
    "        # Get loss from model\n",
    "        if alt_flag:\n",
    "            loss = mc_loss_batch_simul(model, batch, time_array, n, k, time_threshold=205, p_bad=0.05, device=device)\n",
    "        else:\n",
    "            loss = mc_loss_batch_noise_free(model, batch, n, k, predict=False, device=device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.9)\n",
    "    \n",
    "        # Backpropagate\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Iteration {}, Batch: {}, Loss: {}\".format(it, counter, loss.item()))\n",
    "\n",
    "        # Update counter\n",
    "        counter += 1\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    # Update best model\n",
    "    with torch.no_grad():\n",
    "        if loss_total < min_loss:\n",
    "            min_loss = loss_total\n",
    "            model_opt = deepcopy(model)\n",
    "\n",
    "    # Flip\n",
    "    alt_flag = 1 - alt_flag\n",
    "\n",
    "# Save optimal model to save time for generation\n",
    "torch.save(model_opt.state_dict(), \"test_2_opt_350.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f643f20-32ad-44fd-a0bb-ff74ae392479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimal model to save time for generation\n",
    "torch.save(model_opt.state_dict(), \"test_2_opt_350.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed183c6-fa3a-46d0-b697-89a44472f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teacher-forcing?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5066d57e-8355-45b1-a761-1548c6fe41d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n"
     ]
    }
   ],
   "source": [
    "# Make test dataset\n",
    "N = 4000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "test_data = sample_data(N, n, k, device=device)\n",
    "test_dataset = SubmatrixDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=250, shuffle=True)\n",
    "\n",
    "# Load model\n",
    "model_test = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model_test.to(device)\n",
    "model_test.load_state_dict(torch.load(\"test_2_opt_350.pth\", weights_only=False))\n",
    "\n",
    "# Draw a loss curve\n",
    "loss_ls = []\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "    \n",
    "        # Evaluate model\n",
    "        for batch in test_dataloader:\n",
    "            loss_b = mc_loss_batch_fixed(model_test, batch, t, n, k, predict=True, device=device)\n",
    "            loss += loss_b.item() * batch.shape[0]\n",
    "\n",
    "            # Print\n",
    "            print(\"Batch {} finished!\".format(counter))\n",
    "            counter += 1\n",
    "\n",
    "        # Store loss\n",
    "        loss_ls.append(loss / len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec8d5409-8599-4559-ad27-ef25641e426c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n"
     ]
    }
   ],
   "source": [
    "# Make test dataset\n",
    "N = 4000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "test_data = sample_data(N, n, k, device=device)\n",
    "test_dataset = SubmatrixDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=250, shuffle=True)\n",
    "\n",
    "# Load model\n",
    "model_student = TestMPNN_3_old(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model_student.to(device)\n",
    "model_student.load_state_dict(torch.load(\"test_2_opt_350_ft.pth\", weights_only=False))\n",
    "\n",
    "# Draw a loss curve\n",
    "loss_ls_st = []\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "    \n",
    "        # Evaluate model\n",
    "        for batch in test_dataloader:\n",
    "            loss_b = mc_loss_batch_fixed(model_student, batch, t, n, k, predict=True, device=device)\n",
    "            loss += loss_b.item() * batch.shape[0]\n",
    "\n",
    "            # Print\n",
    "            print(\"Batch {} finished!\".format(counter))\n",
    "            counter += 1\n",
    "\n",
    "        # Store loss\n",
    "        loss_ls_st.append(loss / len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9f4f13f-9014-4de9-b304-5c9e392b12fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n"
     ]
    }
   ],
   "source": [
    "# Make test dataset\n",
    "N = 4000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "test_data = sample_data(N, n, k, device=device)\n",
    "test_dataset = SubmatrixDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=250, shuffle=True)\n",
    "\n",
    "# Load model\n",
    "model_test = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model_test.to(device)\n",
    "model_test.load_state_dict(torch.load(\"test_2_opt_350.pth\", weights_only=False))\n",
    "\n",
    "# Draw a loss curve\n",
    "loss_ls = []\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "    \n",
    "        # Evaluate model\n",
    "        for batch in test_dataloader:\n",
    "            loss_b = mc_loss_batch_fixed(model_student, batch, t, n, k, predict=True, device=device)\n",
    "            loss += loss_b.item() * batch.shape[0]\n",
    "\n",
    "            # Print\n",
    "            print(\"Batch {} finished!\".format(counter))\n",
    "            counter += 1\n",
    "\n",
    "        # Store loss\n",
    "        loss_ls.append(loss / len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5516f4d-7329-42a4-85b8-f860ee6482bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07c6e388-eee7-4147-b341-dcdfc6a679c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18648f10-249b-45d4-841b-ab90b7f560bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n"
     ]
    }
   ],
   "source": [
    "# Make test dataset\n",
    "N = 4000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "test_data = sample_data(N, n, k, device=device)\n",
    "test_dataset = SubmatrixDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=250, shuffle=True)\n",
    "\n",
    "# Load model\n",
    "model_350 = TestMPNN_3_a(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model_350.to(device)\n",
    "model_350.load_state_dict(torch.load(\"model_weights_opt_350.pth\", weights_only=False))\n",
    "\n",
    "# Draw a loss curve\n",
    "loss_ls_1 = []\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "    \n",
    "        # Evaluate model\n",
    "        for batch in test_dataloader:\n",
    "            loss_b = mc_loss_batch_fixed(model_350, batch, t, n, k, predict=True, device=device)\n",
    "            loss += loss_b.item() * batch.shape[0]\n",
    "\n",
    "            # Print\n",
    "            print(\"Batch {} finished!\".format(counter))\n",
    "            counter += 1\n",
    "\n",
    "        # Store loss\n",
    "        loss_ls_1.append(loss / len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad89e544-f5a5-49ff-a999-af4a98ef2a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time_array.detach().cpu().numpy() / n, np.array(loss_ls), label=\"Graph NN Denoiser\")\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, np.array(loss_ls_1), label=\"Graph NN Denoiser pre-trained\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61f4a6bf-4f7e-4dff-a53d-748d65599e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.17142857142857143)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(loss_ls) <= np.array(loss_ls_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f6c6c2d2-c8b9-41e2-9cbc-1e63abda8514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "0.02166569186374545\n"
     ]
    }
   ],
   "source": [
    "# Make test dataset\n",
    "N = 4000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "test_data = sample_data(N, n, k, device=device)\n",
    "test_dataset = SubmatrixDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=250, shuffle=True)\n",
    "\"\"\"\n",
    "# Load model\n",
    "model_test = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model_test.to(device)\n",
    "model_test.load_state_dict(torch.load(\"test_2_opt_350.pth\", weights_only=False))\n",
    "\"\"\"\n",
    "# Draw a loss curve\n",
    "loss_ls = []\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(1):\n",
    "        loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "        t = 10000\n",
    "    \n",
    "        # Evaluate model\n",
    "        for batch in test_dataloader:\n",
    "            loss_b = mc_loss_batch_fixed(model_350, batch, t, n, k, predict=True, device=device)\n",
    "            loss += loss_b.item() * batch.shape[0]\n",
    "\n",
    "            # Print\n",
    "            print(\"Batch {} finished!\".format(counter))\n",
    "            counter += 1\n",
    "\n",
    "        # Store loss\n",
    "        print(loss / len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e4b0f33e-58bc-41cc-87a8-8a4adb2bde09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3771, -0.0632],\n",
      "        [ 0.7786,  0.0814],\n",
      "        [-0.7948, -0.1260],\n",
      "        [-0.4099,  0.2299]], device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1720, -0.2952, -0.0098, -0.1209], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.2380, -1.1789,  1.2404, -0.2389]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0262], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model_test.tiny_decoder.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e83b773a-3d22-4292-8899-d0d56f1e1ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([1.1168], device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.log_scale"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66cea66-4e9b-4337-a73e-71675c831e29",
   "metadata": {},
   "source": [
    "**3.2. Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff5f5bd-7473-41da-86a3-c8dc38ef722d",
   "metadata": {},
   "source": [
    "**Train on model, first round**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb5f98d6-97a8-49c0-a898-8105ae684eb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 9\n",
      "Iteration 236, Batch: 0, Loss: 0.05849149450659752\n",
      "Iteration 236, Batch: 1, Loss: 0.039656538516283035\n",
      "Iteration 236, Batch: 2, Loss: 0.03612799569964409\n",
      "Iteration 236, Batch: 3, Loss: 0.04254746809601784\n",
      "Iteration 236, Batch: 4, Loss: 0.04844958335161209\n",
      "Iteration 236, Batch: 5, Loss: 0.041168466210365295\n",
      "Iteration 236, Batch: 6, Loss: 0.05097867548465729\n",
      "Iteration 236, Batch: 7, Loss: 0.05859184265136719\n",
      "Iteration 236, Batch: 8, Loss: 0.03594555705785751\n",
      "Iteration 236, Batch: 9, Loss: 0.06686560064554214\n",
      "Iteration 236, Batch: 10, Loss: 0.07368442416191101\n",
      "Iteration 236, Batch: 11, Loss: 0.03382819518446922\n",
      "Iteration 236, Batch: 12, Loss: 0.05735277011990547\n",
      "Iteration 236, Batch: 13, Loss: 0.07879428565502167\n",
      "Iteration 236, Batch: 14, Loss: 0.05523345246911049\n",
      "Iteration 236, Batch: 15, Loss: 0.052977900952100754\n",
      "Iteration 236, Batch: 16, Loss: 0.05437827110290527\n",
      "Iteration 236, Batch: 17, Loss: 0.02929695136845112\n",
      "Iteration 236, Batch: 18, Loss: 0.0609140507876873\n",
      "Iteration 236, Batch: 19, Loss: 0.07129751890897751\n",
      "Iteration 236, Batch: 20, Loss: 0.08641092479228973\n",
      "Iteration 236, Batch: 21, Loss: 0.06267587840557098\n",
      "Iteration 236, Batch: 22, Loss: 0.11731431633234024\n",
      "Iteration 236, Batch: 23, Loss: 0.11181548982858658\n",
      "Iteration 236, Batch: 24, Loss: 0.09439504146575928\n",
      "Iteration 236, Batch: 25, Loss: 0.10064489394426346\n",
      "Iteration 236, Batch: 26, Loss: 0.07042527198791504\n",
      "Iteration 236, Batch: 27, Loss: 0.05948140472173691\n",
      "Iteration 236, Batch: 28, Loss: 0.07957728952169418\n",
      "Iteration 236, Batch: 29, Loss: 0.05874129757285118\n",
      "Iteration 236, Batch: 30, Loss: 0.0529652014374733\n",
      "Iteration 236, Batch: 31, Loss: 0.03245149180293083\n",
      "Iteration 236, Batch: 32, Loss: 0.049082133919000626\n",
      "Iteration 236, Batch: 33, Loss: 0.07187813520431519\n",
      "Iteration 236, Batch: 34, Loss: 0.04932069033384323\n",
      "Iteration 236, Batch: 35, Loss: 0.053797900676727295\n",
      "Iteration 236, Batch: 36, Loss: 0.04092218726873398\n",
      "Iteration 236, Batch: 37, Loss: 0.05059758946299553\n",
      "Iteration 236, Batch: 38, Loss: 0.04651908949017525\n",
      "Iteration 236, Batch: 39, Loss: 0.06892788410186768\n",
      "Iteration 236, Batch: 40, Loss: 0.0876738652586937\n",
      "Iteration 236, Batch: 41, Loss: 0.07785609364509583\n",
      "Iteration 236, Batch: 42, Loss: 0.06620190292596817\n",
      "Iteration 236, Batch: 43, Loss: 0.048435915261507034\n",
      "Iteration 236, Batch: 44, Loss: 0.05256957933306694\n",
      "Iteration 236, Batch: 45, Loss: 0.033807214349508286\n",
      "Iteration 236, Batch: 46, Loss: 0.04685280844569206\n",
      "Iteration 236, Batch: 47, Loss: 0.057784710079431534\n",
      "Iteration 236, Batch: 48, Loss: 0.05151110887527466\n",
      "Iteration 236, Batch: 49, Loss: 0.045274533331394196\n",
      "Number of layers: 9\n",
      "Iteration 237, Batch: 0, Loss: 0.04682399705052376\n",
      "Iteration 237, Batch: 1, Loss: 0.03245343267917633\n",
      "Iteration 237, Batch: 2, Loss: 0.038550782948732376\n",
      "Iteration 237, Batch: 3, Loss: 0.059007082134485245\n",
      "Iteration 237, Batch: 4, Loss: 0.036485448479652405\n",
      "Iteration 237, Batch: 5, Loss: 0.0669456496834755\n",
      "Iteration 237, Batch: 6, Loss: 0.038340263068675995\n",
      "Iteration 237, Batch: 7, Loss: 0.04560665041208267\n",
      "Iteration 237, Batch: 8, Loss: 0.07369976490736008\n",
      "Iteration 237, Batch: 9, Loss: 0.06457885354757309\n",
      "Iteration 237, Batch: 10, Loss: 0.04950445145368576\n",
      "Iteration 237, Batch: 11, Loss: 0.06547445803880692\n",
      "Iteration 237, Batch: 12, Loss: 0.07147233933210373\n",
      "Iteration 237, Batch: 13, Loss: 0.051224928349256516\n",
      "Iteration 237, Batch: 14, Loss: 0.06391595304012299\n",
      "Iteration 237, Batch: 15, Loss: 0.07695603370666504\n",
      "Iteration 237, Batch: 16, Loss: 0.06374343484640121\n",
      "Iteration 237, Batch: 17, Loss: 0.05667433142662048\n",
      "Iteration 237, Batch: 18, Loss: 0.0650283545255661\n",
      "Iteration 237, Batch: 19, Loss: 0.08684913069009781\n",
      "Iteration 237, Batch: 20, Loss: 0.08779764175415039\n",
      "Iteration 237, Batch: 21, Loss: 0.07135152071714401\n",
      "Iteration 237, Batch: 22, Loss: 0.049222543835639954\n",
      "Iteration 237, Batch: 23, Loss: 0.056570231914520264\n",
      "Iteration 237, Batch: 24, Loss: 0.05320821329951286\n",
      "Iteration 237, Batch: 25, Loss: 0.03730323910713196\n",
      "Iteration 237, Batch: 26, Loss: 0.0480283759534359\n",
      "Iteration 237, Batch: 27, Loss: 0.05133074149489403\n",
      "Iteration 237, Batch: 28, Loss: 0.04908289015293121\n",
      "Iteration 237, Batch: 29, Loss: 0.0773303285241127\n",
      "Iteration 237, Batch: 30, Loss: 0.05210503563284874\n",
      "Iteration 237, Batch: 31, Loss: 0.06295422464609146\n",
      "Iteration 237, Batch: 32, Loss: 0.05819740518927574\n",
      "Iteration 237, Batch: 33, Loss: 0.05460532009601593\n",
      "Iteration 237, Batch: 34, Loss: 0.02793843485414982\n",
      "Iteration 237, Batch: 35, Loss: 0.05787449702620506\n",
      "Iteration 237, Batch: 36, Loss: 0.043175019323825836\n",
      "Iteration 237, Batch: 37, Loss: 0.05857028439640999\n",
      "Iteration 237, Batch: 38, Loss: 0.05128158628940582\n",
      "Iteration 237, Batch: 39, Loss: 0.05150248482823372\n",
      "Iteration 237, Batch: 40, Loss: 0.046498898416757584\n",
      "Iteration 237, Batch: 41, Loss: 0.07992921769618988\n",
      "Iteration 237, Batch: 42, Loss: 0.04229435697197914\n",
      "Iteration 237, Batch: 43, Loss: 0.03355683386325836\n",
      "Iteration 237, Batch: 44, Loss: 0.04551668092608452\n",
      "Iteration 237, Batch: 45, Loss: 0.05521757900714874\n",
      "Iteration 237, Batch: 46, Loss: 0.04945214092731476\n",
      "Iteration 237, Batch: 47, Loss: 0.038698889315128326\n",
      "Iteration 237, Batch: 48, Loss: 0.041358351707458496\n",
      "Iteration 237, Batch: 49, Loss: 0.050562333315610886\n",
      "Number of layers: 9\n",
      "Iteration 238, Batch: 0, Loss: 0.05191746726632118\n",
      "Iteration 238, Batch: 1, Loss: 0.05059000849723816\n",
      "Iteration 238, Batch: 2, Loss: 0.04689197987318039\n",
      "Iteration 238, Batch: 3, Loss: 0.05444848909974098\n",
      "Iteration 238, Batch: 4, Loss: 0.0334177128970623\n",
      "Iteration 238, Batch: 5, Loss: 0.0722787082195282\n",
      "Iteration 238, Batch: 6, Loss: 0.04866671562194824\n",
      "Iteration 238, Batch: 7, Loss: 0.051118314266204834\n",
      "Iteration 238, Batch: 8, Loss: 0.051092423498630524\n",
      "Iteration 238, Batch: 9, Loss: 0.09100393205881119\n",
      "Iteration 238, Batch: 10, Loss: 0.060408204793930054\n",
      "Iteration 238, Batch: 11, Loss: 0.06957121938467026\n",
      "Iteration 238, Batch: 12, Loss: 0.07475879043340683\n",
      "Iteration 238, Batch: 13, Loss: 0.08477159589529037\n",
      "Iteration 238, Batch: 14, Loss: 0.07782828062772751\n",
      "Iteration 238, Batch: 15, Loss: 0.08859175443649292\n",
      "Iteration 238, Batch: 16, Loss: 0.059690721333026886\n",
      "Iteration 238, Batch: 17, Loss: 0.07951029390096664\n",
      "Iteration 238, Batch: 18, Loss: 0.04806608706712723\n",
      "Iteration 238, Batch: 19, Loss: 0.04906294867396355\n",
      "Iteration 238, Batch: 20, Loss: 0.05709075927734375\n",
      "Iteration 238, Batch: 21, Loss: 0.07121048122644424\n",
      "Iteration 238, Batch: 22, Loss: 0.09769725799560547\n",
      "Iteration 238, Batch: 23, Loss: 0.12295495718717575\n",
      "Iteration 238, Batch: 24, Loss: 0.10613855719566345\n",
      "Iteration 238, Batch: 25, Loss: 0.11136898398399353\n",
      "Iteration 238, Batch: 26, Loss: 0.0829242467880249\n",
      "Iteration 238, Batch: 27, Loss: 0.05700025334954262\n",
      "Iteration 238, Batch: 28, Loss: 0.05457810312509537\n",
      "Iteration 238, Batch: 29, Loss: 0.04842030256986618\n",
      "Iteration 238, Batch: 30, Loss: 0.07384349405765533\n",
      "Iteration 238, Batch: 31, Loss: 0.04218762367963791\n",
      "Iteration 238, Batch: 32, Loss: 0.07352572679519653\n",
      "Iteration 238, Batch: 33, Loss: 0.05418640375137329\n",
      "Iteration 238, Batch: 34, Loss: 0.08805973082780838\n",
      "Iteration 238, Batch: 35, Loss: 0.061713263392448425\n",
      "Iteration 238, Batch: 36, Loss: 0.05694114416837692\n",
      "Iteration 238, Batch: 37, Loss: 0.05378905311226845\n",
      "Iteration 238, Batch: 38, Loss: 0.047577280551195145\n",
      "Iteration 238, Batch: 39, Loss: 0.05090610682964325\n",
      "Iteration 238, Batch: 40, Loss: 0.05886200815439224\n",
      "Iteration 238, Batch: 41, Loss: 0.0398348905146122\n",
      "Iteration 238, Batch: 42, Loss: 0.06606810539960861\n",
      "Iteration 238, Batch: 43, Loss: 0.051763731986284256\n",
      "Iteration 238, Batch: 44, Loss: 0.048371572047472\n",
      "Iteration 238, Batch: 45, Loss: 0.08256017416715622\n",
      "Iteration 238, Batch: 46, Loss: 0.10509675741195679\n",
      "Iteration 238, Batch: 47, Loss: 0.04721832647919655\n",
      "Iteration 238, Batch: 48, Loss: 0.044402558356523514\n",
      "Iteration 238, Batch: 49, Loss: 0.06759165972471237\n",
      "Number of layers: 9\n",
      "Iteration 239, Batch: 0, Loss: 0.05708969756960869\n",
      "Iteration 239, Batch: 1, Loss: 0.06419448554515839\n",
      "Iteration 239, Batch: 2, Loss: 0.03518593683838844\n",
      "Iteration 239, Batch: 3, Loss: 0.08178719133138657\n",
      "Iteration 239, Batch: 4, Loss: 0.08491798490285873\n",
      "Iteration 239, Batch: 5, Loss: 0.08419709652662277\n",
      "Iteration 239, Batch: 6, Loss: 0.10193965584039688\n",
      "Iteration 239, Batch: 7, Loss: 0.06720862537622452\n",
      "Iteration 239, Batch: 8, Loss: 0.07751254737377167\n",
      "Iteration 239, Batch: 9, Loss: 0.056622155010700226\n",
      "Iteration 239, Batch: 10, Loss: 0.05470079183578491\n",
      "Iteration 239, Batch: 11, Loss: 0.07771386206150055\n",
      "Iteration 239, Batch: 12, Loss: 0.07342284917831421\n",
      "Iteration 239, Batch: 13, Loss: 0.06120142713189125\n",
      "Iteration 239, Batch: 14, Loss: 0.0765150785446167\n",
      "Iteration 239, Batch: 15, Loss: 0.06037374213337898\n",
      "Iteration 239, Batch: 16, Loss: 0.08575917780399323\n",
      "Iteration 239, Batch: 17, Loss: 0.08145969361066818\n",
      "Iteration 239, Batch: 18, Loss: 0.07637488096952438\n",
      "Iteration 239, Batch: 19, Loss: 0.056828249245882034\n",
      "Iteration 239, Batch: 20, Loss: 0.08009060472249985\n",
      "Iteration 239, Batch: 21, Loss: 0.062146350741386414\n",
      "Iteration 239, Batch: 22, Loss: 0.08647928386926651\n",
      "Iteration 239, Batch: 23, Loss: 0.08260608464479446\n",
      "Iteration 239, Batch: 24, Loss: 0.07604687660932541\n",
      "Iteration 239, Batch: 25, Loss: 0.059321753680706024\n",
      "Iteration 239, Batch: 26, Loss: 0.07762862741947174\n",
      "Iteration 239, Batch: 27, Loss: 0.07397769391536713\n",
      "Iteration 239, Batch: 28, Loss: 0.0961291640996933\n",
      "Iteration 239, Batch: 29, Loss: 0.07273666560649872\n",
      "Iteration 239, Batch: 30, Loss: 0.07813256978988647\n",
      "Iteration 239, Batch: 31, Loss: 0.06676419824361801\n",
      "Iteration 239, Batch: 32, Loss: 0.09508176892995834\n",
      "Iteration 239, Batch: 33, Loss: 0.04768820479512215\n",
      "Iteration 239, Batch: 34, Loss: 0.04829076677560806\n",
      "Iteration 239, Batch: 35, Loss: 0.052326783537864685\n",
      "Iteration 239, Batch: 36, Loss: 0.06585169583559036\n",
      "Iteration 239, Batch: 37, Loss: 0.06272871047258377\n",
      "Iteration 239, Batch: 38, Loss: 0.06410496681928635\n",
      "Iteration 239, Batch: 39, Loss: 0.05826174095273018\n",
      "Iteration 239, Batch: 40, Loss: 0.05717240646481514\n",
      "Iteration 239, Batch: 41, Loss: 0.06662037968635559\n",
      "Iteration 239, Batch: 42, Loss: 0.03957945853471756\n",
      "Iteration 239, Batch: 43, Loss: 0.04167015478014946\n",
      "Iteration 239, Batch: 44, Loss: 0.0369960255920887\n",
      "Iteration 239, Batch: 45, Loss: 0.07577966153621674\n",
      "Iteration 239, Batch: 46, Loss: 0.05391544848680496\n",
      "Iteration 239, Batch: 47, Loss: 0.05478239804506302\n",
      "Iteration 239, Batch: 48, Loss: 0.07868746668100357\n",
      "Iteration 239, Batch: 49, Loss: 0.048897046595811844\n",
      "Number of layers: 9\n",
      "Iteration 240, Batch: 0, Loss: 0.07007629424333572\n",
      "Iteration 240, Batch: 1, Loss: 0.0799679160118103\n",
      "Iteration 240, Batch: 2, Loss: 0.03962760791182518\n",
      "Iteration 240, Batch: 3, Loss: 0.07155841588973999\n",
      "Iteration 240, Batch: 4, Loss: 0.07888099551200867\n",
      "Iteration 240, Batch: 5, Loss: 0.04580945894122124\n",
      "Iteration 240, Batch: 6, Loss: 0.042726412415504456\n",
      "Iteration 240, Batch: 7, Loss: 0.07650783658027649\n",
      "Iteration 240, Batch: 8, Loss: 0.055155713111162186\n",
      "Iteration 240, Batch: 9, Loss: 0.0921175554394722\n",
      "Iteration 240, Batch: 10, Loss: 0.062281254678964615\n",
      "Iteration 240, Batch: 11, Loss: 0.07682251930236816\n",
      "Iteration 240, Batch: 12, Loss: 0.051490530371665955\n",
      "Iteration 240, Batch: 13, Loss: 0.059973038733005524\n",
      "Iteration 240, Batch: 14, Loss: 0.05431152135133743\n",
      "Iteration 240, Batch: 15, Loss: 0.04616597667336464\n",
      "Iteration 240, Batch: 16, Loss: 0.057110074907541275\n",
      "Iteration 240, Batch: 17, Loss: 0.035422321408987045\n",
      "Iteration 240, Batch: 18, Loss: 0.06417319178581238\n",
      "Iteration 240, Batch: 19, Loss: 0.053470153361558914\n",
      "Iteration 240, Batch: 20, Loss: 0.05626721307635307\n",
      "Iteration 240, Batch: 21, Loss: 0.03602929413318634\n",
      "Iteration 240, Batch: 22, Loss: 0.05351540073752403\n",
      "Iteration 240, Batch: 23, Loss: 0.05311422049999237\n",
      "Iteration 240, Batch: 24, Loss: 0.03506730869412422\n",
      "Iteration 240, Batch: 25, Loss: 0.05070225149393082\n",
      "Iteration 240, Batch: 26, Loss: 0.06293286383152008\n",
      "Iteration 240, Batch: 27, Loss: 0.0378112830221653\n",
      "Iteration 240, Batch: 28, Loss: 0.05713799223303795\n",
      "Iteration 240, Batch: 29, Loss: 0.05310889706015587\n",
      "Iteration 240, Batch: 30, Loss: 0.05283030867576599\n",
      "Iteration 240, Batch: 31, Loss: 0.05058542266488075\n",
      "Iteration 240, Batch: 32, Loss: 0.04492303729057312\n",
      "Iteration 240, Batch: 33, Loss: 0.07269831746816635\n",
      "Iteration 240, Batch: 34, Loss: 0.048948876559734344\n",
      "Iteration 240, Batch: 35, Loss: 0.03568994626402855\n",
      "Iteration 240, Batch: 36, Loss: 0.048979613929986954\n",
      "Iteration 240, Batch: 37, Loss: 0.031027328222990036\n",
      "Iteration 240, Batch: 38, Loss: 0.035759054124355316\n",
      "Iteration 240, Batch: 39, Loss: 0.03590792044997215\n",
      "Iteration 240, Batch: 40, Loss: 0.047434043139219284\n",
      "Iteration 240, Batch: 41, Loss: 0.042818039655685425\n",
      "Iteration 240, Batch: 42, Loss: 0.0731339082121849\n",
      "Iteration 240, Batch: 43, Loss: 0.06540455669164658\n",
      "Iteration 240, Batch: 44, Loss: 0.06665249168872833\n",
      "Iteration 240, Batch: 45, Loss: 0.03314774110913277\n",
      "Iteration 240, Batch: 46, Loss: 0.053499601781368256\n",
      "Iteration 240, Batch: 47, Loss: 0.04640667885541916\n",
      "Iteration 240, Batch: 48, Loss: 0.028433647006750107\n",
      "Iteration 240, Batch: 49, Loss: 0.08879657834768295\n",
      "Number of layers: 9\n",
      "Iteration 241, Batch: 0, Loss: 0.08661593496799469\n",
      "Iteration 241, Batch: 1, Loss: 0.05561055615544319\n",
      "Iteration 241, Batch: 2, Loss: 0.047002341598272324\n",
      "Iteration 241, Batch: 3, Loss: 0.04699195921421051\n",
      "Iteration 241, Batch: 4, Loss: 0.05202709883451462\n",
      "Iteration 241, Batch: 5, Loss: 0.07694082707166672\n",
      "Iteration 241, Batch: 6, Loss: 0.07767517864704132\n",
      "Iteration 241, Batch: 7, Loss: 0.06707392632961273\n",
      "Iteration 241, Batch: 8, Loss: 0.0619540810585022\n",
      "Iteration 241, Batch: 9, Loss: 0.03521550074219704\n",
      "Iteration 241, Batch: 10, Loss: 0.044586848467588425\n",
      "Iteration 241, Batch: 11, Loss: 0.0637189969420433\n",
      "Iteration 241, Batch: 12, Loss: 0.03443480283021927\n",
      "Iteration 241, Batch: 13, Loss: 0.045362766832113266\n",
      "Iteration 241, Batch: 14, Loss: 0.04790227860212326\n",
      "Iteration 241, Batch: 15, Loss: 0.07592670619487762\n",
      "Iteration 241, Batch: 16, Loss: 0.0709540992975235\n",
      "Iteration 241, Batch: 17, Loss: 0.02568305842578411\n",
      "Iteration 241, Batch: 18, Loss: 0.05482657253742218\n",
      "Iteration 241, Batch: 19, Loss: 0.047905948013067245\n",
      "Iteration 241, Batch: 20, Loss: 0.03789985179901123\n",
      "Iteration 241, Batch: 21, Loss: 0.04339760169386864\n",
      "Iteration 241, Batch: 22, Loss: 0.04861372709274292\n",
      "Iteration 241, Batch: 23, Loss: 0.06483995914459229\n",
      "Iteration 241, Batch: 24, Loss: 0.030593866482377052\n",
      "Iteration 241, Batch: 25, Loss: 0.038301385939121246\n",
      "Iteration 241, Batch: 26, Loss: 0.05490759760141373\n",
      "Iteration 241, Batch: 27, Loss: 0.06347877532243729\n",
      "Iteration 241, Batch: 28, Loss: 0.048266276717185974\n",
      "Iteration 241, Batch: 29, Loss: 0.05054117739200592\n",
      "Iteration 241, Batch: 30, Loss: 0.048151422291994095\n",
      "Iteration 241, Batch: 31, Loss: 0.08996745944023132\n",
      "Iteration 241, Batch: 32, Loss: 0.0584261417388916\n",
      "Iteration 241, Batch: 33, Loss: 0.06815685331821442\n",
      "Iteration 241, Batch: 34, Loss: 0.05220440775156021\n",
      "Iteration 241, Batch: 35, Loss: 0.0397421158850193\n",
      "Iteration 241, Batch: 36, Loss: 0.04438010975718498\n",
      "Iteration 241, Batch: 37, Loss: 0.03183872252702713\n",
      "Iteration 241, Batch: 38, Loss: 0.047504860907793045\n",
      "Iteration 241, Batch: 39, Loss: 0.04393802210688591\n",
      "Iteration 241, Batch: 40, Loss: 0.055804356932640076\n",
      "Iteration 241, Batch: 41, Loss: 0.04258834570646286\n",
      "Iteration 241, Batch: 42, Loss: 0.05676388740539551\n",
      "Iteration 241, Batch: 43, Loss: 0.030075229704380035\n",
      "Iteration 241, Batch: 44, Loss: 0.050193529576063156\n",
      "Iteration 241, Batch: 45, Loss: 0.03440434858202934\n",
      "Iteration 241, Batch: 46, Loss: 0.038285378366708755\n",
      "Iteration 241, Batch: 47, Loss: 0.03363630175590515\n",
      "Iteration 241, Batch: 48, Loss: 0.04027843475341797\n",
      "Iteration 241, Batch: 49, Loss: 0.06114688143134117\n",
      "Number of layers: 9\n",
      "Iteration 242, Batch: 0, Loss: 0.08044101297855377\n",
      "Iteration 242, Batch: 1, Loss: 0.05394791439175606\n",
      "Iteration 242, Batch: 2, Loss: 0.05880913510918617\n",
      "Iteration 242, Batch: 3, Loss: 0.06213311851024628\n",
      "Iteration 242, Batch: 4, Loss: 0.05175434798002243\n",
      "Iteration 242, Batch: 5, Loss: 0.05246690660715103\n",
      "Iteration 242, Batch: 6, Loss: 0.022451896220445633\n",
      "Iteration 242, Batch: 7, Loss: 0.03071141429245472\n",
      "Iteration 242, Batch: 8, Loss: 0.05968818441033363\n",
      "Iteration 242, Batch: 9, Loss: 0.05871095508337021\n",
      "Iteration 242, Batch: 10, Loss: 0.08700970560312271\n",
      "Iteration 242, Batch: 11, Loss: 0.02948521077632904\n",
      "Iteration 242, Batch: 12, Loss: 0.043222036212682724\n",
      "Iteration 242, Batch: 13, Loss: 0.059403304010629654\n",
      "Iteration 242, Batch: 14, Loss: 0.05816319212317467\n",
      "Iteration 242, Batch: 15, Loss: 0.043281301856040955\n",
      "Iteration 242, Batch: 16, Loss: 0.0576041117310524\n",
      "Iteration 242, Batch: 17, Loss: 0.041089873760938644\n",
      "Iteration 242, Batch: 18, Loss: 0.042474258691072464\n",
      "Iteration 242, Batch: 19, Loss: 0.04485650733113289\n",
      "Iteration 242, Batch: 20, Loss: 0.047919563949108124\n",
      "Iteration 242, Batch: 21, Loss: 0.04767049103975296\n",
      "Iteration 242, Batch: 22, Loss: 0.04303775355219841\n",
      "Iteration 242, Batch: 23, Loss: 0.05838887020945549\n",
      "Iteration 242, Batch: 24, Loss: 0.0449560172855854\n",
      "Iteration 242, Batch: 25, Loss: 0.03703971207141876\n",
      "Iteration 242, Batch: 26, Loss: 0.044038865715265274\n",
      "Iteration 242, Batch: 27, Loss: 0.053643181920051575\n",
      "Iteration 242, Batch: 28, Loss: 0.03454015403985977\n",
      "Iteration 242, Batch: 29, Loss: 0.06435666233301163\n",
      "Iteration 242, Batch: 30, Loss: 0.05873718112707138\n",
      "Iteration 242, Batch: 31, Loss: 0.04103031009435654\n",
      "Iteration 242, Batch: 32, Loss: 0.042178988456726074\n",
      "Iteration 242, Batch: 33, Loss: 0.03192140907049179\n",
      "Iteration 242, Batch: 34, Loss: 0.05635238438844681\n",
      "Iteration 242, Batch: 35, Loss: 0.052891701459884644\n",
      "Iteration 242, Batch: 36, Loss: 0.0677189975976944\n",
      "Iteration 242, Batch: 37, Loss: 0.04152835160493851\n",
      "Iteration 242, Batch: 38, Loss: 0.04501670226454735\n",
      "Iteration 242, Batch: 39, Loss: 0.058183420449495316\n",
      "Iteration 242, Batch: 40, Loss: 0.06315035372972488\n",
      "Iteration 242, Batch: 41, Loss: 0.03602996841073036\n",
      "Iteration 242, Batch: 42, Loss: 0.06252187490463257\n",
      "Iteration 242, Batch: 43, Loss: 0.08296506106853485\n",
      "Iteration 242, Batch: 44, Loss: 0.06289450079202652\n",
      "Iteration 242, Batch: 45, Loss: 0.04012351483106613\n",
      "Iteration 242, Batch: 46, Loss: 0.036679334938526154\n",
      "Iteration 242, Batch: 47, Loss: 0.053300440311431885\n",
      "Iteration 242, Batch: 48, Loss: 0.09068862348794937\n",
      "Iteration 242, Batch: 49, Loss: 0.03910992667078972\n",
      "Number of layers: 9\n",
      "Iteration 243, Batch: 0, Loss: 0.09191293269395828\n",
      "Iteration 243, Batch: 1, Loss: 0.09787628054618835\n",
      "Iteration 243, Batch: 2, Loss: 0.06819634884595871\n",
      "Iteration 243, Batch: 3, Loss: 0.051188234239816666\n",
      "Iteration 243, Batch: 4, Loss: 0.045880965888500214\n",
      "Iteration 243, Batch: 5, Loss: 0.04575281962752342\n",
      "Iteration 243, Batch: 6, Loss: 0.04721245542168617\n",
      "Iteration 243, Batch: 7, Loss: 0.07347002625465393\n",
      "Iteration 243, Batch: 8, Loss: 0.05319943651556969\n",
      "Iteration 243, Batch: 9, Loss: 0.06642264872789383\n",
      "Iteration 243, Batch: 10, Loss: 0.06386732310056686\n",
      "Iteration 243, Batch: 11, Loss: 0.0454740896821022\n",
      "Iteration 243, Batch: 12, Loss: 0.057077109813690186\n",
      "Iteration 243, Batch: 13, Loss: 0.0417666994035244\n",
      "Iteration 243, Batch: 14, Loss: 0.04102924093604088\n",
      "Iteration 243, Batch: 15, Loss: 0.06395264714956284\n",
      "Iteration 243, Batch: 16, Loss: 0.07621555775403976\n",
      "Iteration 243, Batch: 17, Loss: 0.07614407688379288\n",
      "Iteration 243, Batch: 18, Loss: 0.0816221535205841\n",
      "Iteration 243, Batch: 19, Loss: 0.060236748307943344\n",
      "Iteration 243, Batch: 20, Loss: 0.046318333595991135\n",
      "Iteration 243, Batch: 21, Loss: 0.08984456956386566\n",
      "Iteration 243, Batch: 22, Loss: 0.04805757850408554\n",
      "Iteration 243, Batch: 23, Loss: 0.04773695021867752\n",
      "Iteration 243, Batch: 24, Loss: 0.0667247548699379\n",
      "Iteration 243, Batch: 25, Loss: 0.06332797557115555\n",
      "Iteration 243, Batch: 26, Loss: 0.04575634002685547\n",
      "Iteration 243, Batch: 27, Loss: 0.035109858959913254\n",
      "Iteration 243, Batch: 28, Loss: 0.05582394823431969\n",
      "Iteration 243, Batch: 29, Loss: 0.04882550239562988\n",
      "Iteration 243, Batch: 30, Loss: 0.051619019359350204\n",
      "Iteration 243, Batch: 31, Loss: 0.055798862129449844\n",
      "Iteration 243, Batch: 32, Loss: 0.05755438655614853\n",
      "Iteration 243, Batch: 33, Loss: 0.04116804897785187\n",
      "Iteration 243, Batch: 34, Loss: 0.03746536001563072\n",
      "Iteration 243, Batch: 35, Loss: 0.05521806702017784\n",
      "Iteration 243, Batch: 36, Loss: 0.08084901422262192\n",
      "Iteration 243, Batch: 37, Loss: 0.03265954554080963\n",
      "Iteration 243, Batch: 38, Loss: 0.06147293373942375\n",
      "Iteration 243, Batch: 39, Loss: 0.05208037793636322\n",
      "Iteration 243, Batch: 40, Loss: 0.054466135799884796\n",
      "Iteration 243, Batch: 41, Loss: 0.06413177400827408\n",
      "Iteration 243, Batch: 42, Loss: 0.03341247886419296\n",
      "Iteration 243, Batch: 43, Loss: 0.0378437414765358\n",
      "Iteration 243, Batch: 44, Loss: 0.04374760389328003\n",
      "Iteration 243, Batch: 45, Loss: 0.09178968518972397\n",
      "Iteration 243, Batch: 46, Loss: 0.03137081116437912\n",
      "Iteration 243, Batch: 47, Loss: 0.059880442917346954\n",
      "Iteration 243, Batch: 48, Loss: 0.05286692455410957\n",
      "Iteration 243, Batch: 49, Loss: 0.045455243438482285\n",
      "Number of layers: 9\n",
      "Iteration 244, Batch: 0, Loss: 0.060437269508838654\n",
      "Iteration 244, Batch: 1, Loss: 0.04215690493583679\n",
      "Iteration 244, Batch: 2, Loss: 0.03080902434885502\n",
      "Iteration 244, Batch: 3, Loss: 0.031390346586704254\n",
      "Iteration 244, Batch: 4, Loss: 0.06264835596084595\n",
      "Iteration 244, Batch: 5, Loss: 0.06350531429052353\n",
      "Iteration 244, Batch: 6, Loss: 0.05866304412484169\n",
      "Iteration 244, Batch: 7, Loss: 0.05764865130186081\n",
      "Iteration 244, Batch: 8, Loss: 0.06909763067960739\n",
      "Iteration 244, Batch: 9, Loss: 0.09481938928365707\n",
      "Iteration 244, Batch: 10, Loss: 0.042573604732751846\n",
      "Iteration 244, Batch: 11, Loss: 0.05715504661202431\n",
      "Iteration 244, Batch: 12, Loss: 0.042499102652072906\n",
      "Iteration 244, Batch: 13, Loss: 0.05381431430578232\n",
      "Iteration 244, Batch: 14, Loss: 0.06440529972314835\n",
      "Iteration 244, Batch: 15, Loss: 0.05147043988108635\n",
      "Iteration 244, Batch: 16, Loss: 0.05631410330533981\n",
      "Iteration 244, Batch: 17, Loss: 0.05774492025375366\n",
      "Iteration 244, Batch: 18, Loss: 0.049838583916425705\n",
      "Iteration 244, Batch: 19, Loss: 0.04028630256652832\n",
      "Iteration 244, Batch: 20, Loss: 0.07225527614355087\n",
      "Iteration 244, Batch: 21, Loss: 0.049400899559259415\n",
      "Iteration 244, Batch: 22, Loss: 0.0462506078183651\n",
      "Iteration 244, Batch: 23, Loss: 0.06640464067459106\n",
      "Iteration 244, Batch: 24, Loss: 0.043081969022750854\n",
      "Iteration 244, Batch: 25, Loss: 0.04066496714949608\n",
      "Iteration 244, Batch: 26, Loss: 0.04618300870060921\n",
      "Iteration 244, Batch: 27, Loss: 0.059540119022130966\n",
      "Iteration 244, Batch: 28, Loss: 0.02917855978012085\n",
      "Iteration 244, Batch: 29, Loss: 0.02906930074095726\n",
      "Iteration 244, Batch: 30, Loss: 0.06524444371461868\n",
      "Iteration 244, Batch: 31, Loss: 0.0605570524930954\n",
      "Iteration 244, Batch: 32, Loss: 0.05908256396651268\n",
      "Iteration 244, Batch: 33, Loss: 0.05901271849870682\n",
      "Iteration 244, Batch: 34, Loss: 0.03064180724322796\n",
      "Iteration 244, Batch: 35, Loss: 0.07183507084846497\n",
      "Iteration 244, Batch: 36, Loss: 0.06271321326494217\n",
      "Iteration 244, Batch: 37, Loss: 0.05789544805884361\n",
      "Iteration 244, Batch: 38, Loss: 0.04685891419649124\n",
      "Iteration 244, Batch: 39, Loss: 0.06898557394742966\n",
      "Iteration 244, Batch: 40, Loss: 0.06679758429527283\n",
      "Iteration 244, Batch: 41, Loss: 0.06690428406000137\n",
      "Iteration 244, Batch: 42, Loss: 0.07864469289779663\n",
      "Iteration 244, Batch: 43, Loss: 0.08137062191963196\n",
      "Iteration 244, Batch: 44, Loss: 0.07661345601081848\n",
      "Iteration 244, Batch: 45, Loss: 0.045028962194919586\n",
      "Iteration 244, Batch: 46, Loss: 0.07756602019071579\n",
      "Iteration 244, Batch: 47, Loss: 0.05158013850450516\n",
      "Iteration 244, Batch: 48, Loss: 0.056251782923936844\n",
      "Iteration 244, Batch: 49, Loss: 0.04946417734026909\n",
      "Number of layers: 9\n",
      "Iteration 245, Batch: 0, Loss: 0.031986650079488754\n",
      "Iteration 245, Batch: 1, Loss: 0.08381939679384232\n",
      "Iteration 245, Batch: 2, Loss: 0.0748615562915802\n",
      "Iteration 245, Batch: 3, Loss: 0.05887235328555107\n",
      "Iteration 245, Batch: 4, Loss: 0.03795137628912926\n",
      "Iteration 245, Batch: 5, Loss: 0.0444711297750473\n",
      "Iteration 245, Batch: 6, Loss: 0.06526678800582886\n",
      "Iteration 245, Batch: 7, Loss: 0.05669139325618744\n",
      "Iteration 245, Batch: 8, Loss: 0.08302507549524307\n",
      "Iteration 245, Batch: 9, Loss: 0.03821319341659546\n",
      "Iteration 245, Batch: 10, Loss: 0.04265662655234337\n",
      "Iteration 245, Batch: 11, Loss: 0.06002012640237808\n",
      "Iteration 245, Batch: 12, Loss: 0.04899751394987106\n",
      "Iteration 245, Batch: 13, Loss: 0.05954351648688316\n",
      "Iteration 245, Batch: 14, Loss: 0.051754482090473175\n",
      "Iteration 245, Batch: 15, Loss: 0.04105694591999054\n",
      "Iteration 245, Batch: 16, Loss: 0.07318606227636337\n",
      "Iteration 245, Batch: 17, Loss: 0.06510096043348312\n",
      "Iteration 245, Batch: 18, Loss: 0.0664215162396431\n",
      "Iteration 245, Batch: 19, Loss: 0.09557342529296875\n",
      "Iteration 245, Batch: 20, Loss: 0.04171224683523178\n",
      "Iteration 245, Batch: 21, Loss: 0.07380867749452591\n",
      "Iteration 245, Batch: 22, Loss: 0.09517591446638107\n",
      "Iteration 245, Batch: 23, Loss: 0.05604541674256325\n",
      "Iteration 245, Batch: 24, Loss: 0.07973159849643707\n",
      "Iteration 245, Batch: 25, Loss: 0.07435622811317444\n",
      "Iteration 245, Batch: 26, Loss: 0.06733827292919159\n",
      "Iteration 245, Batch: 27, Loss: 0.07099888473749161\n",
      "Iteration 245, Batch: 28, Loss: 0.051574014127254486\n",
      "Iteration 245, Batch: 29, Loss: 0.06320630759000778\n",
      "Iteration 245, Batch: 30, Loss: 0.05069123953580856\n",
      "Iteration 245, Batch: 31, Loss: 0.060185469686985016\n",
      "Iteration 245, Batch: 32, Loss: 0.06963744014501572\n",
      "Iteration 245, Batch: 33, Loss: 0.03981127217411995\n",
      "Iteration 245, Batch: 34, Loss: 0.07951445132493973\n",
      "Iteration 245, Batch: 35, Loss: 0.04064495861530304\n",
      "Iteration 245, Batch: 36, Loss: 0.06450078636407852\n",
      "Iteration 245, Batch: 37, Loss: 0.043290965259075165\n",
      "Iteration 245, Batch: 38, Loss: 0.05916103348135948\n",
      "Iteration 245, Batch: 39, Loss: 0.067887082695961\n",
      "Iteration 245, Batch: 40, Loss: 0.04139333218336105\n",
      "Iteration 245, Batch: 41, Loss: 0.05234929919242859\n",
      "Iteration 245, Batch: 42, Loss: 0.03637828677892685\n",
      "Iteration 245, Batch: 43, Loss: 0.027985217049717903\n",
      "Iteration 245, Batch: 44, Loss: 0.035722482949495316\n",
      "Iteration 245, Batch: 45, Loss: 0.045063432306051254\n",
      "Iteration 245, Batch: 46, Loss: 0.039374060928821564\n",
      "Iteration 245, Batch: 47, Loss: 0.03082994371652603\n",
      "Iteration 245, Batch: 48, Loss: 0.05711640790104866\n",
      "Iteration 245, Batch: 49, Loss: 0.06499482691287994\n",
      "Number of layers: 9\n",
      "Iteration 246, Batch: 0, Loss: 0.08584710955619812\n",
      "Iteration 246, Batch: 1, Loss: 0.08023913949728012\n",
      "Iteration 246, Batch: 2, Loss: 0.05518593639135361\n",
      "Iteration 246, Batch: 3, Loss: 0.039424337446689606\n",
      "Iteration 246, Batch: 4, Loss: 0.06621845066547394\n",
      "Iteration 246, Batch: 5, Loss: 0.049209631979465485\n",
      "Iteration 246, Batch: 6, Loss: 0.037880998104810715\n",
      "Iteration 246, Batch: 7, Loss: 0.036931972950696945\n",
      "Iteration 246, Batch: 8, Loss: 0.06746724247932434\n",
      "Iteration 246, Batch: 9, Loss: 0.057768259197473526\n",
      "Iteration 246, Batch: 10, Loss: 0.032607562839984894\n",
      "Iteration 246, Batch: 11, Loss: 0.06044624373316765\n",
      "Iteration 246, Batch: 12, Loss: 0.041663482785224915\n",
      "Iteration 246, Batch: 13, Loss: 0.05997182056307793\n",
      "Iteration 246, Batch: 14, Loss: 0.0598595067858696\n",
      "Iteration 246, Batch: 15, Loss: 0.05474384129047394\n",
      "Iteration 246, Batch: 16, Loss: 0.06614996492862701\n",
      "Iteration 246, Batch: 17, Loss: 0.051406145095825195\n",
      "Iteration 246, Batch: 18, Loss: 0.06847769767045975\n",
      "Iteration 246, Batch: 19, Loss: 0.05565796419978142\n",
      "Iteration 246, Batch: 20, Loss: 0.0744614228606224\n",
      "Iteration 246, Batch: 21, Loss: 0.04151850566267967\n",
      "Iteration 246, Batch: 22, Loss: 0.04553331434726715\n",
      "Iteration 246, Batch: 23, Loss: 0.05704976245760918\n",
      "Iteration 246, Batch: 24, Loss: 0.043699443340301514\n",
      "Iteration 246, Batch: 25, Loss: 0.052500683814287186\n",
      "Iteration 246, Batch: 26, Loss: 0.04599176347255707\n",
      "Iteration 246, Batch: 27, Loss: 0.03470655530691147\n",
      "Iteration 246, Batch: 28, Loss: 0.05963367968797684\n",
      "Iteration 246, Batch: 29, Loss: 0.05388374999165535\n",
      "Iteration 246, Batch: 30, Loss: 0.03993293270468712\n",
      "Iteration 246, Batch: 31, Loss: 0.04679596424102783\n",
      "Iteration 246, Batch: 32, Loss: 0.045689657330513\n",
      "Iteration 246, Batch: 33, Loss: 0.047222889959812164\n",
      "Iteration 246, Batch: 34, Loss: 0.05724945291876793\n",
      "Iteration 246, Batch: 35, Loss: 0.06213569641113281\n",
      "Iteration 246, Batch: 36, Loss: 0.05172200873494148\n",
      "Iteration 246, Batch: 37, Loss: 0.046738337725400925\n",
      "Iteration 246, Batch: 38, Loss: 0.057394884526729584\n",
      "Iteration 246, Batch: 39, Loss: 0.04323751851916313\n",
      "Iteration 246, Batch: 40, Loss: 0.07542579621076584\n",
      "Iteration 246, Batch: 41, Loss: 0.05542246624827385\n",
      "Iteration 246, Batch: 42, Loss: 0.07711691409349442\n",
      "Iteration 246, Batch: 43, Loss: 0.07317768782377243\n",
      "Iteration 246, Batch: 44, Loss: 0.06612513959407806\n",
      "Iteration 246, Batch: 45, Loss: 0.05649944767355919\n",
      "Iteration 246, Batch: 46, Loss: 0.055318087339401245\n",
      "Iteration 246, Batch: 47, Loss: 0.058132775127887726\n",
      "Iteration 246, Batch: 48, Loss: 0.08514738827943802\n",
      "Iteration 246, Batch: 49, Loss: 0.0559857152402401\n",
      "Number of layers: 9\n",
      "Iteration 247, Batch: 0, Loss: 0.06775019317865372\n",
      "Iteration 247, Batch: 1, Loss: 0.06335142254829407\n",
      "Iteration 247, Batch: 2, Loss: 0.06459706276655197\n",
      "Iteration 247, Batch: 3, Loss: 0.06771095842123032\n",
      "Iteration 247, Batch: 4, Loss: 0.051731858402490616\n",
      "Iteration 247, Batch: 5, Loss: 0.07088076323270798\n",
      "Iteration 247, Batch: 6, Loss: 0.08206994831562042\n",
      "Iteration 247, Batch: 7, Loss: 0.056463852524757385\n",
      "Iteration 247, Batch: 8, Loss: 0.06569615006446838\n",
      "Iteration 247, Batch: 9, Loss: 0.05022311583161354\n",
      "Iteration 247, Batch: 10, Loss: 0.032750483602285385\n",
      "Iteration 247, Batch: 11, Loss: 0.06845723092556\n",
      "Iteration 247, Batch: 12, Loss: 0.05016695708036423\n",
      "Iteration 247, Batch: 13, Loss: 0.06255830824375153\n",
      "Iteration 247, Batch: 14, Loss: 0.062466297298669815\n",
      "Iteration 247, Batch: 15, Loss: 0.07621639221906662\n",
      "Iteration 247, Batch: 16, Loss: 0.04228817671537399\n",
      "Iteration 247, Batch: 17, Loss: 0.05370457470417023\n",
      "Iteration 247, Batch: 18, Loss: 0.0476580411195755\n",
      "Iteration 247, Batch: 19, Loss: 0.04949064925312996\n",
      "Iteration 247, Batch: 20, Loss: 0.06553513556718826\n",
      "Iteration 247, Batch: 21, Loss: 0.02723430097103119\n",
      "Iteration 247, Batch: 22, Loss: 0.05065304785966873\n",
      "Iteration 247, Batch: 23, Loss: 0.06772731989622116\n",
      "Iteration 247, Batch: 24, Loss: 0.07291331142187119\n",
      "Iteration 247, Batch: 25, Loss: 0.06844250112771988\n",
      "Iteration 247, Batch: 26, Loss: 0.06798819452524185\n",
      "Iteration 247, Batch: 27, Loss: 0.05775413289666176\n",
      "Iteration 247, Batch: 28, Loss: 0.047941893339157104\n",
      "Iteration 247, Batch: 29, Loss: 0.06383805721998215\n",
      "Iteration 247, Batch: 30, Loss: 0.06199168413877487\n",
      "Iteration 247, Batch: 31, Loss: 0.061389315873384476\n",
      "Iteration 247, Batch: 32, Loss: 0.06332820653915405\n",
      "Iteration 247, Batch: 33, Loss: 0.06563407927751541\n",
      "Iteration 247, Batch: 34, Loss: 0.03790128231048584\n",
      "Iteration 247, Batch: 35, Loss: 0.08072827756404877\n",
      "Iteration 247, Batch: 36, Loss: 0.06308416277170181\n",
      "Iteration 247, Batch: 37, Loss: 0.08364272117614746\n",
      "Iteration 247, Batch: 38, Loss: 0.07126811891794205\n",
      "Iteration 247, Batch: 39, Loss: 0.05277935788035393\n",
      "Iteration 247, Batch: 40, Loss: 0.07193619757890701\n",
      "Iteration 247, Batch: 41, Loss: 0.06505059450864792\n",
      "Iteration 247, Batch: 42, Loss: 0.04977685958147049\n",
      "Iteration 247, Batch: 43, Loss: 0.10088346153497696\n",
      "Iteration 247, Batch: 44, Loss: 0.06213579699397087\n",
      "Iteration 247, Batch: 45, Loss: 0.12156365066766739\n",
      "Iteration 247, Batch: 46, Loss: 0.10795494168996811\n",
      "Iteration 247, Batch: 47, Loss: 0.11854057013988495\n",
      "Iteration 247, Batch: 48, Loss: 0.08529399335384369\n",
      "Iteration 247, Batch: 49, Loss: 0.10500472038984299\n",
      "Number of layers: 9\n",
      "Iteration 248, Batch: 0, Loss: 0.06138375774025917\n",
      "Iteration 248, Batch: 1, Loss: 0.06416970491409302\n",
      "Iteration 248, Batch: 2, Loss: 0.05443740636110306\n",
      "Iteration 248, Batch: 3, Loss: 0.05950181931257248\n",
      "Iteration 248, Batch: 4, Loss: 0.0705341249704361\n",
      "Iteration 248, Batch: 5, Loss: 0.06724447757005692\n",
      "Iteration 248, Batch: 6, Loss: 0.05498257651925087\n",
      "Iteration 248, Batch: 7, Loss: 0.04990735277533531\n",
      "Iteration 248, Batch: 8, Loss: 0.04861939325928688\n",
      "Iteration 248, Batch: 9, Loss: 0.06963542103767395\n",
      "Iteration 248, Batch: 10, Loss: 0.057019539177417755\n",
      "Iteration 248, Batch: 11, Loss: 0.04036281630396843\n",
      "Iteration 248, Batch: 12, Loss: 0.03529734164476395\n",
      "Iteration 248, Batch: 13, Loss: 0.06909851729869843\n",
      "Iteration 248, Batch: 14, Loss: 0.07900553196668625\n",
      "Iteration 248, Batch: 15, Loss: 0.040266428142786026\n",
      "Iteration 248, Batch: 16, Loss: 0.04898903891444206\n",
      "Iteration 248, Batch: 17, Loss: 0.04064103215932846\n",
      "Iteration 248, Batch: 18, Loss: 0.043192796409130096\n",
      "Iteration 248, Batch: 19, Loss: 0.05378745123744011\n",
      "Iteration 248, Batch: 20, Loss: 0.06949589401483536\n",
      "Iteration 248, Batch: 21, Loss: 0.034207094460725784\n",
      "Iteration 248, Batch: 22, Loss: 0.05965328589081764\n",
      "Iteration 248, Batch: 23, Loss: 0.08321912586688995\n",
      "Iteration 248, Batch: 24, Loss: 0.07943396270275116\n",
      "Iteration 248, Batch: 25, Loss: 0.05485963821411133\n",
      "Iteration 248, Batch: 26, Loss: 0.04888263717293739\n",
      "Iteration 248, Batch: 27, Loss: 0.05899996683001518\n",
      "Iteration 248, Batch: 28, Loss: 0.06993706524372101\n",
      "Iteration 248, Batch: 29, Loss: 0.057618703693151474\n",
      "Iteration 248, Batch: 30, Loss: 0.06887339800596237\n",
      "Iteration 248, Batch: 31, Loss: 0.05762379989027977\n",
      "Iteration 248, Batch: 32, Loss: 0.06539933383464813\n",
      "Iteration 248, Batch: 33, Loss: 0.07074391096830368\n",
      "Iteration 248, Batch: 34, Loss: 0.07520978897809982\n",
      "Iteration 248, Batch: 35, Loss: 0.07601851969957352\n",
      "Iteration 248, Batch: 36, Loss: 0.07692782580852509\n",
      "Iteration 248, Batch: 37, Loss: 0.11249009519815445\n",
      "Iteration 248, Batch: 38, Loss: 0.05910344794392586\n",
      "Iteration 248, Batch: 39, Loss: 0.060486190021038055\n",
      "Iteration 248, Batch: 40, Loss: 0.04600466787815094\n",
      "Iteration 248, Batch: 41, Loss: 0.038299016654491425\n",
      "Iteration 248, Batch: 42, Loss: 0.03930642455816269\n",
      "Iteration 248, Batch: 43, Loss: 0.06753354519605637\n",
      "Iteration 248, Batch: 44, Loss: 0.08337089419364929\n",
      "Iteration 248, Batch: 45, Loss: 0.04424063488841057\n",
      "Iteration 248, Batch: 46, Loss: 0.06678363680839539\n",
      "Iteration 248, Batch: 47, Loss: 0.06681017577648163\n",
      "Iteration 248, Batch: 48, Loss: 0.05544160678982735\n",
      "Iteration 248, Batch: 49, Loss: 0.08510185033082962\n",
      "Number of layers: 9\n",
      "Iteration 249, Batch: 0, Loss: 0.07424557209014893\n",
      "Iteration 249, Batch: 1, Loss: 0.052100975066423416\n",
      "Iteration 249, Batch: 2, Loss: 0.0984019860625267\n",
      "Iteration 249, Batch: 3, Loss: 0.12418901175260544\n",
      "Iteration 249, Batch: 4, Loss: 0.10618633031845093\n",
      "Iteration 249, Batch: 5, Loss: 0.09198180586099625\n",
      "Iteration 249, Batch: 6, Loss: 0.11715028434991837\n",
      "Iteration 249, Batch: 7, Loss: 0.09916848689317703\n",
      "Iteration 249, Batch: 8, Loss: 0.09454386681318283\n",
      "Iteration 249, Batch: 9, Loss: 0.06418947130441666\n",
      "Iteration 249, Batch: 10, Loss: 0.06372665613889694\n",
      "Iteration 249, Batch: 11, Loss: 0.06248597800731659\n",
      "Iteration 249, Batch: 12, Loss: 0.07954388111829758\n",
      "Iteration 249, Batch: 13, Loss: 0.041190411895513535\n",
      "Iteration 249, Batch: 14, Loss: 0.06227370724081993\n",
      "Iteration 249, Batch: 15, Loss: 0.03472404554486275\n",
      "Iteration 249, Batch: 16, Loss: 0.046579860150814056\n",
      "Iteration 249, Batch: 17, Loss: 0.062498193234205246\n",
      "Iteration 249, Batch: 18, Loss: 0.02783142775297165\n",
      "Iteration 249, Batch: 19, Loss: 0.06199212744832039\n",
      "Iteration 249, Batch: 20, Loss: 0.05380341410636902\n",
      "Iteration 249, Batch: 21, Loss: 0.03151661157608032\n",
      "Iteration 249, Batch: 22, Loss: 0.05285392701625824\n",
      "Iteration 249, Batch: 23, Loss: 0.044643811881542206\n",
      "Iteration 249, Batch: 24, Loss: 0.04576323553919792\n",
      "Iteration 249, Batch: 25, Loss: 0.04587537795305252\n",
      "Iteration 249, Batch: 26, Loss: 0.056334011256694794\n",
      "Iteration 249, Batch: 27, Loss: 0.05136483907699585\n",
      "Iteration 249, Batch: 28, Loss: 0.06487589329481125\n",
      "Iteration 249, Batch: 29, Loss: 0.05856916308403015\n",
      "Iteration 249, Batch: 30, Loss: 0.03431791439652443\n",
      "Iteration 249, Batch: 31, Loss: 0.05926591902971268\n",
      "Iteration 249, Batch: 32, Loss: 0.04934399574995041\n",
      "Iteration 249, Batch: 33, Loss: 0.03747224062681198\n",
      "Iteration 249, Batch: 34, Loss: 0.04086597263813019\n",
      "Iteration 249, Batch: 35, Loss: 0.058109480887651443\n",
      "Iteration 249, Batch: 36, Loss: 0.025316424667835236\n",
      "Iteration 249, Batch: 37, Loss: 0.05083547532558441\n",
      "Iteration 249, Batch: 38, Loss: 0.06119539216160774\n",
      "Iteration 249, Batch: 39, Loss: 0.07324191927909851\n",
      "Iteration 249, Batch: 40, Loss: 0.06689490377902985\n",
      "Iteration 249, Batch: 41, Loss: 0.06471574306488037\n",
      "Iteration 249, Batch: 42, Loss: 0.032453570514917374\n",
      "Iteration 249, Batch: 43, Loss: 0.04196854680776596\n",
      "Iteration 249, Batch: 44, Loss: 0.03907211124897003\n",
      "Iteration 249, Batch: 45, Loss: 0.05391092970967293\n",
      "Iteration 249, Batch: 46, Loss: 0.03946254774928093\n",
      "Iteration 249, Batch: 47, Loss: 0.01264185830950737\n",
      "Iteration 249, Batch: 48, Loss: 0.07165808230638504\n",
      "Iteration 249, Batch: 49, Loss: 0.032786428928375244\n",
      "Number of layers: 9\n",
      "Iteration 250, Batch: 0, Loss: 0.06421469897031784\n",
      "Iteration 250, Batch: 1, Loss: 0.0593067966401577\n",
      "Iteration 250, Batch: 2, Loss: 0.0671529620885849\n",
      "Iteration 250, Batch: 3, Loss: 0.05782529339194298\n",
      "Iteration 250, Batch: 4, Loss: 0.03687901794910431\n",
      "Iteration 250, Batch: 5, Loss: 0.0555192269384861\n",
      "Iteration 250, Batch: 6, Loss: 0.061464473605155945\n",
      "Iteration 250, Batch: 7, Loss: 0.07512765377759933\n",
      "Iteration 250, Batch: 8, Loss: 0.05997135490179062\n",
      "Iteration 250, Batch: 9, Loss: 0.06251993775367737\n",
      "Iteration 250, Batch: 10, Loss: 0.07416167855262756\n",
      "Iteration 250, Batch: 11, Loss: 0.09132655709981918\n",
      "Iteration 250, Batch: 12, Loss: 0.07106137275695801\n",
      "Iteration 250, Batch: 13, Loss: 0.10440070182085037\n",
      "Iteration 250, Batch: 14, Loss: 0.062238242477178574\n",
      "Iteration 250, Batch: 15, Loss: 0.09266482293605804\n",
      "Iteration 250, Batch: 16, Loss: 0.0933455228805542\n",
      "Iteration 250, Batch: 17, Loss: 0.08876655250787735\n",
      "Iteration 250, Batch: 18, Loss: 0.07281292229890823\n",
      "Iteration 250, Batch: 19, Loss: 0.07138244807720184\n",
      "Iteration 250, Batch: 20, Loss: 0.08171052485704422\n",
      "Iteration 250, Batch: 21, Loss: 0.11437631398439407\n",
      "Iteration 250, Batch: 22, Loss: 0.08184896409511566\n",
      "Iteration 250, Batch: 23, Loss: 0.13156281411647797\n",
      "Iteration 250, Batch: 24, Loss: 0.12276196479797363\n",
      "Iteration 250, Batch: 25, Loss: 0.126396045088768\n",
      "Iteration 250, Batch: 26, Loss: 0.07562944293022156\n",
      "Iteration 250, Batch: 27, Loss: 0.06598800420761108\n",
      "Iteration 250, Batch: 28, Loss: 0.05002664029598236\n",
      "Iteration 250, Batch: 29, Loss: 0.04282452538609505\n",
      "Iteration 250, Batch: 30, Loss: 0.04976935312151909\n",
      "Iteration 250, Batch: 31, Loss: 0.044236525893211365\n",
      "Iteration 250, Batch: 32, Loss: 0.04550028592348099\n",
      "Iteration 250, Batch: 33, Loss: 0.04564211890101433\n",
      "Iteration 250, Batch: 34, Loss: 0.04054984822869301\n",
      "Iteration 250, Batch: 35, Loss: 0.05265001580119133\n",
      "Iteration 250, Batch: 36, Loss: 0.04016062244772911\n",
      "Iteration 250, Batch: 37, Loss: 0.05487315356731415\n",
      "Iteration 250, Batch: 38, Loss: 0.06097327917814255\n",
      "Iteration 250, Batch: 39, Loss: 0.044274378567934036\n",
      "Iteration 250, Batch: 40, Loss: 0.0680622085928917\n",
      "Iteration 250, Batch: 41, Loss: 0.06492727994918823\n",
      "Iteration 250, Batch: 42, Loss: 0.050011176615953445\n",
      "Iteration 250, Batch: 43, Loss: 0.0438227504491806\n",
      "Iteration 250, Batch: 44, Loss: 0.04154852777719498\n",
      "Iteration 250, Batch: 45, Loss: 0.03207533806562424\n",
      "Iteration 250, Batch: 46, Loss: 0.038864269852638245\n",
      "Iteration 250, Batch: 47, Loss: 0.03288380056619644\n",
      "Iteration 250, Batch: 48, Loss: 0.0355834886431694\n",
      "Iteration 250, Batch: 49, Loss: 0.03912738710641861\n",
      "Number of layers: 9\n",
      "Iteration 251, Batch: 0, Loss: 0.055884283035993576\n",
      "Iteration 251, Batch: 1, Loss: 0.0648769661784172\n",
      "Iteration 251, Batch: 2, Loss: 0.04219041392207146\n",
      "Iteration 251, Batch: 3, Loss: 0.06448370218276978\n",
      "Iteration 251, Batch: 4, Loss: 0.03594812750816345\n",
      "Iteration 251, Batch: 5, Loss: 0.0656745433807373\n",
      "Iteration 251, Batch: 6, Loss: 0.05077560245990753\n",
      "Iteration 251, Batch: 7, Loss: 0.03144035115838051\n",
      "Iteration 251, Batch: 8, Loss: 0.025711053982377052\n",
      "Iteration 251, Batch: 9, Loss: 0.060563381761312485\n",
      "Iteration 251, Batch: 10, Loss: 0.06268385052680969\n",
      "Iteration 251, Batch: 11, Loss: 0.050666868686676025\n",
      "Iteration 251, Batch: 12, Loss: 0.05190100520849228\n",
      "Iteration 251, Batch: 13, Loss: 0.056318461894989014\n",
      "Iteration 251, Batch: 14, Loss: 0.061179034411907196\n",
      "Iteration 251, Batch: 15, Loss: 0.049156177788972855\n",
      "Iteration 251, Batch: 16, Loss: 0.03066505677998066\n",
      "Iteration 251, Batch: 17, Loss: 0.02391972579061985\n",
      "Iteration 251, Batch: 18, Loss: 0.051005490124225616\n",
      "Iteration 251, Batch: 19, Loss: 0.043372418731451035\n",
      "Iteration 251, Batch: 20, Loss: 0.03955315798521042\n",
      "Iteration 251, Batch: 21, Loss: 0.03743739053606987\n",
      "Iteration 251, Batch: 22, Loss: 0.023609228432178497\n",
      "Iteration 251, Batch: 23, Loss: 0.07565515488386154\n",
      "Iteration 251, Batch: 24, Loss: 0.05559168756008148\n",
      "Iteration 251, Batch: 25, Loss: 0.035958610475063324\n",
      "Iteration 251, Batch: 26, Loss: 0.04971277713775635\n",
      "Iteration 251, Batch: 27, Loss: 0.08072374761104584\n",
      "Iteration 251, Batch: 28, Loss: 0.10440701991319656\n",
      "Iteration 251, Batch: 29, Loss: 0.05866284295916557\n",
      "Iteration 251, Batch: 30, Loss: 0.04674692824482918\n",
      "Iteration 251, Batch: 31, Loss: 0.08137395977973938\n",
      "Iteration 251, Batch: 32, Loss: 0.064174123108387\n",
      "Iteration 251, Batch: 33, Loss: 0.05741283670067787\n",
      "Iteration 251, Batch: 34, Loss: 0.08526457846164703\n",
      "Iteration 251, Batch: 35, Loss: 0.08249467611312866\n",
      "Iteration 251, Batch: 36, Loss: 0.08355165272951126\n",
      "Iteration 251, Batch: 37, Loss: 0.07206098735332489\n",
      "Iteration 251, Batch: 38, Loss: 0.05404768884181976\n",
      "Iteration 251, Batch: 39, Loss: 0.057254206389188766\n",
      "Iteration 251, Batch: 40, Loss: 0.07747883349657059\n",
      "Iteration 251, Batch: 41, Loss: 0.05013298988342285\n",
      "Iteration 251, Batch: 42, Loss: 0.055651646107435226\n",
      "Iteration 251, Batch: 43, Loss: 0.07326783984899521\n",
      "Iteration 251, Batch: 44, Loss: 0.0870102122426033\n",
      "Iteration 251, Batch: 45, Loss: 0.08690202236175537\n",
      "Iteration 251, Batch: 46, Loss: 0.0444466769695282\n",
      "Iteration 251, Batch: 47, Loss: 0.06521911919116974\n",
      "Iteration 251, Batch: 48, Loss: 0.06554576754570007\n",
      "Iteration 251, Batch: 49, Loss: 0.06137818098068237\n",
      "Number of layers: 9\n",
      "Iteration 252, Batch: 0, Loss: 0.07618934661149979\n",
      "Iteration 252, Batch: 1, Loss: 0.06488867849111557\n",
      "Iteration 252, Batch: 2, Loss: 0.04258275032043457\n",
      "Iteration 252, Batch: 3, Loss: 0.0408594012260437\n",
      "Iteration 252, Batch: 4, Loss: 0.03495781496167183\n",
      "Iteration 252, Batch: 5, Loss: 0.05959780514240265\n",
      "Iteration 252, Batch: 6, Loss: 0.0485227108001709\n",
      "Iteration 252, Batch: 7, Loss: 0.027616236358880997\n",
      "Iteration 252, Batch: 8, Loss: 0.04333512857556343\n",
      "Iteration 252, Batch: 9, Loss: 0.04860423877835274\n",
      "Iteration 252, Batch: 10, Loss: 0.03933097422122955\n",
      "Iteration 252, Batch: 11, Loss: 0.04277653619647026\n",
      "Iteration 252, Batch: 12, Loss: 0.0395563505589962\n",
      "Iteration 252, Batch: 13, Loss: 0.03978163003921509\n",
      "Iteration 252, Batch: 14, Loss: 0.041724901646375656\n",
      "Iteration 252, Batch: 15, Loss: 0.0573289655148983\n",
      "Iteration 252, Batch: 16, Loss: 0.0494113452732563\n",
      "Iteration 252, Batch: 17, Loss: 0.030301745980978012\n",
      "Iteration 252, Batch: 18, Loss: 0.04177892953157425\n",
      "Iteration 252, Batch: 19, Loss: 0.04211249575018883\n",
      "Iteration 252, Batch: 20, Loss: 0.05116942897439003\n",
      "Iteration 252, Batch: 21, Loss: 0.018040545284748077\n",
      "Iteration 252, Batch: 22, Loss: 0.033848103135824203\n",
      "Iteration 252, Batch: 23, Loss: 0.03766953945159912\n",
      "Iteration 252, Batch: 24, Loss: 0.04742854833602905\n",
      "Iteration 252, Batch: 25, Loss: 0.032330892980098724\n",
      "Iteration 252, Batch: 26, Loss: 0.08361431211233139\n",
      "Iteration 252, Batch: 27, Loss: 0.06570729613304138\n",
      "Iteration 252, Batch: 28, Loss: 0.03476358950138092\n",
      "Iteration 252, Batch: 29, Loss: 0.057787712663412094\n",
      "Iteration 252, Batch: 30, Loss: 0.03881555050611496\n",
      "Iteration 252, Batch: 31, Loss: 0.06318526715040207\n",
      "Iteration 252, Batch: 32, Loss: 0.04039020463824272\n",
      "Iteration 252, Batch: 33, Loss: 0.03798570856451988\n",
      "Iteration 252, Batch: 34, Loss: 0.04497091844677925\n",
      "Iteration 252, Batch: 35, Loss: 0.05370725691318512\n",
      "Iteration 252, Batch: 36, Loss: 0.040547896176576614\n",
      "Iteration 252, Batch: 37, Loss: 0.029342131689190865\n",
      "Iteration 252, Batch: 38, Loss: 0.041720736771821976\n",
      "Iteration 252, Batch: 39, Loss: 0.049421489238739014\n",
      "Iteration 252, Batch: 40, Loss: 0.05188816040754318\n",
      "Iteration 252, Batch: 41, Loss: 0.06998758018016815\n",
      "Iteration 252, Batch: 42, Loss: 0.05870162695646286\n",
      "Iteration 252, Batch: 43, Loss: 0.03600914403796196\n",
      "Iteration 252, Batch: 44, Loss: 0.04315074905753136\n",
      "Iteration 252, Batch: 45, Loss: 0.02654525637626648\n",
      "Iteration 252, Batch: 46, Loss: 0.056585490703582764\n",
      "Iteration 252, Batch: 47, Loss: 0.04828077182173729\n",
      "Iteration 252, Batch: 48, Loss: 0.04161495715379715\n",
      "Iteration 252, Batch: 49, Loss: 0.03562634065747261\n",
      "Number of layers: 9\n",
      "Iteration 253, Batch: 0, Loss: 0.03742891550064087\n",
      "Iteration 253, Batch: 1, Loss: 0.06669460237026215\n",
      "Iteration 253, Batch: 2, Loss: 0.05150367692112923\n",
      "Iteration 253, Batch: 3, Loss: 0.037062063813209534\n",
      "Iteration 253, Batch: 4, Loss: 0.03995342552661896\n",
      "Iteration 253, Batch: 5, Loss: 0.055160410702228546\n",
      "Iteration 253, Batch: 6, Loss: 0.033958788961172104\n",
      "Iteration 253, Batch: 7, Loss: 0.05921636521816254\n",
      "Iteration 253, Batch: 8, Loss: 0.04296329990029335\n",
      "Iteration 253, Batch: 9, Loss: 0.04784753546118736\n",
      "Iteration 253, Batch: 10, Loss: 0.07567736506462097\n",
      "Iteration 253, Batch: 11, Loss: 0.06280338019132614\n",
      "Iteration 253, Batch: 12, Loss: 0.05129805952310562\n",
      "Iteration 253, Batch: 13, Loss: 0.029518626630306244\n",
      "Iteration 253, Batch: 14, Loss: 0.06227744370698929\n",
      "Iteration 253, Batch: 15, Loss: 0.04157540202140808\n",
      "Iteration 253, Batch: 16, Loss: 0.06579097360372543\n",
      "Iteration 253, Batch: 17, Loss: 0.044924333691596985\n",
      "Iteration 253, Batch: 18, Loss: 0.05230157822370529\n",
      "Iteration 253, Batch: 19, Loss: 0.041821181774139404\n",
      "Iteration 253, Batch: 20, Loss: 0.054591331630945206\n",
      "Iteration 253, Batch: 21, Loss: 0.04147936776280403\n",
      "Iteration 253, Batch: 22, Loss: 0.053567517548799515\n",
      "Iteration 253, Batch: 23, Loss: 0.03190833330154419\n",
      "Iteration 253, Batch: 24, Loss: 0.062008894979953766\n",
      "Iteration 253, Batch: 25, Loss: 0.07083456963300705\n",
      "Iteration 253, Batch: 26, Loss: 0.06982024759054184\n",
      "Iteration 253, Batch: 27, Loss: 0.03425627574324608\n",
      "Iteration 253, Batch: 28, Loss: 0.039656542241573334\n",
      "Iteration 253, Batch: 29, Loss: 0.02016364224255085\n",
      "Iteration 253, Batch: 30, Loss: 0.028848018497228622\n",
      "Iteration 253, Batch: 31, Loss: 0.04672909155488014\n",
      "Iteration 253, Batch: 32, Loss: 0.050077810883522034\n",
      "Iteration 253, Batch: 33, Loss: 0.045585259795188904\n",
      "Iteration 253, Batch: 34, Loss: 0.036383602768182755\n",
      "Iteration 253, Batch: 35, Loss: 0.07405558228492737\n",
      "Iteration 253, Batch: 36, Loss: 0.029434489086270332\n",
      "Iteration 253, Batch: 37, Loss: 0.04942310228943825\n",
      "Iteration 253, Batch: 38, Loss: 0.056309010833501816\n",
      "Iteration 253, Batch: 39, Loss: 0.07559429109096527\n",
      "Iteration 253, Batch: 40, Loss: 0.06059889495372772\n",
      "Iteration 253, Batch: 41, Loss: 0.059576913714408875\n",
      "Iteration 253, Batch: 42, Loss: 0.06189994141459465\n",
      "Iteration 253, Batch: 43, Loss: 0.07973544299602509\n",
      "Iteration 253, Batch: 44, Loss: 0.06787529587745667\n",
      "Iteration 253, Batch: 45, Loss: 0.0463096909224987\n",
      "Iteration 253, Batch: 46, Loss: 0.06211736425757408\n",
      "Iteration 253, Batch: 47, Loss: 0.053994324058294296\n",
      "Iteration 253, Batch: 48, Loss: 0.058766551315784454\n",
      "Iteration 253, Batch: 49, Loss: 0.04118982329964638\n",
      "Number of layers: 9\n",
      "Iteration 254, Batch: 0, Loss: 0.03895309194922447\n",
      "Iteration 254, Batch: 1, Loss: 0.11675462126731873\n",
      "Iteration 254, Batch: 2, Loss: 0.033952098339796066\n",
      "Iteration 254, Batch: 3, Loss: 0.05960811302065849\n",
      "Iteration 254, Batch: 4, Loss: 0.05856252461671829\n",
      "Iteration 254, Batch: 5, Loss: 0.047760386019945145\n",
      "Iteration 254, Batch: 6, Loss: 0.0500807948410511\n",
      "Iteration 254, Batch: 7, Loss: 0.04837142676115036\n",
      "Iteration 254, Batch: 8, Loss: 0.07198578864336014\n",
      "Iteration 254, Batch: 9, Loss: 0.08532381057739258\n",
      "Iteration 254, Batch: 10, Loss: 0.06120702251791954\n",
      "Iteration 254, Batch: 11, Loss: 0.0505991131067276\n",
      "Iteration 254, Batch: 12, Loss: 0.04697522893548012\n",
      "Iteration 254, Batch: 13, Loss: 0.07083656638860703\n",
      "Iteration 254, Batch: 14, Loss: 0.06148324906826019\n",
      "Iteration 254, Batch: 15, Loss: 0.06385236233472824\n",
      "Iteration 254, Batch: 16, Loss: 0.0455780029296875\n",
      "Iteration 254, Batch: 17, Loss: 0.04992600902915001\n",
      "Iteration 254, Batch: 18, Loss: 0.0337522067129612\n",
      "Iteration 254, Batch: 19, Loss: 0.07004573196172714\n",
      "Iteration 254, Batch: 20, Loss: 0.05461949110031128\n",
      "Iteration 254, Batch: 21, Loss: 0.052728671580553055\n",
      "Iteration 254, Batch: 22, Loss: 0.07253491878509521\n",
      "Iteration 254, Batch: 23, Loss: 0.07072838395833969\n",
      "Iteration 254, Batch: 24, Loss: 0.05047856643795967\n",
      "Iteration 254, Batch: 25, Loss: 0.06857883930206299\n",
      "Iteration 254, Batch: 26, Loss: 0.04419858008623123\n",
      "Iteration 254, Batch: 27, Loss: 0.06431716680526733\n",
      "Iteration 254, Batch: 28, Loss: 0.09098823368549347\n",
      "Iteration 254, Batch: 29, Loss: 0.060590147972106934\n",
      "Iteration 254, Batch: 30, Loss: 0.07123647630214691\n",
      "Iteration 254, Batch: 31, Loss: 0.03756168484687805\n",
      "Iteration 254, Batch: 32, Loss: 0.03682660311460495\n",
      "Iteration 254, Batch: 33, Loss: 0.06408742815256119\n",
      "Iteration 254, Batch: 34, Loss: 0.04001247137784958\n",
      "Iteration 254, Batch: 35, Loss: 0.059887178242206573\n",
      "Iteration 254, Batch: 36, Loss: 0.038830697536468506\n",
      "Iteration 254, Batch: 37, Loss: 0.058400969952344894\n",
      "Iteration 254, Batch: 38, Loss: 0.060668062418699265\n",
      "Iteration 254, Batch: 39, Loss: 0.060925353318452835\n",
      "Iteration 254, Batch: 40, Loss: 0.0391911081969738\n",
      "Iteration 254, Batch: 41, Loss: 0.057571619749069214\n",
      "Iteration 254, Batch: 42, Loss: 0.06629826128482819\n",
      "Iteration 254, Batch: 43, Loss: 0.060692813247442245\n",
      "Iteration 254, Batch: 44, Loss: 0.05365801230072975\n",
      "Iteration 254, Batch: 45, Loss: 0.03188139200210571\n",
      "Iteration 254, Batch: 46, Loss: 0.049460284411907196\n",
      "Iteration 254, Batch: 47, Loss: 0.036680348217487335\n",
      "Iteration 254, Batch: 48, Loss: 0.03412671014666557\n",
      "Iteration 254, Batch: 49, Loss: 0.06330905109643936\n",
      "Number of layers: 9\n",
      "Iteration 255, Batch: 0, Loss: 0.07039804011583328\n",
      "Iteration 255, Batch: 1, Loss: 0.045825112611055374\n",
      "Iteration 255, Batch: 2, Loss: 0.03943703696131706\n",
      "Iteration 255, Batch: 3, Loss: 0.039933979511260986\n",
      "Iteration 255, Batch: 4, Loss: 0.044815175235271454\n",
      "Iteration 255, Batch: 5, Loss: 0.03699910640716553\n",
      "Iteration 255, Batch: 6, Loss: 0.03608342632651329\n",
      "Iteration 255, Batch: 7, Loss: 0.08612988889217377\n",
      "Iteration 255, Batch: 8, Loss: 0.06805305927991867\n",
      "Iteration 255, Batch: 9, Loss: 0.04668742045760155\n",
      "Iteration 255, Batch: 10, Loss: 0.06291873753070831\n",
      "Iteration 255, Batch: 11, Loss: 0.036829955875873566\n",
      "Iteration 255, Batch: 12, Loss: 0.06424485146999359\n",
      "Iteration 255, Batch: 13, Loss: 0.046523354947566986\n",
      "Iteration 255, Batch: 14, Loss: 0.11987161636352539\n",
      "Iteration 255, Batch: 15, Loss: 0.13718967139720917\n",
      "Iteration 255, Batch: 16, Loss: 0.05767035484313965\n",
      "Iteration 255, Batch: 17, Loss: 0.07571670413017273\n",
      "Iteration 255, Batch: 18, Loss: 0.0719422772526741\n",
      "Iteration 255, Batch: 19, Loss: 0.04829365015029907\n",
      "Iteration 255, Batch: 20, Loss: 0.05741596966981888\n",
      "Iteration 255, Batch: 21, Loss: 0.10441243648529053\n",
      "Iteration 255, Batch: 22, Loss: 0.09317992627620697\n",
      "Iteration 255, Batch: 23, Loss: 0.06626694649457932\n",
      "Iteration 255, Batch: 24, Loss: 0.030265115201473236\n",
      "Iteration 255, Batch: 25, Loss: 0.05471399053931236\n",
      "Iteration 255, Batch: 26, Loss: 0.059082821011543274\n",
      "Iteration 255, Batch: 27, Loss: 0.06983362138271332\n",
      "Iteration 255, Batch: 28, Loss: 0.05984782055020332\n",
      "Iteration 255, Batch: 29, Loss: 0.05473407730460167\n",
      "Iteration 255, Batch: 30, Loss: 0.05439353361725807\n",
      "Iteration 255, Batch: 31, Loss: 0.04939219355583191\n",
      "Iteration 255, Batch: 32, Loss: 0.05334355682134628\n",
      "Iteration 255, Batch: 33, Loss: 0.0642438754439354\n",
      "Iteration 255, Batch: 34, Loss: 0.04434816911816597\n",
      "Iteration 255, Batch: 35, Loss: 0.037748947739601135\n",
      "Iteration 255, Batch: 36, Loss: 0.034082040190696716\n",
      "Iteration 255, Batch: 37, Loss: 0.03647705167531967\n",
      "Iteration 255, Batch: 38, Loss: 0.04916676506400108\n",
      "Iteration 255, Batch: 39, Loss: 0.018194854259490967\n",
      "Iteration 255, Batch: 40, Loss: 0.05207012966275215\n",
      "Iteration 255, Batch: 41, Loss: 0.05738019198179245\n",
      "Iteration 255, Batch: 42, Loss: 0.045259322971105576\n",
      "Iteration 255, Batch: 43, Loss: 0.05476727336645126\n",
      "Iteration 255, Batch: 44, Loss: 0.03696765378117561\n",
      "Iteration 255, Batch: 45, Loss: 0.03285008296370506\n",
      "Iteration 255, Batch: 46, Loss: 0.04874841123819351\n",
      "Iteration 255, Batch: 47, Loss: 0.042567942291498184\n",
      "Iteration 255, Batch: 48, Loss: 0.04169919341802597\n",
      "Iteration 255, Batch: 49, Loss: 0.06076018139719963\n",
      "Number of layers: 9\n",
      "Iteration 256, Batch: 0, Loss: 0.049191735684871674\n",
      "Iteration 256, Batch: 1, Loss: 0.03882813826203346\n",
      "Iteration 256, Batch: 2, Loss: 0.05056341364979744\n",
      "Iteration 256, Batch: 3, Loss: 0.040885552763938904\n",
      "Iteration 256, Batch: 4, Loss: 0.0564243383705616\n",
      "Iteration 256, Batch: 5, Loss: 0.017585717141628265\n",
      "Iteration 256, Batch: 6, Loss: 0.03477706387639046\n",
      "Iteration 256, Batch: 7, Loss: 0.03949703276157379\n",
      "Iteration 256, Batch: 8, Loss: 0.08134005218744278\n",
      "Iteration 256, Batch: 9, Loss: 0.05167146399617195\n",
      "Iteration 256, Batch: 10, Loss: 0.05589886009693146\n",
      "Iteration 256, Batch: 11, Loss: 0.0694449245929718\n",
      "Iteration 256, Batch: 12, Loss: 0.05366196483373642\n",
      "Iteration 256, Batch: 13, Loss: 0.0752074122428894\n",
      "Iteration 256, Batch: 14, Loss: 0.04069020226597786\n",
      "Iteration 256, Batch: 15, Loss: 0.05721215531229973\n",
      "Iteration 256, Batch: 16, Loss: 0.05879192054271698\n",
      "Iteration 256, Batch: 17, Loss: 0.03093268722295761\n",
      "Iteration 256, Batch: 18, Loss: 0.029860638082027435\n",
      "Iteration 256, Batch: 19, Loss: 0.05394701659679413\n",
      "Iteration 256, Batch: 20, Loss: 0.07028193026781082\n",
      "Iteration 256, Batch: 21, Loss: 0.05676254257559776\n",
      "Iteration 256, Batch: 22, Loss: 0.05449746549129486\n",
      "Iteration 256, Batch: 23, Loss: 0.0837857723236084\n",
      "Iteration 256, Batch: 24, Loss: 0.05112912133336067\n",
      "Iteration 256, Batch: 25, Loss: 0.049122486263513565\n",
      "Iteration 256, Batch: 26, Loss: 0.09555164724588394\n",
      "Iteration 256, Batch: 27, Loss: 0.035537999123334885\n",
      "Iteration 256, Batch: 28, Loss: 0.03483879938721657\n",
      "Iteration 256, Batch: 29, Loss: 0.05850476771593094\n",
      "Iteration 256, Batch: 30, Loss: 0.09655867516994476\n",
      "Iteration 256, Batch: 31, Loss: 0.09515684098005295\n",
      "Iteration 256, Batch: 32, Loss: 0.07523999363183975\n",
      "Iteration 256, Batch: 33, Loss: 0.07936941832304001\n",
      "Iteration 256, Batch: 34, Loss: 0.09215341508388519\n",
      "Iteration 256, Batch: 35, Loss: 0.07675641030073166\n",
      "Iteration 256, Batch: 36, Loss: 0.072538822889328\n",
      "Iteration 256, Batch: 37, Loss: 0.08474599570035934\n",
      "Iteration 256, Batch: 38, Loss: 0.09989803284406662\n",
      "Iteration 256, Batch: 39, Loss: 0.09870175272226334\n",
      "Iteration 256, Batch: 40, Loss: 0.05739937350153923\n",
      "Iteration 256, Batch: 41, Loss: 0.0595356822013855\n",
      "Iteration 256, Batch: 42, Loss: 0.0630335882306099\n",
      "Iteration 256, Batch: 43, Loss: 0.03831065446138382\n",
      "Iteration 256, Batch: 44, Loss: 0.04626123234629631\n",
      "Iteration 256, Batch: 45, Loss: 0.0658520981669426\n",
      "Iteration 256, Batch: 46, Loss: 0.043739743530750275\n",
      "Iteration 256, Batch: 47, Loss: 0.11160949617624283\n",
      "Iteration 256, Batch: 48, Loss: 0.08981397747993469\n",
      "Iteration 256, Batch: 49, Loss: 0.09465423971414566\n",
      "Number of layers: 9\n",
      "Iteration 257, Batch: 0, Loss: 0.08770619332790375\n",
      "Iteration 257, Batch: 1, Loss: 0.030749859288334846\n",
      "Iteration 257, Batch: 2, Loss: 0.06982886791229248\n",
      "Iteration 257, Batch: 3, Loss: 0.05199981480836868\n",
      "Iteration 257, Batch: 4, Loss: 0.07276376336812973\n",
      "Iteration 257, Batch: 5, Loss: 0.06627395004034042\n",
      "Iteration 257, Batch: 6, Loss: 0.06596168875694275\n",
      "Iteration 257, Batch: 7, Loss: 0.09170402586460114\n",
      "Iteration 257, Batch: 8, Loss: 0.08233245462179184\n",
      "Iteration 257, Batch: 9, Loss: 0.07888271659612656\n",
      "Iteration 257, Batch: 10, Loss: 0.08074679970741272\n",
      "Iteration 257, Batch: 11, Loss: 0.06265904009342194\n",
      "Iteration 257, Batch: 12, Loss: 0.048033371567726135\n",
      "Iteration 257, Batch: 13, Loss: 0.0636540949344635\n",
      "Iteration 257, Batch: 14, Loss: 0.0684317871928215\n",
      "Iteration 257, Batch: 15, Loss: 0.03826289623975754\n",
      "Iteration 257, Batch: 16, Loss: 0.04122529923915863\n",
      "Iteration 257, Batch: 17, Loss: 0.0732073187828064\n",
      "Iteration 257, Batch: 18, Loss: 0.06277044862508774\n",
      "Iteration 257, Batch: 19, Loss: 0.06241675093770027\n",
      "Iteration 257, Batch: 20, Loss: 0.09578729420900345\n",
      "Iteration 257, Batch: 21, Loss: 0.09828206151723862\n",
      "Iteration 257, Batch: 22, Loss: 0.05783839523792267\n",
      "Iteration 257, Batch: 23, Loss: 0.06846959888935089\n",
      "Iteration 257, Batch: 24, Loss: 0.05549080669879913\n",
      "Iteration 257, Batch: 25, Loss: 0.08716808259487152\n",
      "Iteration 257, Batch: 26, Loss: 0.07009810209274292\n",
      "Iteration 257, Batch: 27, Loss: 0.04808078706264496\n",
      "Iteration 257, Batch: 28, Loss: 0.07055044174194336\n",
      "Iteration 257, Batch: 29, Loss: 0.044746119529008865\n",
      "Iteration 257, Batch: 30, Loss: 0.056667398661375046\n",
      "Iteration 257, Batch: 31, Loss: 0.06075423210859299\n",
      "Iteration 257, Batch: 32, Loss: 0.05386420711874962\n",
      "Iteration 257, Batch: 33, Loss: 0.037991032004356384\n",
      "Iteration 257, Batch: 34, Loss: 0.06277744472026825\n",
      "Iteration 257, Batch: 35, Loss: 0.0420791320502758\n",
      "Iteration 257, Batch: 36, Loss: 0.04194822162389755\n",
      "Iteration 257, Batch: 37, Loss: 0.060366105288267136\n",
      "Iteration 257, Batch: 38, Loss: 0.042127255350351334\n",
      "Iteration 257, Batch: 39, Loss: 0.06926077604293823\n",
      "Iteration 257, Batch: 40, Loss: 0.08709617704153061\n",
      "Iteration 257, Batch: 41, Loss: 0.0648210346698761\n",
      "Iteration 257, Batch: 42, Loss: 0.07235115021467209\n",
      "Iteration 257, Batch: 43, Loss: 0.043549373745918274\n",
      "Iteration 257, Batch: 44, Loss: 0.07598979026079178\n",
      "Iteration 257, Batch: 45, Loss: 0.06484818458557129\n",
      "Iteration 257, Batch: 46, Loss: 0.053655222058296204\n",
      "Iteration 257, Batch: 47, Loss: 0.0964464619755745\n",
      "Iteration 257, Batch: 48, Loss: 0.08429885655641556\n",
      "Iteration 257, Batch: 49, Loss: 0.12147308140993118\n",
      "Number of layers: 9\n",
      "Iteration 258, Batch: 0, Loss: 0.0815708264708519\n",
      "Iteration 258, Batch: 1, Loss: 0.09102479368448257\n",
      "Iteration 258, Batch: 2, Loss: 0.09194034337997437\n",
      "Iteration 258, Batch: 3, Loss: 0.10432395339012146\n",
      "Iteration 258, Batch: 4, Loss: 0.08168920129537582\n",
      "Iteration 258, Batch: 5, Loss: 0.07402801513671875\n",
      "Iteration 258, Batch: 6, Loss: 0.0890435203909874\n",
      "Iteration 258, Batch: 7, Loss: 0.11652922630310059\n",
      "Iteration 258, Batch: 8, Loss: 0.06761660426855087\n",
      "Iteration 258, Batch: 9, Loss: 0.0662587508559227\n",
      "Iteration 258, Batch: 10, Loss: 0.07770057022571564\n",
      "Iteration 258, Batch: 11, Loss: 0.0982784852385521\n",
      "Iteration 258, Batch: 12, Loss: 0.0847632884979248\n",
      "Iteration 258, Batch: 13, Loss: 0.07381582260131836\n",
      "Iteration 258, Batch: 14, Loss: 0.06999613344669342\n",
      "Iteration 258, Batch: 15, Loss: 0.07900960743427277\n",
      "Iteration 258, Batch: 16, Loss: 0.05897977203130722\n",
      "Iteration 258, Batch: 17, Loss: 0.039306264370679855\n",
      "Iteration 258, Batch: 18, Loss: 0.0736839696764946\n",
      "Iteration 258, Batch: 19, Loss: 0.0863925889134407\n",
      "Iteration 258, Batch: 20, Loss: 0.03478153049945831\n",
      "Iteration 258, Batch: 21, Loss: 0.0535842590034008\n",
      "Iteration 258, Batch: 22, Loss: 0.0565078929066658\n",
      "Iteration 258, Batch: 23, Loss: 0.06779906898736954\n",
      "Iteration 258, Batch: 24, Loss: 0.08388952165842056\n",
      "Iteration 258, Batch: 25, Loss: 0.07608041912317276\n",
      "Iteration 258, Batch: 26, Loss: 0.07991119474172592\n",
      "Iteration 258, Batch: 27, Loss: 0.08047974109649658\n",
      "Iteration 258, Batch: 28, Loss: 0.0562744177877903\n",
      "Iteration 258, Batch: 29, Loss: 0.0657653734087944\n",
      "Iteration 258, Batch: 30, Loss: 0.05980420485138893\n",
      "Iteration 258, Batch: 31, Loss: 0.057938143610954285\n",
      "Iteration 258, Batch: 32, Loss: 0.04124581813812256\n",
      "Iteration 258, Batch: 33, Loss: 0.04895571619272232\n",
      "Iteration 258, Batch: 34, Loss: 0.061338551342487335\n",
      "Iteration 258, Batch: 35, Loss: 0.1111251637339592\n",
      "Iteration 258, Batch: 36, Loss: 0.05680500343441963\n",
      "Iteration 258, Batch: 37, Loss: 0.06752942502498627\n",
      "Iteration 258, Batch: 38, Loss: 0.07352888584136963\n",
      "Iteration 258, Batch: 39, Loss: 0.04732639342546463\n",
      "Iteration 258, Batch: 40, Loss: 0.05042165517807007\n",
      "Iteration 258, Batch: 41, Loss: 0.06271619349718094\n",
      "Iteration 258, Batch: 42, Loss: 0.057198092341423035\n",
      "Iteration 258, Batch: 43, Loss: 0.042136386036872864\n",
      "Iteration 258, Batch: 44, Loss: 0.06117533519864082\n",
      "Iteration 258, Batch: 45, Loss: 0.052980467677116394\n",
      "Iteration 258, Batch: 46, Loss: 0.05797098949551582\n",
      "Iteration 258, Batch: 47, Loss: 0.05029825121164322\n",
      "Iteration 258, Batch: 48, Loss: 0.05685894191265106\n",
      "Iteration 258, Batch: 49, Loss: 0.05407122150063515\n",
      "Number of layers: 9\n",
      "Iteration 259, Batch: 0, Loss: 0.04077123478055\n",
      "Iteration 259, Batch: 1, Loss: 0.05636513605713844\n",
      "Iteration 259, Batch: 2, Loss: 0.04663071408867836\n",
      "Iteration 259, Batch: 3, Loss: 0.054364852607250214\n",
      "Iteration 259, Batch: 4, Loss: 0.0438491590321064\n",
      "Iteration 259, Batch: 5, Loss: 0.03701183572411537\n",
      "Iteration 259, Batch: 6, Loss: 0.03603636473417282\n",
      "Iteration 259, Batch: 7, Loss: 0.05509108304977417\n",
      "Iteration 259, Batch: 8, Loss: 0.08096999675035477\n",
      "Iteration 259, Batch: 9, Loss: 0.07660393416881561\n",
      "Iteration 259, Batch: 10, Loss: 0.04525179788470268\n",
      "Iteration 259, Batch: 11, Loss: 0.0410943366587162\n",
      "Iteration 259, Batch: 12, Loss: 0.08295026421546936\n",
      "Iteration 259, Batch: 13, Loss: 0.05867584049701691\n",
      "Iteration 259, Batch: 14, Loss: 0.06105704605579376\n",
      "Iteration 259, Batch: 15, Loss: 0.05764589086174965\n",
      "Iteration 259, Batch: 16, Loss: 0.07338061928749084\n",
      "Iteration 259, Batch: 17, Loss: 0.04341481253504753\n",
      "Iteration 259, Batch: 18, Loss: 0.06740599125623703\n",
      "Iteration 259, Batch: 19, Loss: 0.05553600192070007\n",
      "Iteration 259, Batch: 20, Loss: 0.0325118713080883\n",
      "Iteration 259, Batch: 21, Loss: 0.05675103887915611\n",
      "Iteration 259, Batch: 22, Loss: 0.050937872380018234\n",
      "Iteration 259, Batch: 23, Loss: 0.046643126755952835\n",
      "Iteration 259, Batch: 24, Loss: 0.046076640486717224\n",
      "Iteration 259, Batch: 25, Loss: 0.04159526899456978\n",
      "Iteration 259, Batch: 26, Loss: 0.046708639711141586\n",
      "Iteration 259, Batch: 27, Loss: 0.05302705615758896\n",
      "Iteration 259, Batch: 28, Loss: 0.050188228487968445\n",
      "Iteration 259, Batch: 29, Loss: 0.03151443973183632\n",
      "Iteration 259, Batch: 30, Loss: 0.054915767163038254\n",
      "Iteration 259, Batch: 31, Loss: 0.05320592597126961\n",
      "Iteration 259, Batch: 32, Loss: 0.04697689786553383\n",
      "Iteration 259, Batch: 33, Loss: 0.05164448544383049\n",
      "Iteration 259, Batch: 34, Loss: 0.05469108372926712\n",
      "Iteration 259, Batch: 35, Loss: 0.04016388952732086\n",
      "Iteration 259, Batch: 36, Loss: 0.033900827169418335\n",
      "Iteration 259, Batch: 37, Loss: 0.03314892575144768\n",
      "Iteration 259, Batch: 38, Loss: 0.044773168861866\n",
      "Iteration 259, Batch: 39, Loss: 0.05963340774178505\n",
      "Iteration 259, Batch: 40, Loss: 0.06801837682723999\n",
      "Iteration 259, Batch: 41, Loss: 0.03809301182627678\n",
      "Iteration 259, Batch: 42, Loss: 0.06555323302745819\n",
      "Iteration 259, Batch: 43, Loss: 0.03778266906738281\n",
      "Iteration 259, Batch: 44, Loss: 0.04661595821380615\n",
      "Iteration 259, Batch: 45, Loss: 0.02482975833117962\n",
      "Iteration 259, Batch: 46, Loss: 0.041850071400403976\n",
      "Iteration 259, Batch: 47, Loss: 0.03992130234837532\n",
      "Iteration 259, Batch: 48, Loss: 0.07467389106750488\n",
      "Iteration 259, Batch: 49, Loss: 0.04136437177658081\n",
      "Number of layers: 9\n",
      "Iteration 260, Batch: 0, Loss: 0.055363692343235016\n",
      "Iteration 260, Batch: 1, Loss: 0.058781832456588745\n",
      "Iteration 260, Batch: 2, Loss: 0.04167572408914566\n",
      "Iteration 260, Batch: 3, Loss: 0.05501517280936241\n",
      "Iteration 260, Batch: 4, Loss: 0.05026192218065262\n",
      "Iteration 260, Batch: 5, Loss: 0.05069639906287193\n",
      "Iteration 260, Batch: 6, Loss: 0.022573044523596764\n",
      "Iteration 260, Batch: 7, Loss: 0.035405248403549194\n",
      "Iteration 260, Batch: 8, Loss: 0.03800582140684128\n",
      "Iteration 260, Batch: 9, Loss: 0.053617753088474274\n",
      "Iteration 260, Batch: 10, Loss: 0.0709194615483284\n",
      "Iteration 260, Batch: 11, Loss: 0.06040129065513611\n",
      "Iteration 260, Batch: 12, Loss: 0.042921941727399826\n",
      "Iteration 260, Batch: 13, Loss: 0.0636151134967804\n",
      "Iteration 260, Batch: 14, Loss: 0.048146676272153854\n",
      "Iteration 260, Batch: 15, Loss: 0.0535728819668293\n",
      "Iteration 260, Batch: 16, Loss: 0.06482212245464325\n",
      "Iteration 260, Batch: 17, Loss: 0.06147073581814766\n",
      "Iteration 260, Batch: 18, Loss: 0.04919351637363434\n",
      "Iteration 260, Batch: 19, Loss: 0.0490998737514019\n",
      "Iteration 260, Batch: 20, Loss: 0.07166366279125214\n",
      "Iteration 260, Batch: 21, Loss: 0.05230383202433586\n",
      "Iteration 260, Batch: 22, Loss: 0.04651637002825737\n",
      "Iteration 260, Batch: 23, Loss: 0.06239460036158562\n",
      "Iteration 260, Batch: 24, Loss: 0.05185290798544884\n",
      "Iteration 260, Batch: 25, Loss: 0.04216868802905083\n",
      "Iteration 260, Batch: 26, Loss: 0.033140480518341064\n",
      "Iteration 260, Batch: 27, Loss: 0.05624612793326378\n",
      "Iteration 260, Batch: 28, Loss: 0.05616176500916481\n",
      "Iteration 260, Batch: 29, Loss: 0.047557227313518524\n",
      "Iteration 260, Batch: 30, Loss: 0.05400941148400307\n",
      "Iteration 260, Batch: 31, Loss: 0.03575761988759041\n",
      "Iteration 260, Batch: 32, Loss: 0.02700488455593586\n",
      "Iteration 260, Batch: 33, Loss: 0.06630432605743408\n",
      "Iteration 260, Batch: 34, Loss: 0.05024314299225807\n",
      "Iteration 260, Batch: 35, Loss: 0.07113112509250641\n",
      "Iteration 260, Batch: 36, Loss: 0.05395298823714256\n",
      "Iteration 260, Batch: 37, Loss: 0.11050379276275635\n",
      "Iteration 260, Batch: 38, Loss: 0.11136334389448166\n",
      "Iteration 260, Batch: 39, Loss: 0.08719363808631897\n",
      "Iteration 260, Batch: 40, Loss: 0.024498209357261658\n",
      "Iteration 260, Batch: 41, Loss: 0.06329274922609329\n",
      "Iteration 260, Batch: 42, Loss: 0.037723761051893234\n",
      "Iteration 260, Batch: 43, Loss: 0.06057080999016762\n",
      "Iteration 260, Batch: 44, Loss: 0.06369447708129883\n",
      "Iteration 260, Batch: 45, Loss: 0.051029011607170105\n",
      "Iteration 260, Batch: 46, Loss: 0.06388181447982788\n",
      "Iteration 260, Batch: 47, Loss: 0.08140932023525238\n",
      "Iteration 260, Batch: 48, Loss: 0.04777827113866806\n",
      "Iteration 260, Batch: 49, Loss: 0.045140303671360016\n",
      "Number of layers: 9\n",
      "Iteration 261, Batch: 0, Loss: 0.038675036281347275\n",
      "Iteration 261, Batch: 1, Loss: 0.047052521258592606\n",
      "Iteration 261, Batch: 2, Loss: 0.08756259083747864\n",
      "Iteration 261, Batch: 3, Loss: 0.04775113984942436\n",
      "Iteration 261, Batch: 4, Loss: 0.05947014316916466\n",
      "Iteration 261, Batch: 5, Loss: 0.041208796203136444\n",
      "Iteration 261, Batch: 6, Loss: 0.024807991459965706\n",
      "Iteration 261, Batch: 7, Loss: 0.05787172168493271\n",
      "Iteration 261, Batch: 8, Loss: 0.04050429165363312\n",
      "Iteration 261, Batch: 9, Loss: 0.05161750689148903\n",
      "Iteration 261, Batch: 10, Loss: 0.06980453431606293\n",
      "Iteration 261, Batch: 11, Loss: 0.06650405377149582\n",
      "Iteration 261, Batch: 12, Loss: 0.08096900582313538\n",
      "Iteration 261, Batch: 13, Loss: 0.04123419150710106\n",
      "Iteration 261, Batch: 14, Loss: 0.0569474957883358\n",
      "Iteration 261, Batch: 15, Loss: 0.03899786248803139\n",
      "Iteration 261, Batch: 16, Loss: 0.056445587426424026\n",
      "Iteration 261, Batch: 17, Loss: 0.06325925886631012\n",
      "Iteration 261, Batch: 18, Loss: 0.048102881759405136\n",
      "Iteration 261, Batch: 19, Loss: 0.05797313526272774\n",
      "Iteration 261, Batch: 20, Loss: 0.051575031131505966\n",
      "Iteration 261, Batch: 21, Loss: 0.04982660338282585\n",
      "Iteration 261, Batch: 22, Loss: 0.06419084221124649\n",
      "Iteration 261, Batch: 23, Loss: 0.06169263646006584\n",
      "Iteration 261, Batch: 24, Loss: 0.08231023699045181\n",
      "Iteration 261, Batch: 25, Loss: 0.047660425305366516\n",
      "Iteration 261, Batch: 26, Loss: 0.029885834082961082\n",
      "Iteration 261, Batch: 27, Loss: 0.06173946335911751\n",
      "Iteration 261, Batch: 28, Loss: 0.058538902550935745\n",
      "Iteration 261, Batch: 29, Loss: 0.0688755214214325\n",
      "Iteration 261, Batch: 30, Loss: 0.04741881415247917\n",
      "Iteration 261, Batch: 31, Loss: 0.04718862101435661\n",
      "Iteration 261, Batch: 32, Loss: 0.07456253468990326\n",
      "Iteration 261, Batch: 33, Loss: 0.06007552146911621\n",
      "Iteration 261, Batch: 34, Loss: 0.06327848136425018\n",
      "Iteration 261, Batch: 35, Loss: 0.04378524050116539\n",
      "Iteration 261, Batch: 36, Loss: 0.07135891914367676\n",
      "Iteration 261, Batch: 37, Loss: 0.05246013030409813\n",
      "Iteration 261, Batch: 38, Loss: 0.05631072074174881\n",
      "Iteration 261, Batch: 39, Loss: 0.04327728971838951\n",
      "Iteration 261, Batch: 40, Loss: 0.07193318754434586\n",
      "Iteration 261, Batch: 41, Loss: 0.05287805572152138\n",
      "Iteration 261, Batch: 42, Loss: 0.07740163803100586\n",
      "Iteration 261, Batch: 43, Loss: 0.05506138503551483\n",
      "Iteration 261, Batch: 44, Loss: 0.0524505116045475\n",
      "Iteration 261, Batch: 45, Loss: 0.0698363408446312\n",
      "Iteration 261, Batch: 46, Loss: 0.06682728976011276\n",
      "Iteration 261, Batch: 47, Loss: 0.08236551284790039\n",
      "Iteration 261, Batch: 48, Loss: 0.03435654193162918\n",
      "Iteration 261, Batch: 49, Loss: 0.05153786391019821\n",
      "Number of layers: 9\n",
      "Iteration 262, Batch: 0, Loss: 0.038105159997940063\n",
      "Iteration 262, Batch: 1, Loss: 0.04822351783514023\n",
      "Iteration 262, Batch: 2, Loss: 0.05993080139160156\n",
      "Iteration 262, Batch: 3, Loss: 0.05529305338859558\n",
      "Iteration 262, Batch: 4, Loss: 0.03825180605053902\n",
      "Iteration 262, Batch: 5, Loss: 0.04876770079135895\n",
      "Iteration 262, Batch: 6, Loss: 0.0598653219640255\n",
      "Iteration 262, Batch: 7, Loss: 0.052236709743738174\n",
      "Iteration 262, Batch: 8, Loss: 0.07259812206029892\n",
      "Iteration 262, Batch: 9, Loss: 0.057699304074048996\n",
      "Iteration 262, Batch: 10, Loss: 0.06020699068903923\n",
      "Iteration 262, Batch: 11, Loss: 0.055953968316316605\n",
      "Iteration 262, Batch: 12, Loss: 0.04230019822716713\n",
      "Iteration 262, Batch: 13, Loss: 0.053736135363578796\n",
      "Iteration 262, Batch: 14, Loss: 0.06276889145374298\n",
      "Iteration 262, Batch: 15, Loss: 0.029897507280111313\n",
      "Iteration 262, Batch: 16, Loss: 0.05838814005255699\n",
      "Iteration 262, Batch: 17, Loss: 0.052416037768125534\n",
      "Iteration 262, Batch: 18, Loss: 0.07055837661027908\n",
      "Iteration 262, Batch: 19, Loss: 0.04698406159877777\n",
      "Iteration 262, Batch: 20, Loss: 0.0619630329310894\n",
      "Iteration 262, Batch: 21, Loss: 0.052556928247213364\n",
      "Iteration 262, Batch: 22, Loss: 0.04037103429436684\n",
      "Iteration 262, Batch: 23, Loss: 0.07404138892889023\n",
      "Iteration 262, Batch: 24, Loss: 0.08075112849473953\n",
      "Iteration 262, Batch: 25, Loss: 0.06941340118646622\n",
      "Iteration 262, Batch: 26, Loss: 0.042183760553598404\n",
      "Iteration 262, Batch: 27, Loss: 0.05687057226896286\n",
      "Iteration 262, Batch: 28, Loss: 0.07837697863578796\n",
      "Iteration 262, Batch: 29, Loss: 0.04872710630297661\n",
      "Iteration 262, Batch: 30, Loss: 0.03322592005133629\n",
      "Iteration 262, Batch: 31, Loss: 0.06251857429742813\n",
      "Iteration 262, Batch: 32, Loss: 0.058488473296165466\n",
      "Iteration 262, Batch: 33, Loss: 0.052530739456415176\n",
      "Iteration 262, Batch: 34, Loss: 0.06880884617567062\n",
      "Iteration 262, Batch: 35, Loss: 0.05510151758790016\n",
      "Iteration 262, Batch: 36, Loss: 0.059362880885601044\n",
      "Iteration 262, Batch: 37, Loss: 0.059323593974113464\n",
      "Iteration 262, Batch: 38, Loss: 0.0587986521422863\n",
      "Iteration 262, Batch: 39, Loss: 0.05430199205875397\n",
      "Iteration 262, Batch: 40, Loss: 0.05516192317008972\n",
      "Iteration 262, Batch: 41, Loss: 0.060646235942840576\n",
      "Iteration 262, Batch: 42, Loss: 0.040249280631542206\n",
      "Iteration 262, Batch: 43, Loss: 0.04604603722691536\n",
      "Iteration 262, Batch: 44, Loss: 0.05005505308508873\n",
      "Iteration 262, Batch: 45, Loss: 0.060097645968198776\n",
      "Iteration 262, Batch: 46, Loss: 0.05585233122110367\n",
      "Iteration 262, Batch: 47, Loss: 0.04796842485666275\n",
      "Iteration 262, Batch: 48, Loss: 0.04863238334655762\n",
      "Iteration 262, Batch: 49, Loss: 0.05528218299150467\n",
      "Number of layers: 9\n",
      "Iteration 263, Batch: 0, Loss: 0.0506141223013401\n",
      "Iteration 263, Batch: 1, Loss: 0.061778414994478226\n",
      "Iteration 263, Batch: 2, Loss: 0.06332714855670929\n",
      "Iteration 263, Batch: 3, Loss: 0.06482838839292526\n",
      "Iteration 263, Batch: 4, Loss: 0.06393134593963623\n",
      "Iteration 263, Batch: 5, Loss: 0.07860197871923447\n",
      "Iteration 263, Batch: 6, Loss: 0.06631051003932953\n",
      "Iteration 263, Batch: 7, Loss: 0.08682039380073547\n",
      "Iteration 263, Batch: 8, Loss: 0.05262390151619911\n",
      "Iteration 263, Batch: 9, Loss: 0.07405328750610352\n",
      "Iteration 263, Batch: 10, Loss: 0.06888172775506973\n",
      "Iteration 263, Batch: 11, Loss: 0.058688320219516754\n",
      "Iteration 263, Batch: 12, Loss: 0.058181796222925186\n",
      "Iteration 263, Batch: 13, Loss: 0.0677727609872818\n",
      "Iteration 263, Batch: 14, Loss: 0.055081844329833984\n",
      "Iteration 263, Batch: 15, Loss: 0.04647568613290787\n",
      "Iteration 263, Batch: 16, Loss: 0.037129729986190796\n",
      "Iteration 263, Batch: 17, Loss: 0.048909738659858704\n",
      "Iteration 263, Batch: 18, Loss: 0.057186421006917953\n",
      "Iteration 263, Batch: 19, Loss: 0.06694509834051132\n",
      "Iteration 263, Batch: 20, Loss: 0.04501420259475708\n",
      "Iteration 263, Batch: 21, Loss: 0.08091861754655838\n",
      "Iteration 263, Batch: 22, Loss: 0.09023908525705338\n",
      "Iteration 263, Batch: 23, Loss: 0.0641375333070755\n",
      "Iteration 263, Batch: 24, Loss: 0.0420718789100647\n",
      "Iteration 263, Batch: 25, Loss: 0.0625453069806099\n",
      "Iteration 263, Batch: 26, Loss: 0.03797705098986626\n",
      "Iteration 263, Batch: 27, Loss: 0.07936665415763855\n",
      "Iteration 263, Batch: 28, Loss: 0.07773322612047195\n",
      "Iteration 263, Batch: 29, Loss: 0.06069432199001312\n",
      "Iteration 263, Batch: 30, Loss: 0.06327363848686218\n",
      "Iteration 263, Batch: 31, Loss: 0.07169879972934723\n",
      "Iteration 263, Batch: 32, Loss: 0.10162610560655594\n",
      "Iteration 263, Batch: 33, Loss: 0.08237121999263763\n",
      "Iteration 263, Batch: 34, Loss: 0.05123768746852875\n",
      "Iteration 263, Batch: 35, Loss: 0.06656871736049652\n",
      "Iteration 263, Batch: 36, Loss: 0.036065325140953064\n",
      "Iteration 263, Batch: 37, Loss: 0.038695577532052994\n",
      "Iteration 263, Batch: 38, Loss: 0.06807790696620941\n",
      "Iteration 263, Batch: 39, Loss: 0.04973362386226654\n",
      "Iteration 263, Batch: 40, Loss: 0.06438762694597244\n",
      "Iteration 263, Batch: 41, Loss: 0.03395678848028183\n",
      "Iteration 263, Batch: 42, Loss: 0.05773033946752548\n",
      "Iteration 263, Batch: 43, Loss: 0.06571854650974274\n",
      "Iteration 263, Batch: 44, Loss: 0.04812991991639137\n",
      "Iteration 263, Batch: 45, Loss: 0.07481993734836578\n",
      "Iteration 263, Batch: 46, Loss: 0.06316403299570084\n",
      "Iteration 263, Batch: 47, Loss: 0.06099068373441696\n",
      "Iteration 263, Batch: 48, Loss: 0.033255431801080704\n",
      "Iteration 263, Batch: 49, Loss: 0.06788855791091919\n",
      "Number of layers: 9\n",
      "Iteration 264, Batch: 0, Loss: 0.07594601809978485\n",
      "Iteration 264, Batch: 1, Loss: 0.06977230310440063\n",
      "Iteration 264, Batch: 2, Loss: 0.053231894969940186\n",
      "Iteration 264, Batch: 3, Loss: 0.05930430442094803\n",
      "Iteration 264, Batch: 4, Loss: 0.06471502035856247\n",
      "Iteration 264, Batch: 5, Loss: 0.06028691679239273\n",
      "Iteration 264, Batch: 6, Loss: 0.06072673201560974\n",
      "Iteration 264, Batch: 7, Loss: 0.04106607288122177\n",
      "Iteration 264, Batch: 8, Loss: 0.07301803678274155\n",
      "Iteration 264, Batch: 9, Loss: 0.039561666548252106\n",
      "Iteration 264, Batch: 10, Loss: 0.06784398853778839\n",
      "Iteration 264, Batch: 11, Loss: 0.054473504424095154\n",
      "Iteration 264, Batch: 12, Loss: 0.048038873821496964\n",
      "Iteration 264, Batch: 13, Loss: 0.054344549775123596\n",
      "Iteration 264, Batch: 14, Loss: 0.09663886576890945\n",
      "Iteration 264, Batch: 15, Loss: 0.055600643157958984\n",
      "Iteration 264, Batch: 16, Loss: 0.04650036618113518\n",
      "Iteration 264, Batch: 17, Loss: 0.04680604487657547\n",
      "Iteration 264, Batch: 18, Loss: 0.040011607110500336\n",
      "Iteration 264, Batch: 19, Loss: 0.057609956711530685\n",
      "Iteration 264, Batch: 20, Loss: 0.06460500508546829\n",
      "Iteration 264, Batch: 21, Loss: 0.05293595790863037\n",
      "Iteration 264, Batch: 22, Loss: 0.08145508915185928\n",
      "Iteration 264, Batch: 23, Loss: 0.07368563115596771\n",
      "Iteration 264, Batch: 24, Loss: 0.05719740688800812\n",
      "Iteration 264, Batch: 25, Loss: 0.060364577919244766\n",
      "Iteration 264, Batch: 26, Loss: 0.04162198305130005\n",
      "Iteration 264, Batch: 27, Loss: 0.06496994197368622\n",
      "Iteration 264, Batch: 28, Loss: 0.044228166341781616\n",
      "Iteration 264, Batch: 29, Loss: 0.02763080596923828\n",
      "Iteration 264, Batch: 30, Loss: 0.07221673429012299\n",
      "Iteration 264, Batch: 31, Loss: 0.0485164076089859\n",
      "Iteration 264, Batch: 32, Loss: 0.04266108572483063\n",
      "Iteration 264, Batch: 33, Loss: 0.03310323506593704\n",
      "Iteration 264, Batch: 34, Loss: 0.0345207080245018\n",
      "Iteration 264, Batch: 35, Loss: 0.03201321139931679\n",
      "Iteration 264, Batch: 36, Loss: 0.08451581746339798\n",
      "Iteration 264, Batch: 37, Loss: 0.0943247377872467\n",
      "Iteration 264, Batch: 38, Loss: 0.05869143828749657\n",
      "Iteration 264, Batch: 39, Loss: 0.06443575024604797\n",
      "Iteration 264, Batch: 40, Loss: 0.05864586681127548\n",
      "Iteration 264, Batch: 41, Loss: 0.0661037266254425\n",
      "Iteration 264, Batch: 42, Loss: 0.05839955061674118\n",
      "Iteration 264, Batch: 43, Loss: 0.06742031872272491\n",
      "Iteration 264, Batch: 44, Loss: 0.058987803757190704\n",
      "Iteration 264, Batch: 45, Loss: 0.04809214174747467\n",
      "Iteration 264, Batch: 46, Loss: 0.05258578807115555\n",
      "Iteration 264, Batch: 47, Loss: 0.0519072599709034\n",
      "Iteration 264, Batch: 48, Loss: 0.059634651988744736\n",
      "Iteration 264, Batch: 49, Loss: 0.06581434607505798\n",
      "Number of layers: 9\n",
      "Iteration 265, Batch: 0, Loss: 0.03732322156429291\n",
      "Iteration 265, Batch: 1, Loss: 0.05762796103954315\n",
      "Iteration 265, Batch: 2, Loss: 0.053017958998680115\n",
      "Iteration 265, Batch: 3, Loss: 0.0689522922039032\n",
      "Iteration 265, Batch: 4, Loss: 0.059510305523872375\n",
      "Iteration 265, Batch: 5, Loss: 0.03927820920944214\n",
      "Iteration 265, Batch: 6, Loss: 0.04118761047720909\n",
      "Iteration 265, Batch: 7, Loss: 0.060457952320575714\n",
      "Iteration 265, Batch: 8, Loss: 0.05330263823270798\n",
      "Iteration 265, Batch: 9, Loss: 0.06393522024154663\n",
      "Iteration 265, Batch: 10, Loss: 0.016815073788166046\n",
      "Iteration 265, Batch: 11, Loss: 0.037202633917331696\n",
      "Iteration 265, Batch: 12, Loss: 0.06517190486192703\n",
      "Iteration 265, Batch: 13, Loss: 0.054036956280469894\n",
      "Iteration 265, Batch: 14, Loss: 0.07162346690893173\n",
      "Iteration 265, Batch: 15, Loss: 0.05908617004752159\n",
      "Iteration 265, Batch: 16, Loss: 0.06439254432916641\n",
      "Iteration 265, Batch: 17, Loss: 0.04105629399418831\n",
      "Iteration 265, Batch: 18, Loss: 0.06640807539224625\n",
      "Iteration 265, Batch: 19, Loss: 0.049456529319286346\n",
      "Iteration 265, Batch: 20, Loss: 0.03931553661823273\n",
      "Iteration 265, Batch: 21, Loss: 0.04674148932099342\n",
      "Iteration 265, Batch: 22, Loss: 0.07098424434661865\n",
      "Iteration 265, Batch: 23, Loss: 0.07440876215696335\n",
      "Iteration 265, Batch: 24, Loss: 0.057161133736371994\n",
      "Iteration 265, Batch: 25, Loss: 0.051424503326416016\n",
      "Iteration 265, Batch: 26, Loss: 0.05897205322980881\n",
      "Iteration 265, Batch: 27, Loss: 0.05399773642420769\n",
      "Iteration 265, Batch: 28, Loss: 0.05549449473619461\n",
      "Iteration 265, Batch: 29, Loss: 0.055406052619218826\n",
      "Iteration 265, Batch: 30, Loss: 0.06241583079099655\n",
      "Iteration 265, Batch: 31, Loss: 0.05378066748380661\n",
      "Iteration 265, Batch: 32, Loss: 0.033587951213121414\n",
      "Iteration 265, Batch: 33, Loss: 0.058623574674129486\n",
      "Iteration 265, Batch: 34, Loss: 0.06409802287817001\n",
      "Iteration 265, Batch: 35, Loss: 0.056928157806396484\n",
      "Iteration 265, Batch: 36, Loss: 0.05119325965642929\n",
      "Iteration 265, Batch: 37, Loss: 0.06593232601881027\n",
      "Iteration 265, Batch: 38, Loss: 0.05856529250741005\n",
      "Iteration 265, Batch: 39, Loss: 0.04012949764728546\n",
      "Iteration 265, Batch: 40, Loss: 0.0677240714430809\n",
      "Iteration 265, Batch: 41, Loss: 0.14254887402057648\n",
      "Iteration 265, Batch: 42, Loss: 0.0731278657913208\n",
      "Iteration 265, Batch: 43, Loss: 0.0752616599202156\n",
      "Iteration 265, Batch: 44, Loss: 0.06091928482055664\n",
      "Iteration 265, Batch: 45, Loss: 0.04312628507614136\n",
      "Iteration 265, Batch: 46, Loss: 0.051148202270269394\n",
      "Iteration 265, Batch: 47, Loss: 0.048319458961486816\n",
      "Iteration 265, Batch: 48, Loss: 0.05259716883301735\n",
      "Iteration 265, Batch: 49, Loss: 0.046761758625507355\n",
      "Number of layers: 9\n",
      "Iteration 266, Batch: 0, Loss: 0.044495370239019394\n",
      "Iteration 266, Batch: 1, Loss: 0.09543167799711227\n",
      "Iteration 266, Batch: 2, Loss: 0.07470326125621796\n",
      "Iteration 266, Batch: 3, Loss: 0.04001219570636749\n",
      "Iteration 266, Batch: 4, Loss: 0.03612777218222618\n",
      "Iteration 266, Batch: 5, Loss: 0.04422116279602051\n",
      "Iteration 266, Batch: 6, Loss: 0.05115325748920441\n",
      "Iteration 266, Batch: 7, Loss: 0.05192049220204353\n",
      "Iteration 266, Batch: 8, Loss: 0.04408092051744461\n",
      "Iteration 266, Batch: 9, Loss: 0.03609491139650345\n",
      "Iteration 266, Batch: 10, Loss: 0.05945107340812683\n",
      "Iteration 266, Batch: 11, Loss: 0.027477839961647987\n",
      "Iteration 266, Batch: 12, Loss: 0.07180983573198318\n",
      "Iteration 266, Batch: 13, Loss: 0.05396295338869095\n",
      "Iteration 266, Batch: 14, Loss: 0.04376457631587982\n",
      "Iteration 266, Batch: 15, Loss: 0.07556172460317612\n",
      "Iteration 266, Batch: 16, Loss: 0.04342586547136307\n",
      "Iteration 266, Batch: 17, Loss: 0.04215225577354431\n",
      "Iteration 266, Batch: 18, Loss: 0.039600398391485214\n",
      "Iteration 266, Batch: 19, Loss: 0.038117874413728714\n",
      "Iteration 266, Batch: 20, Loss: 0.060872383415699005\n",
      "Iteration 266, Batch: 21, Loss: 0.043530967086553574\n",
      "Iteration 266, Batch: 22, Loss: 0.05487852543592453\n",
      "Iteration 266, Batch: 23, Loss: 0.050465479493141174\n",
      "Iteration 266, Batch: 24, Loss: 0.051216285675764084\n",
      "Iteration 266, Batch: 25, Loss: 0.0446174181997776\n",
      "Iteration 266, Batch: 26, Loss: 0.04404069855809212\n",
      "Iteration 266, Batch: 27, Loss: 0.05224382132291794\n",
      "Iteration 266, Batch: 28, Loss: 0.050437457859516144\n",
      "Iteration 266, Batch: 29, Loss: 0.10584694892168045\n",
      "Iteration 266, Batch: 30, Loss: 0.08741975575685501\n",
      "Iteration 266, Batch: 31, Loss: 0.07245616614818573\n",
      "Iteration 266, Batch: 32, Loss: 0.07925240695476532\n",
      "Iteration 266, Batch: 33, Loss: 0.06326818466186523\n",
      "Iteration 266, Batch: 34, Loss: 0.05257548391819\n",
      "Iteration 266, Batch: 35, Loss: 0.0481109544634819\n",
      "Iteration 266, Batch: 36, Loss: 0.03501337394118309\n",
      "Iteration 266, Batch: 37, Loss: 0.05253543704748154\n",
      "Iteration 266, Batch: 38, Loss: 0.07401414960622787\n",
      "Iteration 266, Batch: 39, Loss: 0.04310230165719986\n",
      "Iteration 266, Batch: 40, Loss: 0.0436396524310112\n",
      "Iteration 266, Batch: 41, Loss: 0.05566203594207764\n",
      "Iteration 266, Batch: 42, Loss: 0.04741649702191353\n",
      "Iteration 266, Batch: 43, Loss: 0.06643582880496979\n",
      "Iteration 266, Batch: 44, Loss: 0.05012546107172966\n",
      "Iteration 266, Batch: 45, Loss: 0.05979088321328163\n",
      "Iteration 266, Batch: 46, Loss: 0.03293384611606598\n",
      "Iteration 266, Batch: 47, Loss: 0.06442252546548843\n",
      "Iteration 266, Batch: 48, Loss: 0.03839179128408432\n",
      "Iteration 266, Batch: 49, Loss: 0.03143199905753136\n",
      "Number of layers: 10\n",
      "Iteration 267, Batch: 0, Loss: 0.26843342185020447\n",
      "Iteration 267, Batch: 1, Loss: 0.13336831331253052\n",
      "Iteration 267, Batch: 2, Loss: 0.10709530860185623\n",
      "Iteration 267, Batch: 3, Loss: 0.0998479351401329\n",
      "Iteration 267, Batch: 4, Loss: 0.13011597096920013\n",
      "Iteration 267, Batch: 5, Loss: 0.20361371338367462\n",
      "Iteration 267, Batch: 6, Loss: 0.18960784375667572\n",
      "Iteration 267, Batch: 7, Loss: 0.1560298353433609\n",
      "Iteration 267, Batch: 8, Loss: 0.13439719378948212\n",
      "Iteration 267, Batch: 9, Loss: 0.09098222851753235\n",
      "Iteration 267, Batch: 10, Loss: 0.0717613473534584\n",
      "Iteration 267, Batch: 11, Loss: 0.113499715924263\n",
      "Iteration 267, Batch: 12, Loss: 0.05683659017086029\n",
      "Iteration 267, Batch: 13, Loss: 0.07213429361581802\n",
      "Iteration 267, Batch: 14, Loss: 0.06841367483139038\n",
      "Iteration 267, Batch: 15, Loss: 0.06287012994289398\n",
      "Iteration 267, Batch: 16, Loss: 0.0696391612291336\n",
      "Iteration 267, Batch: 17, Loss: 0.05199785530567169\n",
      "Iteration 267, Batch: 18, Loss: 0.0916067361831665\n",
      "Iteration 267, Batch: 19, Loss: 0.06675440073013306\n",
      "Iteration 267, Batch: 20, Loss: 0.06585977971553802\n",
      "Iteration 267, Batch: 21, Loss: 0.06419612467288971\n",
      "Iteration 267, Batch: 22, Loss: 0.08170255273580551\n",
      "Iteration 267, Batch: 23, Loss: 0.054556190967559814\n",
      "Iteration 267, Batch: 24, Loss: 0.053440485149621964\n",
      "Iteration 267, Batch: 25, Loss: 0.035123080015182495\n",
      "Iteration 267, Batch: 26, Loss: 0.06906839460134506\n",
      "Iteration 267, Batch: 27, Loss: 0.061848413199186325\n",
      "Iteration 267, Batch: 28, Loss: 0.05373363941907883\n",
      "Iteration 267, Batch: 29, Loss: 0.04570983350276947\n",
      "Iteration 267, Batch: 30, Loss: 0.06072884425520897\n",
      "Iteration 267, Batch: 31, Loss: 0.0545293428003788\n",
      "Iteration 267, Batch: 32, Loss: 0.06684912741184235\n",
      "Iteration 267, Batch: 33, Loss: 0.058664511889219284\n",
      "Iteration 267, Batch: 34, Loss: 0.06737934798002243\n",
      "Iteration 267, Batch: 35, Loss: 0.05418999865651131\n",
      "Iteration 267, Batch: 36, Loss: 0.04012148827314377\n",
      "Iteration 267, Batch: 37, Loss: 0.07197104394435883\n",
      "Iteration 267, Batch: 38, Loss: 0.049162495881319046\n",
      "Iteration 267, Batch: 39, Loss: 0.05155238136649132\n",
      "Iteration 267, Batch: 40, Loss: 0.056854020804166794\n",
      "Iteration 267, Batch: 41, Loss: 0.04737892001867294\n",
      "Iteration 267, Batch: 42, Loss: 0.053357064723968506\n",
      "Iteration 267, Batch: 43, Loss: 0.06101338192820549\n",
      "Iteration 267, Batch: 44, Loss: 0.05259731039404869\n",
      "Iteration 267, Batch: 45, Loss: 0.04682713374495506\n",
      "Iteration 267, Batch: 46, Loss: 0.04428514465689659\n",
      "Iteration 267, Batch: 47, Loss: 0.05629335343837738\n",
      "Iteration 267, Batch: 48, Loss: 0.03724970296025276\n",
      "Iteration 267, Batch: 49, Loss: 0.0507793091237545\n",
      "Number of layers: 10\n",
      "Iteration 268, Batch: 0, Loss: 0.052693624049425125\n",
      "Iteration 268, Batch: 1, Loss: 0.06202077865600586\n",
      "Iteration 268, Batch: 2, Loss: 0.07589231431484222\n",
      "Iteration 268, Batch: 3, Loss: 0.05792856588959694\n",
      "Iteration 268, Batch: 4, Loss: 0.055570609867572784\n",
      "Iteration 268, Batch: 5, Loss: 0.038156021386384964\n",
      "Iteration 268, Batch: 6, Loss: 0.07539112865924835\n",
      "Iteration 268, Batch: 7, Loss: 0.06999342143535614\n",
      "Iteration 268, Batch: 8, Loss: 0.023175017908215523\n",
      "Iteration 268, Batch: 9, Loss: 0.07908418774604797\n",
      "Iteration 268, Batch: 10, Loss: 0.05632192641496658\n",
      "Iteration 268, Batch: 11, Loss: 0.042292091995477676\n",
      "Iteration 268, Batch: 12, Loss: 0.0655265673995018\n",
      "Iteration 268, Batch: 13, Loss: 0.08006633818149567\n",
      "Iteration 268, Batch: 14, Loss: 0.04089720547199249\n",
      "Iteration 268, Batch: 15, Loss: 0.053644225001335144\n",
      "Iteration 268, Batch: 16, Loss: 0.05290474742650986\n",
      "Iteration 268, Batch: 17, Loss: 0.06539129465818405\n",
      "Iteration 268, Batch: 18, Loss: 0.048311498016119\n",
      "Iteration 268, Batch: 19, Loss: 0.056274354457855225\n",
      "Iteration 268, Batch: 20, Loss: 0.08161212503910065\n",
      "Iteration 268, Batch: 21, Loss: 0.053957488387823105\n",
      "Iteration 268, Batch: 22, Loss: 0.03255617991089821\n",
      "Iteration 268, Batch: 23, Loss: 0.06265582889318466\n",
      "Iteration 268, Batch: 24, Loss: 0.05235861614346504\n",
      "Iteration 268, Batch: 25, Loss: 0.053333982825279236\n",
      "Iteration 268, Batch: 26, Loss: 0.048490751534700394\n",
      "Iteration 268, Batch: 27, Loss: 0.06561312824487686\n",
      "Iteration 268, Batch: 28, Loss: 0.05799498409032822\n",
      "Iteration 268, Batch: 29, Loss: 0.07624731212854385\n",
      "Iteration 268, Batch: 30, Loss: 0.03903171792626381\n",
      "Iteration 268, Batch: 31, Loss: 0.037107862532138824\n",
      "Iteration 268, Batch: 32, Loss: 0.04304873198270798\n",
      "Iteration 268, Batch: 33, Loss: 0.0446985624730587\n",
      "Iteration 268, Batch: 34, Loss: 0.05241675302386284\n",
      "Iteration 268, Batch: 35, Loss: 0.04220157489180565\n",
      "Iteration 268, Batch: 36, Loss: 0.06497693806886673\n",
      "Iteration 268, Batch: 37, Loss: 0.03383940830826759\n",
      "Iteration 268, Batch: 38, Loss: 0.03796886280179024\n",
      "Iteration 268, Batch: 39, Loss: 0.03306298330426216\n",
      "Iteration 268, Batch: 40, Loss: 0.04253588616847992\n",
      "Iteration 268, Batch: 41, Loss: 0.031922921538352966\n",
      "Iteration 268, Batch: 42, Loss: 0.06911491602659225\n",
      "Iteration 268, Batch: 43, Loss: 0.027820220217108727\n",
      "Iteration 268, Batch: 44, Loss: 0.05678378790616989\n",
      "Iteration 268, Batch: 45, Loss: 0.06449583172798157\n",
      "Iteration 268, Batch: 46, Loss: 0.061374809592962265\n",
      "Iteration 268, Batch: 47, Loss: 0.06948281079530716\n",
      "Iteration 268, Batch: 48, Loss: 0.08281126618385315\n",
      "Iteration 268, Batch: 49, Loss: 0.05274088308215141\n",
      "Number of layers: 10\n",
      "Iteration 269, Batch: 0, Loss: 0.05059167370200157\n",
      "Iteration 269, Batch: 1, Loss: 0.07557091861963272\n",
      "Iteration 269, Batch: 2, Loss: 0.05301053076982498\n",
      "Iteration 269, Batch: 3, Loss: 0.06586203724145889\n",
      "Iteration 269, Batch: 4, Loss: 0.07166587561368942\n",
      "Iteration 269, Batch: 5, Loss: 0.036183912307024\n",
      "Iteration 269, Batch: 6, Loss: 0.04297711327672005\n",
      "Iteration 269, Batch: 7, Loss: 0.03203301504254341\n",
      "Iteration 269, Batch: 8, Loss: 0.05223536491394043\n",
      "Iteration 269, Batch: 9, Loss: 0.05055705085396767\n",
      "Iteration 269, Batch: 10, Loss: 0.06498754024505615\n",
      "Iteration 269, Batch: 11, Loss: 0.04812707379460335\n",
      "Iteration 269, Batch: 12, Loss: 0.05791746452450752\n",
      "Iteration 269, Batch: 13, Loss: 0.05672018229961395\n",
      "Iteration 269, Batch: 14, Loss: 0.06370151787996292\n",
      "Iteration 269, Batch: 15, Loss: 0.05087146535515785\n",
      "Iteration 269, Batch: 16, Loss: 0.06026303023099899\n",
      "Iteration 269, Batch: 17, Loss: 0.06687123328447342\n",
      "Iteration 269, Batch: 18, Loss: 0.03999053314328194\n",
      "Iteration 269, Batch: 19, Loss: 0.04531698301434517\n",
      "Iteration 269, Batch: 20, Loss: 0.04553217440843582\n",
      "Iteration 269, Batch: 21, Loss: 0.043143536895513535\n",
      "Iteration 269, Batch: 22, Loss: 0.036555662751197815\n",
      "Iteration 269, Batch: 23, Loss: 0.05638160556554794\n",
      "Iteration 269, Batch: 24, Loss: 0.06083018705248833\n",
      "Iteration 269, Batch: 25, Loss: 0.033016882836818695\n",
      "Iteration 269, Batch: 26, Loss: 0.039284419268369675\n",
      "Iteration 269, Batch: 27, Loss: 0.05398973450064659\n",
      "Iteration 269, Batch: 28, Loss: 0.06757888942956924\n",
      "Iteration 269, Batch: 29, Loss: 0.04308343678712845\n",
      "Iteration 269, Batch: 30, Loss: 0.049578264355659485\n",
      "Iteration 269, Batch: 31, Loss: 0.04619761183857918\n",
      "Iteration 269, Batch: 32, Loss: 0.04255620762705803\n",
      "Iteration 269, Batch: 33, Loss: 0.06269046664237976\n",
      "Iteration 269, Batch: 34, Loss: 0.11544956266880035\n",
      "Iteration 269, Batch: 35, Loss: 0.07341720163822174\n",
      "Iteration 269, Batch: 36, Loss: 0.04304293170571327\n",
      "Iteration 269, Batch: 37, Loss: 0.06358328461647034\n",
      "Iteration 269, Batch: 38, Loss: 0.06736371666193008\n",
      "Iteration 269, Batch: 39, Loss: 0.05249626561999321\n",
      "Iteration 269, Batch: 40, Loss: 0.07798837125301361\n",
      "Iteration 269, Batch: 41, Loss: 0.07206884026527405\n",
      "Iteration 269, Batch: 42, Loss: 0.04999199137091637\n",
      "Iteration 269, Batch: 43, Loss: 0.057579316198825836\n",
      "Iteration 269, Batch: 44, Loss: 0.06050169840455055\n",
      "Iteration 269, Batch: 45, Loss: 0.07347648590803146\n",
      "Iteration 269, Batch: 46, Loss: 0.06744053959846497\n",
      "Iteration 269, Batch: 47, Loss: 0.06579313427209854\n",
      "Iteration 269, Batch: 48, Loss: 0.04562420770525932\n",
      "Iteration 269, Batch: 49, Loss: 0.05564083158969879\n",
      "Number of layers: 10\n",
      "Iteration 270, Batch: 0, Loss: 0.0473172664642334\n",
      "Iteration 270, Batch: 1, Loss: 0.05247165635228157\n",
      "Iteration 270, Batch: 2, Loss: 0.07529287040233612\n",
      "Iteration 270, Batch: 3, Loss: 0.05527849122881889\n",
      "Iteration 270, Batch: 4, Loss: 0.05202419310808182\n",
      "Iteration 270, Batch: 5, Loss: 0.06747422367334366\n",
      "Iteration 270, Batch: 6, Loss: 0.06988893449306488\n",
      "Iteration 270, Batch: 7, Loss: 0.07563546299934387\n",
      "Iteration 270, Batch: 8, Loss: 0.0629531592130661\n",
      "Iteration 270, Batch: 9, Loss: 0.044364847242832184\n",
      "Iteration 270, Batch: 10, Loss: 0.05383215472102165\n",
      "Iteration 270, Batch: 11, Loss: 0.057862862944602966\n",
      "Iteration 270, Batch: 12, Loss: 0.03639852628111839\n",
      "Iteration 270, Batch: 13, Loss: 0.035288259387016296\n",
      "Iteration 270, Batch: 14, Loss: 0.07363364845514297\n",
      "Iteration 270, Batch: 15, Loss: 0.05590393766760826\n",
      "Iteration 270, Batch: 16, Loss: 0.06416518241167068\n",
      "Iteration 270, Batch: 17, Loss: 0.07880135625600815\n",
      "Iteration 270, Batch: 18, Loss: 0.07847902923822403\n",
      "Iteration 270, Batch: 19, Loss: 0.06919337809085846\n",
      "Iteration 270, Batch: 20, Loss: 0.04571891203522682\n",
      "Iteration 270, Batch: 21, Loss: 0.05442972853779793\n",
      "Iteration 270, Batch: 22, Loss: 0.06917545199394226\n",
      "Iteration 270, Batch: 23, Loss: 0.05382547900080681\n",
      "Iteration 270, Batch: 24, Loss: 0.07248423993587494\n",
      "Iteration 270, Batch: 25, Loss: 0.06317459791898727\n",
      "Iteration 270, Batch: 26, Loss: 0.0749804824590683\n",
      "Iteration 270, Batch: 27, Loss: 0.0795970931649208\n",
      "Iteration 270, Batch: 28, Loss: 0.05909552052617073\n",
      "Iteration 270, Batch: 29, Loss: 0.04083765298128128\n",
      "Iteration 270, Batch: 30, Loss: 0.10246583074331284\n",
      "Iteration 270, Batch: 31, Loss: 0.06463616341352463\n",
      "Iteration 270, Batch: 32, Loss: 0.08559249341487885\n",
      "Iteration 270, Batch: 33, Loss: 0.11616472154855728\n",
      "Iteration 270, Batch: 34, Loss: 0.10689946264028549\n",
      "Iteration 270, Batch: 35, Loss: 0.07879222184419632\n",
      "Iteration 270, Batch: 36, Loss: 0.06891238689422607\n",
      "Iteration 270, Batch: 37, Loss: 0.09672585874795914\n",
      "Iteration 270, Batch: 38, Loss: 0.047252997756004333\n",
      "Iteration 270, Batch: 39, Loss: 0.039366550743579865\n",
      "Iteration 270, Batch: 40, Loss: 0.07416043430566788\n",
      "Iteration 270, Batch: 41, Loss: 0.07987479865550995\n",
      "Iteration 270, Batch: 42, Loss: 0.04831346869468689\n",
      "Iteration 270, Batch: 43, Loss: 0.04348309338092804\n",
      "Iteration 270, Batch: 44, Loss: 0.03974220156669617\n",
      "Iteration 270, Batch: 45, Loss: 0.09479844570159912\n",
      "Iteration 270, Batch: 46, Loss: 0.04631337895989418\n",
      "Iteration 270, Batch: 47, Loss: 0.045593906193971634\n",
      "Iteration 270, Batch: 48, Loss: 0.037031665444374084\n",
      "Iteration 270, Batch: 49, Loss: 0.04974043741822243\n",
      "Number of layers: 10\n",
      "Iteration 271, Batch: 0, Loss: 0.050885897129774094\n",
      "Iteration 271, Batch: 1, Loss: 0.07980401813983917\n",
      "Iteration 271, Batch: 2, Loss: 0.10910524427890778\n",
      "Iteration 271, Batch: 3, Loss: 0.0774628221988678\n",
      "Iteration 271, Batch: 4, Loss: 0.06125840172171593\n",
      "Iteration 271, Batch: 5, Loss: 0.04695188254117966\n",
      "Iteration 271, Batch: 6, Loss: 0.09179665148258209\n",
      "Iteration 271, Batch: 7, Loss: 0.0606304407119751\n",
      "Iteration 271, Batch: 8, Loss: 0.09946758300065994\n",
      "Iteration 271, Batch: 9, Loss: 0.09372995048761368\n",
      "Iteration 271, Batch: 10, Loss: 0.08584772795438766\n",
      "Iteration 271, Batch: 11, Loss: 0.05049256980419159\n",
      "Iteration 271, Batch: 12, Loss: 0.0554240345954895\n",
      "Iteration 271, Batch: 13, Loss: 0.0461696982383728\n",
      "Iteration 271, Batch: 14, Loss: 0.055859994143247604\n",
      "Iteration 271, Batch: 15, Loss: 0.038193490356206894\n",
      "Iteration 271, Batch: 16, Loss: 0.07209285348653793\n",
      "Iteration 271, Batch: 17, Loss: 0.053742170333862305\n",
      "Iteration 271, Batch: 18, Loss: 0.062313634902238846\n",
      "Iteration 271, Batch: 19, Loss: 0.06353896111249924\n",
      "Iteration 271, Batch: 20, Loss: 0.04598856717348099\n",
      "Iteration 271, Batch: 21, Loss: 0.08309028297662735\n",
      "Iteration 271, Batch: 22, Loss: 0.035424623638391495\n",
      "Iteration 271, Batch: 23, Loss: 0.04441981390118599\n",
      "Iteration 271, Batch: 24, Loss: 0.036545202136039734\n",
      "Iteration 271, Batch: 25, Loss: 0.0623437762260437\n",
      "Iteration 271, Batch: 26, Loss: 0.06324072927236557\n",
      "Iteration 271, Batch: 27, Loss: 0.04062648117542267\n",
      "Iteration 271, Batch: 28, Loss: 0.08959241956472397\n",
      "Iteration 271, Batch: 29, Loss: 0.09446888417005539\n",
      "Iteration 271, Batch: 30, Loss: 0.09880359470844269\n",
      "Iteration 271, Batch: 31, Loss: 0.09992524981498718\n",
      "Iteration 271, Batch: 32, Loss: 0.07607398927211761\n",
      "Iteration 271, Batch: 33, Loss: 0.07674596458673477\n",
      "Iteration 271, Batch: 34, Loss: 0.07144133001565933\n",
      "Iteration 271, Batch: 35, Loss: 0.08011896908283234\n",
      "Iteration 271, Batch: 36, Loss: 0.07932724058628082\n",
      "Iteration 271, Batch: 37, Loss: 0.057732343673706055\n",
      "Iteration 271, Batch: 38, Loss: 0.05212702974677086\n",
      "Iteration 271, Batch: 39, Loss: 0.06663215905427933\n",
      "Iteration 271, Batch: 40, Loss: 0.05922939255833626\n",
      "Iteration 271, Batch: 41, Loss: 0.039453815668821335\n",
      "Iteration 271, Batch: 42, Loss: 0.036003630608320236\n",
      "Iteration 271, Batch: 43, Loss: 0.03606485575437546\n",
      "Iteration 271, Batch: 44, Loss: 0.051777005195617676\n",
      "Iteration 271, Batch: 45, Loss: 0.060278281569480896\n",
      "Iteration 271, Batch: 46, Loss: 0.032572191208601\n",
      "Iteration 271, Batch: 47, Loss: 0.029747996479272842\n",
      "Iteration 271, Batch: 48, Loss: 0.03487955406308174\n",
      "Iteration 271, Batch: 49, Loss: 0.06385823339223862\n",
      "Number of layers: 10\n",
      "Iteration 272, Batch: 0, Loss: 0.07695715874433517\n",
      "Iteration 272, Batch: 1, Loss: 0.06681572645902634\n",
      "Iteration 272, Batch: 2, Loss: 0.03972172364592552\n",
      "Iteration 272, Batch: 3, Loss: 0.047267939895391464\n",
      "Iteration 272, Batch: 4, Loss: 0.06715342402458191\n",
      "Iteration 272, Batch: 5, Loss: 0.05136793851852417\n",
      "Iteration 272, Batch: 6, Loss: 0.05243442952632904\n",
      "Iteration 272, Batch: 7, Loss: 0.057491838932037354\n",
      "Iteration 272, Batch: 8, Loss: 0.05512246489524841\n",
      "Iteration 272, Batch: 9, Loss: 0.05043800547719002\n",
      "Iteration 272, Batch: 10, Loss: 0.048032406717538834\n",
      "Iteration 272, Batch: 11, Loss: 0.06259236484766006\n",
      "Iteration 272, Batch: 12, Loss: 0.04453964903950691\n",
      "Iteration 272, Batch: 13, Loss: 0.055892571806907654\n",
      "Iteration 272, Batch: 14, Loss: 0.056174781173467636\n",
      "Iteration 272, Batch: 15, Loss: 0.046896837651729584\n",
      "Iteration 272, Batch: 16, Loss: 0.037002597004175186\n",
      "Iteration 272, Batch: 17, Loss: 0.07665354013442993\n",
      "Iteration 272, Batch: 18, Loss: 0.0727807804942131\n",
      "Iteration 272, Batch: 19, Loss: 0.08259602636098862\n",
      "Iteration 272, Batch: 20, Loss: 0.06343664973974228\n",
      "Iteration 272, Batch: 21, Loss: 0.04738099128007889\n",
      "Iteration 272, Batch: 22, Loss: 0.060565005987882614\n",
      "Iteration 272, Batch: 23, Loss: 0.038979966193437576\n",
      "Iteration 272, Batch: 24, Loss: 0.056295573711395264\n",
      "Iteration 272, Batch: 25, Loss: 0.029380621388554573\n",
      "Iteration 272, Batch: 26, Loss: 0.053854215890169144\n",
      "Iteration 272, Batch: 27, Loss: 0.0505492277443409\n",
      "Iteration 272, Batch: 28, Loss: 0.058325108140707016\n",
      "Iteration 272, Batch: 29, Loss: 0.04883147031068802\n",
      "Iteration 272, Batch: 30, Loss: 0.04008765146136284\n",
      "Iteration 272, Batch: 31, Loss: 0.07053041458129883\n",
      "Iteration 272, Batch: 32, Loss: 0.04955487698316574\n",
      "Iteration 272, Batch: 33, Loss: 0.041427161544561386\n",
      "Iteration 272, Batch: 34, Loss: 0.0676523745059967\n",
      "Iteration 272, Batch: 35, Loss: 0.03116590343415737\n",
      "Iteration 272, Batch: 36, Loss: 0.0376800037920475\n",
      "Iteration 272, Batch: 37, Loss: 0.060976412147283554\n",
      "Iteration 272, Batch: 38, Loss: 0.05229983851313591\n",
      "Iteration 272, Batch: 39, Loss: 0.052137717604637146\n",
      "Iteration 272, Batch: 40, Loss: 0.05413035303354263\n",
      "Iteration 272, Batch: 41, Loss: 0.036903925240039825\n",
      "Iteration 272, Batch: 42, Loss: 0.02615581452846527\n",
      "Iteration 272, Batch: 43, Loss: 0.06308560818433762\n",
      "Iteration 272, Batch: 44, Loss: 0.05547716096043587\n",
      "Iteration 272, Batch: 45, Loss: 0.060260575264692307\n",
      "Iteration 272, Batch: 46, Loss: 0.042571552097797394\n",
      "Iteration 272, Batch: 47, Loss: 0.02066938579082489\n",
      "Iteration 272, Batch: 48, Loss: 0.020774146541953087\n",
      "Iteration 272, Batch: 49, Loss: 0.042347028851509094\n",
      "Number of layers: 10\n",
      "Iteration 273, Batch: 0, Loss: 0.035346850752830505\n",
      "Iteration 273, Batch: 1, Loss: 0.03177618980407715\n",
      "Iteration 273, Batch: 2, Loss: 0.026800960302352905\n",
      "Iteration 273, Batch: 3, Loss: 0.027152642607688904\n",
      "Iteration 273, Batch: 4, Loss: 0.043337445706129074\n",
      "Iteration 273, Batch: 5, Loss: 0.03964972868561745\n",
      "Iteration 273, Batch: 6, Loss: 0.04142949357628822\n",
      "Iteration 273, Batch: 7, Loss: 0.03223976865410805\n",
      "Iteration 273, Batch: 8, Loss: 0.045113783329725266\n",
      "Iteration 273, Batch: 9, Loss: 0.044863976538181305\n",
      "Iteration 273, Batch: 10, Loss: 0.052717361599206924\n",
      "Iteration 273, Batch: 11, Loss: 0.07522313296794891\n",
      "Iteration 273, Batch: 12, Loss: 0.06819307804107666\n",
      "Iteration 273, Batch: 13, Loss: 0.04615950584411621\n",
      "Iteration 273, Batch: 14, Loss: 0.06667720526456833\n",
      "Iteration 273, Batch: 15, Loss: 0.04772963374853134\n",
      "Iteration 273, Batch: 16, Loss: 0.052146829664707184\n",
      "Iteration 273, Batch: 17, Loss: 0.038114190101623535\n",
      "Iteration 273, Batch: 18, Loss: 0.05024338141083717\n",
      "Iteration 273, Batch: 19, Loss: 0.04731999337673187\n",
      "Iteration 273, Batch: 20, Loss: 0.06018034368753433\n",
      "Iteration 273, Batch: 21, Loss: 0.05852261558175087\n",
      "Iteration 273, Batch: 22, Loss: 0.044909700751304626\n",
      "Iteration 273, Batch: 23, Loss: 0.046478427946567535\n",
      "Iteration 273, Batch: 24, Loss: 0.04480131343007088\n",
      "Iteration 273, Batch: 25, Loss: 0.0644628033041954\n",
      "Iteration 273, Batch: 26, Loss: 0.05192306265234947\n",
      "Iteration 273, Batch: 27, Loss: 0.03795218840241432\n",
      "Iteration 273, Batch: 28, Loss: 0.0450558066368103\n",
      "Iteration 273, Batch: 29, Loss: 0.05169118940830231\n",
      "Iteration 273, Batch: 30, Loss: 0.045713890343904495\n",
      "Iteration 273, Batch: 31, Loss: 0.027417289093136787\n",
      "Iteration 273, Batch: 32, Loss: 0.04266032949090004\n",
      "Iteration 273, Batch: 33, Loss: 0.044014062732458115\n",
      "Iteration 273, Batch: 34, Loss: 0.03323090821504593\n",
      "Iteration 273, Batch: 35, Loss: 0.042555030435323715\n",
      "Iteration 273, Batch: 36, Loss: 0.04953591153025627\n",
      "Iteration 273, Batch: 37, Loss: 0.05423087626695633\n",
      "Iteration 273, Batch: 38, Loss: 0.04837622120976448\n",
      "Iteration 273, Batch: 39, Loss: 0.037929046899080276\n",
      "Iteration 273, Batch: 40, Loss: 0.05368812754750252\n",
      "Iteration 273, Batch: 41, Loss: 0.029508991166949272\n",
      "Iteration 273, Batch: 42, Loss: 0.026541151106357574\n",
      "Iteration 273, Batch: 43, Loss: 0.04445945471525192\n",
      "Iteration 273, Batch: 44, Loss: 0.04264025762677193\n",
      "Iteration 273, Batch: 45, Loss: 0.05076449364423752\n",
      "Iteration 273, Batch: 46, Loss: 0.0512373261153698\n",
      "Iteration 273, Batch: 47, Loss: 0.04529485106468201\n",
      "Iteration 273, Batch: 48, Loss: 0.04824679344892502\n",
      "Iteration 273, Batch: 49, Loss: 0.05023876205086708\n",
      "Number of layers: 10\n",
      "Iteration 274, Batch: 0, Loss: 0.06635810434818268\n",
      "Iteration 274, Batch: 1, Loss: 0.0651487335562706\n",
      "Iteration 274, Batch: 2, Loss: 0.032444167882204056\n",
      "Iteration 274, Batch: 3, Loss: 0.06332545727491379\n",
      "Iteration 274, Batch: 4, Loss: 0.03368258848786354\n",
      "Iteration 274, Batch: 5, Loss: 0.04865557700395584\n",
      "Iteration 274, Batch: 6, Loss: 0.05390537157654762\n",
      "Iteration 274, Batch: 7, Loss: 0.06640403717756271\n",
      "Iteration 274, Batch: 8, Loss: 0.06627222150564194\n",
      "Iteration 274, Batch: 9, Loss: 0.07905415445566177\n",
      "Iteration 274, Batch: 10, Loss: 0.04009683430194855\n",
      "Iteration 274, Batch: 11, Loss: 0.060000307857990265\n",
      "Iteration 274, Batch: 12, Loss: 0.05533250793814659\n",
      "Iteration 274, Batch: 13, Loss: 0.050131723284721375\n",
      "Iteration 274, Batch: 14, Loss: 0.05597202107310295\n",
      "Iteration 274, Batch: 15, Loss: 0.05400094762444496\n",
      "Iteration 274, Batch: 16, Loss: 0.03792735934257507\n",
      "Iteration 274, Batch: 17, Loss: 0.02009832300245762\n",
      "Iteration 274, Batch: 18, Loss: 0.06649278104305267\n",
      "Iteration 274, Batch: 19, Loss: 0.06803911179304123\n",
      "Iteration 274, Batch: 20, Loss: 0.08565202355384827\n",
      "Iteration 274, Batch: 21, Loss: 0.0757267102599144\n",
      "Iteration 274, Batch: 22, Loss: 0.052336737513542175\n",
      "Iteration 274, Batch: 23, Loss: 0.027204513549804688\n",
      "Iteration 274, Batch: 24, Loss: 0.07608877867460251\n",
      "Iteration 274, Batch: 25, Loss: 0.04376917704939842\n",
      "Iteration 274, Batch: 26, Loss: 0.09098239988088608\n",
      "Iteration 274, Batch: 27, Loss: 0.0852683037519455\n",
      "Iteration 274, Batch: 28, Loss: 0.05940162390470505\n",
      "Iteration 274, Batch: 29, Loss: 0.05886391922831535\n",
      "Iteration 274, Batch: 30, Loss: 0.0433213971555233\n",
      "Iteration 274, Batch: 31, Loss: 0.07113418728113174\n",
      "Iteration 274, Batch: 32, Loss: 0.0770694836974144\n",
      "Iteration 274, Batch: 33, Loss: 0.03260171040892601\n",
      "Iteration 274, Batch: 34, Loss: 0.04935586825013161\n",
      "Iteration 274, Batch: 35, Loss: 0.05773875117301941\n",
      "Iteration 274, Batch: 36, Loss: 0.09773030877113342\n",
      "Iteration 274, Batch: 37, Loss: 0.09869357943534851\n",
      "Iteration 274, Batch: 38, Loss: 0.06488163024187088\n",
      "Iteration 274, Batch: 39, Loss: 0.05371559038758278\n",
      "Iteration 274, Batch: 40, Loss: 0.05920063704252243\n",
      "Iteration 274, Batch: 41, Loss: 0.05080857500433922\n",
      "Iteration 274, Batch: 42, Loss: 0.07642214745283127\n",
      "Iteration 274, Batch: 43, Loss: 0.08197522908449173\n",
      "Iteration 274, Batch: 44, Loss: 0.0643213614821434\n",
      "Iteration 274, Batch: 45, Loss: 0.05146796256303787\n",
      "Iteration 274, Batch: 46, Loss: 0.03878747671842575\n",
      "Iteration 274, Batch: 47, Loss: 0.05899184197187424\n",
      "Iteration 274, Batch: 48, Loss: 0.06604396551847458\n",
      "Iteration 274, Batch: 49, Loss: 0.06808621436357498\n",
      "Number of layers: 10\n",
      "Iteration 275, Batch: 0, Loss: 0.03711995109915733\n",
      "Iteration 275, Batch: 1, Loss: 0.041360042989254\n",
      "Iteration 275, Batch: 2, Loss: 0.03805827349424362\n",
      "Iteration 275, Batch: 3, Loss: 0.03978012129664421\n",
      "Iteration 275, Batch: 4, Loss: 0.037461958825588226\n",
      "Iteration 275, Batch: 5, Loss: 0.06980617344379425\n",
      "Iteration 275, Batch: 6, Loss: 0.026252606883645058\n",
      "Iteration 275, Batch: 7, Loss: 0.046196069568395615\n",
      "Iteration 275, Batch: 8, Loss: 0.05476483702659607\n",
      "Iteration 275, Batch: 9, Loss: 0.053988803178071976\n",
      "Iteration 275, Batch: 10, Loss: 0.0423969067633152\n",
      "Iteration 275, Batch: 11, Loss: 0.06325174123048782\n",
      "Iteration 275, Batch: 12, Loss: 0.03463933244347572\n",
      "Iteration 275, Batch: 13, Loss: 0.040621280670166016\n",
      "Iteration 275, Batch: 14, Loss: 0.04591783881187439\n",
      "Iteration 275, Batch: 15, Loss: 0.04811733216047287\n",
      "Iteration 275, Batch: 16, Loss: 0.04455118998885155\n",
      "Iteration 275, Batch: 17, Loss: 0.04322481155395508\n",
      "Iteration 275, Batch: 18, Loss: 0.06174257770180702\n",
      "Iteration 275, Batch: 19, Loss: 0.055922944098711014\n",
      "Iteration 275, Batch: 20, Loss: 0.04555397853255272\n",
      "Iteration 275, Batch: 21, Loss: 0.06387010216712952\n",
      "Iteration 275, Batch: 22, Loss: 0.025311265140771866\n",
      "Iteration 275, Batch: 23, Loss: 0.047837480902671814\n",
      "Iteration 275, Batch: 24, Loss: 0.04036207124590874\n",
      "Iteration 275, Batch: 25, Loss: 0.048121947795152664\n",
      "Iteration 275, Batch: 26, Loss: 0.05296669900417328\n",
      "Iteration 275, Batch: 27, Loss: 0.03922058641910553\n",
      "Iteration 275, Batch: 28, Loss: 0.04210689291357994\n",
      "Iteration 275, Batch: 29, Loss: 0.044137243181467056\n",
      "Iteration 275, Batch: 30, Loss: 0.05298346281051636\n",
      "Iteration 275, Batch: 31, Loss: 0.06800612062215805\n",
      "Iteration 275, Batch: 32, Loss: 0.06297551840543747\n",
      "Iteration 275, Batch: 33, Loss: 0.06050320342183113\n",
      "Iteration 275, Batch: 34, Loss: 0.051396626979112625\n",
      "Iteration 275, Batch: 35, Loss: 0.06531007587909698\n",
      "Iteration 275, Batch: 36, Loss: 0.042135514318943024\n",
      "Iteration 275, Batch: 37, Loss: 0.04996445029973984\n",
      "Iteration 275, Batch: 38, Loss: 0.05767848715186119\n",
      "Iteration 275, Batch: 39, Loss: 0.05323483422398567\n",
      "Iteration 275, Batch: 40, Loss: 0.04634947329759598\n",
      "Iteration 275, Batch: 41, Loss: 0.06242508813738823\n",
      "Iteration 275, Batch: 42, Loss: 0.04213475435972214\n",
      "Iteration 275, Batch: 43, Loss: 0.025958789512515068\n",
      "Iteration 275, Batch: 44, Loss: 0.048868339508771896\n",
      "Iteration 275, Batch: 45, Loss: 0.08390683680772781\n",
      "Iteration 275, Batch: 46, Loss: 0.06163183972239494\n",
      "Iteration 275, Batch: 47, Loss: 0.02812992036342621\n",
      "Iteration 275, Batch: 48, Loss: 0.04884184151887894\n",
      "Iteration 275, Batch: 49, Loss: 0.04372326284646988\n",
      "Number of layers: 10\n",
      "Iteration 276, Batch: 0, Loss: 0.09043974429368973\n",
      "Iteration 276, Batch: 1, Loss: 0.078172966837883\n",
      "Iteration 276, Batch: 2, Loss: 0.1196255087852478\n",
      "Iteration 276, Batch: 3, Loss: 0.12386272102594376\n",
      "Iteration 276, Batch: 4, Loss: 0.0739745944738388\n",
      "Iteration 276, Batch: 5, Loss: 0.07529253512620926\n",
      "Iteration 276, Batch: 6, Loss: 0.08630462735891342\n",
      "Iteration 276, Batch: 7, Loss: 0.09899722039699554\n",
      "Iteration 276, Batch: 8, Loss: 0.09378883987665176\n",
      "Iteration 276, Batch: 9, Loss: 0.10747836530208588\n",
      "Iteration 276, Batch: 10, Loss: 0.08618570119142532\n",
      "Iteration 276, Batch: 11, Loss: 0.08626720309257507\n",
      "Iteration 276, Batch: 12, Loss: 0.05867713689804077\n",
      "Iteration 276, Batch: 13, Loss: 0.0859258696436882\n",
      "Iteration 276, Batch: 14, Loss: 0.08866773545742035\n",
      "Iteration 276, Batch: 15, Loss: 0.05588395893573761\n",
      "Iteration 276, Batch: 16, Loss: 0.101744145154953\n",
      "Iteration 276, Batch: 17, Loss: 0.08464059978723526\n",
      "Iteration 276, Batch: 18, Loss: 0.07973072677850723\n",
      "Iteration 276, Batch: 19, Loss: 0.07258258014917374\n",
      "Iteration 276, Batch: 20, Loss: 0.0662720575928688\n",
      "Iteration 276, Batch: 21, Loss: 0.06527812033891678\n",
      "Iteration 276, Batch: 22, Loss: 0.09142682701349258\n",
      "Iteration 276, Batch: 23, Loss: 0.08663442730903625\n",
      "Iteration 276, Batch: 24, Loss: 0.06048073619604111\n",
      "Iteration 276, Batch: 25, Loss: 0.09792826324701309\n",
      "Iteration 276, Batch: 26, Loss: 0.0497504360973835\n",
      "Iteration 276, Batch: 27, Loss: 0.06481567025184631\n",
      "Iteration 276, Batch: 28, Loss: 0.07491357624530792\n",
      "Iteration 276, Batch: 29, Loss: 0.0296262726187706\n",
      "Iteration 276, Batch: 30, Loss: 0.048629507422447205\n",
      "Iteration 276, Batch: 31, Loss: 0.04462435469031334\n",
      "Iteration 276, Batch: 32, Loss: 0.06882747262716293\n",
      "Iteration 276, Batch: 33, Loss: 0.05169602856040001\n",
      "Iteration 276, Batch: 34, Loss: 0.06756692379713058\n",
      "Iteration 276, Batch: 35, Loss: 0.037651143968105316\n",
      "Iteration 276, Batch: 36, Loss: 0.036099016666412354\n",
      "Iteration 276, Batch: 37, Loss: 0.0540224127471447\n",
      "Iteration 276, Batch: 38, Loss: 0.04777026176452637\n",
      "Iteration 276, Batch: 39, Loss: 0.05989699810743332\n",
      "Iteration 276, Batch: 40, Loss: 0.07605849206447601\n",
      "Iteration 276, Batch: 41, Loss: 0.08334722369909286\n",
      "Iteration 276, Batch: 42, Loss: 0.07496295869350433\n",
      "Iteration 276, Batch: 43, Loss: 0.10008653253316879\n",
      "Iteration 276, Batch: 44, Loss: 0.07001784443855286\n",
      "Iteration 276, Batch: 45, Loss: 0.05602404475212097\n",
      "Iteration 276, Batch: 46, Loss: 0.08091974258422852\n",
      "Iteration 276, Batch: 47, Loss: 0.06955022364854813\n",
      "Iteration 276, Batch: 48, Loss: 0.09043718874454498\n",
      "Iteration 276, Batch: 49, Loss: 0.08015704900026321\n",
      "Number of layers: 10\n",
      "Iteration 277, Batch: 0, Loss: 0.07331112027168274\n",
      "Iteration 277, Batch: 1, Loss: 0.07412632554769516\n",
      "Iteration 277, Batch: 2, Loss: 0.08627306669950485\n",
      "Iteration 277, Batch: 3, Loss: 0.08864577114582062\n",
      "Iteration 277, Batch: 4, Loss: 0.06925457715988159\n",
      "Iteration 277, Batch: 5, Loss: 0.07256066799163818\n",
      "Iteration 277, Batch: 6, Loss: 0.040169600397348404\n",
      "Iteration 277, Batch: 7, Loss: 0.059599749743938446\n",
      "Iteration 277, Batch: 8, Loss: 0.09561853110790253\n",
      "Iteration 277, Batch: 9, Loss: 0.050295133143663406\n",
      "Iteration 277, Batch: 10, Loss: 0.0920296236872673\n",
      "Iteration 277, Batch: 11, Loss: 0.07696504145860672\n",
      "Iteration 277, Batch: 12, Loss: 0.08434770256280899\n",
      "Iteration 277, Batch: 13, Loss: 0.0902172103524208\n",
      "Iteration 277, Batch: 14, Loss: 0.04762227088212967\n",
      "Iteration 277, Batch: 15, Loss: 0.04465801268815994\n",
      "Iteration 277, Batch: 16, Loss: 0.07842351496219635\n",
      "Iteration 277, Batch: 17, Loss: 0.08824026584625244\n",
      "Iteration 277, Batch: 18, Loss: 0.08775261789560318\n",
      "Iteration 277, Batch: 19, Loss: 0.04341675341129303\n",
      "Iteration 277, Batch: 20, Loss: 0.04131903126835823\n",
      "Iteration 277, Batch: 21, Loss: 0.07668451219797134\n",
      "Iteration 277, Batch: 22, Loss: 0.06513925641775131\n",
      "Iteration 277, Batch: 23, Loss: 0.06743064522743225\n",
      "Iteration 277, Batch: 24, Loss: 0.0688844546675682\n",
      "Iteration 277, Batch: 25, Loss: 0.03750845044851303\n",
      "Iteration 277, Batch: 26, Loss: 0.07036948204040527\n",
      "Iteration 277, Batch: 27, Loss: 0.03676017001271248\n",
      "Iteration 277, Batch: 28, Loss: 0.051401566714048386\n",
      "Iteration 277, Batch: 29, Loss: 0.0421098954975605\n",
      "Iteration 277, Batch: 30, Loss: 0.07120006531476974\n",
      "Iteration 277, Batch: 31, Loss: 0.0542575903236866\n",
      "Iteration 277, Batch: 32, Loss: 0.03356776759028435\n",
      "Iteration 277, Batch: 33, Loss: 0.04851100221276283\n",
      "Iteration 277, Batch: 34, Loss: 0.03757501766085625\n",
      "Iteration 277, Batch: 35, Loss: 0.06629301607608795\n",
      "Iteration 277, Batch: 36, Loss: 0.09502528607845306\n",
      "Iteration 277, Batch: 37, Loss: 0.06039523333311081\n",
      "Iteration 277, Batch: 38, Loss: 0.03885181248188019\n",
      "Iteration 277, Batch: 39, Loss: 0.06825383752584457\n",
      "Iteration 277, Batch: 40, Loss: 0.0710015520453453\n",
      "Iteration 277, Batch: 41, Loss: 0.07019928842782974\n",
      "Iteration 277, Batch: 42, Loss: 0.0906180739402771\n",
      "Iteration 277, Batch: 43, Loss: 0.05830197036266327\n",
      "Iteration 277, Batch: 44, Loss: 0.0669935941696167\n",
      "Iteration 277, Batch: 45, Loss: 0.05693746358156204\n",
      "Iteration 277, Batch: 46, Loss: 0.0701151192188263\n",
      "Iteration 277, Batch: 47, Loss: 0.03973601758480072\n",
      "Iteration 277, Batch: 48, Loss: 0.04354887828230858\n",
      "Iteration 277, Batch: 49, Loss: 0.07129516452550888\n",
      "Number of layers: 10\n",
      "Iteration 278, Batch: 0, Loss: 0.06852415204048157\n",
      "Iteration 278, Batch: 1, Loss: 0.05761194974184036\n",
      "Iteration 278, Batch: 2, Loss: 0.04505026713013649\n",
      "Iteration 278, Batch: 3, Loss: 0.0411866195499897\n",
      "Iteration 278, Batch: 4, Loss: 0.04103374481201172\n",
      "Iteration 278, Batch: 5, Loss: 0.055108580738306046\n",
      "Iteration 278, Batch: 6, Loss: 0.049719542264938354\n",
      "Iteration 278, Batch: 7, Loss: 0.06078355014324188\n",
      "Iteration 278, Batch: 8, Loss: 0.06922069936990738\n",
      "Iteration 278, Batch: 9, Loss: 0.03668031468987465\n",
      "Iteration 278, Batch: 10, Loss: 0.05669379234313965\n",
      "Iteration 278, Batch: 11, Loss: 0.06209444999694824\n",
      "Iteration 278, Batch: 12, Loss: 0.0466352216899395\n",
      "Iteration 278, Batch: 13, Loss: 0.06835789978504181\n",
      "Iteration 278, Batch: 14, Loss: 0.06534066051244736\n",
      "Iteration 278, Batch: 15, Loss: 0.09490655362606049\n",
      "Iteration 278, Batch: 16, Loss: 0.07950052618980408\n",
      "Iteration 278, Batch: 17, Loss: 0.08943947404623032\n",
      "Iteration 278, Batch: 18, Loss: 0.07107851654291153\n",
      "Iteration 278, Batch: 19, Loss: 0.0692412406206131\n",
      "Iteration 278, Batch: 20, Loss: 0.01968436688184738\n",
      "Iteration 278, Batch: 21, Loss: 0.07779305428266525\n",
      "Iteration 278, Batch: 22, Loss: 0.049736179411411285\n",
      "Iteration 278, Batch: 23, Loss: 0.08843223005533218\n",
      "Iteration 278, Batch: 24, Loss: 0.08478672057390213\n",
      "Iteration 278, Batch: 25, Loss: 0.03406696394085884\n",
      "Iteration 278, Batch: 26, Loss: 0.06894782185554504\n",
      "Iteration 278, Batch: 27, Loss: 0.053260911256074905\n",
      "Iteration 278, Batch: 28, Loss: 0.09156373143196106\n",
      "Iteration 278, Batch: 29, Loss: 0.11159983277320862\n",
      "Iteration 278, Batch: 30, Loss: 0.10726740956306458\n",
      "Iteration 278, Batch: 31, Loss: 0.027728872373700142\n",
      "Iteration 278, Batch: 32, Loss: 0.05639834702014923\n",
      "Iteration 278, Batch: 33, Loss: 0.08722119778394699\n",
      "Iteration 278, Batch: 34, Loss: 0.06253819167613983\n",
      "Iteration 278, Batch: 35, Loss: 0.06262913346290588\n",
      "Iteration 278, Batch: 36, Loss: 0.0728677287697792\n",
      "Iteration 278, Batch: 37, Loss: 0.09403257071971893\n",
      "Iteration 278, Batch: 38, Loss: 0.10306856036186218\n",
      "Iteration 278, Batch: 39, Loss: 0.10800908505916595\n",
      "Iteration 278, Batch: 40, Loss: 0.06888367235660553\n",
      "Iteration 278, Batch: 41, Loss: 0.07101864367723465\n",
      "Iteration 278, Batch: 42, Loss: 0.04293571785092354\n",
      "Iteration 278, Batch: 43, Loss: 0.059458427131175995\n",
      "Iteration 278, Batch: 44, Loss: 0.07226572930812836\n",
      "Iteration 278, Batch: 45, Loss: 0.04228153079748154\n",
      "Iteration 278, Batch: 46, Loss: 0.08102471381425858\n",
      "Iteration 278, Batch: 47, Loss: 0.07891509681940079\n",
      "Iteration 278, Batch: 48, Loss: 0.0743878036737442\n",
      "Iteration 278, Batch: 49, Loss: 0.07913444191217422\n",
      "Number of layers: 10\n",
      "Iteration 279, Batch: 0, Loss: 0.06297282874584198\n",
      "Iteration 279, Batch: 1, Loss: 0.043243613094091415\n",
      "Iteration 279, Batch: 2, Loss: 0.07493124157190323\n",
      "Iteration 279, Batch: 3, Loss: 0.09136560559272766\n",
      "Iteration 279, Batch: 4, Loss: 0.090752974152565\n",
      "Iteration 279, Batch: 5, Loss: 0.09078890830278397\n",
      "Iteration 279, Batch: 6, Loss: 0.08669053763151169\n",
      "Iteration 279, Batch: 7, Loss: 0.0720633789896965\n",
      "Iteration 279, Batch: 8, Loss: 0.06790247559547424\n",
      "Iteration 279, Batch: 9, Loss: 0.09987781941890717\n",
      "Iteration 279, Batch: 10, Loss: 0.09836358577013016\n",
      "Iteration 279, Batch: 11, Loss: 0.07039692252874374\n",
      "Iteration 279, Batch: 12, Loss: 0.055538613349199295\n",
      "Iteration 279, Batch: 13, Loss: 0.10629867017269135\n",
      "Iteration 279, Batch: 14, Loss: 0.10487236082553864\n",
      "Iteration 279, Batch: 15, Loss: 0.09885424375534058\n",
      "Iteration 279, Batch: 16, Loss: 0.07993808388710022\n",
      "Iteration 279, Batch: 17, Loss: 0.03975965827703476\n",
      "Iteration 279, Batch: 18, Loss: 0.0619589164853096\n",
      "Iteration 279, Batch: 19, Loss: 0.12512700259685516\n",
      "Iteration 279, Batch: 20, Loss: 0.0850859209895134\n",
      "Iteration 279, Batch: 21, Loss: 0.07001373916864395\n",
      "Iteration 279, Batch: 22, Loss: 0.06831701844930649\n",
      "Iteration 279, Batch: 23, Loss: 0.06893143802881241\n",
      "Iteration 279, Batch: 24, Loss: 0.059539489448070526\n",
      "Iteration 279, Batch: 25, Loss: 0.052673082798719406\n",
      "Iteration 279, Batch: 26, Loss: 0.05453922227025032\n",
      "Iteration 279, Batch: 27, Loss: 0.04933122545480728\n",
      "Iteration 279, Batch: 28, Loss: 0.052030038088560104\n",
      "Iteration 279, Batch: 29, Loss: 0.05732540041208267\n",
      "Iteration 279, Batch: 30, Loss: 0.08075833320617676\n",
      "Iteration 279, Batch: 31, Loss: 0.042109373956918716\n",
      "Iteration 279, Batch: 32, Loss: 0.07209866493940353\n",
      "Iteration 279, Batch: 33, Loss: 0.04599248245358467\n",
      "Iteration 279, Batch: 34, Loss: 0.04134391248226166\n",
      "Iteration 279, Batch: 35, Loss: 0.03701748326420784\n",
      "Iteration 279, Batch: 36, Loss: 0.0508539080619812\n",
      "Iteration 279, Batch: 37, Loss: 0.09090174734592438\n",
      "Iteration 279, Batch: 38, Loss: 0.07597779482603073\n",
      "Iteration 279, Batch: 39, Loss: 0.07473208010196686\n",
      "Iteration 279, Batch: 40, Loss: 0.0704333707690239\n",
      "Iteration 279, Batch: 41, Loss: 0.09074334800243378\n",
      "Iteration 279, Batch: 42, Loss: 0.07000809907913208\n",
      "Iteration 279, Batch: 43, Loss: 0.07044980674982071\n",
      "Iteration 279, Batch: 44, Loss: 0.04940667003393173\n",
      "Iteration 279, Batch: 45, Loss: 0.10529454797506332\n",
      "Iteration 279, Batch: 46, Loss: 0.08043811470270157\n",
      "Iteration 279, Batch: 47, Loss: 0.05923392251133919\n",
      "Iteration 279, Batch: 48, Loss: 0.06303323805332184\n",
      "Iteration 279, Batch: 49, Loss: 0.060422319918870926\n",
      "Number of layers: 10\n",
      "Iteration 280, Batch: 0, Loss: 0.05165912210941315\n",
      "Iteration 280, Batch: 1, Loss: 0.04593180492520332\n",
      "Iteration 280, Batch: 2, Loss: 0.07052528113126755\n",
      "Iteration 280, Batch: 3, Loss: 0.06055224686861038\n",
      "Iteration 280, Batch: 4, Loss: 0.05512702837586403\n",
      "Iteration 280, Batch: 5, Loss: 0.06856175512075424\n",
      "Iteration 280, Batch: 6, Loss: 0.09037148207426071\n",
      "Iteration 280, Batch: 7, Loss: 0.06163080036640167\n",
      "Iteration 280, Batch: 8, Loss: 0.05384384095668793\n",
      "Iteration 280, Batch: 9, Loss: 0.025667496025562286\n",
      "Iteration 280, Batch: 10, Loss: 0.054647039622068405\n",
      "Iteration 280, Batch: 11, Loss: 0.04337906837463379\n",
      "Iteration 280, Batch: 12, Loss: 0.035529084503650665\n",
      "Iteration 280, Batch: 13, Loss: 0.055786702781915665\n",
      "Iteration 280, Batch: 14, Loss: 0.03372683748602867\n",
      "Iteration 280, Batch: 15, Loss: 0.04839463531970978\n",
      "Iteration 280, Batch: 16, Loss: 0.04913978651165962\n",
      "Iteration 280, Batch: 17, Loss: 0.035614725202322006\n",
      "Iteration 280, Batch: 18, Loss: 0.04806823655962944\n",
      "Iteration 280, Batch: 19, Loss: 0.06241469830274582\n",
      "Iteration 280, Batch: 20, Loss: 0.05659796670079231\n",
      "Iteration 280, Batch: 21, Loss: 0.055255331099033356\n",
      "Iteration 280, Batch: 22, Loss: 0.06263216584920883\n",
      "Iteration 280, Batch: 23, Loss: 0.04552706703543663\n",
      "Iteration 280, Batch: 24, Loss: 0.031057771295309067\n",
      "Iteration 280, Batch: 25, Loss: 0.04447285458445549\n",
      "Iteration 280, Batch: 26, Loss: 0.05298520624637604\n",
      "Iteration 280, Batch: 27, Loss: 0.05709146335721016\n",
      "Iteration 280, Batch: 28, Loss: 0.04490679129958153\n",
      "Iteration 280, Batch: 29, Loss: 0.06814639270305634\n",
      "Iteration 280, Batch: 30, Loss: 0.15193235874176025\n",
      "Iteration 280, Batch: 31, Loss: 0.1407471001148224\n",
      "Iteration 280, Batch: 32, Loss: 0.10002048313617706\n",
      "Iteration 280, Batch: 33, Loss: 0.09489064663648605\n",
      "Iteration 280, Batch: 34, Loss: 0.06217114254832268\n",
      "Iteration 280, Batch: 35, Loss: 0.042858656495809555\n",
      "Iteration 280, Batch: 36, Loss: 0.05738241225481033\n",
      "Iteration 280, Batch: 37, Loss: 0.07713918387889862\n",
      "Iteration 280, Batch: 38, Loss: 0.07451830804347992\n",
      "Iteration 280, Batch: 39, Loss: 0.06010306254029274\n",
      "Iteration 280, Batch: 40, Loss: 0.08139263093471527\n",
      "Iteration 280, Batch: 41, Loss: 0.052557095885276794\n",
      "Iteration 280, Batch: 42, Loss: 0.053517475724220276\n",
      "Iteration 280, Batch: 43, Loss: 0.04330194368958473\n",
      "Iteration 280, Batch: 44, Loss: 0.056385792791843414\n",
      "Iteration 280, Batch: 45, Loss: 0.06066184490919113\n",
      "Iteration 280, Batch: 46, Loss: 0.060962092131376266\n",
      "Iteration 280, Batch: 47, Loss: 0.10177179425954819\n",
      "Iteration 280, Batch: 48, Loss: 0.1143629401922226\n",
      "Iteration 280, Batch: 49, Loss: 0.07860521972179413\n",
      "Number of layers: 10\n",
      "Iteration 281, Batch: 0, Loss: 0.05665986239910126\n",
      "Iteration 281, Batch: 1, Loss: 0.033904220908880234\n",
      "Iteration 281, Batch: 2, Loss: 0.05523024499416351\n",
      "Iteration 281, Batch: 3, Loss: 0.03907468542456627\n",
      "Iteration 281, Batch: 4, Loss: 0.07658657431602478\n",
      "Iteration 281, Batch: 5, Loss: 0.05129426717758179\n",
      "Iteration 281, Batch: 6, Loss: 0.047259263694286346\n",
      "Iteration 281, Batch: 7, Loss: 0.046426255255937576\n",
      "Iteration 281, Batch: 8, Loss: 0.06272594630718231\n",
      "Iteration 281, Batch: 9, Loss: 0.06605499237775803\n",
      "Iteration 281, Batch: 10, Loss: 0.058651652187108994\n",
      "Iteration 281, Batch: 11, Loss: 0.07265792042016983\n",
      "Iteration 281, Batch: 12, Loss: 0.058652594685554504\n",
      "Iteration 281, Batch: 13, Loss: 0.041847340762615204\n",
      "Iteration 281, Batch: 14, Loss: 0.063706174492836\n",
      "Iteration 281, Batch: 15, Loss: 0.09382220357656479\n",
      "Iteration 281, Batch: 16, Loss: 0.08848060667514801\n",
      "Iteration 281, Batch: 17, Loss: 0.07929178327322006\n",
      "Iteration 281, Batch: 18, Loss: 0.08775080740451813\n",
      "Iteration 281, Batch: 19, Loss: 0.07602491974830627\n",
      "Iteration 281, Batch: 20, Loss: 0.07823552191257477\n",
      "Iteration 281, Batch: 21, Loss: 0.0718974694609642\n",
      "Iteration 281, Batch: 22, Loss: 0.06283862143754959\n",
      "Iteration 281, Batch: 23, Loss: 0.06252118200063705\n",
      "Iteration 281, Batch: 24, Loss: 0.03988223150372505\n",
      "Iteration 281, Batch: 25, Loss: 0.07068640738725662\n",
      "Iteration 281, Batch: 26, Loss: 0.06764841824769974\n",
      "Iteration 281, Batch: 27, Loss: 0.05693827196955681\n",
      "Iteration 281, Batch: 28, Loss: 0.06123863160610199\n",
      "Iteration 281, Batch: 29, Loss: 0.041919153183698654\n",
      "Iteration 281, Batch: 30, Loss: 0.046968501061201096\n",
      "Iteration 281, Batch: 31, Loss: 0.05275566875934601\n",
      "Iteration 281, Batch: 32, Loss: 0.0572555810213089\n",
      "Iteration 281, Batch: 33, Loss: 0.0760786384344101\n",
      "Iteration 281, Batch: 34, Loss: 0.0561392717063427\n",
      "Iteration 281, Batch: 35, Loss: 0.06189752370119095\n",
      "Iteration 281, Batch: 36, Loss: 0.05795890837907791\n",
      "Iteration 281, Batch: 37, Loss: 0.050694435834884644\n",
      "Iteration 281, Batch: 38, Loss: 0.04551950469613075\n",
      "Iteration 281, Batch: 39, Loss: 0.05637175589799881\n",
      "Iteration 281, Batch: 40, Loss: 0.05198461562395096\n",
      "Iteration 281, Batch: 41, Loss: 0.06610202044248581\n",
      "Iteration 281, Batch: 42, Loss: 0.09379155933856964\n",
      "Iteration 281, Batch: 43, Loss: 0.07082020491361618\n",
      "Iteration 281, Batch: 44, Loss: 0.09995877742767334\n",
      "Iteration 281, Batch: 45, Loss: 0.03596033900976181\n",
      "Iteration 281, Batch: 46, Loss: 0.05546237528324127\n",
      "Iteration 281, Batch: 47, Loss: 0.11549211293458939\n",
      "Iteration 281, Batch: 48, Loss: 0.06932539492845535\n",
      "Iteration 281, Batch: 49, Loss: 0.06609401851892471\n",
      "Number of layers: 10\n",
      "Iteration 282, Batch: 0, Loss: 0.04798998683691025\n",
      "Iteration 282, Batch: 1, Loss: 0.05963883176445961\n",
      "Iteration 282, Batch: 2, Loss: 0.06151237711310387\n",
      "Iteration 282, Batch: 3, Loss: 0.04774744063615799\n",
      "Iteration 282, Batch: 4, Loss: 0.07099492102861404\n",
      "Iteration 282, Batch: 5, Loss: 0.05156955495476723\n",
      "Iteration 282, Batch: 6, Loss: 0.06548360735177994\n",
      "Iteration 282, Batch: 7, Loss: 0.077603779733181\n",
      "Iteration 282, Batch: 8, Loss: 0.0626726970076561\n",
      "Iteration 282, Batch: 9, Loss: 0.043436698615550995\n",
      "Iteration 282, Batch: 10, Loss: 0.0509071908891201\n",
      "Iteration 282, Batch: 11, Loss: 0.03209551423788071\n",
      "Iteration 282, Batch: 12, Loss: 0.044405244290828705\n",
      "Iteration 282, Batch: 13, Loss: 0.04338335245847702\n",
      "Iteration 282, Batch: 14, Loss: 0.04592956602573395\n",
      "Iteration 282, Batch: 15, Loss: 0.049075622111558914\n",
      "Iteration 282, Batch: 16, Loss: 0.05943521484732628\n",
      "Iteration 282, Batch: 17, Loss: 0.05889393016695976\n",
      "Iteration 282, Batch: 18, Loss: 0.07044386118650436\n",
      "Iteration 282, Batch: 19, Loss: 0.050197504460811615\n",
      "Iteration 282, Batch: 20, Loss: 0.08291540294885635\n",
      "Iteration 282, Batch: 21, Loss: 0.06001934036612511\n",
      "Iteration 282, Batch: 22, Loss: 0.07508914172649384\n",
      "Iteration 282, Batch: 23, Loss: 0.053885363042354584\n",
      "Iteration 282, Batch: 24, Loss: 0.04917311668395996\n",
      "Iteration 282, Batch: 25, Loss: 0.035800956189632416\n",
      "Iteration 282, Batch: 26, Loss: 0.050460223108530045\n",
      "Iteration 282, Batch: 27, Loss: 0.06613662838935852\n",
      "Iteration 282, Batch: 28, Loss: 0.052292630076408386\n",
      "Iteration 282, Batch: 29, Loss: 0.03560122102499008\n",
      "Iteration 282, Batch: 30, Loss: 0.04291129112243652\n",
      "Iteration 282, Batch: 31, Loss: 0.07700757682323456\n",
      "Iteration 282, Batch: 32, Loss: 0.04210629314184189\n",
      "Iteration 282, Batch: 33, Loss: 0.08029826730489731\n",
      "Iteration 282, Batch: 34, Loss: 0.036952368915081024\n",
      "Iteration 282, Batch: 35, Loss: 0.04352514073252678\n",
      "Iteration 282, Batch: 36, Loss: 0.03900880366563797\n",
      "Iteration 282, Batch: 37, Loss: 0.05243290960788727\n",
      "Iteration 282, Batch: 38, Loss: 0.04910316318273544\n",
      "Iteration 282, Batch: 39, Loss: 0.06538010388612747\n",
      "Iteration 282, Batch: 40, Loss: 0.05462437495589256\n",
      "Iteration 282, Batch: 41, Loss: 0.06791676580905914\n",
      "Iteration 282, Batch: 42, Loss: 0.04767559841275215\n",
      "Iteration 282, Batch: 43, Loss: 0.04349775239825249\n",
      "Iteration 282, Batch: 44, Loss: 0.06368297338485718\n",
      "Iteration 282, Batch: 45, Loss: 0.046056345105171204\n",
      "Iteration 282, Batch: 46, Loss: 0.04907903075218201\n",
      "Iteration 282, Batch: 47, Loss: 0.0392812117934227\n",
      "Iteration 282, Batch: 48, Loss: 0.06702543050050735\n",
      "Iteration 282, Batch: 49, Loss: 0.08110254257917404\n",
      "Number of layers: 10\n",
      "Iteration 283, Batch: 0, Loss: 0.04556891322135925\n",
      "Iteration 283, Batch: 1, Loss: 0.05841760337352753\n",
      "Iteration 283, Batch: 2, Loss: 0.1029277890920639\n",
      "Iteration 283, Batch: 3, Loss: 0.05614350363612175\n",
      "Iteration 283, Batch: 4, Loss: 0.06740868836641312\n",
      "Iteration 283, Batch: 5, Loss: 0.07699929922819138\n",
      "Iteration 283, Batch: 6, Loss: 0.042806271463632584\n",
      "Iteration 283, Batch: 7, Loss: 0.054151762276887894\n",
      "Iteration 283, Batch: 8, Loss: 0.05019932985305786\n",
      "Iteration 283, Batch: 9, Loss: 0.06727594137191772\n",
      "Iteration 283, Batch: 10, Loss: 0.0465332493185997\n",
      "Iteration 283, Batch: 11, Loss: 0.04439286142587662\n",
      "Iteration 283, Batch: 12, Loss: 0.04349822551012039\n",
      "Iteration 283, Batch: 13, Loss: 0.04845213145017624\n",
      "Iteration 283, Batch: 14, Loss: 0.05279175564646721\n",
      "Iteration 283, Batch: 15, Loss: 0.08344323933124542\n",
      "Iteration 283, Batch: 16, Loss: 0.037204571068286896\n",
      "Iteration 283, Batch: 17, Loss: 0.03703766688704491\n",
      "Iteration 283, Batch: 18, Loss: 0.06815887987613678\n",
      "Iteration 283, Batch: 19, Loss: 0.0641433596611023\n",
      "Iteration 283, Batch: 20, Loss: 0.04732655733823776\n",
      "Iteration 283, Batch: 21, Loss: 0.06498534232378006\n",
      "Iteration 283, Batch: 22, Loss: 0.050732240080833435\n",
      "Iteration 283, Batch: 23, Loss: 0.04931308329105377\n",
      "Iteration 283, Batch: 24, Loss: 0.05597419664263725\n",
      "Iteration 283, Batch: 25, Loss: 0.03867553174495697\n",
      "Iteration 283, Batch: 26, Loss: 0.06042511761188507\n",
      "Iteration 283, Batch: 27, Loss: 0.05150812491774559\n",
      "Iteration 283, Batch: 28, Loss: 0.03451214358210564\n",
      "Iteration 283, Batch: 29, Loss: 0.06425623595714569\n",
      "Iteration 283, Batch: 30, Loss: 0.05454280599951744\n",
      "Iteration 283, Batch: 31, Loss: 0.05088982358574867\n",
      "Iteration 283, Batch: 32, Loss: 0.03249404951930046\n",
      "Iteration 283, Batch: 33, Loss: 0.04646189138293266\n",
      "Iteration 283, Batch: 34, Loss: 0.06466081738471985\n",
      "Iteration 283, Batch: 35, Loss: 0.04846014082431793\n",
      "Iteration 283, Batch: 36, Loss: 0.026231521740555763\n",
      "Iteration 283, Batch: 37, Loss: 0.032359778881073\n",
      "Iteration 283, Batch: 38, Loss: 0.04682677984237671\n",
      "Iteration 283, Batch: 39, Loss: 0.05088508501648903\n",
      "Iteration 283, Batch: 40, Loss: 0.05204719305038452\n",
      "Iteration 283, Batch: 41, Loss: 0.056148018687963486\n",
      "Iteration 283, Batch: 42, Loss: 0.05031045898795128\n",
      "Iteration 283, Batch: 43, Loss: 0.03803609684109688\n",
      "Iteration 283, Batch: 44, Loss: 0.0405423529446125\n",
      "Iteration 283, Batch: 45, Loss: 0.054994650185108185\n",
      "Iteration 283, Batch: 46, Loss: 0.06857168674468994\n",
      "Iteration 283, Batch: 47, Loss: 0.07822010666131973\n",
      "Iteration 283, Batch: 48, Loss: 0.1016254797577858\n",
      "Iteration 283, Batch: 49, Loss: 0.08283264189958572\n",
      "Number of layers: 10\n",
      "Iteration 284, Batch: 0, Loss: 0.10246812552213669\n",
      "Iteration 284, Batch: 1, Loss: 0.07056833803653717\n",
      "Iteration 284, Batch: 2, Loss: 0.08573749661445618\n",
      "Iteration 284, Batch: 3, Loss: 0.056481216102838516\n",
      "Iteration 284, Batch: 4, Loss: 0.08875908702611923\n",
      "Iteration 284, Batch: 5, Loss: 0.11534079909324646\n",
      "Iteration 284, Batch: 6, Loss: 0.1126132383942604\n",
      "Iteration 284, Batch: 7, Loss: 0.09215995669364929\n",
      "Iteration 284, Batch: 8, Loss: 0.07776528596878052\n",
      "Iteration 284, Batch: 9, Loss: 0.05806860700249672\n",
      "Iteration 284, Batch: 10, Loss: 0.09488542377948761\n",
      "Iteration 284, Batch: 11, Loss: 0.04589059203863144\n",
      "Iteration 284, Batch: 12, Loss: 0.08740190416574478\n",
      "Iteration 284, Batch: 13, Loss: 0.07866733521223068\n",
      "Iteration 284, Batch: 14, Loss: 0.09974077343940735\n",
      "Iteration 284, Batch: 15, Loss: 0.05258089303970337\n",
      "Iteration 284, Batch: 16, Loss: 0.053022611886262894\n",
      "Iteration 284, Batch: 17, Loss: 0.040860217064619064\n",
      "Iteration 284, Batch: 18, Loss: 0.0558071993291378\n",
      "Iteration 284, Batch: 19, Loss: 0.0677456483244896\n",
      "Iteration 284, Batch: 20, Loss: 0.06475605815649033\n",
      "Iteration 284, Batch: 21, Loss: 0.059662941843271255\n",
      "Iteration 284, Batch: 22, Loss: 0.06624370068311691\n",
      "Iteration 284, Batch: 23, Loss: 0.06780485063791275\n",
      "Iteration 284, Batch: 24, Loss: 0.0744122788310051\n",
      "Iteration 284, Batch: 25, Loss: 0.07472460716962814\n",
      "Iteration 284, Batch: 26, Loss: 0.052643608301877975\n",
      "Iteration 284, Batch: 27, Loss: 0.06926864385604858\n",
      "Iteration 284, Batch: 28, Loss: 0.04791655391454697\n",
      "Iteration 284, Batch: 29, Loss: 0.04019952565431595\n",
      "Iteration 284, Batch: 30, Loss: 0.036606092005968094\n",
      "Iteration 284, Batch: 31, Loss: 0.03610601648688316\n",
      "Iteration 284, Batch: 32, Loss: 0.0520176962018013\n",
      "Iteration 284, Batch: 33, Loss: 0.06160486489534378\n",
      "Iteration 284, Batch: 34, Loss: 0.05935949087142944\n",
      "Iteration 284, Batch: 35, Loss: 0.04050561040639877\n",
      "Iteration 284, Batch: 36, Loss: 0.040031760931015015\n",
      "Iteration 284, Batch: 37, Loss: 0.055681273341178894\n",
      "Iteration 284, Batch: 38, Loss: 0.06896889954805374\n",
      "Iteration 284, Batch: 39, Loss: 0.05722793564200401\n",
      "Iteration 284, Batch: 40, Loss: 0.06404219567775726\n",
      "Iteration 284, Batch: 41, Loss: 0.10754109919071198\n",
      "Iteration 284, Batch: 42, Loss: 0.058893464505672455\n",
      "Iteration 284, Batch: 43, Loss: 0.0484749972820282\n",
      "Iteration 284, Batch: 44, Loss: 0.04725077375769615\n",
      "Iteration 284, Batch: 45, Loss: 0.07129445672035217\n",
      "Iteration 284, Batch: 46, Loss: 0.07375795394182205\n",
      "Iteration 284, Batch: 47, Loss: 0.0829261988401413\n",
      "Iteration 284, Batch: 48, Loss: 0.07159430533647537\n",
      "Iteration 284, Batch: 49, Loss: 0.06158415228128433\n",
      "Number of layers: 10\n",
      "Iteration 285, Batch: 0, Loss: 0.07693438231945038\n",
      "Iteration 285, Batch: 1, Loss: 0.05858183652162552\n",
      "Iteration 285, Batch: 2, Loss: 0.03512421250343323\n",
      "Iteration 285, Batch: 3, Loss: 0.04571191594004631\n",
      "Iteration 285, Batch: 4, Loss: 0.05732373893260956\n",
      "Iteration 285, Batch: 5, Loss: 0.04681956022977829\n",
      "Iteration 285, Batch: 6, Loss: 0.052762582898139954\n",
      "Iteration 285, Batch: 7, Loss: 0.046313636004924774\n",
      "Iteration 285, Batch: 8, Loss: 0.04975343495607376\n",
      "Iteration 285, Batch: 9, Loss: 0.03634900972247124\n",
      "Iteration 285, Batch: 10, Loss: 0.05740257725119591\n",
      "Iteration 285, Batch: 11, Loss: 0.03717472031712532\n",
      "Iteration 285, Batch: 12, Loss: 0.08290158212184906\n",
      "Iteration 285, Batch: 13, Loss: 0.04180469736456871\n",
      "Iteration 285, Batch: 14, Loss: 0.04740384593605995\n",
      "Iteration 285, Batch: 15, Loss: 0.06696834415197372\n",
      "Iteration 285, Batch: 16, Loss: 0.05781705304980278\n",
      "Iteration 285, Batch: 17, Loss: 0.05137333646416664\n",
      "Iteration 285, Batch: 18, Loss: 0.06150253117084503\n",
      "Iteration 285, Batch: 19, Loss: 0.040312521159648895\n",
      "Iteration 285, Batch: 20, Loss: 0.053562626242637634\n",
      "Iteration 285, Batch: 21, Loss: 0.02070515789091587\n",
      "Iteration 285, Batch: 22, Loss: 0.044547680765390396\n",
      "Iteration 285, Batch: 23, Loss: 0.05846536532044411\n",
      "Iteration 285, Batch: 24, Loss: 0.036713920533657074\n",
      "Iteration 285, Batch: 25, Loss: 0.07705291360616684\n",
      "Iteration 285, Batch: 26, Loss: 0.04256882146000862\n",
      "Iteration 285, Batch: 27, Loss: 0.07276392728090286\n",
      "Iteration 285, Batch: 28, Loss: 0.06549948453903198\n",
      "Iteration 285, Batch: 29, Loss: 0.05603737756609917\n",
      "Iteration 285, Batch: 30, Loss: 0.06233016774058342\n",
      "Iteration 285, Batch: 31, Loss: 0.07609684765338898\n",
      "Iteration 285, Batch: 32, Loss: 0.08607565611600876\n",
      "Iteration 285, Batch: 33, Loss: 0.05760369077324867\n",
      "Iteration 285, Batch: 34, Loss: 0.06672680377960205\n",
      "Iteration 285, Batch: 35, Loss: 0.06584545969963074\n",
      "Iteration 285, Batch: 36, Loss: 0.0803046002984047\n",
      "Iteration 285, Batch: 37, Loss: 0.05763491615653038\n",
      "Iteration 285, Batch: 38, Loss: 0.06344613432884216\n",
      "Iteration 285, Batch: 39, Loss: 0.058939751237630844\n",
      "Iteration 285, Batch: 40, Loss: 0.0696338340640068\n",
      "Iteration 285, Batch: 41, Loss: 0.05670774355530739\n",
      "Iteration 285, Batch: 42, Loss: 0.06831525266170502\n",
      "Iteration 285, Batch: 43, Loss: 0.051216572523117065\n",
      "Iteration 285, Batch: 44, Loss: 0.059156034141778946\n",
      "Iteration 285, Batch: 45, Loss: 0.0563616119325161\n",
      "Iteration 285, Batch: 46, Loss: 0.061637360602617264\n",
      "Iteration 285, Batch: 47, Loss: 0.043760109692811966\n",
      "Iteration 285, Batch: 48, Loss: 0.03347177430987358\n",
      "Iteration 285, Batch: 49, Loss: 0.05144811049103737\n",
      "Number of layers: 10\n",
      "Iteration 286, Batch: 0, Loss: 0.04774573817849159\n",
      "Iteration 286, Batch: 1, Loss: 0.06378395855426788\n",
      "Iteration 286, Batch: 2, Loss: 0.053785696625709534\n",
      "Iteration 286, Batch: 3, Loss: 0.05786380171775818\n",
      "Iteration 286, Batch: 4, Loss: 0.07156328111886978\n",
      "Iteration 286, Batch: 5, Loss: 0.060429204255342484\n",
      "Iteration 286, Batch: 6, Loss: 0.05099063739180565\n",
      "Iteration 286, Batch: 7, Loss: 0.0403907336294651\n",
      "Iteration 286, Batch: 8, Loss: 0.08506438881158829\n",
      "Iteration 286, Batch: 9, Loss: 0.06257966160774231\n",
      "Iteration 286, Batch: 10, Loss: 0.08884795755147934\n",
      "Iteration 286, Batch: 11, Loss: 0.08996731787919998\n",
      "Iteration 286, Batch: 12, Loss: 0.07179026305675507\n",
      "Iteration 286, Batch: 13, Loss: 0.06810032576322556\n",
      "Iteration 286, Batch: 14, Loss: 0.046585455536842346\n",
      "Iteration 286, Batch: 15, Loss: 0.03425361588597298\n",
      "Iteration 286, Batch: 16, Loss: 0.04873301088809967\n",
      "Iteration 286, Batch: 17, Loss: 0.08968370407819748\n",
      "Iteration 286, Batch: 18, Loss: 0.054215386509895325\n",
      "Iteration 286, Batch: 19, Loss: 0.054652199149131775\n",
      "Iteration 286, Batch: 20, Loss: 0.04784251004457474\n",
      "Iteration 286, Batch: 21, Loss: 0.0660911276936531\n",
      "Iteration 286, Batch: 22, Loss: 0.05887537822127342\n",
      "Iteration 286, Batch: 23, Loss: 0.06522738933563232\n",
      "Iteration 286, Batch: 24, Loss: 0.05549765005707741\n",
      "Iteration 286, Batch: 25, Loss: 0.04867241904139519\n",
      "Iteration 286, Batch: 26, Loss: 0.06367092579603195\n",
      "Iteration 286, Batch: 27, Loss: 0.06428461521863937\n",
      "Iteration 286, Batch: 28, Loss: 0.05287550017237663\n",
      "Iteration 286, Batch: 29, Loss: 0.04075091704726219\n",
      "Iteration 286, Batch: 30, Loss: 0.04647941514849663\n",
      "Iteration 286, Batch: 31, Loss: 0.06661408394575119\n",
      "Iteration 286, Batch: 32, Loss: 0.05530152842402458\n",
      "Iteration 286, Batch: 33, Loss: 0.07460058480501175\n",
      "Iteration 286, Batch: 34, Loss: 0.06445292383432388\n",
      "Iteration 286, Batch: 35, Loss: 0.06742920726537704\n",
      "Iteration 286, Batch: 36, Loss: 0.07946223020553589\n",
      "Iteration 286, Batch: 37, Loss: 0.06120556592941284\n",
      "Iteration 286, Batch: 38, Loss: 0.047884080559015274\n",
      "Iteration 286, Batch: 39, Loss: 0.06107427552342415\n",
      "Iteration 286, Batch: 40, Loss: 0.02624942548573017\n",
      "Iteration 286, Batch: 41, Loss: 0.04608637094497681\n",
      "Iteration 286, Batch: 42, Loss: 0.044453635811805725\n",
      "Iteration 286, Batch: 43, Loss: 0.10009413212537766\n",
      "Iteration 286, Batch: 44, Loss: 0.08457313477993011\n",
      "Iteration 286, Batch: 45, Loss: 0.07828350365161896\n",
      "Iteration 286, Batch: 46, Loss: 0.053399547934532166\n",
      "Iteration 286, Batch: 47, Loss: 0.05793365463614464\n",
      "Iteration 286, Batch: 48, Loss: 0.047617681324481964\n",
      "Iteration 286, Batch: 49, Loss: 0.04480655863881111\n",
      "Number of layers: 10\n",
      "Iteration 287, Batch: 0, Loss: 0.06448467820882797\n",
      "Iteration 287, Batch: 1, Loss: 0.034424517303705215\n",
      "Iteration 287, Batch: 2, Loss: 0.04778062552213669\n",
      "Iteration 287, Batch: 3, Loss: 0.025487830862402916\n",
      "Iteration 287, Batch: 4, Loss: 0.051128167659044266\n",
      "Iteration 287, Batch: 5, Loss: 0.027847247198224068\n",
      "Iteration 287, Batch: 6, Loss: 0.04753373563289642\n",
      "Iteration 287, Batch: 7, Loss: 0.07266771048307419\n",
      "Iteration 287, Batch: 8, Loss: 0.06529548764228821\n",
      "Iteration 287, Batch: 9, Loss: 0.06986603885889053\n",
      "Iteration 287, Batch: 10, Loss: 0.07832021266222\n",
      "Iteration 287, Batch: 11, Loss: 0.08609813451766968\n",
      "Iteration 287, Batch: 12, Loss: 0.06114904209971428\n",
      "Iteration 287, Batch: 13, Loss: 0.059287670999765396\n",
      "Iteration 287, Batch: 14, Loss: 0.04220843315124512\n",
      "Iteration 287, Batch: 15, Loss: 0.03879494592547417\n",
      "Iteration 287, Batch: 16, Loss: 0.061018407344818115\n",
      "Iteration 287, Batch: 17, Loss: 0.05663798749446869\n",
      "Iteration 287, Batch: 18, Loss: 0.05644945800304413\n",
      "Iteration 287, Batch: 19, Loss: 0.05819402262568474\n",
      "Iteration 287, Batch: 20, Loss: 0.05919305980205536\n",
      "Iteration 287, Batch: 21, Loss: 0.07290428876876831\n",
      "Iteration 287, Batch: 22, Loss: 0.06173669546842575\n",
      "Iteration 287, Batch: 23, Loss: 0.053293269127607346\n",
      "Iteration 287, Batch: 24, Loss: 0.07221642136573792\n",
      "Iteration 287, Batch: 25, Loss: 0.06756257265806198\n",
      "Iteration 287, Batch: 26, Loss: 0.07452452927827835\n",
      "Iteration 287, Batch: 27, Loss: 0.07571636140346527\n",
      "Iteration 287, Batch: 28, Loss: 0.0653885081410408\n",
      "Iteration 287, Batch: 29, Loss: 0.12743458151817322\n",
      "Iteration 287, Batch: 30, Loss: 0.1451006382703781\n",
      "Iteration 287, Batch: 31, Loss: 0.08916491270065308\n",
      "Iteration 287, Batch: 32, Loss: 0.08632628619670868\n",
      "Iteration 287, Batch: 33, Loss: 0.07109009474515915\n",
      "Iteration 287, Batch: 34, Loss: 0.06346094608306885\n",
      "Iteration 287, Batch: 35, Loss: 0.02178233489394188\n",
      "Iteration 287, Batch: 36, Loss: 0.07259627431631088\n",
      "Iteration 287, Batch: 37, Loss: 0.09561454504728317\n",
      "Iteration 287, Batch: 38, Loss: 0.07588975876569748\n",
      "Iteration 287, Batch: 39, Loss: 0.08678201586008072\n",
      "Iteration 287, Batch: 40, Loss: 0.05729205533862114\n",
      "Iteration 287, Batch: 41, Loss: 0.08112737536430359\n",
      "Iteration 287, Batch: 42, Loss: 0.11263297498226166\n",
      "Iteration 287, Batch: 43, Loss: 0.06957439333200455\n",
      "Iteration 287, Batch: 44, Loss: 0.06672351807355881\n",
      "Iteration 287, Batch: 45, Loss: 0.08457083255052567\n",
      "Iteration 287, Batch: 46, Loss: 0.03193753957748413\n",
      "Iteration 287, Batch: 47, Loss: 0.06379139423370361\n",
      "Iteration 287, Batch: 48, Loss: 0.057839054614305496\n",
      "Iteration 287, Batch: 49, Loss: 0.06566163897514343\n",
      "Number of layers: 10\n",
      "Iteration 288, Batch: 0, Loss: 0.05890288203954697\n",
      "Iteration 288, Batch: 1, Loss: 0.06119217723608017\n",
      "Iteration 288, Batch: 2, Loss: 0.04935586825013161\n",
      "Iteration 288, Batch: 3, Loss: 0.049344975501298904\n",
      "Iteration 288, Batch: 4, Loss: 0.06402911990880966\n",
      "Iteration 288, Batch: 5, Loss: 0.09304448962211609\n",
      "Iteration 288, Batch: 6, Loss: 0.09001406282186508\n",
      "Iteration 288, Batch: 7, Loss: 0.08323197811841965\n",
      "Iteration 288, Batch: 8, Loss: 0.06053127720952034\n",
      "Iteration 288, Batch: 9, Loss: 0.06052495911717415\n",
      "Iteration 288, Batch: 10, Loss: 0.06358816474676132\n",
      "Iteration 288, Batch: 11, Loss: 0.05163291469216347\n",
      "Iteration 288, Batch: 12, Loss: 0.06387820839881897\n",
      "Iteration 288, Batch: 13, Loss: 0.05161372572183609\n",
      "Iteration 288, Batch: 14, Loss: 0.06527858972549438\n",
      "Iteration 288, Batch: 15, Loss: 0.06706779450178146\n",
      "Iteration 288, Batch: 16, Loss: 0.06982959806919098\n",
      "Iteration 288, Batch: 17, Loss: 0.08877403289079666\n",
      "Iteration 288, Batch: 18, Loss: 0.057840507477521896\n",
      "Iteration 288, Batch: 19, Loss: 0.06431110948324203\n",
      "Iteration 288, Batch: 20, Loss: 0.04267725721001625\n",
      "Iteration 288, Batch: 21, Loss: 0.07274686545133591\n",
      "Iteration 288, Batch: 22, Loss: 0.07412751019001007\n",
      "Iteration 288, Batch: 23, Loss: 0.04140352085232735\n",
      "Iteration 288, Batch: 24, Loss: 0.07355830818414688\n",
      "Iteration 288, Batch: 25, Loss: 0.047594767063856125\n",
      "Iteration 288, Batch: 26, Loss: 0.04382915422320366\n",
      "Iteration 288, Batch: 27, Loss: 0.031672537326812744\n",
      "Iteration 288, Batch: 28, Loss: 0.051398277282714844\n",
      "Iteration 288, Batch: 29, Loss: 0.07125091552734375\n",
      "Iteration 288, Batch: 30, Loss: 0.059945981949567795\n",
      "Iteration 288, Batch: 31, Loss: 0.05754784867167473\n",
      "Iteration 288, Batch: 32, Loss: 0.05758675932884216\n",
      "Iteration 288, Batch: 33, Loss: 0.061984408646821976\n",
      "Iteration 288, Batch: 34, Loss: 0.054857149720191956\n",
      "Iteration 288, Batch: 35, Loss: 0.05976058170199394\n",
      "Iteration 288, Batch: 36, Loss: 0.06169654801487923\n",
      "Iteration 288, Batch: 37, Loss: 0.06742192059755325\n",
      "Iteration 288, Batch: 38, Loss: 0.06844363361597061\n",
      "Iteration 288, Batch: 39, Loss: 0.06667240709066391\n",
      "Iteration 288, Batch: 40, Loss: 0.0587599091231823\n",
      "Iteration 288, Batch: 41, Loss: 0.060484379529953\n",
      "Iteration 288, Batch: 42, Loss: 0.046228859573602676\n",
      "Iteration 288, Batch: 43, Loss: 0.101397804915905\n",
      "Iteration 288, Batch: 44, Loss: 0.08429479598999023\n",
      "Iteration 288, Batch: 45, Loss: 0.06494443863630295\n",
      "Iteration 288, Batch: 46, Loss: 0.07554183155298233\n",
      "Iteration 288, Batch: 47, Loss: 0.08897116780281067\n",
      "Iteration 288, Batch: 48, Loss: 0.05857280641794205\n",
      "Iteration 288, Batch: 49, Loss: 0.05538307875394821\n",
      "Number of layers: 10\n",
      "Iteration 289, Batch: 0, Loss: 0.0779496431350708\n",
      "Iteration 289, Batch: 1, Loss: 0.07537119835615158\n",
      "Iteration 289, Batch: 2, Loss: 0.06165843456983566\n",
      "Iteration 289, Batch: 3, Loss: 0.08003556728363037\n",
      "Iteration 289, Batch: 4, Loss: 0.09108159691095352\n",
      "Iteration 289, Batch: 5, Loss: 0.05426909402012825\n",
      "Iteration 289, Batch: 6, Loss: 0.04578027129173279\n",
      "Iteration 289, Batch: 7, Loss: 0.059749145060777664\n",
      "Iteration 289, Batch: 8, Loss: 0.05231035128235817\n",
      "Iteration 289, Batch: 9, Loss: 0.06749342381954193\n",
      "Iteration 289, Batch: 10, Loss: 0.0834479108452797\n",
      "Iteration 289, Batch: 11, Loss: 0.0350646935403347\n",
      "Iteration 289, Batch: 12, Loss: 0.08003018796443939\n",
      "Iteration 289, Batch: 13, Loss: 0.06714463233947754\n",
      "Iteration 289, Batch: 14, Loss: 0.06282825767993927\n",
      "Iteration 289, Batch: 15, Loss: 0.05168462172150612\n",
      "Iteration 289, Batch: 16, Loss: 0.05361386761069298\n",
      "Iteration 289, Batch: 17, Loss: 0.040965642780065536\n",
      "Iteration 289, Batch: 18, Loss: 0.02752591483294964\n",
      "Iteration 289, Batch: 19, Loss: 0.040367014706134796\n",
      "Iteration 289, Batch: 20, Loss: 0.04332985728979111\n",
      "Iteration 289, Batch: 21, Loss: 0.06750272959470749\n",
      "Iteration 289, Batch: 22, Loss: 0.03333690017461777\n",
      "Iteration 289, Batch: 23, Loss: 0.05063379555940628\n",
      "Iteration 289, Batch: 24, Loss: 0.05690232664346695\n",
      "Iteration 289, Batch: 25, Loss: 0.0633765310049057\n",
      "Iteration 289, Batch: 26, Loss: 0.0475015714764595\n",
      "Iteration 289, Batch: 27, Loss: 0.06279396265745163\n",
      "Iteration 289, Batch: 28, Loss: 0.04712265357375145\n",
      "Iteration 289, Batch: 29, Loss: 0.04329410567879677\n",
      "Iteration 289, Batch: 30, Loss: 0.03144311532378197\n",
      "Iteration 289, Batch: 31, Loss: 0.05890382453799248\n",
      "Iteration 289, Batch: 32, Loss: 0.08631179481744766\n",
      "Iteration 289, Batch: 33, Loss: 0.02732560597360134\n",
      "Iteration 289, Batch: 34, Loss: 0.022170452401041985\n",
      "Iteration 289, Batch: 35, Loss: 0.05021393671631813\n",
      "Iteration 289, Batch: 36, Loss: 0.03621892258524895\n",
      "Iteration 289, Batch: 37, Loss: 0.04738401249051094\n",
      "Iteration 289, Batch: 38, Loss: 0.05310908332467079\n",
      "Iteration 289, Batch: 39, Loss: 0.04764166846871376\n",
      "Iteration 289, Batch: 40, Loss: 0.05243879184126854\n",
      "Iteration 289, Batch: 41, Loss: 0.06669185310602188\n",
      "Iteration 289, Batch: 42, Loss: 0.0475916750729084\n",
      "Iteration 289, Batch: 43, Loss: 0.043416403234004974\n",
      "Iteration 289, Batch: 44, Loss: 0.037243060767650604\n",
      "Iteration 289, Batch: 45, Loss: 0.0504579059779644\n",
      "Iteration 289, Batch: 46, Loss: 0.045797936618328094\n",
      "Iteration 289, Batch: 47, Loss: 0.04807928577065468\n",
      "Iteration 289, Batch: 48, Loss: 0.08899866044521332\n",
      "Iteration 289, Batch: 49, Loss: 0.046507786959409714\n",
      "Number of layers: 10\n",
      "Iteration 290, Batch: 0, Loss: 0.052315693348646164\n",
      "Iteration 290, Batch: 1, Loss: 0.05760038644075394\n",
      "Iteration 290, Batch: 2, Loss: 0.04285347834229469\n",
      "Iteration 290, Batch: 3, Loss: 0.0825682058930397\n",
      "Iteration 290, Batch: 4, Loss: 0.05462271720170975\n",
      "Iteration 290, Batch: 5, Loss: 0.044146984815597534\n",
      "Iteration 290, Batch: 6, Loss: 0.0702115148305893\n",
      "Iteration 290, Batch: 7, Loss: 0.040020234882831573\n",
      "Iteration 290, Batch: 8, Loss: 0.05426206439733505\n",
      "Iteration 290, Batch: 9, Loss: 0.05966766178607941\n",
      "Iteration 290, Batch: 10, Loss: 0.0355430506169796\n",
      "Iteration 290, Batch: 11, Loss: 0.06128337234258652\n",
      "Iteration 290, Batch: 12, Loss: 0.08511202037334442\n",
      "Iteration 290, Batch: 13, Loss: 0.06944756954908371\n",
      "Iteration 290, Batch: 14, Loss: 0.05576905980706215\n",
      "Iteration 290, Batch: 15, Loss: 0.024997252970933914\n",
      "Iteration 290, Batch: 16, Loss: 0.05206229165196419\n",
      "Iteration 290, Batch: 17, Loss: 0.04933670535683632\n",
      "Iteration 290, Batch: 18, Loss: 0.09293975681066513\n",
      "Iteration 290, Batch: 19, Loss: 0.06481947749853134\n",
      "Iteration 290, Batch: 20, Loss: 0.06198020651936531\n",
      "Iteration 290, Batch: 21, Loss: 0.06745894998311996\n",
      "Iteration 290, Batch: 22, Loss: 0.07139535248279572\n",
      "Iteration 290, Batch: 23, Loss: 0.04358944669365883\n",
      "Iteration 290, Batch: 24, Loss: 0.05447721853852272\n",
      "Iteration 290, Batch: 25, Loss: 0.045693300664424896\n",
      "Iteration 290, Batch: 26, Loss: 0.042393360286951065\n",
      "Iteration 290, Batch: 27, Loss: 0.05385482311248779\n",
      "Iteration 290, Batch: 28, Loss: 0.05149613320827484\n",
      "Iteration 290, Batch: 29, Loss: 0.039547499269247055\n",
      "Iteration 290, Batch: 30, Loss: 0.04084097594022751\n",
      "Iteration 290, Batch: 31, Loss: 0.04536138474941254\n",
      "Iteration 290, Batch: 32, Loss: 0.05489685758948326\n",
      "Iteration 290, Batch: 33, Loss: 0.05298222601413727\n",
      "Iteration 290, Batch: 34, Loss: 0.01947977766394615\n",
      "Iteration 290, Batch: 35, Loss: 0.04681282863020897\n",
      "Iteration 290, Batch: 36, Loss: 0.06050526350736618\n",
      "Iteration 290, Batch: 37, Loss: 0.04845143109560013\n",
      "Iteration 290, Batch: 38, Loss: 0.047754917293787\n",
      "Iteration 290, Batch: 39, Loss: 0.07411005347967148\n",
      "Iteration 290, Batch: 40, Loss: 0.04781629517674446\n",
      "Iteration 290, Batch: 41, Loss: 0.02374543435871601\n",
      "Iteration 290, Batch: 42, Loss: 0.041815135627985\n",
      "Iteration 290, Batch: 43, Loss: 0.06315493583679199\n",
      "Iteration 290, Batch: 44, Loss: 0.0607425756752491\n",
      "Iteration 290, Batch: 45, Loss: 0.027531037107110023\n",
      "Iteration 290, Batch: 46, Loss: 0.031906936317682266\n",
      "Iteration 290, Batch: 47, Loss: 0.030160583555698395\n",
      "Iteration 290, Batch: 48, Loss: 0.039713501930236816\n",
      "Iteration 290, Batch: 49, Loss: 0.04176913946866989\n",
      "Number of layers: 10\n",
      "Iteration 291, Batch: 0, Loss: 0.03609612211585045\n",
      "Iteration 291, Batch: 1, Loss: 0.043882016092538834\n",
      "Iteration 291, Batch: 2, Loss: 0.03440939262509346\n",
      "Iteration 291, Batch: 3, Loss: 0.05202089250087738\n",
      "Iteration 291, Batch: 4, Loss: 0.03957697004079819\n",
      "Iteration 291, Batch: 5, Loss: 0.04555797576904297\n",
      "Iteration 291, Batch: 6, Loss: 0.03580435365438461\n",
      "Iteration 291, Batch: 7, Loss: 0.06278572976589203\n",
      "Iteration 291, Batch: 8, Loss: 0.053842008113861084\n",
      "Iteration 291, Batch: 9, Loss: 0.06336255371570587\n",
      "Iteration 291, Batch: 10, Loss: 0.03774675726890564\n",
      "Iteration 291, Batch: 11, Loss: 0.02658250741660595\n",
      "Iteration 291, Batch: 12, Loss: 0.04102383926510811\n",
      "Iteration 291, Batch: 13, Loss: 0.03534067049622536\n",
      "Iteration 291, Batch: 14, Loss: 0.03518849238753319\n",
      "Iteration 291, Batch: 15, Loss: 0.027735471725463867\n",
      "Iteration 291, Batch: 16, Loss: 0.051941823214292526\n",
      "Iteration 291, Batch: 17, Loss: 0.061738595366477966\n",
      "Iteration 291, Batch: 18, Loss: 0.05605141818523407\n",
      "Iteration 291, Batch: 19, Loss: 0.03253898024559021\n",
      "Iteration 291, Batch: 20, Loss: 0.03856617957353592\n",
      "Iteration 291, Batch: 21, Loss: 0.06449280679225922\n",
      "Iteration 291, Batch: 22, Loss: 0.02998039685189724\n",
      "Iteration 291, Batch: 23, Loss: 0.028016014024615288\n",
      "Iteration 291, Batch: 24, Loss: 0.042511820793151855\n",
      "Iteration 291, Batch: 25, Loss: 0.04708005487918854\n",
      "Iteration 291, Batch: 26, Loss: 0.038991011679172516\n",
      "Iteration 291, Batch: 27, Loss: 0.047426942735910416\n",
      "Iteration 291, Batch: 28, Loss: 0.05006035044789314\n",
      "Iteration 291, Batch: 29, Loss: 0.06320950388908386\n",
      "Iteration 291, Batch: 30, Loss: 0.050969820469617844\n",
      "Iteration 291, Batch: 31, Loss: 0.04037158936262131\n",
      "Iteration 291, Batch: 32, Loss: 0.06637819856405258\n",
      "Iteration 291, Batch: 33, Loss: 0.040646810084581375\n",
      "Iteration 291, Batch: 34, Loss: 0.051525313407182693\n",
      "Iteration 291, Batch: 35, Loss: 0.06053784117102623\n",
      "Iteration 291, Batch: 36, Loss: 0.03598509356379509\n",
      "Iteration 291, Batch: 37, Loss: 0.06521745771169662\n",
      "Iteration 291, Batch: 38, Loss: 0.02675473317503929\n",
      "Iteration 291, Batch: 39, Loss: 0.039780233055353165\n",
      "Iteration 291, Batch: 40, Loss: 0.03386576846241951\n",
      "Iteration 291, Batch: 41, Loss: 0.06059243902564049\n",
      "Iteration 291, Batch: 42, Loss: 0.06969606131315231\n",
      "Iteration 291, Batch: 43, Loss: 0.041447389870882034\n",
      "Iteration 291, Batch: 44, Loss: 0.04098378121852875\n",
      "Iteration 291, Batch: 45, Loss: 0.03864492103457451\n",
      "Iteration 291, Batch: 46, Loss: 0.04559663310647011\n",
      "Iteration 291, Batch: 47, Loss: 0.04725901409983635\n",
      "Iteration 291, Batch: 48, Loss: 0.03475545719265938\n",
      "Iteration 291, Batch: 49, Loss: 0.046755217015743256\n",
      "Number of layers: 10\n",
      "Iteration 292, Batch: 0, Loss: 0.05555712431669235\n",
      "Iteration 292, Batch: 1, Loss: 0.040747277438640594\n",
      "Iteration 292, Batch: 2, Loss: 0.03904198482632637\n",
      "Iteration 292, Batch: 3, Loss: 0.04687843099236488\n",
      "Iteration 292, Batch: 4, Loss: 0.041381604969501495\n",
      "Iteration 292, Batch: 5, Loss: 0.030224427580833435\n",
      "Iteration 292, Batch: 6, Loss: 0.02778507210314274\n",
      "Iteration 292, Batch: 7, Loss: 0.05701969563961029\n",
      "Iteration 292, Batch: 8, Loss: 0.06911363452672958\n",
      "Iteration 292, Batch: 9, Loss: 0.047266747802495956\n",
      "Iteration 292, Batch: 10, Loss: 0.03364893049001694\n",
      "Iteration 292, Batch: 11, Loss: 0.03960685059428215\n",
      "Iteration 292, Batch: 12, Loss: 0.04858223721385002\n",
      "Iteration 292, Batch: 13, Loss: 0.07009006291627884\n",
      "Iteration 292, Batch: 14, Loss: 0.03066634200513363\n",
      "Iteration 292, Batch: 15, Loss: 0.04846549779176712\n",
      "Iteration 292, Batch: 16, Loss: 0.034246381372213364\n",
      "Iteration 292, Batch: 17, Loss: 0.06699215620756149\n",
      "Iteration 292, Batch: 18, Loss: 0.09558358788490295\n",
      "Iteration 292, Batch: 19, Loss: 0.0770336240530014\n",
      "Iteration 292, Batch: 20, Loss: 0.04305342584848404\n",
      "Iteration 292, Batch: 21, Loss: 0.06206100434064865\n",
      "Iteration 292, Batch: 22, Loss: 0.04144721478223801\n",
      "Iteration 292, Batch: 23, Loss: 0.05905458331108093\n",
      "Iteration 292, Batch: 24, Loss: 0.07028143107891083\n",
      "Iteration 292, Batch: 25, Loss: 0.050582803785800934\n",
      "Iteration 292, Batch: 26, Loss: 0.02517961896955967\n",
      "Iteration 292, Batch: 27, Loss: 0.04680124297738075\n",
      "Iteration 292, Batch: 28, Loss: 0.050729990005493164\n",
      "Iteration 292, Batch: 29, Loss: 0.032589562237262726\n",
      "Iteration 292, Batch: 30, Loss: 0.048814162611961365\n",
      "Iteration 292, Batch: 31, Loss: 0.03195764496922493\n",
      "Iteration 292, Batch: 32, Loss: 0.03622592240571976\n",
      "Iteration 292, Batch: 33, Loss: 0.05304450914263725\n",
      "Iteration 292, Batch: 34, Loss: 0.04648066312074661\n",
      "Iteration 292, Batch: 35, Loss: 0.0764228105545044\n",
      "Iteration 292, Batch: 36, Loss: 0.03703119605779648\n",
      "Iteration 292, Batch: 37, Loss: 0.05392848700284958\n",
      "Iteration 292, Batch: 38, Loss: 0.04906430095434189\n",
      "Iteration 292, Batch: 39, Loss: 0.054106634110212326\n",
      "Iteration 292, Batch: 40, Loss: 0.06131814047694206\n",
      "Iteration 292, Batch: 41, Loss: 0.04596855863928795\n",
      "Iteration 292, Batch: 42, Loss: 0.06023770198225975\n",
      "Iteration 292, Batch: 43, Loss: 0.06807229667901993\n",
      "Iteration 292, Batch: 44, Loss: 0.053355760872364044\n",
      "Iteration 292, Batch: 45, Loss: 0.06938294321298599\n",
      "Iteration 292, Batch: 46, Loss: 0.12331763654947281\n",
      "Iteration 292, Batch: 47, Loss: 0.11170240491628647\n",
      "Iteration 292, Batch: 48, Loss: 0.13730613887310028\n",
      "Iteration 292, Batch: 49, Loss: 0.07565203309059143\n",
      "Number of layers: 10\n",
      "Iteration 293, Batch: 0, Loss: 0.11241626739501953\n",
      "Iteration 293, Batch: 1, Loss: 0.07071580737829208\n",
      "Iteration 293, Batch: 2, Loss: 0.06474214792251587\n",
      "Iteration 293, Batch: 3, Loss: 0.06719283759593964\n",
      "Iteration 293, Batch: 4, Loss: 0.059218406677246094\n",
      "Iteration 293, Batch: 5, Loss: 0.05586691200733185\n",
      "Iteration 293, Batch: 6, Loss: 0.0564110092818737\n",
      "Iteration 293, Batch: 7, Loss: 0.04088272154331207\n",
      "Iteration 293, Batch: 8, Loss: 0.06182512640953064\n",
      "Iteration 293, Batch: 9, Loss: 0.05551918223500252\n",
      "Iteration 293, Batch: 10, Loss: 0.06001146137714386\n",
      "Iteration 293, Batch: 11, Loss: 0.03365630283951759\n",
      "Iteration 293, Batch: 12, Loss: 0.05832960829138756\n",
      "Iteration 293, Batch: 13, Loss: 0.05300549417734146\n",
      "Iteration 293, Batch: 14, Loss: 0.0706094279885292\n",
      "Iteration 293, Batch: 15, Loss: 0.05410532280802727\n",
      "Iteration 293, Batch: 16, Loss: 0.05950441211462021\n",
      "Iteration 293, Batch: 17, Loss: 0.057670362293720245\n",
      "Iteration 293, Batch: 18, Loss: 0.038681041449308395\n",
      "Iteration 293, Batch: 19, Loss: 0.05454806610941887\n",
      "Iteration 293, Batch: 20, Loss: 0.03397325053811073\n",
      "Iteration 293, Batch: 21, Loss: 0.04617253318428993\n",
      "Iteration 293, Batch: 22, Loss: 0.0634002611041069\n",
      "Iteration 293, Batch: 23, Loss: 0.05792552977800369\n",
      "Iteration 293, Batch: 24, Loss: 0.05719027295708656\n",
      "Iteration 293, Batch: 25, Loss: 0.048132121562957764\n",
      "Iteration 293, Batch: 26, Loss: 0.059264760464429855\n",
      "Iteration 293, Batch: 27, Loss: 0.06692468374967575\n",
      "Iteration 293, Batch: 28, Loss: 0.03943195566534996\n",
      "Iteration 293, Batch: 29, Loss: 0.04953831061720848\n",
      "Iteration 293, Batch: 30, Loss: 0.018623260781168938\n",
      "Iteration 293, Batch: 31, Loss: 0.047834161669015884\n",
      "Iteration 293, Batch: 32, Loss: 0.06153520569205284\n",
      "Iteration 293, Batch: 33, Loss: 0.04176699370145798\n",
      "Iteration 293, Batch: 34, Loss: 0.05835656076669693\n",
      "Iteration 293, Batch: 35, Loss: 0.08082322776317596\n",
      "Iteration 293, Batch: 36, Loss: 0.055959757417440414\n",
      "Iteration 293, Batch: 37, Loss: 0.043150395154953\n",
      "Iteration 293, Batch: 38, Loss: 0.04556683450937271\n",
      "Iteration 293, Batch: 39, Loss: 0.046034373342990875\n",
      "Iteration 293, Batch: 40, Loss: 0.04022042080760002\n",
      "Iteration 293, Batch: 41, Loss: 0.06863638013601303\n",
      "Iteration 293, Batch: 42, Loss: 0.03891787678003311\n",
      "Iteration 293, Batch: 43, Loss: 0.05407975986599922\n",
      "Iteration 293, Batch: 44, Loss: 0.05143536999821663\n",
      "Iteration 293, Batch: 45, Loss: 0.06375377625226974\n",
      "Iteration 293, Batch: 46, Loss: 0.06240158900618553\n",
      "Iteration 293, Batch: 47, Loss: 0.05026489123702049\n",
      "Iteration 293, Batch: 48, Loss: 0.0340486615896225\n",
      "Iteration 293, Batch: 49, Loss: 0.05771847441792488\n",
      "Number of layers: 10\n",
      "Iteration 294, Batch: 0, Loss: 0.08672498166561127\n",
      "Iteration 294, Batch: 1, Loss: 0.042218971997499466\n",
      "Iteration 294, Batch: 2, Loss: 0.04298774152994156\n",
      "Iteration 294, Batch: 3, Loss: 0.0474228598177433\n",
      "Iteration 294, Batch: 4, Loss: 0.07230567932128906\n",
      "Iteration 294, Batch: 5, Loss: 0.06183333322405815\n",
      "Iteration 294, Batch: 6, Loss: 0.0316922701895237\n",
      "Iteration 294, Batch: 7, Loss: 0.04677002876996994\n",
      "Iteration 294, Batch: 8, Loss: 0.08378742635250092\n",
      "Iteration 294, Batch: 9, Loss: 0.06891744583845139\n",
      "Iteration 294, Batch: 10, Loss: 0.07825911045074463\n",
      "Iteration 294, Batch: 11, Loss: 0.04617200419306755\n",
      "Iteration 294, Batch: 12, Loss: 0.08167101442813873\n",
      "Iteration 294, Batch: 13, Loss: 0.03614169731736183\n",
      "Iteration 294, Batch: 14, Loss: 0.04105377197265625\n",
      "Iteration 294, Batch: 15, Loss: 0.06324800103902817\n",
      "Iteration 294, Batch: 16, Loss: 0.03318523243069649\n",
      "Iteration 294, Batch: 17, Loss: 0.07069329917430878\n",
      "Iteration 294, Batch: 18, Loss: 0.02530704438686371\n",
      "Iteration 294, Batch: 19, Loss: 0.035826653242111206\n",
      "Iteration 294, Batch: 20, Loss: 0.04489867016673088\n",
      "Iteration 294, Batch: 21, Loss: 0.05506276339292526\n",
      "Iteration 294, Batch: 22, Loss: 0.050510916858911514\n",
      "Iteration 294, Batch: 23, Loss: 0.050876446068286896\n",
      "Iteration 294, Batch: 24, Loss: 0.02978501468896866\n",
      "Iteration 294, Batch: 25, Loss: 0.05291931331157684\n",
      "Iteration 294, Batch: 26, Loss: 0.04985947161912918\n",
      "Iteration 294, Batch: 27, Loss: 0.050409700721502304\n",
      "Iteration 294, Batch: 28, Loss: 0.05139900743961334\n",
      "Iteration 294, Batch: 29, Loss: 0.03091401420533657\n",
      "Iteration 294, Batch: 30, Loss: 0.03426705673336983\n",
      "Iteration 294, Batch: 31, Loss: 0.07807781547307968\n",
      "Iteration 294, Batch: 32, Loss: 0.03952888771891594\n",
      "Iteration 294, Batch: 33, Loss: 0.049249906092882156\n",
      "Iteration 294, Batch: 34, Loss: 0.056313637644052505\n",
      "Iteration 294, Batch: 35, Loss: 0.07032592594623566\n",
      "Iteration 294, Batch: 36, Loss: 0.0687008872628212\n",
      "Iteration 294, Batch: 37, Loss: 0.0622733011841774\n",
      "Iteration 294, Batch: 38, Loss: 0.057217568159103394\n",
      "Iteration 294, Batch: 39, Loss: 0.04511215165257454\n",
      "Iteration 294, Batch: 40, Loss: 0.05876864120364189\n",
      "Iteration 294, Batch: 41, Loss: 0.054413583129644394\n",
      "Iteration 294, Batch: 42, Loss: 0.05656380578875542\n",
      "Iteration 294, Batch: 43, Loss: 0.0612943060696125\n",
      "Iteration 294, Batch: 44, Loss: 0.0798933207988739\n",
      "Iteration 294, Batch: 45, Loss: 0.07315250486135483\n",
      "Iteration 294, Batch: 46, Loss: 0.07156716287136078\n",
      "Iteration 294, Batch: 47, Loss: 0.045446258038282394\n",
      "Iteration 294, Batch: 48, Loss: 0.05875721946358681\n",
      "Iteration 294, Batch: 49, Loss: 0.043294284492731094\n",
      "Number of layers: 10\n",
      "Iteration 295, Batch: 0, Loss: 0.07442867010831833\n",
      "Iteration 295, Batch: 1, Loss: 0.11035545915365219\n",
      "Iteration 295, Batch: 2, Loss: 0.10848625004291534\n",
      "Iteration 295, Batch: 3, Loss: 0.0936456024646759\n",
      "Iteration 295, Batch: 4, Loss: 0.087403804063797\n",
      "Iteration 295, Batch: 5, Loss: 0.06592506915330887\n",
      "Iteration 295, Batch: 6, Loss: 0.058037128299474716\n",
      "Iteration 295, Batch: 7, Loss: 0.04458797723054886\n",
      "Iteration 295, Batch: 8, Loss: 0.029834764078259468\n",
      "Iteration 295, Batch: 9, Loss: 0.06896340101957321\n",
      "Iteration 295, Batch: 10, Loss: 0.05226924642920494\n",
      "Iteration 295, Batch: 11, Loss: 0.04653611034154892\n",
      "Iteration 295, Batch: 12, Loss: 0.03385210037231445\n",
      "Iteration 295, Batch: 13, Loss: 0.02639353647828102\n",
      "Iteration 295, Batch: 14, Loss: 0.05279186740517616\n",
      "Iteration 295, Batch: 15, Loss: 0.04251664876937866\n",
      "Iteration 295, Batch: 16, Loss: 0.0801173597574234\n",
      "Iteration 295, Batch: 17, Loss: 0.06379780918359756\n",
      "Iteration 295, Batch: 18, Loss: 0.05551391839981079\n",
      "Iteration 295, Batch: 19, Loss: 0.0797828882932663\n",
      "Iteration 295, Batch: 20, Loss: 0.048861246556043625\n",
      "Iteration 295, Batch: 21, Loss: 0.05323195457458496\n",
      "Iteration 295, Batch: 22, Loss: 0.05495394766330719\n",
      "Iteration 295, Batch: 23, Loss: 0.041193608194589615\n",
      "Iteration 295, Batch: 24, Loss: 0.040667928755283356\n",
      "Iteration 295, Batch: 25, Loss: 0.053480084985494614\n",
      "Iteration 295, Batch: 26, Loss: 0.03695354238152504\n",
      "Iteration 295, Batch: 27, Loss: 0.06978759914636612\n",
      "Iteration 295, Batch: 28, Loss: 0.039269495755434036\n",
      "Iteration 295, Batch: 29, Loss: 0.017660625278949738\n",
      "Iteration 295, Batch: 30, Loss: 0.061503197997808456\n",
      "Iteration 295, Batch: 31, Loss: 0.05706680193543434\n",
      "Iteration 295, Batch: 32, Loss: 0.06236432120203972\n",
      "Iteration 295, Batch: 33, Loss: 0.04454652592539787\n",
      "Iteration 295, Batch: 34, Loss: 0.034204863011837006\n",
      "Iteration 295, Batch: 35, Loss: 0.04967116191983223\n",
      "Iteration 295, Batch: 36, Loss: 0.04521478712558746\n",
      "Iteration 295, Batch: 37, Loss: 0.040103740990161896\n",
      "Iteration 295, Batch: 38, Loss: 0.07703525573015213\n",
      "Iteration 295, Batch: 39, Loss: 0.038711898028850555\n",
      "Iteration 295, Batch: 40, Loss: 0.0449325256049633\n",
      "Iteration 295, Batch: 41, Loss: 0.08160703629255295\n",
      "Iteration 295, Batch: 42, Loss: 0.05668981000781059\n",
      "Iteration 295, Batch: 43, Loss: 0.04534774273633957\n",
      "Iteration 295, Batch: 44, Loss: 0.036718063056468964\n",
      "Iteration 295, Batch: 45, Loss: 0.05949089303612709\n",
      "Iteration 295, Batch: 46, Loss: 0.05065320059657097\n",
      "Iteration 295, Batch: 47, Loss: 0.049980729818344116\n",
      "Iteration 295, Batch: 48, Loss: 0.07168738543987274\n",
      "Iteration 295, Batch: 49, Loss: 0.03324221447110176\n",
      "Number of layers: 10\n",
      "Iteration 296, Batch: 0, Loss: 0.049964774399995804\n",
      "Iteration 296, Batch: 1, Loss: 0.03510241210460663\n",
      "Iteration 296, Batch: 2, Loss: 0.04692639410495758\n",
      "Iteration 296, Batch: 3, Loss: 0.04952823370695114\n",
      "Iteration 296, Batch: 4, Loss: 0.05401432886719704\n",
      "Iteration 296, Batch: 5, Loss: 0.04851604253053665\n",
      "Iteration 296, Batch: 6, Loss: 0.040294911712408066\n",
      "Iteration 296, Batch: 7, Loss: 0.050171736627817154\n",
      "Iteration 296, Batch: 8, Loss: 0.03606995567679405\n",
      "Iteration 296, Batch: 9, Loss: 0.043918363749980927\n",
      "Iteration 296, Batch: 10, Loss: 0.03665262088179588\n",
      "Iteration 296, Batch: 11, Loss: 0.036281418055295944\n",
      "Iteration 296, Batch: 12, Loss: 0.03965340554714203\n",
      "Iteration 296, Batch: 13, Loss: 0.027992086485028267\n",
      "Iteration 296, Batch: 14, Loss: 0.06790489703416824\n",
      "Iteration 296, Batch: 15, Loss: 0.03144945576786995\n",
      "Iteration 296, Batch: 16, Loss: 0.05219074711203575\n",
      "Iteration 296, Batch: 17, Loss: 0.05563292279839516\n",
      "Iteration 296, Batch: 18, Loss: 0.04172361269593239\n",
      "Iteration 296, Batch: 19, Loss: 0.04041554778814316\n",
      "Iteration 296, Batch: 20, Loss: 0.029072578996419907\n",
      "Iteration 296, Batch: 21, Loss: 0.05405924841761589\n",
      "Iteration 296, Batch: 22, Loss: 0.05680893734097481\n",
      "Iteration 296, Batch: 23, Loss: 0.04059632867574692\n",
      "Iteration 296, Batch: 24, Loss: 0.044325072318315506\n",
      "Iteration 296, Batch: 25, Loss: 0.07101187855005264\n",
      "Iteration 296, Batch: 26, Loss: 0.05872950330376625\n",
      "Iteration 296, Batch: 27, Loss: 0.06120201572775841\n",
      "Iteration 296, Batch: 28, Loss: 0.028233293443918228\n",
      "Iteration 296, Batch: 29, Loss: 0.05277634412050247\n",
      "Iteration 296, Batch: 30, Loss: 0.03557834029197693\n",
      "Iteration 296, Batch: 31, Loss: 0.04390005022287369\n",
      "Iteration 296, Batch: 32, Loss: 0.05190129578113556\n",
      "Iteration 296, Batch: 33, Loss: 0.05094184726476669\n",
      "Iteration 296, Batch: 34, Loss: 0.04941129684448242\n",
      "Iteration 296, Batch: 35, Loss: 0.030348310247063637\n",
      "Iteration 296, Batch: 36, Loss: 0.0547402985394001\n",
      "Iteration 296, Batch: 37, Loss: 0.03936074301600456\n",
      "Iteration 296, Batch: 38, Loss: 0.023838255554437637\n",
      "Iteration 296, Batch: 39, Loss: 0.02579834870994091\n",
      "Iteration 296, Batch: 40, Loss: 0.044295020401477814\n",
      "Iteration 296, Batch: 41, Loss: 0.04412011057138443\n",
      "Iteration 296, Batch: 42, Loss: 0.028771748766303062\n",
      "Iteration 296, Batch: 43, Loss: 0.027571868151426315\n",
      "Iteration 296, Batch: 44, Loss: 0.056693367660045624\n",
      "Iteration 296, Batch: 45, Loss: 0.0669531598687172\n",
      "Iteration 296, Batch: 46, Loss: 0.04027627408504486\n",
      "Iteration 296, Batch: 47, Loss: 0.04539930075407028\n",
      "Iteration 296, Batch: 48, Loss: 0.06388134509325027\n",
      "Iteration 296, Batch: 49, Loss: 0.038802746683359146\n",
      "Number of layers: 10\n",
      "Iteration 297, Batch: 0, Loss: 0.05295915529131889\n",
      "Iteration 297, Batch: 1, Loss: 0.030560612678527832\n",
      "Iteration 297, Batch: 2, Loss: 0.03665907680988312\n",
      "Iteration 297, Batch: 3, Loss: 0.03373438119888306\n",
      "Iteration 297, Batch: 4, Loss: 0.050553224980831146\n",
      "Iteration 297, Batch: 5, Loss: 0.044418271631002426\n",
      "Iteration 297, Batch: 6, Loss: 0.032771311700344086\n",
      "Iteration 297, Batch: 7, Loss: 0.04838643595576286\n",
      "Iteration 297, Batch: 8, Loss: 0.03557255491614342\n",
      "Iteration 297, Batch: 9, Loss: 0.05566137284040451\n",
      "Iteration 297, Batch: 10, Loss: 0.044733524322509766\n",
      "Iteration 297, Batch: 11, Loss: 0.047727640718221664\n",
      "Iteration 297, Batch: 12, Loss: 0.0600871779024601\n",
      "Iteration 297, Batch: 13, Loss: 0.08741777390241623\n",
      "Iteration 297, Batch: 14, Loss: 0.07255709171295166\n",
      "Iteration 297, Batch: 15, Loss: 0.06186429038643837\n",
      "Iteration 297, Batch: 16, Loss: 0.07172857969999313\n",
      "Iteration 297, Batch: 17, Loss: 0.0186642836779356\n",
      "Iteration 297, Batch: 18, Loss: 0.04304758459329605\n",
      "Iteration 297, Batch: 19, Loss: 0.03894614428281784\n",
      "Iteration 297, Batch: 20, Loss: 0.05457425117492676\n",
      "Iteration 297, Batch: 21, Loss: 0.04790179058909416\n",
      "Iteration 297, Batch: 22, Loss: 0.05057959258556366\n",
      "Iteration 297, Batch: 23, Loss: 0.05436630919575691\n",
      "Iteration 297, Batch: 24, Loss: 0.058396052569150925\n",
      "Iteration 297, Batch: 25, Loss: 0.029121750965714455\n",
      "Iteration 297, Batch: 26, Loss: 0.0571565218269825\n",
      "Iteration 297, Batch: 27, Loss: 0.05478103831410408\n",
      "Iteration 297, Batch: 28, Loss: 0.06903263181447983\n",
      "Iteration 297, Batch: 29, Loss: 0.0164907556027174\n",
      "Iteration 297, Batch: 30, Loss: 0.03969141095876694\n",
      "Iteration 297, Batch: 31, Loss: 0.0480823889374733\n",
      "Iteration 297, Batch: 32, Loss: 0.04493780806660652\n",
      "Iteration 297, Batch: 33, Loss: 0.03676678240299225\n",
      "Iteration 297, Batch: 34, Loss: 0.045432593673467636\n",
      "Iteration 297, Batch: 35, Loss: 0.03396986424922943\n",
      "Iteration 297, Batch: 36, Loss: 0.049245890229940414\n",
      "Iteration 297, Batch: 37, Loss: 0.03485511243343353\n",
      "Iteration 297, Batch: 38, Loss: 0.03624659776687622\n",
      "Iteration 297, Batch: 39, Loss: 0.04226730763912201\n",
      "Iteration 297, Batch: 40, Loss: 0.034724459052085876\n",
      "Iteration 297, Batch: 41, Loss: 0.041728675365448\n",
      "Iteration 297, Batch: 42, Loss: 0.02374599315226078\n",
      "Iteration 297, Batch: 43, Loss: 0.056570615619421005\n",
      "Iteration 297, Batch: 44, Loss: 0.05001603066921234\n",
      "Iteration 297, Batch: 45, Loss: 0.05451955273747444\n",
      "Iteration 297, Batch: 46, Loss: 0.06284696608781815\n",
      "Iteration 297, Batch: 47, Loss: 0.03455424681305885\n",
      "Iteration 297, Batch: 48, Loss: 0.04400189593434334\n",
      "Iteration 297, Batch: 49, Loss: 0.05179636925458908\n",
      "Number of layers: 10\n",
      "Iteration 298, Batch: 0, Loss: 0.04953836277127266\n",
      "Iteration 298, Batch: 1, Loss: 0.02115800604224205\n",
      "Iteration 298, Batch: 2, Loss: 0.04345322027802467\n",
      "Iteration 298, Batch: 3, Loss: 0.0617816187441349\n",
      "Iteration 298, Batch: 4, Loss: 0.03647143766283989\n",
      "Iteration 298, Batch: 5, Loss: 0.04389044642448425\n",
      "Iteration 298, Batch: 6, Loss: 0.03590819612145424\n",
      "Iteration 298, Batch: 7, Loss: 0.03025023825466633\n",
      "Iteration 298, Batch: 8, Loss: 0.04628632590174675\n",
      "Iteration 298, Batch: 9, Loss: 0.06216242164373398\n",
      "Iteration 298, Batch: 10, Loss: 0.046351708471775055\n",
      "Iteration 298, Batch: 11, Loss: 0.04155464470386505\n",
      "Iteration 298, Batch: 12, Loss: 0.03882454335689545\n",
      "Iteration 298, Batch: 13, Loss: 0.05040598288178444\n",
      "Iteration 298, Batch: 14, Loss: 0.0368955135345459\n",
      "Iteration 298, Batch: 15, Loss: 0.044522300362586975\n",
      "Iteration 298, Batch: 16, Loss: 0.05735047161579132\n",
      "Iteration 298, Batch: 17, Loss: 0.029751425608992577\n",
      "Iteration 298, Batch: 18, Loss: 0.03322341665625572\n",
      "Iteration 298, Batch: 19, Loss: 0.03565886989235878\n",
      "Iteration 298, Batch: 20, Loss: 0.040593236684799194\n",
      "Iteration 298, Batch: 21, Loss: 0.02271212451159954\n",
      "Iteration 298, Batch: 22, Loss: 0.04719895124435425\n",
      "Iteration 298, Batch: 23, Loss: 0.03850509226322174\n",
      "Iteration 298, Batch: 24, Loss: 0.06190095841884613\n",
      "Iteration 298, Batch: 25, Loss: 0.07837720215320587\n",
      "Iteration 298, Batch: 26, Loss: 0.03932115063071251\n",
      "Iteration 298, Batch: 27, Loss: 0.04371495544910431\n",
      "Iteration 298, Batch: 28, Loss: 0.04224434122443199\n",
      "Iteration 298, Batch: 29, Loss: 0.07595767825841904\n",
      "Iteration 298, Batch: 30, Loss: 0.030406204983592033\n",
      "Iteration 298, Batch: 31, Loss: 0.06260547786951065\n",
      "Iteration 298, Batch: 32, Loss: 0.05375067889690399\n",
      "Iteration 298, Batch: 33, Loss: 0.05951349064707756\n",
      "Iteration 298, Batch: 34, Loss: 0.030852660536766052\n",
      "Iteration 298, Batch: 35, Loss: 0.02843475341796875\n",
      "Iteration 298, Batch: 36, Loss: 0.06678549945354462\n",
      "Iteration 298, Batch: 37, Loss: 0.05900430679321289\n",
      "Iteration 298, Batch: 38, Loss: 0.03491194546222687\n",
      "Iteration 298, Batch: 39, Loss: 0.03208876773715019\n",
      "Iteration 298, Batch: 40, Loss: 0.04311743006110191\n",
      "Iteration 298, Batch: 41, Loss: 0.03869002312421799\n",
      "Iteration 298, Batch: 42, Loss: 0.05262642353773117\n",
      "Iteration 298, Batch: 43, Loss: 0.03688519820570946\n",
      "Iteration 298, Batch: 44, Loss: 0.060507018119096756\n",
      "Iteration 298, Batch: 45, Loss: 0.06338971108198166\n",
      "Iteration 298, Batch: 46, Loss: 0.05971279367804527\n",
      "Iteration 298, Batch: 47, Loss: 0.053875263780355453\n",
      "Iteration 298, Batch: 48, Loss: 0.045393798500299454\n",
      "Iteration 298, Batch: 49, Loss: 0.040688030421733856\n",
      "Number of layers: 10\n",
      "Iteration 299, Batch: 0, Loss: 0.029006900265812874\n",
      "Iteration 299, Batch: 1, Loss: 0.03047928772866726\n",
      "Iteration 299, Batch: 2, Loss: 0.050041504204273224\n",
      "Iteration 299, Batch: 3, Loss: 0.04889613017439842\n",
      "Iteration 299, Batch: 4, Loss: 0.051169268786907196\n",
      "Iteration 299, Batch: 5, Loss: 0.05083373934030533\n",
      "Iteration 299, Batch: 6, Loss: 0.03780270740389824\n",
      "Iteration 299, Batch: 7, Loss: 0.046231117099523544\n",
      "Iteration 299, Batch: 8, Loss: 0.03523493930697441\n",
      "Iteration 299, Batch: 9, Loss: 0.04541810229420662\n",
      "Iteration 299, Batch: 10, Loss: 0.03453224152326584\n",
      "Iteration 299, Batch: 11, Loss: 0.046837661415338516\n",
      "Iteration 299, Batch: 12, Loss: 0.036187075078487396\n",
      "Iteration 299, Batch: 13, Loss: 0.04575352743268013\n",
      "Iteration 299, Batch: 14, Loss: 0.03886769711971283\n",
      "Iteration 299, Batch: 15, Loss: 0.035025257617235184\n",
      "Iteration 299, Batch: 16, Loss: 0.05314125493168831\n",
      "Iteration 299, Batch: 17, Loss: 0.06483236700296402\n",
      "Iteration 299, Batch: 18, Loss: 0.04458652436733246\n",
      "Iteration 299, Batch: 19, Loss: 0.05474346876144409\n",
      "Iteration 299, Batch: 20, Loss: 0.04093659669160843\n",
      "Iteration 299, Batch: 21, Loss: 0.03866090625524521\n",
      "Iteration 299, Batch: 22, Loss: 0.054170187562704086\n",
      "Iteration 299, Batch: 23, Loss: 0.037992361932992935\n",
      "Iteration 299, Batch: 24, Loss: 0.02411559969186783\n",
      "Iteration 299, Batch: 25, Loss: 0.037469688802957535\n",
      "Iteration 299, Batch: 26, Loss: 0.03768947347998619\n",
      "Iteration 299, Batch: 27, Loss: 0.0479058213531971\n",
      "Iteration 299, Batch: 28, Loss: 0.024958442896604538\n",
      "Iteration 299, Batch: 29, Loss: 0.06523947417736053\n",
      "Iteration 299, Batch: 30, Loss: 0.03301646187901497\n",
      "Iteration 299, Batch: 31, Loss: 0.05039648711681366\n",
      "Iteration 299, Batch: 32, Loss: 0.05868333578109741\n",
      "Iteration 299, Batch: 33, Loss: 0.036051347851753235\n",
      "Iteration 299, Batch: 34, Loss: 0.08141719549894333\n",
      "Iteration 299, Batch: 35, Loss: 0.07438519597053528\n",
      "Iteration 299, Batch: 36, Loss: 0.03376501053571701\n",
      "Iteration 299, Batch: 37, Loss: 0.028129350394010544\n",
      "Iteration 299, Batch: 38, Loss: 0.037822429090738297\n",
      "Iteration 299, Batch: 39, Loss: 0.03913022577762604\n",
      "Iteration 299, Batch: 40, Loss: 0.05118577927350998\n",
      "Iteration 299, Batch: 41, Loss: 0.05125877261161804\n",
      "Iteration 299, Batch: 42, Loss: 0.055482663214206696\n",
      "Iteration 299, Batch: 43, Loss: 0.05817461758852005\n",
      "Iteration 299, Batch: 44, Loss: 0.02450738102197647\n",
      "Iteration 299, Batch: 45, Loss: 0.0619671605527401\n",
      "Iteration 299, Batch: 46, Loss: 0.029169995337724686\n",
      "Iteration 299, Batch: 47, Loss: 0.03878369554877281\n",
      "Iteration 299, Batch: 48, Loss: 0.05094229429960251\n",
      "Iteration 299, Batch: 49, Loss: 0.05165504664182663\n",
      "Number of layers: 10\n",
      "Iteration 300, Batch: 0, Loss: 0.0432862751185894\n",
      "Iteration 300, Batch: 1, Loss: 0.05565660819411278\n",
      "Iteration 300, Batch: 2, Loss: 0.03639606758952141\n",
      "Iteration 300, Batch: 3, Loss: 0.03411667421460152\n",
      "Iteration 300, Batch: 4, Loss: 0.04569859430193901\n",
      "Iteration 300, Batch: 5, Loss: 0.03804105520248413\n",
      "Iteration 300, Batch: 6, Loss: 0.03338604047894478\n",
      "Iteration 300, Batch: 7, Loss: 0.035337895154953\n",
      "Iteration 300, Batch: 8, Loss: 0.05622365325689316\n",
      "Iteration 300, Batch: 9, Loss: 0.038004741072654724\n",
      "Iteration 300, Batch: 10, Loss: 0.034812506288290024\n",
      "Iteration 300, Batch: 11, Loss: 0.03503239527344704\n",
      "Iteration 300, Batch: 12, Loss: 0.03515290841460228\n",
      "Iteration 300, Batch: 13, Loss: 0.031045736744999886\n",
      "Iteration 300, Batch: 14, Loss: 0.039228491485118866\n",
      "Iteration 300, Batch: 15, Loss: 0.040979914367198944\n",
      "Iteration 300, Batch: 16, Loss: 0.04689951241016388\n",
      "Iteration 300, Batch: 17, Loss: 0.05543774366378784\n",
      "Iteration 300, Batch: 18, Loss: 0.04644549638032913\n",
      "Iteration 300, Batch: 19, Loss: 0.07386908680200577\n",
      "Iteration 300, Batch: 20, Loss: 0.04467333108186722\n",
      "Iteration 300, Batch: 21, Loss: 0.06729859113693237\n",
      "Iteration 300, Batch: 22, Loss: 0.0550420805811882\n",
      "Iteration 300, Batch: 23, Loss: 0.029002083465456963\n",
      "Iteration 300, Batch: 24, Loss: 0.05040225014090538\n",
      "Iteration 300, Batch: 25, Loss: 0.05552871525287628\n",
      "Iteration 300, Batch: 26, Loss: 0.06985768675804138\n",
      "Iteration 300, Batch: 27, Loss: 0.07416491955518723\n",
      "Iteration 300, Batch: 28, Loss: 0.0677165612578392\n",
      "Iteration 300, Batch: 29, Loss: 0.04270559176802635\n",
      "Iteration 300, Batch: 30, Loss: 0.03029807098209858\n",
      "Iteration 300, Batch: 31, Loss: 0.04761975258588791\n",
      "Iteration 300, Batch: 32, Loss: 0.02712402679026127\n",
      "Iteration 300, Batch: 33, Loss: 0.052071187645196915\n",
      "Iteration 300, Batch: 34, Loss: 0.05555129051208496\n",
      "Iteration 300, Batch: 35, Loss: 0.046594951301813126\n",
      "Iteration 300, Batch: 36, Loss: 0.027238689363002777\n",
      "Iteration 300, Batch: 37, Loss: 0.037433989346027374\n",
      "Iteration 300, Batch: 38, Loss: 0.045798130333423615\n",
      "Iteration 300, Batch: 39, Loss: 0.060856327414512634\n",
      "Iteration 300, Batch: 40, Loss: 0.03556506708264351\n",
      "Iteration 300, Batch: 41, Loss: 0.034062739461660385\n",
      "Iteration 300, Batch: 42, Loss: 0.04343926161527634\n",
      "Iteration 300, Batch: 43, Loss: 0.04708049073815346\n",
      "Iteration 300, Batch: 44, Loss: 0.0429324246942997\n",
      "Iteration 300, Batch: 45, Loss: 0.042330216616392136\n",
      "Iteration 300, Batch: 46, Loss: 0.04934334754943848\n",
      "Iteration 300, Batch: 47, Loss: 0.04809298366308212\n",
      "Iteration 300, Batch: 48, Loss: 0.0360708050429821\n",
      "Iteration 300, Batch: 49, Loss: 0.04050813242793083\n",
      "Number of layers: 10\n",
      "Iteration 301, Batch: 0, Loss: 0.0573653019964695\n",
      "Iteration 301, Batch: 1, Loss: 0.0354674868285656\n",
      "Iteration 301, Batch: 2, Loss: 0.0572158619761467\n",
      "Iteration 301, Batch: 3, Loss: 0.06198723986744881\n",
      "Iteration 301, Batch: 4, Loss: 0.047147322446107864\n",
      "Iteration 301, Batch: 5, Loss: 0.06084994971752167\n",
      "Iteration 301, Batch: 6, Loss: 0.05694727227091789\n",
      "Iteration 301, Batch: 7, Loss: 0.0615248940885067\n",
      "Iteration 301, Batch: 8, Loss: 0.04297507181763649\n",
      "Iteration 301, Batch: 9, Loss: 0.05175060033798218\n",
      "Iteration 301, Batch: 10, Loss: 0.060745056718587875\n",
      "Iteration 301, Batch: 11, Loss: 0.028472309932112694\n",
      "Iteration 301, Batch: 12, Loss: 0.07170688360929489\n",
      "Iteration 301, Batch: 13, Loss: 0.043777551501989365\n",
      "Iteration 301, Batch: 14, Loss: 0.029376577585935593\n",
      "Iteration 301, Batch: 15, Loss: 0.049747828394174576\n",
      "Iteration 301, Batch: 16, Loss: 0.03209967538714409\n",
      "Iteration 301, Batch: 17, Loss: 0.03777384012937546\n",
      "Iteration 301, Batch: 18, Loss: 0.044267185032367706\n",
      "Iteration 301, Batch: 19, Loss: 0.03529104217886925\n",
      "Iteration 301, Batch: 20, Loss: 0.027170833200216293\n",
      "Iteration 301, Batch: 21, Loss: 0.03251850605010986\n",
      "Iteration 301, Batch: 22, Loss: 0.05818600580096245\n",
      "Iteration 301, Batch: 23, Loss: 0.05433440953493118\n",
      "Iteration 301, Batch: 24, Loss: 0.06704749166965485\n",
      "Iteration 301, Batch: 25, Loss: 0.06703746318817139\n",
      "Iteration 301, Batch: 26, Loss: 0.046256132423877716\n",
      "Iteration 301, Batch: 27, Loss: 0.06440524011850357\n",
      "Iteration 301, Batch: 28, Loss: 0.048261165618896484\n",
      "Iteration 301, Batch: 29, Loss: 0.08676336705684662\n",
      "Iteration 301, Batch: 30, Loss: 0.06361237168312073\n",
      "Iteration 301, Batch: 31, Loss: 0.045837998390197754\n",
      "Iteration 301, Batch: 32, Loss: 0.05667516961693764\n",
      "Iteration 301, Batch: 33, Loss: 0.04326896741986275\n",
      "Iteration 301, Batch: 34, Loss: 0.056211743503808975\n",
      "Iteration 301, Batch: 35, Loss: 0.03950369358062744\n",
      "Iteration 301, Batch: 36, Loss: 0.04594017565250397\n",
      "Iteration 301, Batch: 37, Loss: 0.04177164286375046\n",
      "Iteration 301, Batch: 38, Loss: 0.04598813131451607\n",
      "Iteration 301, Batch: 39, Loss: 0.04051162302494049\n",
      "Iteration 301, Batch: 40, Loss: 0.07574690878391266\n",
      "Iteration 301, Batch: 41, Loss: 0.05607764050364494\n",
      "Iteration 301, Batch: 42, Loss: 0.046428926289081573\n",
      "Iteration 301, Batch: 43, Loss: 0.044998832046985626\n",
      "Iteration 301, Batch: 44, Loss: 0.035833124071359634\n",
      "Iteration 301, Batch: 45, Loss: 0.06513719260692596\n",
      "Iteration 301, Batch: 46, Loss: 0.08010368794202805\n",
      "Iteration 301, Batch: 47, Loss: 0.056950852274894714\n",
      "Iteration 301, Batch: 48, Loss: 0.04233955219388008\n",
      "Iteration 301, Batch: 49, Loss: 0.06882905215024948\n",
      "Number of layers: 10\n",
      "Iteration 302, Batch: 0, Loss: 0.06617820262908936\n",
      "Iteration 302, Batch: 1, Loss: 0.051610317081213\n",
      "Iteration 302, Batch: 2, Loss: 0.07388998568058014\n",
      "Iteration 302, Batch: 3, Loss: 0.06321316212415695\n",
      "Iteration 302, Batch: 4, Loss: 0.07598134130239487\n",
      "Iteration 302, Batch: 5, Loss: 0.06347135454416275\n",
      "Iteration 302, Batch: 6, Loss: 0.060674943029880524\n",
      "Iteration 302, Batch: 7, Loss: 0.050176285207271576\n",
      "Iteration 302, Batch: 8, Loss: 0.06406379491090775\n",
      "Iteration 302, Batch: 9, Loss: 0.061595361679792404\n",
      "Iteration 302, Batch: 10, Loss: 0.06654184311628342\n",
      "Iteration 302, Batch: 11, Loss: 0.07609216123819351\n",
      "Iteration 302, Batch: 12, Loss: 0.0796830952167511\n",
      "Iteration 302, Batch: 13, Loss: 0.07339399307966232\n",
      "Iteration 302, Batch: 14, Loss: 0.0486789308488369\n",
      "Iteration 302, Batch: 15, Loss: 0.051582805812358856\n",
      "Iteration 302, Batch: 16, Loss: 0.04431525245308876\n",
      "Iteration 302, Batch: 17, Loss: 0.03015921078622341\n",
      "Iteration 302, Batch: 18, Loss: 0.03438761457800865\n",
      "Iteration 302, Batch: 19, Loss: 0.028862938284873962\n",
      "Iteration 302, Batch: 20, Loss: 0.050767235457897186\n",
      "Iteration 302, Batch: 21, Loss: 0.0740116685628891\n",
      "Iteration 302, Batch: 22, Loss: 0.043832890689373016\n",
      "Iteration 302, Batch: 23, Loss: 0.07087961584329605\n",
      "Iteration 302, Batch: 24, Loss: 0.06222175434231758\n",
      "Iteration 302, Batch: 25, Loss: 0.03843695670366287\n",
      "Iteration 302, Batch: 26, Loss: 0.05449831858277321\n",
      "Iteration 302, Batch: 27, Loss: 0.05384986847639084\n",
      "Iteration 302, Batch: 28, Loss: 0.042081184685230255\n",
      "Iteration 302, Batch: 29, Loss: 0.07452469319105148\n",
      "Iteration 302, Batch: 30, Loss: 0.04650877043604851\n",
      "Iteration 302, Batch: 31, Loss: 0.054122354835271835\n",
      "Iteration 302, Batch: 32, Loss: 0.046767864376306534\n",
      "Iteration 302, Batch: 33, Loss: 0.027491886168718338\n",
      "Iteration 302, Batch: 34, Loss: 0.06054473668336868\n",
      "Iteration 302, Batch: 35, Loss: 0.029894525185227394\n",
      "Iteration 302, Batch: 36, Loss: 0.03813670575618744\n",
      "Iteration 302, Batch: 37, Loss: 0.04099118709564209\n",
      "Iteration 302, Batch: 38, Loss: 0.06159714609384537\n",
      "Iteration 302, Batch: 39, Loss: 0.022709615528583527\n",
      "Iteration 302, Batch: 40, Loss: 0.028180845081806183\n",
      "Iteration 302, Batch: 41, Loss: 0.0407126322388649\n",
      "Iteration 302, Batch: 42, Loss: 0.04338585585355759\n",
      "Iteration 302, Batch: 43, Loss: 0.042596474289894104\n",
      "Iteration 302, Batch: 44, Loss: 0.04704766348004341\n",
      "Iteration 302, Batch: 45, Loss: 0.046686723828315735\n",
      "Iteration 302, Batch: 46, Loss: 0.06739241629838943\n",
      "Iteration 302, Batch: 47, Loss: 0.025672797113656998\n",
      "Iteration 302, Batch: 48, Loss: 0.05692581087350845\n",
      "Iteration 302, Batch: 49, Loss: 0.05195537582039833\n",
      "Number of layers: 10\n",
      "Iteration 303, Batch: 0, Loss: 0.04229497164487839\n",
      "Iteration 303, Batch: 1, Loss: 0.02977132797241211\n",
      "Iteration 303, Batch: 2, Loss: 0.05345071852207184\n",
      "Iteration 303, Batch: 3, Loss: 0.038152627646923065\n",
      "Iteration 303, Batch: 4, Loss: 0.04814201220870018\n",
      "Iteration 303, Batch: 5, Loss: 0.049732230603694916\n",
      "Iteration 303, Batch: 6, Loss: 0.05873284116387367\n",
      "Iteration 303, Batch: 7, Loss: 0.04214184358716011\n",
      "Iteration 303, Batch: 8, Loss: 0.05096695199608803\n",
      "Iteration 303, Batch: 9, Loss: 0.03141113743185997\n",
      "Iteration 303, Batch: 10, Loss: 0.042536698281764984\n",
      "Iteration 303, Batch: 11, Loss: 0.04797669127583504\n",
      "Iteration 303, Batch: 12, Loss: 0.054609306156635284\n",
      "Iteration 303, Batch: 13, Loss: 0.07527544349431992\n",
      "Iteration 303, Batch: 14, Loss: 0.06110117956995964\n",
      "Iteration 303, Batch: 15, Loss: 0.05677711218595505\n",
      "Iteration 303, Batch: 16, Loss: 0.030164804309606552\n",
      "Iteration 303, Batch: 17, Loss: 0.044518355280160904\n",
      "Iteration 303, Batch: 18, Loss: 0.03587084636092186\n",
      "Iteration 303, Batch: 19, Loss: 0.055921848863363266\n",
      "Iteration 303, Batch: 20, Loss: 0.05232221633195877\n",
      "Iteration 303, Batch: 21, Loss: 0.043983835726976395\n",
      "Iteration 303, Batch: 22, Loss: 0.028899502009153366\n",
      "Iteration 303, Batch: 23, Loss: 0.03640872985124588\n",
      "Iteration 303, Batch: 24, Loss: 0.04819117486476898\n",
      "Iteration 303, Batch: 25, Loss: 0.05210031569004059\n",
      "Iteration 303, Batch: 26, Loss: 0.04936278238892555\n",
      "Iteration 303, Batch: 27, Loss: 0.03333562612533569\n",
      "Iteration 303, Batch: 28, Loss: 0.04366691783070564\n",
      "Iteration 303, Batch: 29, Loss: 0.056134145706892014\n",
      "Iteration 303, Batch: 30, Loss: 0.034790705889463425\n",
      "Iteration 303, Batch: 31, Loss: 0.029709825292229652\n",
      "Iteration 303, Batch: 32, Loss: 0.03331609070301056\n",
      "Iteration 303, Batch: 33, Loss: 0.05764050781726837\n",
      "Iteration 303, Batch: 34, Loss: 0.05803508684039116\n",
      "Iteration 303, Batch: 35, Loss: 0.03949667140841484\n",
      "Iteration 303, Batch: 36, Loss: 0.05126647278666496\n",
      "Iteration 303, Batch: 37, Loss: 0.08121441304683685\n",
      "Iteration 303, Batch: 38, Loss: 0.060076188296079636\n",
      "Iteration 303, Batch: 39, Loss: 0.05457279086112976\n",
      "Iteration 303, Batch: 40, Loss: 0.04031722620129585\n",
      "Iteration 303, Batch: 41, Loss: 0.06780491769313812\n",
      "Iteration 303, Batch: 42, Loss: 0.06818526983261108\n",
      "Iteration 303, Batch: 43, Loss: 0.061114601790905\n",
      "Iteration 303, Batch: 44, Loss: 0.07992907613515854\n",
      "Iteration 303, Batch: 45, Loss: 0.09748747944831848\n",
      "Iteration 303, Batch: 46, Loss: 0.0729721337556839\n",
      "Iteration 303, Batch: 47, Loss: 0.08923632651567459\n",
      "Iteration 303, Batch: 48, Loss: 0.059569165110588074\n",
      "Iteration 303, Batch: 49, Loss: 0.052935514599084854\n",
      "Number of layers: 10\n",
      "Iteration 304, Batch: 0, Loss: 0.08171873539686203\n",
      "Iteration 304, Batch: 1, Loss: 0.084492526948452\n",
      "Iteration 304, Batch: 2, Loss: 0.07737782597541809\n",
      "Iteration 304, Batch: 3, Loss: 0.042816162109375\n",
      "Iteration 304, Batch: 4, Loss: 0.05314275249838829\n",
      "Iteration 304, Batch: 5, Loss: 0.05306953936815262\n",
      "Iteration 304, Batch: 6, Loss: 0.044790904968976974\n",
      "Iteration 304, Batch: 7, Loss: 0.041035089641809464\n",
      "Iteration 304, Batch: 8, Loss: 0.028498968109488487\n",
      "Iteration 304, Batch: 9, Loss: 0.04723924398422241\n",
      "Iteration 304, Batch: 10, Loss: 0.04191963002085686\n",
      "Iteration 304, Batch: 11, Loss: 0.05972853675484657\n",
      "Iteration 304, Batch: 12, Loss: 0.04898105561733246\n",
      "Iteration 304, Batch: 13, Loss: 0.04616774618625641\n",
      "Iteration 304, Batch: 14, Loss: 0.03420234099030495\n",
      "Iteration 304, Batch: 15, Loss: 0.045663852244615555\n",
      "Iteration 304, Batch: 16, Loss: 0.06060270220041275\n",
      "Iteration 304, Batch: 17, Loss: 0.06483393162488937\n",
      "Iteration 304, Batch: 18, Loss: 0.0710814818739891\n",
      "Iteration 304, Batch: 19, Loss: 0.06443876028060913\n",
      "Iteration 304, Batch: 20, Loss: 0.058680664747953415\n",
      "Iteration 304, Batch: 21, Loss: 0.03215169534087181\n",
      "Iteration 304, Batch: 22, Loss: 0.035510849207639694\n",
      "Iteration 304, Batch: 23, Loss: 0.040294576436281204\n",
      "Iteration 304, Batch: 24, Loss: 0.07215536385774612\n",
      "Iteration 304, Batch: 25, Loss: 0.06278453767299652\n",
      "Iteration 304, Batch: 26, Loss: 0.03325771912932396\n",
      "Iteration 304, Batch: 27, Loss: 0.05231492221355438\n",
      "Iteration 304, Batch: 28, Loss: 0.038499023765325546\n",
      "Iteration 304, Batch: 29, Loss: 0.06222185119986534\n",
      "Iteration 304, Batch: 30, Loss: 0.033441584557294846\n",
      "Iteration 304, Batch: 31, Loss: 0.03473230451345444\n",
      "Iteration 304, Batch: 32, Loss: 0.04572077468037605\n",
      "Iteration 304, Batch: 33, Loss: 0.07574819773435593\n",
      "Iteration 304, Batch: 34, Loss: 0.045784108340740204\n",
      "Iteration 304, Batch: 35, Loss: 0.08588743209838867\n",
      "Iteration 304, Batch: 36, Loss: 0.05815831199288368\n",
      "Iteration 304, Batch: 37, Loss: 0.04281456023454666\n",
      "Iteration 304, Batch: 38, Loss: 0.06040055304765701\n",
      "Iteration 304, Batch: 39, Loss: 0.05156462639570236\n",
      "Iteration 304, Batch: 40, Loss: 0.06067315489053726\n",
      "Iteration 304, Batch: 41, Loss: 0.052355993539094925\n",
      "Iteration 304, Batch: 42, Loss: 0.04891638085246086\n",
      "Iteration 304, Batch: 43, Loss: 0.0494239404797554\n",
      "Iteration 304, Batch: 44, Loss: 0.05063211917877197\n",
      "Iteration 304, Batch: 45, Loss: 0.07263171672821045\n",
      "Iteration 304, Batch: 46, Loss: 0.05077698454260826\n",
      "Iteration 304, Batch: 47, Loss: 0.03984234109520912\n",
      "Iteration 304, Batch: 48, Loss: 0.04210067167878151\n",
      "Iteration 304, Batch: 49, Loss: 0.03277426213026047\n",
      "Number of layers: 10\n",
      "Iteration 305, Batch: 0, Loss: 0.06240638718008995\n",
      "Iteration 305, Batch: 1, Loss: 0.06162300333380699\n",
      "Iteration 305, Batch: 2, Loss: 0.05679926648736\n",
      "Iteration 305, Batch: 3, Loss: 0.04623720049858093\n",
      "Iteration 305, Batch: 4, Loss: 0.03553255647420883\n",
      "Iteration 305, Batch: 5, Loss: 0.05635228008031845\n",
      "Iteration 305, Batch: 6, Loss: 0.06115828454494476\n",
      "Iteration 305, Batch: 7, Loss: 0.04736986383795738\n",
      "Iteration 305, Batch: 8, Loss: 0.04457885026931763\n",
      "Iteration 305, Batch: 9, Loss: 0.03267330676317215\n",
      "Iteration 305, Batch: 10, Loss: 0.03583957254886627\n",
      "Iteration 305, Batch: 11, Loss: 0.04862542077898979\n",
      "Iteration 305, Batch: 12, Loss: 0.054160136729478836\n",
      "Iteration 305, Batch: 13, Loss: 0.06970041990280151\n",
      "Iteration 305, Batch: 14, Loss: 0.04157460108399391\n",
      "Iteration 305, Batch: 15, Loss: 0.06692490726709366\n",
      "Iteration 305, Batch: 16, Loss: 0.05241428315639496\n",
      "Iteration 305, Batch: 17, Loss: 0.08731979131698608\n",
      "Iteration 305, Batch: 18, Loss: 0.051638197153806686\n",
      "Iteration 305, Batch: 19, Loss: 0.06927380710840225\n",
      "Iteration 305, Batch: 20, Loss: 0.05848066136240959\n",
      "Iteration 305, Batch: 21, Loss: 0.06068816035985947\n",
      "Iteration 305, Batch: 22, Loss: 0.06753190606832504\n",
      "Iteration 305, Batch: 23, Loss: 0.04271065443754196\n",
      "Iteration 305, Batch: 24, Loss: 0.06313151121139526\n",
      "Iteration 305, Batch: 25, Loss: 0.027501020580530167\n",
      "Iteration 305, Batch: 26, Loss: 0.02674507535994053\n",
      "Iteration 305, Batch: 27, Loss: 0.05949843302369118\n",
      "Iteration 305, Batch: 28, Loss: 0.059845760464668274\n",
      "Iteration 305, Batch: 29, Loss: 0.06379423290491104\n",
      "Iteration 305, Batch: 30, Loss: 0.08720264583826065\n",
      "Iteration 305, Batch: 31, Loss: 0.028226951137185097\n",
      "Iteration 305, Batch: 32, Loss: 0.05946587771177292\n",
      "Iteration 305, Batch: 33, Loss: 0.05657513812184334\n",
      "Iteration 305, Batch: 34, Loss: 0.0794801115989685\n",
      "Iteration 305, Batch: 35, Loss: 0.044795624911785126\n",
      "Iteration 305, Batch: 36, Loss: 0.05953687056899071\n",
      "Iteration 305, Batch: 37, Loss: 0.09361410140991211\n",
      "Iteration 305, Batch: 38, Loss: 0.06311172246932983\n",
      "Iteration 305, Batch: 39, Loss: 0.03515806794166565\n",
      "Iteration 305, Batch: 40, Loss: 0.055565718561410904\n",
      "Iteration 305, Batch: 41, Loss: 0.04222036153078079\n",
      "Iteration 305, Batch: 42, Loss: 0.09140830487012863\n",
      "Iteration 305, Batch: 43, Loss: 0.0496586449444294\n",
      "Iteration 305, Batch: 44, Loss: 0.0503913052380085\n",
      "Iteration 305, Batch: 45, Loss: 0.058525361120700836\n",
      "Iteration 305, Batch: 46, Loss: 0.08272439986467361\n",
      "Iteration 305, Batch: 47, Loss: 0.030636068433523178\n",
      "Iteration 305, Batch: 48, Loss: 0.06533804535865784\n",
      "Iteration 305, Batch: 49, Loss: 0.06081310659646988\n",
      "Number of layers: 10\n",
      "Iteration 306, Batch: 0, Loss: 0.043090637773275375\n",
      "Iteration 306, Batch: 1, Loss: 0.05894874408841133\n",
      "Iteration 306, Batch: 2, Loss: 0.07199980318546295\n",
      "Iteration 306, Batch: 3, Loss: 0.06423653662204742\n",
      "Iteration 306, Batch: 4, Loss: 0.07008979469537735\n",
      "Iteration 306, Batch: 5, Loss: 0.05665874481201172\n",
      "Iteration 306, Batch: 6, Loss: 0.08075760304927826\n",
      "Iteration 306, Batch: 7, Loss: 0.07071216404438019\n",
      "Iteration 306, Batch: 8, Loss: 0.04937825724482536\n",
      "Iteration 306, Batch: 9, Loss: 0.08656038343906403\n",
      "Iteration 306, Batch: 10, Loss: 0.08213744312524796\n",
      "Iteration 306, Batch: 11, Loss: 0.07810492068529129\n",
      "Iteration 306, Batch: 12, Loss: 0.09375603497028351\n",
      "Iteration 306, Batch: 13, Loss: 0.08506487309932709\n",
      "Iteration 306, Batch: 14, Loss: 0.07002633064985275\n",
      "Iteration 306, Batch: 15, Loss: 0.08086360991001129\n",
      "Iteration 306, Batch: 16, Loss: 0.08411269634962082\n",
      "Iteration 306, Batch: 17, Loss: 0.08525948971509933\n",
      "Iteration 306, Batch: 18, Loss: 0.06539600342512131\n",
      "Iteration 306, Batch: 19, Loss: 0.04115015268325806\n",
      "Iteration 306, Batch: 20, Loss: 0.07589231431484222\n",
      "Iteration 306, Batch: 21, Loss: 0.05980823561549187\n",
      "Iteration 306, Batch: 22, Loss: 0.08754672110080719\n",
      "Iteration 306, Batch: 23, Loss: 0.06226169317960739\n",
      "Iteration 306, Batch: 24, Loss: 0.06359855085611343\n",
      "Iteration 306, Batch: 25, Loss: 0.0676131397485733\n",
      "Iteration 306, Batch: 26, Loss: 0.08441612124443054\n",
      "Iteration 306, Batch: 27, Loss: 0.05959077551960945\n",
      "Iteration 306, Batch: 28, Loss: 0.03533197194337845\n",
      "Iteration 306, Batch: 29, Loss: 0.054307520389556885\n",
      "Iteration 306, Batch: 30, Loss: 0.033875077962875366\n",
      "Iteration 306, Batch: 31, Loss: 0.04957062751054764\n",
      "Iteration 306, Batch: 32, Loss: 0.06845289468765259\n",
      "Iteration 306, Batch: 33, Loss: 0.0881342664361\n",
      "Iteration 306, Batch: 34, Loss: 0.05823280289769173\n",
      "Iteration 306, Batch: 35, Loss: 0.04527117311954498\n",
      "Iteration 306, Batch: 36, Loss: 0.03328805789351463\n",
      "Iteration 306, Batch: 37, Loss: 0.053880028426647186\n",
      "Iteration 306, Batch: 38, Loss: 0.07238879054784775\n",
      "Iteration 306, Batch: 39, Loss: 0.03153245896100998\n",
      "Iteration 306, Batch: 40, Loss: 0.07277771830558777\n",
      "Iteration 306, Batch: 41, Loss: 0.07703091204166412\n",
      "Iteration 306, Batch: 42, Loss: 0.029762696474790573\n",
      "Iteration 306, Batch: 43, Loss: 0.05224471911787987\n",
      "Iteration 306, Batch: 44, Loss: 0.06593973934650421\n",
      "Iteration 306, Batch: 45, Loss: 0.045186612755060196\n",
      "Iteration 306, Batch: 46, Loss: 0.05772387236356735\n",
      "Iteration 306, Batch: 47, Loss: 0.0364513099193573\n",
      "Iteration 306, Batch: 48, Loss: 0.03708891198039055\n",
      "Iteration 306, Batch: 49, Loss: 0.06185724586248398\n",
      "Number of layers: 10\n",
      "Iteration 307, Batch: 0, Loss: 0.037613168358802795\n",
      "Iteration 307, Batch: 1, Loss: 0.05978179723024368\n",
      "Iteration 307, Batch: 2, Loss: 0.04715948924422264\n",
      "Iteration 307, Batch: 3, Loss: 0.0608840212225914\n",
      "Iteration 307, Batch: 4, Loss: 0.0569501630961895\n",
      "Iteration 307, Batch: 5, Loss: 0.03816145658493042\n",
      "Iteration 307, Batch: 6, Loss: 0.07605192810297012\n",
      "Iteration 307, Batch: 7, Loss: 0.040003325790166855\n",
      "Iteration 307, Batch: 8, Loss: 0.08157048374414444\n",
      "Iteration 307, Batch: 9, Loss: 0.04206499829888344\n",
      "Iteration 307, Batch: 10, Loss: 0.05188790336251259\n",
      "Iteration 307, Batch: 11, Loss: 0.06980111449956894\n",
      "Iteration 307, Batch: 12, Loss: 0.06403591483831406\n",
      "Iteration 307, Batch: 13, Loss: 0.06312742829322815\n",
      "Iteration 307, Batch: 14, Loss: 0.037036437541246414\n",
      "Iteration 307, Batch: 15, Loss: 0.06889531016349792\n",
      "Iteration 307, Batch: 16, Loss: 0.059781961143016815\n",
      "Iteration 307, Batch: 17, Loss: 0.08977607637643814\n",
      "Iteration 307, Batch: 18, Loss: 0.10701704025268555\n",
      "Iteration 307, Batch: 19, Loss: 0.06473714113235474\n",
      "Iteration 307, Batch: 20, Loss: 0.053341224789619446\n",
      "Iteration 307, Batch: 21, Loss: 0.045549917966127396\n",
      "Iteration 307, Batch: 22, Loss: 0.04935208335518837\n",
      "Iteration 307, Batch: 23, Loss: 0.029859622940421104\n",
      "Iteration 307, Batch: 24, Loss: 0.07021885365247726\n",
      "Iteration 307, Batch: 25, Loss: 0.0807354599237442\n",
      "Iteration 307, Batch: 26, Loss: 0.049586329609155655\n",
      "Iteration 307, Batch: 27, Loss: 0.054513540118932724\n",
      "Iteration 307, Batch: 28, Loss: 0.06734361499547958\n",
      "Iteration 307, Batch: 29, Loss: 0.05993172526359558\n",
      "Iteration 307, Batch: 30, Loss: 0.0701165497303009\n",
      "Iteration 307, Batch: 31, Loss: 0.031557101756334305\n",
      "Iteration 307, Batch: 32, Loss: 0.04269958287477493\n",
      "Iteration 307, Batch: 33, Loss: 0.03147616237401962\n",
      "Iteration 307, Batch: 34, Loss: 0.056707799434661865\n",
      "Iteration 307, Batch: 35, Loss: 0.04145805165171623\n",
      "Iteration 307, Batch: 36, Loss: 0.05009828507900238\n",
      "Iteration 307, Batch: 37, Loss: 0.0349653959274292\n",
      "Iteration 307, Batch: 38, Loss: 0.042993512004613876\n",
      "Iteration 307, Batch: 39, Loss: 0.044681109488010406\n",
      "Iteration 307, Batch: 40, Loss: 0.03338506445288658\n",
      "Iteration 307, Batch: 41, Loss: 0.0433502271771431\n",
      "Iteration 307, Batch: 42, Loss: 0.033405911177396774\n",
      "Iteration 307, Batch: 43, Loss: 0.03364051878452301\n",
      "Iteration 307, Batch: 44, Loss: 0.0465051531791687\n",
      "Iteration 307, Batch: 45, Loss: 0.04860183969140053\n",
      "Iteration 307, Batch: 46, Loss: 0.014663485810160637\n",
      "Iteration 307, Batch: 47, Loss: 0.04671468585729599\n",
      "Iteration 307, Batch: 48, Loss: 0.04768599942326546\n",
      "Iteration 307, Batch: 49, Loss: 0.05609554052352905\n",
      "Number of layers: 10\n",
      "Iteration 308, Batch: 0, Loss: 0.06493932753801346\n",
      "Iteration 308, Batch: 1, Loss: 0.02449793927371502\n",
      "Iteration 308, Batch: 2, Loss: 0.04509863629937172\n",
      "Iteration 308, Batch: 3, Loss: 0.07583528012037277\n",
      "Iteration 308, Batch: 4, Loss: 0.051343392580747604\n",
      "Iteration 308, Batch: 5, Loss: 0.03895174711942673\n",
      "Iteration 308, Batch: 6, Loss: 0.051467351615428925\n",
      "Iteration 308, Batch: 7, Loss: 0.0664580762386322\n",
      "Iteration 308, Batch: 8, Loss: 0.04693099856376648\n",
      "Iteration 308, Batch: 9, Loss: 0.06393007934093475\n",
      "Iteration 308, Batch: 10, Loss: 0.04439385607838631\n",
      "Iteration 308, Batch: 11, Loss: 0.042949188500642776\n",
      "Iteration 308, Batch: 12, Loss: 0.046622224152088165\n",
      "Iteration 308, Batch: 13, Loss: 0.057127177715301514\n",
      "Iteration 308, Batch: 14, Loss: 0.0722954049706459\n",
      "Iteration 308, Batch: 15, Loss: 0.06818723678588867\n",
      "Iteration 308, Batch: 16, Loss: 0.07989776134490967\n",
      "Iteration 308, Batch: 17, Loss: 0.09261955320835114\n",
      "Iteration 308, Batch: 18, Loss: 0.04682654142379761\n",
      "Iteration 308, Batch: 19, Loss: 0.050003185868263245\n",
      "Iteration 308, Batch: 20, Loss: 0.062293604016304016\n",
      "Iteration 308, Batch: 21, Loss: 0.07467756420373917\n",
      "Iteration 308, Batch: 22, Loss: 0.06379619240760803\n",
      "Iteration 308, Batch: 23, Loss: 0.08119695633649826\n",
      "Iteration 308, Batch: 24, Loss: 0.06502367556095123\n",
      "Iteration 308, Batch: 25, Loss: 0.04798942431807518\n",
      "Iteration 308, Batch: 26, Loss: 0.07537069171667099\n",
      "Iteration 308, Batch: 27, Loss: 0.056231774389743805\n",
      "Iteration 308, Batch: 28, Loss: 0.06277202814817429\n",
      "Iteration 308, Batch: 29, Loss: 0.08695907145738602\n",
      "Iteration 308, Batch: 30, Loss: 0.04472321271896362\n",
      "Iteration 308, Batch: 31, Loss: 0.04594118893146515\n",
      "Iteration 308, Batch: 32, Loss: 0.039107952266931534\n",
      "Iteration 308, Batch: 33, Loss: 0.03925124183297157\n",
      "Iteration 308, Batch: 34, Loss: 0.032659437507390976\n",
      "Iteration 308, Batch: 35, Loss: 0.046339478343725204\n",
      "Iteration 308, Batch: 36, Loss: 0.05535464733839035\n",
      "Iteration 308, Batch: 37, Loss: 0.06919779628515244\n",
      "Iteration 308, Batch: 38, Loss: 0.07740136235952377\n",
      "Iteration 308, Batch: 39, Loss: 0.03993551805615425\n",
      "Iteration 308, Batch: 40, Loss: 0.043330490589141846\n",
      "Iteration 308, Batch: 41, Loss: 0.044112466275691986\n",
      "Iteration 308, Batch: 42, Loss: 0.03541858494281769\n",
      "Iteration 308, Batch: 43, Loss: 0.0475093349814415\n",
      "Iteration 308, Batch: 44, Loss: 0.06847596168518066\n",
      "Iteration 308, Batch: 45, Loss: 0.04258548468351364\n",
      "Iteration 308, Batch: 46, Loss: 0.035359688103199005\n",
      "Iteration 308, Batch: 47, Loss: 0.050143178552389145\n",
      "Iteration 308, Batch: 48, Loss: 0.04549577459692955\n",
      "Iteration 308, Batch: 49, Loss: 0.03669147938489914\n",
      "Number of layers: 10\n",
      "Iteration 309, Batch: 0, Loss: 0.0766732320189476\n",
      "Iteration 309, Batch: 1, Loss: 0.04895709082484245\n",
      "Iteration 309, Batch: 2, Loss: 0.045836761593818665\n",
      "Iteration 309, Batch: 3, Loss: 0.08195864409208298\n",
      "Iteration 309, Batch: 4, Loss: 0.023304250091314316\n",
      "Iteration 309, Batch: 5, Loss: 0.04460141062736511\n",
      "Iteration 309, Batch: 6, Loss: 0.051035016775131226\n",
      "Iteration 309, Batch: 7, Loss: 0.036362629383802414\n",
      "Iteration 309, Batch: 8, Loss: 0.04614986851811409\n",
      "Iteration 309, Batch: 9, Loss: 0.06311026215553284\n",
      "Iteration 309, Batch: 10, Loss: 0.05867476388812065\n",
      "Iteration 309, Batch: 11, Loss: 0.035217445343732834\n",
      "Iteration 309, Batch: 12, Loss: 0.051923707127571106\n",
      "Iteration 309, Batch: 13, Loss: 0.05372723564505577\n",
      "Iteration 309, Batch: 14, Loss: 0.053404226899147034\n",
      "Iteration 309, Batch: 15, Loss: 0.032854899764060974\n",
      "Iteration 309, Batch: 16, Loss: 0.04927963763475418\n",
      "Iteration 309, Batch: 17, Loss: 0.050845690071582794\n",
      "Iteration 309, Batch: 18, Loss: 0.07949650287628174\n",
      "Iteration 309, Batch: 19, Loss: 0.0606374554336071\n",
      "Iteration 309, Batch: 20, Loss: 0.059711337089538574\n",
      "Iteration 309, Batch: 21, Loss: 0.03774483874440193\n",
      "Iteration 309, Batch: 22, Loss: 0.04288014397025108\n",
      "Iteration 309, Batch: 23, Loss: 0.024657130241394043\n",
      "Iteration 309, Batch: 24, Loss: 0.04530702903866768\n",
      "Iteration 309, Batch: 25, Loss: 0.028451794758439064\n",
      "Iteration 309, Batch: 26, Loss: 0.031207561492919922\n",
      "Iteration 309, Batch: 27, Loss: 0.05342606455087662\n",
      "Iteration 309, Batch: 28, Loss: 0.04459822550415993\n",
      "Iteration 309, Batch: 29, Loss: 0.0503036230802536\n",
      "Iteration 309, Batch: 30, Loss: 0.045522768050432205\n",
      "Iteration 309, Batch: 31, Loss: 0.04954390600323677\n",
      "Iteration 309, Batch: 32, Loss: 0.05015334486961365\n",
      "Iteration 309, Batch: 33, Loss: 0.02395423874258995\n",
      "Iteration 309, Batch: 34, Loss: 0.034128930419683456\n",
      "Iteration 309, Batch: 35, Loss: 0.04587385058403015\n",
      "Iteration 309, Batch: 36, Loss: 0.03557363897562027\n",
      "Iteration 309, Batch: 37, Loss: 0.057499270886182785\n",
      "Iteration 309, Batch: 38, Loss: 0.0550532341003418\n",
      "Iteration 309, Batch: 39, Loss: 0.03486422449350357\n",
      "Iteration 309, Batch: 40, Loss: 0.06617826968431473\n",
      "Iteration 309, Batch: 41, Loss: 0.024120785295963287\n",
      "Iteration 309, Batch: 42, Loss: 0.03173375874757767\n",
      "Iteration 309, Batch: 43, Loss: 0.039698533713817596\n",
      "Iteration 309, Batch: 44, Loss: 0.06414157152175903\n",
      "Iteration 309, Batch: 45, Loss: 0.0244656503200531\n",
      "Iteration 309, Batch: 46, Loss: 0.026842186227440834\n",
      "Iteration 309, Batch: 47, Loss: 0.04373551160097122\n",
      "Iteration 309, Batch: 48, Loss: 0.040565796196460724\n",
      "Iteration 309, Batch: 49, Loss: 0.026697615161538124\n",
      "Number of layers: 10\n",
      "Iteration 310, Batch: 0, Loss: 0.03372950106859207\n",
      "Iteration 310, Batch: 1, Loss: 0.057475823909044266\n",
      "Iteration 310, Batch: 2, Loss: 0.05913939327001572\n",
      "Iteration 310, Batch: 3, Loss: 0.039124272763729095\n",
      "Iteration 310, Batch: 4, Loss: 0.053477268666028976\n",
      "Iteration 310, Batch: 5, Loss: 0.036593735218048096\n",
      "Iteration 310, Batch: 6, Loss: 0.037913449108600616\n",
      "Iteration 310, Batch: 7, Loss: 0.054380111396312714\n",
      "Iteration 310, Batch: 8, Loss: 0.051914844661951065\n",
      "Iteration 310, Batch: 9, Loss: 0.04064849391579628\n",
      "Iteration 310, Batch: 10, Loss: 0.04136741906404495\n",
      "Iteration 310, Batch: 11, Loss: 0.04293631762266159\n",
      "Iteration 310, Batch: 12, Loss: 0.03219194710254669\n",
      "Iteration 310, Batch: 13, Loss: 0.03462647646665573\n",
      "Iteration 310, Batch: 14, Loss: 0.02677406743168831\n",
      "Iteration 310, Batch: 15, Loss: 0.05231477692723274\n",
      "Iteration 310, Batch: 16, Loss: 0.0485772043466568\n",
      "Iteration 310, Batch: 17, Loss: 0.04021112248301506\n",
      "Iteration 310, Batch: 18, Loss: 0.05034291744232178\n",
      "Iteration 310, Batch: 19, Loss: 0.03629427030682564\n",
      "Iteration 310, Batch: 20, Loss: 0.04002274572849274\n",
      "Iteration 310, Batch: 21, Loss: 0.04691974073648453\n",
      "Iteration 310, Batch: 22, Loss: 0.02964881621301174\n",
      "Iteration 310, Batch: 23, Loss: 0.07307100296020508\n",
      "Iteration 310, Batch: 24, Loss: 0.03365694358944893\n",
      "Iteration 310, Batch: 25, Loss: 0.05830011144280434\n",
      "Iteration 310, Batch: 26, Loss: 0.03786211833357811\n",
      "Iteration 310, Batch: 27, Loss: 0.05716807395219803\n",
      "Iteration 310, Batch: 28, Loss: 0.04350697249174118\n",
      "Iteration 310, Batch: 29, Loss: 0.0293427724391222\n",
      "Iteration 310, Batch: 30, Loss: 0.05462551862001419\n",
      "Iteration 310, Batch: 31, Loss: 0.04283163323998451\n",
      "Iteration 310, Batch: 32, Loss: 0.02800052985548973\n",
      "Iteration 310, Batch: 33, Loss: 0.03788964822888374\n",
      "Iteration 310, Batch: 34, Loss: 0.051265615969896317\n",
      "Iteration 310, Batch: 35, Loss: 0.05065689980983734\n",
      "Iteration 310, Batch: 36, Loss: 0.05539046227931976\n",
      "Iteration 310, Batch: 37, Loss: 0.04631160944700241\n",
      "Iteration 310, Batch: 38, Loss: 0.035445671528577805\n",
      "Iteration 310, Batch: 39, Loss: 0.044203680008649826\n",
      "Iteration 310, Batch: 40, Loss: 0.05274365469813347\n",
      "Iteration 310, Batch: 41, Loss: 0.0363459587097168\n",
      "Iteration 310, Batch: 42, Loss: 0.055386848747730255\n",
      "Iteration 310, Batch: 43, Loss: 0.05380358174443245\n",
      "Iteration 310, Batch: 44, Loss: 0.038026101887226105\n",
      "Iteration 310, Batch: 45, Loss: 0.05046306550502777\n",
      "Iteration 310, Batch: 46, Loss: 0.07835713028907776\n",
      "Iteration 310, Batch: 47, Loss: 0.06095454469323158\n",
      "Iteration 310, Batch: 48, Loss: 0.033944595605134964\n",
      "Iteration 310, Batch: 49, Loss: 0.04583501070737839\n",
      "Number of layers: 10\n",
      "Iteration 311, Batch: 0, Loss: 0.05323902145028114\n",
      "Iteration 311, Batch: 1, Loss: 0.03271128237247467\n",
      "Iteration 311, Batch: 2, Loss: 0.05383360758423805\n",
      "Iteration 311, Batch: 3, Loss: 0.037272024899721146\n",
      "Iteration 311, Batch: 4, Loss: 0.04424801096320152\n",
      "Iteration 311, Batch: 5, Loss: 0.052053358405828476\n",
      "Iteration 311, Batch: 6, Loss: 0.025630414485931396\n",
      "Iteration 311, Batch: 7, Loss: 0.044597186148166656\n",
      "Iteration 311, Batch: 8, Loss: 0.05272113159298897\n",
      "Iteration 311, Batch: 9, Loss: 0.0463109016418457\n",
      "Iteration 311, Batch: 10, Loss: 0.03214037045836449\n",
      "Iteration 311, Batch: 11, Loss: 0.05272289738059044\n",
      "Iteration 311, Batch: 12, Loss: 0.05341080576181412\n",
      "Iteration 311, Batch: 13, Loss: 0.0378144308924675\n",
      "Iteration 311, Batch: 14, Loss: 0.02180194854736328\n",
      "Iteration 311, Batch: 15, Loss: 0.04152795672416687\n",
      "Iteration 311, Batch: 16, Loss: 0.049639444798231125\n",
      "Iteration 311, Batch: 17, Loss: 0.022714486345648766\n",
      "Iteration 311, Batch: 18, Loss: 0.05108742415904999\n",
      "Iteration 311, Batch: 19, Loss: 0.06166201829910278\n",
      "Iteration 311, Batch: 20, Loss: 0.036505475640296936\n",
      "Iteration 311, Batch: 21, Loss: 0.05505368113517761\n",
      "Iteration 311, Batch: 22, Loss: 0.03404610604047775\n",
      "Iteration 311, Batch: 23, Loss: 0.050206415355205536\n",
      "Iteration 311, Batch: 24, Loss: 0.036788228899240494\n",
      "Iteration 311, Batch: 25, Loss: 0.03933056816458702\n",
      "Iteration 311, Batch: 26, Loss: 0.0580923818051815\n",
      "Iteration 311, Batch: 27, Loss: 0.04363032057881355\n",
      "Iteration 311, Batch: 28, Loss: 0.028405550867319107\n",
      "Iteration 311, Batch: 29, Loss: 0.05798210948705673\n",
      "Iteration 311, Batch: 30, Loss: 0.047039467841386795\n",
      "Iteration 311, Batch: 31, Loss: 0.044767942279577255\n",
      "Iteration 311, Batch: 32, Loss: 0.029810480773448944\n",
      "Iteration 311, Batch: 33, Loss: 0.04030769690871239\n",
      "Iteration 311, Batch: 34, Loss: 0.04986034333705902\n",
      "Iteration 311, Batch: 35, Loss: 0.05746911093592644\n",
      "Iteration 311, Batch: 36, Loss: 0.04722927138209343\n",
      "Iteration 311, Batch: 37, Loss: 0.03051481582224369\n",
      "Iteration 311, Batch: 38, Loss: 0.033462267369031906\n",
      "Iteration 311, Batch: 39, Loss: 0.06350256502628326\n",
      "Iteration 311, Batch: 40, Loss: 0.07413534075021744\n",
      "Iteration 311, Batch: 41, Loss: 0.03710073232650757\n",
      "Iteration 311, Batch: 42, Loss: 0.03247550502419472\n",
      "Iteration 311, Batch: 43, Loss: 0.04374826326966286\n",
      "Iteration 311, Batch: 44, Loss: 0.04427710548043251\n",
      "Iteration 311, Batch: 45, Loss: 0.05748482421040535\n",
      "Iteration 311, Batch: 46, Loss: 0.056853294372558594\n",
      "Iteration 311, Batch: 47, Loss: 0.06485617905855179\n",
      "Iteration 311, Batch: 48, Loss: 0.054425839334726334\n",
      "Iteration 311, Batch: 49, Loss: 0.06317867338657379\n",
      "Number of layers: 10\n",
      "Iteration 312, Batch: 0, Loss: 0.09236135333776474\n",
      "Iteration 312, Batch: 1, Loss: 0.09493745863437653\n",
      "Iteration 312, Batch: 2, Loss: 0.08923717588186264\n",
      "Iteration 312, Batch: 3, Loss: 0.053388576954603195\n",
      "Iteration 312, Batch: 4, Loss: 0.07064519077539444\n",
      "Iteration 312, Batch: 5, Loss: 0.04273250326514244\n",
      "Iteration 312, Batch: 6, Loss: 0.04142778739333153\n",
      "Iteration 312, Batch: 7, Loss: 0.03292315453290939\n",
      "Iteration 312, Batch: 8, Loss: 0.0530802346765995\n",
      "Iteration 312, Batch: 9, Loss: 0.048243120312690735\n",
      "Iteration 312, Batch: 10, Loss: 0.08223994821310043\n",
      "Iteration 312, Batch: 11, Loss: 0.0721367746591568\n",
      "Iteration 312, Batch: 12, Loss: 0.08351585268974304\n",
      "Iteration 312, Batch: 13, Loss: 0.08880753070116043\n",
      "Iteration 312, Batch: 14, Loss: 0.05676060542464256\n",
      "Iteration 312, Batch: 15, Loss: 0.04985947161912918\n",
      "Iteration 312, Batch: 16, Loss: 0.03018905594944954\n",
      "Iteration 312, Batch: 17, Loss: 0.06197008863091469\n",
      "Iteration 312, Batch: 18, Loss: 0.0923430398106575\n",
      "Iteration 312, Batch: 19, Loss: 0.06995756179094315\n",
      "Iteration 312, Batch: 20, Loss: 0.04409642890095711\n",
      "Iteration 312, Batch: 21, Loss: 0.025908516719937325\n",
      "Iteration 312, Batch: 22, Loss: 0.048128802329301834\n",
      "Iteration 312, Batch: 23, Loss: 0.05896875262260437\n",
      "Iteration 312, Batch: 24, Loss: 0.050303854048252106\n",
      "Iteration 312, Batch: 25, Loss: 0.031052932143211365\n",
      "Iteration 312, Batch: 26, Loss: 0.046189192682504654\n",
      "Iteration 312, Batch: 27, Loss: 0.03634700924158096\n",
      "Iteration 312, Batch: 28, Loss: 0.05649283155798912\n",
      "Iteration 312, Batch: 29, Loss: 0.04759199172258377\n",
      "Iteration 312, Batch: 30, Loss: 0.03913440555334091\n",
      "Iteration 312, Batch: 31, Loss: 0.05035686492919922\n",
      "Iteration 312, Batch: 32, Loss: 0.049216706305742264\n",
      "Iteration 312, Batch: 33, Loss: 0.04531525447964668\n",
      "Iteration 312, Batch: 34, Loss: 0.05115186423063278\n",
      "Iteration 312, Batch: 35, Loss: 0.05822832137346268\n",
      "Iteration 312, Batch: 36, Loss: 0.054835040122270584\n",
      "Iteration 312, Batch: 37, Loss: 0.06471323221921921\n",
      "Iteration 312, Batch: 38, Loss: 0.05836722254753113\n",
      "Iteration 312, Batch: 39, Loss: 0.038666997104883194\n",
      "Iteration 312, Batch: 40, Loss: 0.07232174277305603\n",
      "Iteration 312, Batch: 41, Loss: 0.0439533032476902\n",
      "Iteration 312, Batch: 42, Loss: 0.057017482817173004\n",
      "Iteration 312, Batch: 43, Loss: 0.045821040868759155\n",
      "Iteration 312, Batch: 44, Loss: 0.04024265334010124\n",
      "Iteration 312, Batch: 45, Loss: 0.10153164714574814\n",
      "Iteration 312, Batch: 46, Loss: 0.06027177721261978\n",
      "Iteration 312, Batch: 47, Loss: 0.04492959752678871\n",
      "Iteration 312, Batch: 48, Loss: 0.05070621520280838\n",
      "Iteration 312, Batch: 49, Loss: 0.048604030162096024\n",
      "Number of layers: 10\n",
      "Iteration 313, Batch: 0, Loss: 0.0403485968708992\n",
      "Iteration 313, Batch: 1, Loss: 0.06027703732252121\n",
      "Iteration 313, Batch: 2, Loss: 0.037720758467912674\n",
      "Iteration 313, Batch: 3, Loss: 0.02822575345635414\n",
      "Iteration 313, Batch: 4, Loss: 0.060370072722435\n",
      "Iteration 313, Batch: 5, Loss: 0.03911356255412102\n",
      "Iteration 313, Batch: 6, Loss: 0.0583251528441906\n",
      "Iteration 313, Batch: 7, Loss: 0.05343169718980789\n",
      "Iteration 313, Batch: 8, Loss: 0.04314667731523514\n",
      "Iteration 313, Batch: 9, Loss: 0.04792483150959015\n",
      "Iteration 313, Batch: 10, Loss: 0.05174050107598305\n",
      "Iteration 313, Batch: 11, Loss: 0.06059020757675171\n",
      "Iteration 313, Batch: 12, Loss: 0.05416419357061386\n",
      "Iteration 313, Batch: 13, Loss: 0.07597079128026962\n",
      "Iteration 313, Batch: 14, Loss: 0.03577413409948349\n",
      "Iteration 313, Batch: 15, Loss: 0.053446367383003235\n",
      "Iteration 313, Batch: 16, Loss: 0.06650146842002869\n",
      "Iteration 313, Batch: 17, Loss: 0.06277140229940414\n",
      "Iteration 313, Batch: 18, Loss: 0.038821328431367874\n",
      "Iteration 313, Batch: 19, Loss: 0.03706924989819527\n",
      "Iteration 313, Batch: 20, Loss: 0.0525900274515152\n",
      "Iteration 313, Batch: 21, Loss: 0.04698219150304794\n",
      "Iteration 313, Batch: 22, Loss: 0.04307259991765022\n",
      "Iteration 313, Batch: 23, Loss: 0.04627474769949913\n",
      "Iteration 313, Batch: 24, Loss: 0.036454103887081146\n",
      "Iteration 313, Batch: 25, Loss: 0.03526770696043968\n",
      "Iteration 313, Batch: 26, Loss: 0.06044619530439377\n",
      "Iteration 313, Batch: 27, Loss: 0.058051519095897675\n",
      "Iteration 313, Batch: 28, Loss: 0.048854559659957886\n",
      "Iteration 313, Batch: 29, Loss: 0.05796952173113823\n",
      "Iteration 313, Batch: 30, Loss: 0.07043761014938354\n",
      "Iteration 313, Batch: 31, Loss: 0.06935056298971176\n",
      "Iteration 313, Batch: 32, Loss: 0.023925436660647392\n",
      "Iteration 313, Batch: 33, Loss: 0.031946565955877304\n",
      "Iteration 313, Batch: 34, Loss: 0.03820199891924858\n",
      "Iteration 313, Batch: 35, Loss: 0.07178060710430145\n",
      "Iteration 313, Batch: 36, Loss: 0.03302402049303055\n",
      "Iteration 313, Batch: 37, Loss: 0.07671505957841873\n",
      "Iteration 313, Batch: 38, Loss: 0.05389804393053055\n",
      "Iteration 313, Batch: 39, Loss: 0.03564896434545517\n",
      "Iteration 313, Batch: 40, Loss: 0.05174854025244713\n",
      "Iteration 313, Batch: 41, Loss: 0.025646492838859558\n",
      "Iteration 313, Batch: 42, Loss: 0.03681205213069916\n",
      "Iteration 313, Batch: 43, Loss: 0.024204954504966736\n",
      "Iteration 313, Batch: 44, Loss: 0.047800395637750626\n",
      "Iteration 313, Batch: 45, Loss: 0.08287059515714645\n",
      "Iteration 313, Batch: 46, Loss: 0.06065806373953819\n",
      "Iteration 313, Batch: 47, Loss: 0.08738035708665848\n",
      "Iteration 313, Batch: 48, Loss: 0.036488767713308334\n",
      "Iteration 313, Batch: 49, Loss: 0.061754900962114334\n",
      "Number of layers: 10\n",
      "Iteration 314, Batch: 0, Loss: 0.060271795839071274\n",
      "Iteration 314, Batch: 1, Loss: 0.07088672369718552\n",
      "Iteration 314, Batch: 2, Loss: 0.045641377568244934\n",
      "Iteration 314, Batch: 3, Loss: 0.06915845721960068\n",
      "Iteration 314, Batch: 4, Loss: 0.04281553626060486\n",
      "Iteration 314, Batch: 5, Loss: 0.07954689115285873\n",
      "Iteration 314, Batch: 6, Loss: 0.0816601812839508\n",
      "Iteration 314, Batch: 7, Loss: 0.06468512117862701\n",
      "Iteration 314, Batch: 8, Loss: 0.026555262506008148\n",
      "Iteration 314, Batch: 9, Loss: 0.034169841557741165\n",
      "Iteration 314, Batch: 10, Loss: 0.048552725464105606\n",
      "Iteration 314, Batch: 11, Loss: 0.054238736629486084\n",
      "Iteration 314, Batch: 12, Loss: 0.0631873682141304\n",
      "Iteration 314, Batch: 13, Loss: 0.07461962848901749\n",
      "Iteration 314, Batch: 14, Loss: 0.05213075503706932\n",
      "Iteration 314, Batch: 15, Loss: 0.08421916514635086\n",
      "Iteration 314, Batch: 16, Loss: 0.08707810193300247\n",
      "Iteration 314, Batch: 17, Loss: 0.0635075569152832\n",
      "Iteration 314, Batch: 18, Loss: 0.058173827826976776\n",
      "Iteration 314, Batch: 19, Loss: 0.07080282270908356\n",
      "Iteration 314, Batch: 20, Loss: 0.05062847584486008\n",
      "Iteration 314, Batch: 21, Loss: 0.053040917962789536\n",
      "Iteration 314, Batch: 22, Loss: 0.08926522731781006\n",
      "Iteration 314, Batch: 23, Loss: 0.10107001662254333\n",
      "Iteration 314, Batch: 24, Loss: 0.06534921377897263\n",
      "Iteration 314, Batch: 25, Loss: 0.07377602159976959\n",
      "Iteration 314, Batch: 26, Loss: 0.04215138778090477\n",
      "Iteration 314, Batch: 27, Loss: 0.06394898891448975\n",
      "Iteration 314, Batch: 28, Loss: 0.07693106681108475\n",
      "Iteration 314, Batch: 29, Loss: 0.04506837576627731\n",
      "Iteration 314, Batch: 30, Loss: 0.062731072306633\n",
      "Iteration 314, Batch: 31, Loss: 0.0454975962638855\n",
      "Iteration 314, Batch: 32, Loss: 0.05052339285612106\n",
      "Iteration 314, Batch: 33, Loss: 0.04551484435796738\n",
      "Iteration 314, Batch: 34, Loss: 0.0688820406794548\n",
      "Iteration 314, Batch: 35, Loss: 0.05957696959376335\n",
      "Iteration 314, Batch: 36, Loss: 0.04210805892944336\n",
      "Iteration 314, Batch: 37, Loss: 0.06077473983168602\n",
      "Iteration 314, Batch: 38, Loss: 0.06998665630817413\n",
      "Iteration 314, Batch: 39, Loss: 0.07681261003017426\n",
      "Iteration 314, Batch: 40, Loss: 0.048203546553850174\n",
      "Iteration 314, Batch: 41, Loss: 0.04387976974248886\n",
      "Iteration 314, Batch: 42, Loss: 0.03429301828145981\n",
      "Iteration 314, Batch: 43, Loss: 0.04393928498029709\n",
      "Iteration 314, Batch: 44, Loss: 0.03485558182001114\n",
      "Iteration 314, Batch: 45, Loss: 0.060587551444768906\n",
      "Iteration 314, Batch: 46, Loss: 0.04945617541670799\n",
      "Iteration 314, Batch: 47, Loss: 0.03012133203446865\n",
      "Iteration 314, Batch: 48, Loss: 0.02936163917183876\n",
      "Iteration 314, Batch: 49, Loss: 0.05779050663113594\n",
      "Number of layers: 10\n",
      "Iteration 315, Batch: 0, Loss: 0.04532710835337639\n",
      "Iteration 315, Batch: 1, Loss: 0.024872412905097008\n",
      "Iteration 315, Batch: 2, Loss: 0.07336652278900146\n",
      "Iteration 315, Batch: 3, Loss: 0.026870181784033775\n",
      "Iteration 315, Batch: 4, Loss: 0.04607989266514778\n",
      "Iteration 315, Batch: 5, Loss: 0.031980302184820175\n",
      "Iteration 315, Batch: 6, Loss: 0.031832579523324966\n",
      "Iteration 315, Batch: 7, Loss: 0.04317223280668259\n",
      "Iteration 315, Batch: 8, Loss: 0.042284153401851654\n",
      "Iteration 315, Batch: 9, Loss: 0.028862394392490387\n",
      "Iteration 315, Batch: 10, Loss: 0.05410214141011238\n",
      "Iteration 315, Batch: 11, Loss: 0.036126911640167236\n",
      "Iteration 315, Batch: 12, Loss: 0.0439968965947628\n",
      "Iteration 315, Batch: 13, Loss: 0.033962756395339966\n",
      "Iteration 315, Batch: 14, Loss: 0.03880779817700386\n",
      "Iteration 315, Batch: 15, Loss: 0.04250563681125641\n",
      "Iteration 315, Batch: 16, Loss: 0.027622977271676064\n",
      "Iteration 315, Batch: 17, Loss: 0.05478493124246597\n",
      "Iteration 315, Batch: 18, Loss: 0.03813809156417847\n",
      "Iteration 315, Batch: 19, Loss: 0.04402193799614906\n",
      "Iteration 315, Batch: 20, Loss: 0.05504457652568817\n",
      "Iteration 315, Batch: 21, Loss: 0.04560531675815582\n",
      "Iteration 315, Batch: 22, Loss: 0.03493519872426987\n",
      "Iteration 315, Batch: 23, Loss: 0.07768384367227554\n",
      "Iteration 315, Batch: 24, Loss: 0.04937698319554329\n",
      "Iteration 315, Batch: 25, Loss: 0.03777475282549858\n",
      "Iteration 315, Batch: 26, Loss: 0.06343698501586914\n",
      "Iteration 315, Batch: 27, Loss: 0.06465087831020355\n",
      "Iteration 315, Batch: 28, Loss: 0.03403332084417343\n",
      "Iteration 315, Batch: 29, Loss: 0.04585621878504753\n",
      "Iteration 315, Batch: 30, Loss: 0.03460125997662544\n",
      "Iteration 315, Batch: 31, Loss: 0.03448162600398064\n",
      "Iteration 315, Batch: 32, Loss: 0.040699128061532974\n",
      "Iteration 315, Batch: 33, Loss: 0.039476171135902405\n",
      "Iteration 315, Batch: 34, Loss: 0.05800448730587959\n",
      "Iteration 315, Batch: 35, Loss: 0.03410593420267105\n",
      "Iteration 315, Batch: 36, Loss: 0.04971109703183174\n",
      "Iteration 315, Batch: 37, Loss: 0.04223709926009178\n",
      "Iteration 315, Batch: 38, Loss: 0.04928971081972122\n",
      "Iteration 315, Batch: 39, Loss: 0.04409671574831009\n",
      "Iteration 315, Batch: 40, Loss: 0.03664777800440788\n",
      "Iteration 315, Batch: 41, Loss: 0.05657392367720604\n",
      "Iteration 315, Batch: 42, Loss: 0.05371538922190666\n",
      "Iteration 315, Batch: 43, Loss: 0.033534422516822815\n",
      "Iteration 315, Batch: 44, Loss: 0.05222753435373306\n",
      "Iteration 315, Batch: 45, Loss: 0.046497639268636703\n",
      "Iteration 315, Batch: 46, Loss: 0.034018225967884064\n",
      "Iteration 315, Batch: 47, Loss: 0.04027765989303589\n",
      "Iteration 315, Batch: 48, Loss: 0.030259994789958\n",
      "Iteration 315, Batch: 49, Loss: 0.029054906219244003\n",
      "Number of layers: 10\n",
      "Iteration 316, Batch: 0, Loss: 0.03667399659752846\n",
      "Iteration 316, Batch: 1, Loss: 0.039448197931051254\n",
      "Iteration 316, Batch: 2, Loss: 0.026437310501933098\n",
      "Iteration 316, Batch: 3, Loss: 0.0460285022854805\n",
      "Iteration 316, Batch: 4, Loss: 0.03452066332101822\n",
      "Iteration 316, Batch: 5, Loss: 0.056262996047735214\n",
      "Iteration 316, Batch: 6, Loss: 0.0369294211268425\n",
      "Iteration 316, Batch: 7, Loss: 0.08676712960004807\n",
      "Iteration 316, Batch: 8, Loss: 0.04657929390668869\n",
      "Iteration 316, Batch: 9, Loss: 0.026502609252929688\n",
      "Iteration 316, Batch: 10, Loss: 0.04020455852150917\n",
      "Iteration 316, Batch: 11, Loss: 0.04348557069897652\n",
      "Iteration 316, Batch: 12, Loss: 0.04383895918726921\n",
      "Iteration 316, Batch: 13, Loss: 0.05284712836146355\n",
      "Iteration 316, Batch: 14, Loss: 0.03333150967955589\n",
      "Iteration 316, Batch: 15, Loss: 0.05737675353884697\n",
      "Iteration 316, Batch: 16, Loss: 0.039579812437295914\n",
      "Iteration 316, Batch: 17, Loss: 0.02513444609940052\n",
      "Iteration 316, Batch: 18, Loss: 0.05819132551550865\n",
      "Iteration 316, Batch: 19, Loss: 0.0390993170440197\n",
      "Iteration 316, Batch: 20, Loss: 0.056729502975940704\n",
      "Iteration 316, Batch: 21, Loss: 0.04977230355143547\n",
      "Iteration 316, Batch: 22, Loss: 0.05700714886188507\n",
      "Iteration 316, Batch: 23, Loss: 0.040411196649074554\n",
      "Iteration 316, Batch: 24, Loss: 0.05201542750000954\n",
      "Iteration 316, Batch: 25, Loss: 0.02616680972278118\n",
      "Iteration 316, Batch: 26, Loss: 0.04447091370820999\n",
      "Iteration 316, Batch: 27, Loss: 0.04226073995232582\n",
      "Iteration 316, Batch: 28, Loss: 0.04954635351896286\n",
      "Iteration 316, Batch: 29, Loss: 0.07139065861701965\n",
      "Iteration 316, Batch: 30, Loss: 0.07190324366092682\n",
      "Iteration 316, Batch: 31, Loss: 0.05586846172809601\n",
      "Iteration 316, Batch: 32, Loss: 0.04958851635456085\n",
      "Iteration 316, Batch: 33, Loss: 0.039761751890182495\n",
      "Iteration 316, Batch: 34, Loss: 0.043460726737976074\n",
      "Iteration 316, Batch: 35, Loss: 0.026712937280535698\n",
      "Iteration 316, Batch: 36, Loss: 0.04464302957057953\n",
      "Iteration 316, Batch: 37, Loss: 0.03590432181954384\n",
      "Iteration 316, Batch: 38, Loss: 0.0629807859659195\n",
      "Iteration 316, Batch: 39, Loss: 0.0320911630988121\n",
      "Iteration 316, Batch: 40, Loss: 0.04311776161193848\n",
      "Iteration 316, Batch: 41, Loss: 0.04905584082007408\n",
      "Iteration 316, Batch: 42, Loss: 0.04275991767644882\n",
      "Iteration 316, Batch: 43, Loss: 0.08344082534313202\n",
      "Iteration 316, Batch: 44, Loss: 0.056559108197689056\n",
      "Iteration 316, Batch: 45, Loss: 0.05042881518602371\n",
      "Iteration 316, Batch: 46, Loss: 0.04557269811630249\n",
      "Iteration 316, Batch: 47, Loss: 0.0776825100183487\n",
      "Iteration 316, Batch: 48, Loss: 0.024216771125793457\n",
      "Iteration 316, Batch: 49, Loss: 0.04508407413959503\n",
      "Number of layers: 10\n",
      "Iteration 317, Batch: 0, Loss: 0.04035783186554909\n",
      "Iteration 317, Batch: 1, Loss: 0.03899265080690384\n",
      "Iteration 317, Batch: 2, Loss: 0.05051276087760925\n",
      "Iteration 317, Batch: 3, Loss: 0.05053364485502243\n",
      "Iteration 317, Batch: 4, Loss: 0.02950320579111576\n",
      "Iteration 317, Batch: 5, Loss: 0.029238592833280563\n",
      "Iteration 317, Batch: 6, Loss: 0.03244905546307564\n",
      "Iteration 317, Batch: 7, Loss: 0.049785833805799484\n",
      "Iteration 317, Batch: 8, Loss: 0.03620106726884842\n",
      "Iteration 317, Batch: 9, Loss: 0.057890813797712326\n",
      "Iteration 317, Batch: 10, Loss: 0.06382951140403748\n",
      "Iteration 317, Batch: 11, Loss: 0.1149464100599289\n",
      "Iteration 317, Batch: 12, Loss: 0.05216548964381218\n",
      "Iteration 317, Batch: 13, Loss: 0.07263324409723282\n",
      "Iteration 317, Batch: 14, Loss: 0.06378669291734695\n",
      "Iteration 317, Batch: 15, Loss: 0.061728380620479584\n",
      "Iteration 317, Batch: 16, Loss: 0.03517153114080429\n",
      "Iteration 317, Batch: 17, Loss: 0.043930597603321075\n",
      "Iteration 317, Batch: 18, Loss: 0.0410507395863533\n",
      "Iteration 317, Batch: 19, Loss: 0.03689103573560715\n",
      "Iteration 317, Batch: 20, Loss: 0.03310013189911842\n",
      "Iteration 317, Batch: 21, Loss: 0.03497592732310295\n",
      "Iteration 317, Batch: 22, Loss: 0.04109939560294151\n",
      "Iteration 317, Batch: 23, Loss: 0.02323988452553749\n",
      "Iteration 317, Batch: 24, Loss: 0.035126037895679474\n",
      "Iteration 317, Batch: 25, Loss: 0.04958530142903328\n",
      "Iteration 317, Batch: 26, Loss: 0.038330078125\n",
      "Iteration 317, Batch: 27, Loss: 0.04308660328388214\n",
      "Iteration 317, Batch: 28, Loss: 0.04599349945783615\n",
      "Iteration 317, Batch: 29, Loss: 0.03201640397310257\n",
      "Iteration 317, Batch: 30, Loss: 0.0604751817882061\n",
      "Iteration 317, Batch: 31, Loss: 0.036653418093919754\n",
      "Iteration 317, Batch: 32, Loss: 0.028081554919481277\n",
      "Iteration 317, Batch: 33, Loss: 0.0531727597117424\n",
      "Iteration 317, Batch: 34, Loss: 0.04557891562581062\n",
      "Iteration 317, Batch: 35, Loss: 0.018808534368872643\n",
      "Iteration 317, Batch: 36, Loss: 0.037988487631082535\n",
      "Iteration 317, Batch: 37, Loss: 0.031146913766860962\n",
      "Iteration 317, Batch: 38, Loss: 0.0471021793782711\n",
      "Iteration 317, Batch: 39, Loss: 0.04004581272602081\n",
      "Iteration 317, Batch: 40, Loss: 0.05201929807662964\n",
      "Iteration 317, Batch: 41, Loss: 0.03148066997528076\n",
      "Iteration 317, Batch: 42, Loss: 0.02435593493282795\n",
      "Iteration 317, Batch: 43, Loss: 0.04740648344159126\n",
      "Iteration 317, Batch: 44, Loss: 0.043207116425037384\n",
      "Iteration 317, Batch: 45, Loss: 0.03089834190905094\n",
      "Iteration 317, Batch: 46, Loss: 0.044182732701301575\n",
      "Iteration 317, Batch: 47, Loss: 0.055764950811862946\n",
      "Iteration 317, Batch: 48, Loss: 0.05622989684343338\n",
      "Iteration 317, Batch: 49, Loss: 0.031527694314718246\n",
      "Number of layers: 10\n",
      "Iteration 318, Batch: 0, Loss: 0.03307807818055153\n",
      "Iteration 318, Batch: 1, Loss: 0.0571930967271328\n",
      "Iteration 318, Batch: 2, Loss: 0.08734960108995438\n",
      "Iteration 318, Batch: 3, Loss: 0.03146858885884285\n",
      "Iteration 318, Batch: 4, Loss: 0.04081331193447113\n",
      "Iteration 318, Batch: 5, Loss: 0.06126893684267998\n",
      "Iteration 318, Batch: 6, Loss: 0.031553998589515686\n",
      "Iteration 318, Batch: 7, Loss: 0.052685219794511795\n",
      "Iteration 318, Batch: 8, Loss: 0.046791017055511475\n",
      "Iteration 318, Batch: 9, Loss: 0.029513482004404068\n",
      "Iteration 318, Batch: 10, Loss: 0.04727533087134361\n",
      "Iteration 318, Batch: 11, Loss: 0.03540421277284622\n",
      "Iteration 318, Batch: 12, Loss: 0.039256274700164795\n",
      "Iteration 318, Batch: 13, Loss: 0.04001189023256302\n",
      "Iteration 318, Batch: 14, Loss: 0.050881583243608475\n",
      "Iteration 318, Batch: 15, Loss: 0.04005416855216026\n",
      "Iteration 318, Batch: 16, Loss: 0.04051881656050682\n",
      "Iteration 318, Batch: 17, Loss: 0.031118864193558693\n",
      "Iteration 318, Batch: 18, Loss: 0.04207862913608551\n",
      "Iteration 318, Batch: 19, Loss: 0.06729517877101898\n",
      "Iteration 318, Batch: 20, Loss: 0.047878947108983994\n",
      "Iteration 318, Batch: 21, Loss: 0.029202627018094063\n",
      "Iteration 318, Batch: 22, Loss: 0.07396898418664932\n",
      "Iteration 318, Batch: 23, Loss: 0.04814011603593826\n",
      "Iteration 318, Batch: 24, Loss: 0.07206857204437256\n",
      "Iteration 318, Batch: 25, Loss: 0.039573367685079575\n",
      "Iteration 318, Batch: 26, Loss: 0.028586450964212418\n",
      "Iteration 318, Batch: 27, Loss: 0.05910264328122139\n",
      "Iteration 318, Batch: 28, Loss: 0.03713984042406082\n",
      "Iteration 318, Batch: 29, Loss: 0.04132091999053955\n",
      "Iteration 318, Batch: 30, Loss: 0.07080677151679993\n",
      "Iteration 318, Batch: 31, Loss: 0.03579305112361908\n",
      "Iteration 318, Batch: 32, Loss: 0.03579925000667572\n",
      "Iteration 318, Batch: 33, Loss: 0.050688762217760086\n",
      "Iteration 318, Batch: 34, Loss: 0.037624556571245193\n",
      "Iteration 318, Batch: 35, Loss: 0.034680843353271484\n",
      "Iteration 318, Batch: 36, Loss: 0.03848665952682495\n",
      "Iteration 318, Batch: 37, Loss: 0.021242333576083183\n",
      "Iteration 318, Batch: 38, Loss: 0.04245491325855255\n",
      "Iteration 318, Batch: 39, Loss: 0.053954027593135834\n",
      "Iteration 318, Batch: 40, Loss: 0.08918986469507217\n",
      "Iteration 318, Batch: 41, Loss: 0.07007395476102829\n",
      "Iteration 318, Batch: 42, Loss: 0.062324244529008865\n",
      "Iteration 318, Batch: 43, Loss: 0.05619863420724869\n",
      "Iteration 318, Batch: 44, Loss: 0.05526484549045563\n",
      "Iteration 318, Batch: 45, Loss: 0.0339839868247509\n",
      "Iteration 318, Batch: 46, Loss: 0.08507486432790756\n",
      "Iteration 318, Batch: 47, Loss: 0.0884682834148407\n",
      "Iteration 318, Batch: 48, Loss: 0.0687224343419075\n",
      "Iteration 318, Batch: 49, Loss: 0.08204137533903122\n",
      "Number of layers: 10\n",
      "Iteration 319, Batch: 0, Loss: 0.07489372044801712\n",
      "Iteration 319, Batch: 1, Loss: 0.10114289820194244\n",
      "Iteration 319, Batch: 2, Loss: 0.07227659970521927\n",
      "Iteration 319, Batch: 3, Loss: 0.11332827061414719\n",
      "Iteration 319, Batch: 4, Loss: 0.03409113362431526\n",
      "Iteration 319, Batch: 5, Loss: 0.04248262196779251\n",
      "Iteration 319, Batch: 6, Loss: 0.03867819160223007\n",
      "Iteration 319, Batch: 7, Loss: 0.04313305765390396\n",
      "Iteration 319, Batch: 8, Loss: 0.07053542882204056\n",
      "Iteration 319, Batch: 9, Loss: 0.08956736326217651\n",
      "Iteration 319, Batch: 10, Loss: 0.06213007867336273\n",
      "Iteration 319, Batch: 11, Loss: 0.04230345040559769\n",
      "Iteration 319, Batch: 12, Loss: 0.078681580722332\n",
      "Iteration 319, Batch: 13, Loss: 0.03764398396015167\n",
      "Iteration 319, Batch: 14, Loss: 0.033116307109594345\n",
      "Iteration 319, Batch: 15, Loss: 0.05165153369307518\n",
      "Iteration 319, Batch: 16, Loss: 0.05328836292028427\n",
      "Iteration 319, Batch: 17, Loss: 0.05200015380978584\n",
      "Iteration 319, Batch: 18, Loss: 0.06560225039720535\n",
      "Iteration 319, Batch: 19, Loss: 0.06628276407718658\n",
      "Iteration 319, Batch: 20, Loss: 0.04775106906890869\n",
      "Iteration 319, Batch: 21, Loss: 0.07519859820604324\n",
      "Iteration 319, Batch: 22, Loss: 0.04415736719965935\n",
      "Iteration 319, Batch: 23, Loss: 0.053956083953380585\n",
      "Iteration 319, Batch: 24, Loss: 0.046458709985017776\n",
      "Iteration 319, Batch: 25, Loss: 0.06358017772436142\n",
      "Iteration 319, Batch: 26, Loss: 0.06018998473882675\n",
      "Iteration 319, Batch: 27, Loss: 0.03994526341557503\n",
      "Iteration 319, Batch: 28, Loss: 0.04110455513000488\n",
      "Iteration 319, Batch: 29, Loss: 0.06131559610366821\n",
      "Iteration 319, Batch: 30, Loss: 0.05192996934056282\n",
      "Iteration 319, Batch: 31, Loss: 0.04921668395400047\n",
      "Iteration 319, Batch: 32, Loss: 0.0659579336643219\n",
      "Iteration 319, Batch: 33, Loss: 0.07865478843450546\n",
      "Iteration 319, Batch: 34, Loss: 0.06234355643391609\n",
      "Iteration 319, Batch: 35, Loss: 0.04275410622358322\n",
      "Iteration 319, Batch: 36, Loss: 0.07802493125200272\n",
      "Iteration 319, Batch: 37, Loss: 0.03244516998529434\n",
      "Iteration 319, Batch: 38, Loss: 0.07517991214990616\n",
      "Iteration 319, Batch: 39, Loss: 0.06265351176261902\n",
      "Iteration 319, Batch: 40, Loss: 0.06319639831781387\n",
      "Iteration 319, Batch: 41, Loss: 0.05621451139450073\n",
      "Iteration 319, Batch: 42, Loss: 0.07356350123882294\n",
      "Iteration 319, Batch: 43, Loss: 0.05225037783384323\n",
      "Iteration 319, Batch: 44, Loss: 0.04764293506741524\n",
      "Iteration 319, Batch: 45, Loss: 0.057301606982946396\n",
      "Iteration 319, Batch: 46, Loss: 0.06394635885953903\n",
      "Iteration 319, Batch: 47, Loss: 0.06308827549219131\n",
      "Iteration 319, Batch: 48, Loss: 0.06407784670591354\n",
      "Iteration 319, Batch: 49, Loss: 0.035922273993492126\n",
      "Number of layers: 10\n",
      "Iteration 320, Batch: 0, Loss: 0.05863533914089203\n",
      "Iteration 320, Batch: 1, Loss: 0.07932048290967941\n",
      "Iteration 320, Batch: 2, Loss: 0.07703390717506409\n",
      "Iteration 320, Batch: 3, Loss: 0.11455835402011871\n",
      "Iteration 320, Batch: 4, Loss: 0.10751351714134216\n",
      "Iteration 320, Batch: 5, Loss: 0.11066805571317673\n",
      "Iteration 320, Batch: 6, Loss: 0.09112151712179184\n",
      "Iteration 320, Batch: 7, Loss: 0.08115305006504059\n",
      "Iteration 320, Batch: 8, Loss: 0.08040394634008408\n",
      "Iteration 320, Batch: 9, Loss: 0.07363718003034592\n",
      "Iteration 320, Batch: 10, Loss: 0.05959056690335274\n",
      "Iteration 320, Batch: 11, Loss: 0.050796497613191605\n",
      "Iteration 320, Batch: 12, Loss: 0.09890743345022202\n",
      "Iteration 320, Batch: 13, Loss: 0.05218710005283356\n",
      "Iteration 320, Batch: 14, Loss: 0.05930648744106293\n",
      "Iteration 320, Batch: 15, Loss: 0.08654332906007767\n",
      "Iteration 320, Batch: 16, Loss: 0.10621404647827148\n",
      "Iteration 320, Batch: 17, Loss: 0.08851281553506851\n",
      "Iteration 320, Batch: 18, Loss: 0.06747064739465714\n",
      "Iteration 320, Batch: 19, Loss: 0.03721103444695473\n",
      "Iteration 320, Batch: 20, Loss: 0.0492144376039505\n",
      "Iteration 320, Batch: 21, Loss: 0.07031114399433136\n",
      "Iteration 320, Batch: 22, Loss: 0.06953932344913483\n",
      "Iteration 320, Batch: 23, Loss: 0.05185459926724434\n",
      "Iteration 320, Batch: 24, Loss: 0.022384511306881905\n",
      "Iteration 320, Batch: 25, Loss: 0.0553315132856369\n",
      "Iteration 320, Batch: 26, Loss: 0.058018434792757034\n",
      "Iteration 320, Batch: 27, Loss: 0.04702489823102951\n",
      "Iteration 320, Batch: 28, Loss: 0.04563829302787781\n",
      "Iteration 320, Batch: 29, Loss: 0.06943489611148834\n",
      "Iteration 320, Batch: 30, Loss: 0.09911119192838669\n",
      "Iteration 320, Batch: 31, Loss: 0.04297954589128494\n",
      "Iteration 320, Batch: 32, Loss: 0.05011172965168953\n",
      "Iteration 320, Batch: 33, Loss: 0.045786503702402115\n",
      "Iteration 320, Batch: 34, Loss: 0.048766110092401505\n",
      "Iteration 320, Batch: 35, Loss: 0.06524834781885147\n",
      "Iteration 320, Batch: 36, Loss: 0.06056174635887146\n",
      "Iteration 320, Batch: 37, Loss: 0.06298638135194778\n",
      "Iteration 320, Batch: 38, Loss: 0.06338968127965927\n",
      "Iteration 320, Batch: 39, Loss: 0.06513439863920212\n",
      "Iteration 320, Batch: 40, Loss: 0.06904406100511551\n",
      "Iteration 320, Batch: 41, Loss: 0.07051965594291687\n",
      "Iteration 320, Batch: 42, Loss: 0.03366995230317116\n",
      "Iteration 320, Batch: 43, Loss: 0.05043340474367142\n",
      "Iteration 320, Batch: 44, Loss: 0.046615079045295715\n",
      "Iteration 320, Batch: 45, Loss: 0.035533562302589417\n",
      "Iteration 320, Batch: 46, Loss: 0.044075287878513336\n",
      "Iteration 320, Batch: 47, Loss: 0.06791071593761444\n",
      "Iteration 320, Batch: 48, Loss: 0.02647664211690426\n",
      "Iteration 320, Batch: 49, Loss: 0.05936283990740776\n",
      "Number of layers: 10\n",
      "Iteration 321, Batch: 0, Loss: 0.05299299210309982\n",
      "Iteration 321, Batch: 1, Loss: 0.04677363857626915\n",
      "Iteration 321, Batch: 2, Loss: 0.03506249561905861\n",
      "Iteration 321, Batch: 3, Loss: 0.06312662363052368\n",
      "Iteration 321, Batch: 4, Loss: 0.04700099676847458\n",
      "Iteration 321, Batch: 5, Loss: 0.049488458782434464\n",
      "Iteration 321, Batch: 6, Loss: 0.058878906071186066\n",
      "Iteration 321, Batch: 7, Loss: 0.06508032977581024\n",
      "Iteration 321, Batch: 8, Loss: 0.048896368592977524\n",
      "Iteration 321, Batch: 9, Loss: 0.05712653324007988\n",
      "Iteration 321, Batch: 10, Loss: 0.038377344608306885\n",
      "Iteration 321, Batch: 11, Loss: 0.05916137993335724\n",
      "Iteration 321, Batch: 12, Loss: 0.06089473143219948\n",
      "Iteration 321, Batch: 13, Loss: 0.06774089485406876\n",
      "Iteration 321, Batch: 14, Loss: 0.05428719520568848\n",
      "Iteration 321, Batch: 15, Loss: 0.03882427141070366\n",
      "Iteration 321, Batch: 16, Loss: 0.06271430104970932\n",
      "Iteration 321, Batch: 17, Loss: 0.08348649740219116\n",
      "Iteration 321, Batch: 18, Loss: 0.09105155616998672\n",
      "Iteration 321, Batch: 19, Loss: 0.09348805993795395\n",
      "Iteration 321, Batch: 20, Loss: 0.11756554991006851\n",
      "Iteration 321, Batch: 21, Loss: 0.10477513819932938\n",
      "Iteration 321, Batch: 22, Loss: 0.07824571430683136\n",
      "Iteration 321, Batch: 23, Loss: 0.0889892727136612\n",
      "Iteration 321, Batch: 24, Loss: 0.15290014445781708\n",
      "Iteration 321, Batch: 25, Loss: 0.16753096878528595\n",
      "Iteration 321, Batch: 26, Loss: 0.09676065295934677\n",
      "Iteration 321, Batch: 27, Loss: 0.07649767398834229\n",
      "Iteration 321, Batch: 28, Loss: 0.07268598675727844\n",
      "Iteration 321, Batch: 29, Loss: 0.07166585326194763\n",
      "Iteration 321, Batch: 30, Loss: 0.06011590734124184\n",
      "Iteration 321, Batch: 31, Loss: 0.05695253610610962\n",
      "Iteration 321, Batch: 32, Loss: 0.03900638595223427\n",
      "Iteration 321, Batch: 33, Loss: 0.05521481856703758\n",
      "Iteration 321, Batch: 34, Loss: 0.06128578633069992\n",
      "Iteration 321, Batch: 35, Loss: 0.04035645350813866\n",
      "Iteration 321, Batch: 36, Loss: 0.038362499326467514\n",
      "Iteration 321, Batch: 37, Loss: 0.05369270592927933\n",
      "Iteration 321, Batch: 38, Loss: 0.07731649279594421\n",
      "Iteration 321, Batch: 39, Loss: 0.0538489893078804\n",
      "Iteration 321, Batch: 40, Loss: 0.04196998476982117\n",
      "Iteration 321, Batch: 41, Loss: 0.026970719918608665\n",
      "Iteration 321, Batch: 42, Loss: 0.026219086721539497\n",
      "Iteration 321, Batch: 43, Loss: 0.05013936758041382\n",
      "Iteration 321, Batch: 44, Loss: 0.05354436859488487\n",
      "Iteration 321, Batch: 45, Loss: 0.044407233595848083\n",
      "Iteration 321, Batch: 46, Loss: 0.07038675248622894\n",
      "Iteration 321, Batch: 47, Loss: 0.09247425198554993\n",
      "Iteration 321, Batch: 48, Loss: 0.07998466491699219\n",
      "Iteration 321, Batch: 49, Loss: 0.05988415703177452\n",
      "Number of layers: 10\n",
      "Iteration 322, Batch: 0, Loss: 0.06717722117900848\n",
      "Iteration 322, Batch: 1, Loss: 0.06781788915395737\n",
      "Iteration 322, Batch: 2, Loss: 0.04963188245892525\n",
      "Iteration 322, Batch: 3, Loss: 0.0495808869600296\n",
      "Iteration 322, Batch: 4, Loss: 0.06920662522315979\n",
      "Iteration 322, Batch: 5, Loss: 0.06817049533128738\n",
      "Iteration 322, Batch: 6, Loss: 0.08905976265668869\n",
      "Iteration 322, Batch: 7, Loss: 0.04865783452987671\n",
      "Iteration 322, Batch: 8, Loss: 0.06246228888630867\n",
      "Iteration 322, Batch: 9, Loss: 0.05146828666329384\n",
      "Iteration 322, Batch: 10, Loss: 0.05526529997587204\n",
      "Iteration 322, Batch: 11, Loss: 0.07586610317230225\n",
      "Iteration 322, Batch: 12, Loss: 0.07473136484622955\n",
      "Iteration 322, Batch: 13, Loss: 0.03229564055800438\n",
      "Iteration 322, Batch: 14, Loss: 0.0726323127746582\n",
      "Iteration 322, Batch: 15, Loss: 0.10943740606307983\n",
      "Iteration 322, Batch: 16, Loss: 0.08771398663520813\n",
      "Iteration 322, Batch: 17, Loss: 0.07729322463274002\n",
      "Iteration 322, Batch: 18, Loss: 0.10492457449436188\n",
      "Iteration 322, Batch: 19, Loss: 0.0927082970738411\n",
      "Iteration 322, Batch: 20, Loss: 0.03825029358267784\n",
      "Iteration 322, Batch: 21, Loss: 0.0397290363907814\n",
      "Iteration 322, Batch: 22, Loss: 0.080513134598732\n",
      "Iteration 322, Batch: 23, Loss: 0.05474425107240677\n",
      "Iteration 322, Batch: 24, Loss: 0.03657444566488266\n",
      "Iteration 322, Batch: 25, Loss: 0.0331885889172554\n",
      "Iteration 322, Batch: 26, Loss: 0.03634309023618698\n",
      "Iteration 322, Batch: 27, Loss: 0.06269033998250961\n",
      "Iteration 322, Batch: 28, Loss: 0.05620897188782692\n",
      "Iteration 322, Batch: 29, Loss: 0.09478513151407242\n",
      "Iteration 322, Batch: 30, Loss: 0.11650551855564117\n",
      "Iteration 322, Batch: 31, Loss: 0.16681285202503204\n",
      "Iteration 322, Batch: 32, Loss: 0.1503153294324875\n",
      "Iteration 322, Batch: 33, Loss: 0.09132968634366989\n",
      "Iteration 322, Batch: 34, Loss: 0.047499340027570724\n",
      "Iteration 322, Batch: 35, Loss: 0.06468046456575394\n",
      "Iteration 322, Batch: 36, Loss: 0.04465871676802635\n",
      "Iteration 322, Batch: 37, Loss: 0.0531812384724617\n",
      "Iteration 322, Batch: 38, Loss: 0.06301077455282211\n",
      "Iteration 322, Batch: 39, Loss: 0.04934488609433174\n",
      "Iteration 322, Batch: 40, Loss: 0.048719216138124466\n",
      "Iteration 322, Batch: 41, Loss: 0.06145557388663292\n",
      "Iteration 322, Batch: 42, Loss: 0.08159171789884567\n",
      "Iteration 322, Batch: 43, Loss: 0.06680675595998764\n",
      "Iteration 322, Batch: 44, Loss: 0.04771846532821655\n",
      "Iteration 322, Batch: 45, Loss: 0.031955521553754807\n",
      "Iteration 322, Batch: 46, Loss: 0.05655251443386078\n",
      "Iteration 322, Batch: 47, Loss: 0.02981361374258995\n",
      "Iteration 322, Batch: 48, Loss: 0.03353548049926758\n",
      "Iteration 322, Batch: 49, Loss: 0.04797973111271858\n",
      "Number of layers: 10\n",
      "Iteration 323, Batch: 0, Loss: 0.03578272834420204\n",
      "Iteration 323, Batch: 1, Loss: 0.19333802163600922\n",
      "Iteration 323, Batch: 2, Loss: 0.3613211214542389\n",
      "Iteration 323, Batch: 3, Loss: 0.42990124225616455\n",
      "Iteration 323, Batch: 4, Loss: 0.20280656218528748\n",
      "Iteration 323, Batch: 5, Loss: 0.06651220470666885\n",
      "Iteration 323, Batch: 6, Loss: 0.07267387956380844\n",
      "Iteration 323, Batch: 7, Loss: 0.060624103993177414\n",
      "Iteration 323, Batch: 8, Loss: 0.06871200352907181\n",
      "Iteration 323, Batch: 9, Loss: 0.0370168536901474\n",
      "Iteration 323, Batch: 10, Loss: 0.06308849900960922\n",
      "Iteration 323, Batch: 11, Loss: 0.08873143047094345\n",
      "Iteration 323, Batch: 12, Loss: 0.04898419231176376\n",
      "Iteration 323, Batch: 13, Loss: 0.12396678328514099\n",
      "Iteration 323, Batch: 14, Loss: 0.10356831550598145\n",
      "Iteration 323, Batch: 15, Loss: 0.11813817918300629\n",
      "Iteration 323, Batch: 16, Loss: 0.07719715684652328\n",
      "Iteration 323, Batch: 17, Loss: 0.08433537930250168\n",
      "Iteration 323, Batch: 18, Loss: 0.08093259483575821\n",
      "Iteration 323, Batch: 19, Loss: 0.05663125962018967\n",
      "Iteration 323, Batch: 20, Loss: 0.07580553740262985\n",
      "Iteration 323, Batch: 21, Loss: 0.08311106264591217\n",
      "Iteration 323, Batch: 22, Loss: 0.11063247174024582\n",
      "Iteration 323, Batch: 23, Loss: 0.06122181937098503\n",
      "Iteration 323, Batch: 24, Loss: 0.042549047619104385\n",
      "Iteration 323, Batch: 25, Loss: 0.07115998864173889\n",
      "Iteration 323, Batch: 26, Loss: 0.06905842572450638\n",
      "Iteration 323, Batch: 27, Loss: 0.07648925483226776\n",
      "Iteration 323, Batch: 28, Loss: 0.04957713931798935\n",
      "Iteration 323, Batch: 29, Loss: 0.05481398478150368\n",
      "Iteration 323, Batch: 30, Loss: 0.056181009858846664\n",
      "Iteration 323, Batch: 31, Loss: 0.06280427426099777\n",
      "Iteration 323, Batch: 32, Loss: 0.054124124348163605\n",
      "Iteration 323, Batch: 33, Loss: 0.044367264956235886\n",
      "Iteration 323, Batch: 34, Loss: 0.04536384344100952\n",
      "Iteration 323, Batch: 35, Loss: 0.028739290311932564\n",
      "Iteration 323, Batch: 36, Loss: 0.08411137759685516\n",
      "Iteration 323, Batch: 37, Loss: 0.07775365561246872\n",
      "Iteration 323, Batch: 38, Loss: 0.04531565681099892\n",
      "Iteration 323, Batch: 39, Loss: 0.08608819544315338\n",
      "Iteration 323, Batch: 40, Loss: 0.06704890727996826\n",
      "Iteration 323, Batch: 41, Loss: 0.07638388127088547\n",
      "Iteration 323, Batch: 42, Loss: 0.07563949376344681\n",
      "Iteration 323, Batch: 43, Loss: 0.09748155623674393\n",
      "Iteration 323, Batch: 44, Loss: 0.06478729099035263\n",
      "Iteration 323, Batch: 45, Loss: 0.04773213714361191\n",
      "Iteration 323, Batch: 46, Loss: 0.043107565492391586\n",
      "Iteration 323, Batch: 47, Loss: 0.050591785460710526\n",
      "Iteration 323, Batch: 48, Loss: 0.07170125097036362\n",
      "Iteration 323, Batch: 49, Loss: 0.04622381180524826\n",
      "Number of layers: 10\n",
      "Iteration 324, Batch: 0, Loss: 0.06388480961322784\n",
      "Iteration 324, Batch: 1, Loss: 0.0590747632086277\n",
      "Iteration 324, Batch: 2, Loss: 0.030310463160276413\n",
      "Iteration 324, Batch: 3, Loss: 0.06581573188304901\n",
      "Iteration 324, Batch: 4, Loss: 0.04118431359529495\n",
      "Iteration 324, Batch: 5, Loss: 0.07947713881731033\n",
      "Iteration 324, Batch: 6, Loss: 0.07233209162950516\n",
      "Iteration 324, Batch: 7, Loss: 0.07391811907291412\n",
      "Iteration 324, Batch: 8, Loss: 0.02851814590394497\n",
      "Iteration 324, Batch: 9, Loss: 0.060381438583135605\n",
      "Iteration 324, Batch: 10, Loss: 0.039862021803855896\n",
      "Iteration 324, Batch: 11, Loss: 0.045694202184677124\n",
      "Iteration 324, Batch: 12, Loss: 0.061924681067466736\n",
      "Iteration 324, Batch: 13, Loss: 0.06345315277576447\n",
      "Iteration 324, Batch: 14, Loss: 0.05010116472840309\n",
      "Iteration 324, Batch: 15, Loss: 0.03170574828982353\n",
      "Iteration 324, Batch: 16, Loss: 0.053920697420835495\n",
      "Iteration 324, Batch: 17, Loss: 0.029997853562235832\n",
      "Iteration 324, Batch: 18, Loss: 0.06608416140079498\n",
      "Iteration 324, Batch: 19, Loss: 0.04741271957755089\n",
      "Iteration 324, Batch: 20, Loss: 0.05560354143381119\n",
      "Iteration 324, Batch: 21, Loss: 0.024850938469171524\n",
      "Iteration 324, Batch: 22, Loss: 0.060498714447021484\n",
      "Iteration 324, Batch: 23, Loss: 0.04531555995345116\n",
      "Iteration 324, Batch: 24, Loss: 0.07117478549480438\n",
      "Iteration 324, Batch: 25, Loss: 0.06046300381422043\n",
      "Iteration 324, Batch: 26, Loss: 0.07670916616916656\n",
      "Iteration 324, Batch: 27, Loss: 0.052085112780332565\n",
      "Iteration 324, Batch: 28, Loss: 0.07811108976602554\n",
      "Iteration 324, Batch: 29, Loss: 0.049879662692546844\n",
      "Iteration 324, Batch: 30, Loss: 0.06639130413532257\n",
      "Iteration 324, Batch: 31, Loss: 0.04838002473115921\n",
      "Iteration 324, Batch: 32, Loss: 0.04842941835522652\n",
      "Iteration 324, Batch: 33, Loss: 0.07126560062170029\n",
      "Iteration 324, Batch: 34, Loss: 0.05011049285531044\n",
      "Iteration 324, Batch: 35, Loss: 0.07689658552408218\n",
      "Iteration 324, Batch: 36, Loss: 0.05662579461932182\n",
      "Iteration 324, Batch: 37, Loss: 0.027510439977049828\n",
      "Iteration 324, Batch: 38, Loss: 0.03068562224507332\n",
      "Iteration 324, Batch: 39, Loss: 0.04402079060673714\n",
      "Iteration 324, Batch: 40, Loss: 0.049925465136766434\n",
      "Iteration 324, Batch: 41, Loss: 0.059883348643779755\n",
      "Iteration 324, Batch: 42, Loss: 0.08710061758756638\n",
      "Iteration 324, Batch: 43, Loss: 0.0652061179280281\n",
      "Iteration 324, Batch: 44, Loss: 0.054612331092357635\n",
      "Iteration 324, Batch: 45, Loss: 0.025760507211089134\n",
      "Iteration 324, Batch: 46, Loss: 0.048917148262262344\n",
      "Iteration 324, Batch: 47, Loss: 0.05067173391580582\n",
      "Iteration 324, Batch: 48, Loss: 0.05339696630835533\n",
      "Iteration 324, Batch: 49, Loss: 0.06870575994253159\n",
      "Number of layers: 10\n",
      "Iteration 325, Batch: 0, Loss: 0.0659509152173996\n",
      "Iteration 325, Batch: 1, Loss: 0.05331709608435631\n",
      "Iteration 325, Batch: 2, Loss: 0.04161195829510689\n",
      "Iteration 325, Batch: 3, Loss: 0.04813624173402786\n",
      "Iteration 325, Batch: 4, Loss: 0.08057302236557007\n",
      "Iteration 325, Batch: 5, Loss: 0.03585308417677879\n",
      "Iteration 325, Batch: 6, Loss: 0.06412604451179504\n",
      "Iteration 325, Batch: 7, Loss: 0.060216207057237625\n",
      "Iteration 325, Batch: 8, Loss: 0.05125761777162552\n",
      "Iteration 325, Batch: 9, Loss: 0.07683330029249191\n",
      "Iteration 325, Batch: 10, Loss: 0.06052401289343834\n",
      "Iteration 325, Batch: 11, Loss: 0.042527761310338974\n",
      "Iteration 325, Batch: 12, Loss: 0.07966981828212738\n",
      "Iteration 325, Batch: 13, Loss: 0.061712197959423065\n",
      "Iteration 325, Batch: 14, Loss: 0.06873749196529388\n",
      "Iteration 325, Batch: 15, Loss: 0.05450865998864174\n",
      "Iteration 325, Batch: 16, Loss: 0.042514875531196594\n",
      "Iteration 325, Batch: 17, Loss: 0.07898897677659988\n",
      "Iteration 325, Batch: 18, Loss: 0.06438601762056351\n",
      "Iteration 325, Batch: 19, Loss: 0.05435136705636978\n",
      "Iteration 325, Batch: 20, Loss: 0.07388744503259659\n",
      "Iteration 325, Batch: 21, Loss: 0.08874914050102234\n",
      "Iteration 325, Batch: 22, Loss: 0.03232540190219879\n",
      "Iteration 325, Batch: 23, Loss: 0.055845193564891815\n",
      "Iteration 325, Batch: 24, Loss: 0.05880260467529297\n",
      "Iteration 325, Batch: 25, Loss: 0.05321504548192024\n",
      "Iteration 325, Batch: 26, Loss: 0.035294581204652786\n",
      "Iteration 325, Batch: 27, Loss: 0.059688910841941833\n",
      "Iteration 325, Batch: 28, Loss: 0.03983524069190025\n",
      "Iteration 325, Batch: 29, Loss: 0.060544222593307495\n",
      "Iteration 325, Batch: 30, Loss: 0.036003656685352325\n",
      "Iteration 325, Batch: 31, Loss: 0.030716590583324432\n",
      "Iteration 325, Batch: 32, Loss: 0.041032712906599045\n",
      "Iteration 325, Batch: 33, Loss: 0.08204444497823715\n",
      "Iteration 325, Batch: 34, Loss: 0.07921135425567627\n",
      "Iteration 325, Batch: 35, Loss: 0.053055353462696075\n",
      "Iteration 325, Batch: 36, Loss: 0.08803647011518478\n",
      "Iteration 325, Batch: 37, Loss: 0.06984276324510574\n",
      "Iteration 325, Batch: 38, Loss: 0.06460370868444443\n",
      "Iteration 325, Batch: 39, Loss: 0.05066626891493797\n",
      "Iteration 325, Batch: 40, Loss: 0.06349645555019379\n",
      "Iteration 325, Batch: 41, Loss: 0.05305943265557289\n",
      "Iteration 325, Batch: 42, Loss: 0.029650697484612465\n",
      "Iteration 325, Batch: 43, Loss: 0.04361921176314354\n",
      "Iteration 325, Batch: 44, Loss: 0.05760646611452103\n",
      "Iteration 325, Batch: 45, Loss: 0.05870785564184189\n",
      "Iteration 325, Batch: 46, Loss: 0.04526495933532715\n",
      "Iteration 325, Batch: 47, Loss: 0.05158223211765289\n",
      "Iteration 325, Batch: 48, Loss: 0.06923550367355347\n",
      "Iteration 325, Batch: 49, Loss: 0.046051833778619766\n",
      "Number of layers: 10\n",
      "Iteration 326, Batch: 0, Loss: 0.0414254330098629\n",
      "Iteration 326, Batch: 1, Loss: 0.05184139311313629\n",
      "Iteration 326, Batch: 2, Loss: 0.0528319887816906\n",
      "Iteration 326, Batch: 3, Loss: 0.07727820426225662\n",
      "Iteration 326, Batch: 4, Loss: 0.05248584225773811\n",
      "Iteration 326, Batch: 5, Loss: 0.05077901855111122\n",
      "Iteration 326, Batch: 6, Loss: 0.05750255286693573\n",
      "Iteration 326, Batch: 7, Loss: 0.05519063398241997\n",
      "Iteration 326, Batch: 8, Loss: 0.04565572738647461\n",
      "Iteration 326, Batch: 9, Loss: 0.04463627189397812\n",
      "Iteration 326, Batch: 10, Loss: 0.06153693050146103\n",
      "Iteration 326, Batch: 11, Loss: 0.046072568744421005\n",
      "Iteration 326, Batch: 12, Loss: 0.04254771023988724\n",
      "Iteration 326, Batch: 13, Loss: 0.10233660787343979\n",
      "Iteration 326, Batch: 14, Loss: 0.061715710908174515\n",
      "Iteration 326, Batch: 15, Loss: 0.04740886762738228\n",
      "Iteration 326, Batch: 16, Loss: 0.06756965816020966\n",
      "Iteration 326, Batch: 17, Loss: 0.05696145445108414\n",
      "Iteration 326, Batch: 18, Loss: 0.05076080188155174\n",
      "Iteration 326, Batch: 19, Loss: 0.05266110226511955\n",
      "Iteration 326, Batch: 20, Loss: 0.05562097951769829\n",
      "Iteration 326, Batch: 21, Loss: 0.0479576401412487\n",
      "Iteration 326, Batch: 22, Loss: 0.040772780776023865\n",
      "Iteration 326, Batch: 23, Loss: 0.046056412160396576\n",
      "Iteration 326, Batch: 24, Loss: 0.025668280199170113\n",
      "Iteration 326, Batch: 25, Loss: 0.04804688319563866\n",
      "Iteration 326, Batch: 26, Loss: 0.042887602001428604\n",
      "Iteration 326, Batch: 27, Loss: 0.05778195336461067\n",
      "Iteration 326, Batch: 28, Loss: 0.04698127508163452\n",
      "Iteration 326, Batch: 29, Loss: 0.053437430411577225\n",
      "Iteration 326, Batch: 30, Loss: 0.06588663905858994\n",
      "Iteration 326, Batch: 31, Loss: 0.09846869111061096\n",
      "Iteration 326, Batch: 32, Loss: 0.08643266558647156\n",
      "Iteration 326, Batch: 33, Loss: 0.05800996720790863\n",
      "Iteration 326, Batch: 34, Loss: 0.040041740983724594\n",
      "Iteration 326, Batch: 35, Loss: 0.05292629078030586\n",
      "Iteration 326, Batch: 36, Loss: 0.08021993190050125\n",
      "Iteration 326, Batch: 37, Loss: 0.09341158717870712\n",
      "Iteration 326, Batch: 38, Loss: 0.06549667567014694\n",
      "Iteration 326, Batch: 39, Loss: 0.03739726170897484\n",
      "Iteration 326, Batch: 40, Loss: 0.053861040621995926\n",
      "Iteration 326, Batch: 41, Loss: 0.03167814016342163\n",
      "Iteration 326, Batch: 42, Loss: 0.09432239085435867\n",
      "Iteration 326, Batch: 43, Loss: 0.04447360336780548\n",
      "Iteration 326, Batch: 44, Loss: 0.05042440444231033\n",
      "Iteration 326, Batch: 45, Loss: 0.07233399897813797\n",
      "Iteration 326, Batch: 46, Loss: 0.056583743542432785\n",
      "Iteration 326, Batch: 47, Loss: 0.055418290197849274\n",
      "Iteration 326, Batch: 48, Loss: 0.04723503068089485\n",
      "Iteration 326, Batch: 49, Loss: 0.06396778672933578\n",
      "Number of layers: 10\n",
      "Iteration 327, Batch: 0, Loss: 0.04023152217268944\n",
      "Iteration 327, Batch: 1, Loss: 0.0640953928232193\n",
      "Iteration 327, Batch: 2, Loss: 0.07359671592712402\n",
      "Iteration 327, Batch: 3, Loss: 0.05761788412928581\n",
      "Iteration 327, Batch: 4, Loss: 0.06743722409009933\n",
      "Iteration 327, Batch: 5, Loss: 0.05880460515618324\n",
      "Iteration 327, Batch: 6, Loss: 0.0778195708990097\n",
      "Iteration 327, Batch: 7, Loss: 0.0603518933057785\n",
      "Iteration 327, Batch: 8, Loss: 0.06787833571434021\n",
      "Iteration 327, Batch: 9, Loss: 0.0532817542552948\n",
      "Iteration 327, Batch: 10, Loss: 0.05614839494228363\n",
      "Iteration 327, Batch: 11, Loss: 0.04019154608249664\n",
      "Iteration 327, Batch: 12, Loss: 0.06255584955215454\n",
      "Iteration 327, Batch: 13, Loss: 0.062052078545093536\n",
      "Iteration 327, Batch: 14, Loss: 0.05910981073975563\n",
      "Iteration 327, Batch: 15, Loss: 0.04774006828665733\n",
      "Iteration 327, Batch: 16, Loss: 0.04928239807486534\n",
      "Iteration 327, Batch: 17, Loss: 0.050210364162921906\n",
      "Iteration 327, Batch: 18, Loss: 0.06719954311847687\n",
      "Iteration 327, Batch: 19, Loss: 0.06648080050945282\n",
      "Iteration 327, Batch: 20, Loss: 0.07936317473649979\n",
      "Iteration 327, Batch: 21, Loss: 0.050506241619586945\n",
      "Iteration 327, Batch: 22, Loss: 0.04282775893807411\n",
      "Iteration 327, Batch: 23, Loss: 0.0427987277507782\n",
      "Iteration 327, Batch: 24, Loss: 0.041213784366846085\n",
      "Iteration 327, Batch: 25, Loss: 0.04321989044547081\n",
      "Iteration 327, Batch: 26, Loss: 0.04585462436079979\n",
      "Iteration 327, Batch: 27, Loss: 0.044722650200128555\n",
      "Iteration 327, Batch: 28, Loss: 0.0440957248210907\n",
      "Iteration 327, Batch: 29, Loss: 0.052780456840991974\n",
      "Iteration 327, Batch: 30, Loss: 0.047908179461956024\n",
      "Iteration 327, Batch: 31, Loss: 0.04621538147330284\n",
      "Iteration 327, Batch: 32, Loss: 0.04508329927921295\n",
      "Iteration 327, Batch: 33, Loss: 0.06097207963466644\n",
      "Iteration 327, Batch: 34, Loss: 0.04768277704715729\n",
      "Iteration 327, Batch: 35, Loss: 0.033955615013837814\n",
      "Iteration 327, Batch: 36, Loss: 0.06899016350507736\n",
      "Iteration 327, Batch: 37, Loss: 0.03503420576453209\n",
      "Iteration 327, Batch: 38, Loss: 0.0620432011783123\n",
      "Iteration 327, Batch: 39, Loss: 0.05828741937875748\n",
      "Iteration 327, Batch: 40, Loss: 0.026958826929330826\n",
      "Iteration 327, Batch: 41, Loss: 0.06234290823340416\n",
      "Iteration 327, Batch: 42, Loss: 0.04667413979768753\n",
      "Iteration 327, Batch: 43, Loss: 0.03398628532886505\n",
      "Iteration 327, Batch: 44, Loss: 0.038913238793611526\n",
      "Iteration 327, Batch: 45, Loss: 0.05318949744105339\n",
      "Iteration 327, Batch: 46, Loss: 0.02703058160841465\n",
      "Iteration 327, Batch: 47, Loss: 0.03418669477105141\n",
      "Iteration 327, Batch: 48, Loss: 0.03385048359632492\n",
      "Iteration 327, Batch: 49, Loss: 0.033340223133563995\n",
      "Number of layers: 10\n",
      "Iteration 328, Batch: 0, Loss: 0.054263386875391006\n",
      "Iteration 328, Batch: 1, Loss: 0.04376088082790375\n",
      "Iteration 328, Batch: 2, Loss: 0.0542222335934639\n",
      "Iteration 328, Batch: 3, Loss: 0.05763706937432289\n",
      "Iteration 328, Batch: 4, Loss: 0.05650068074464798\n",
      "Iteration 328, Batch: 5, Loss: 0.055235061794519424\n",
      "Iteration 328, Batch: 6, Loss: 0.04993138089776039\n",
      "Iteration 328, Batch: 7, Loss: 0.03687797114253044\n",
      "Iteration 328, Batch: 8, Loss: 0.05238658934831619\n",
      "Iteration 328, Batch: 9, Loss: 0.02711012214422226\n",
      "Iteration 328, Batch: 10, Loss: 0.042692117393016815\n",
      "Iteration 328, Batch: 11, Loss: 0.06406157463788986\n",
      "Iteration 328, Batch: 12, Loss: 0.03837624564766884\n",
      "Iteration 328, Batch: 13, Loss: 0.05220584198832512\n",
      "Iteration 328, Batch: 14, Loss: 0.06473179906606674\n",
      "Iteration 328, Batch: 15, Loss: 0.05053454637527466\n",
      "Iteration 328, Batch: 16, Loss: 0.06959843635559082\n",
      "Iteration 328, Batch: 17, Loss: 0.03782296180725098\n",
      "Iteration 328, Batch: 18, Loss: 0.04314759001135826\n",
      "Iteration 328, Batch: 19, Loss: 0.04796449467539787\n",
      "Iteration 328, Batch: 20, Loss: 0.03358552232384682\n",
      "Iteration 328, Batch: 21, Loss: 0.05350892245769501\n",
      "Iteration 328, Batch: 22, Loss: 0.04905787482857704\n",
      "Iteration 328, Batch: 23, Loss: 0.04649989679455757\n",
      "Iteration 328, Batch: 24, Loss: 0.04709823802113533\n",
      "Iteration 328, Batch: 25, Loss: 0.031031064689159393\n",
      "Iteration 328, Batch: 26, Loss: 0.028466302901506424\n",
      "Iteration 328, Batch: 27, Loss: 0.04010233283042908\n",
      "Iteration 328, Batch: 28, Loss: 0.03726363182067871\n",
      "Iteration 328, Batch: 29, Loss: 0.03719841688871384\n",
      "Iteration 328, Batch: 30, Loss: 0.05469265207648277\n",
      "Iteration 328, Batch: 31, Loss: 0.02893189527094364\n",
      "Iteration 328, Batch: 32, Loss: 0.02873591147363186\n",
      "Iteration 328, Batch: 33, Loss: 0.02920185588300228\n",
      "Iteration 328, Batch: 34, Loss: 0.032924894243478775\n",
      "Iteration 328, Batch: 35, Loss: 0.03878755122423172\n",
      "Iteration 328, Batch: 36, Loss: 0.03432192653417587\n",
      "Iteration 328, Batch: 37, Loss: 0.06992581486701965\n",
      "Iteration 328, Batch: 38, Loss: 0.05903089419007301\n",
      "Iteration 328, Batch: 39, Loss: 0.03658347204327583\n",
      "Iteration 328, Batch: 40, Loss: 0.03417585790157318\n",
      "Iteration 328, Batch: 41, Loss: 0.04081692919135094\n",
      "Iteration 328, Batch: 42, Loss: 0.044732898473739624\n",
      "Iteration 328, Batch: 43, Loss: 0.029888061806559563\n",
      "Iteration 328, Batch: 44, Loss: 0.04092367738485336\n",
      "Iteration 328, Batch: 45, Loss: 0.031901221722364426\n",
      "Iteration 328, Batch: 46, Loss: 0.03608715906739235\n",
      "Iteration 328, Batch: 47, Loss: 0.04000171646475792\n",
      "Iteration 328, Batch: 48, Loss: 0.042388010770082474\n",
      "Iteration 328, Batch: 49, Loss: 0.031919240951538086\n",
      "Number of layers: 10\n",
      "Iteration 329, Batch: 0, Loss: 0.04515806958079338\n",
      "Iteration 329, Batch: 1, Loss: 0.04974738135933876\n",
      "Iteration 329, Batch: 2, Loss: 0.04056466370820999\n",
      "Iteration 329, Batch: 3, Loss: 0.029083967208862305\n",
      "Iteration 329, Batch: 4, Loss: 0.027876444160938263\n",
      "Iteration 329, Batch: 5, Loss: 0.035375457257032394\n",
      "Iteration 329, Batch: 6, Loss: 0.0468597412109375\n",
      "Iteration 329, Batch: 7, Loss: 0.01765984669327736\n",
      "Iteration 329, Batch: 8, Loss: 0.03494728356599808\n",
      "Iteration 329, Batch: 9, Loss: 0.033314045518636703\n",
      "Iteration 329, Batch: 10, Loss: 0.058039724826812744\n",
      "Iteration 329, Batch: 11, Loss: 0.05417361110448837\n",
      "Iteration 329, Batch: 12, Loss: 0.04069794714450836\n",
      "Iteration 329, Batch: 13, Loss: 0.0618661567568779\n",
      "Iteration 329, Batch: 14, Loss: 0.05729461833834648\n",
      "Iteration 329, Batch: 15, Loss: 0.029056290164589882\n",
      "Iteration 329, Batch: 16, Loss: 0.05025787651538849\n",
      "Iteration 329, Batch: 17, Loss: 0.032368794083595276\n",
      "Iteration 329, Batch: 18, Loss: 0.05847897380590439\n",
      "Iteration 329, Batch: 19, Loss: 0.039354246109724045\n",
      "Iteration 329, Batch: 20, Loss: 0.05864614248275757\n",
      "Iteration 329, Batch: 21, Loss: 0.06988415867090225\n",
      "Iteration 329, Batch: 22, Loss: 0.02243731915950775\n",
      "Iteration 329, Batch: 23, Loss: 0.05931245535612106\n",
      "Iteration 329, Batch: 24, Loss: 0.05130211263895035\n",
      "Iteration 329, Batch: 25, Loss: 0.06222059205174446\n",
      "Iteration 329, Batch: 26, Loss: 0.05471087992191315\n",
      "Iteration 329, Batch: 27, Loss: 0.046450138092041016\n",
      "Iteration 329, Batch: 28, Loss: 0.040480922907590866\n",
      "Iteration 329, Batch: 29, Loss: 0.040482815355062485\n",
      "Iteration 329, Batch: 30, Loss: 0.07421273738145828\n",
      "Iteration 329, Batch: 31, Loss: 0.036813512444496155\n",
      "Iteration 329, Batch: 32, Loss: 0.06715543568134308\n",
      "Iteration 329, Batch: 33, Loss: 0.03939218074083328\n",
      "Iteration 329, Batch: 34, Loss: 0.04676776006817818\n",
      "Iteration 329, Batch: 35, Loss: 0.05049937590956688\n",
      "Iteration 329, Batch: 36, Loss: 0.06288611143827438\n",
      "Iteration 329, Batch: 37, Loss: 0.03307744115591049\n",
      "Iteration 329, Batch: 38, Loss: 0.038357365876436234\n",
      "Iteration 329, Batch: 39, Loss: 0.04335366562008858\n",
      "Iteration 329, Batch: 40, Loss: 0.03600568324327469\n",
      "Iteration 329, Batch: 41, Loss: 0.04426272213459015\n",
      "Iteration 329, Batch: 42, Loss: 0.03344116732478142\n",
      "Iteration 329, Batch: 43, Loss: 0.033040087670087814\n",
      "Iteration 329, Batch: 44, Loss: 0.05204102396965027\n",
      "Iteration 329, Batch: 45, Loss: 0.046493589878082275\n",
      "Iteration 329, Batch: 46, Loss: 0.09787670522928238\n",
      "Iteration 329, Batch: 47, Loss: 0.06921888887882233\n",
      "Iteration 329, Batch: 48, Loss: 0.06674497574567795\n",
      "Iteration 329, Batch: 49, Loss: 0.05247874930500984\n",
      "Number of layers: 10\n",
      "Iteration 330, Batch: 0, Loss: 0.04629107192158699\n",
      "Iteration 330, Batch: 1, Loss: 0.06289008259773254\n",
      "Iteration 330, Batch: 2, Loss: 0.05995209142565727\n",
      "Iteration 330, Batch: 3, Loss: 0.06246110796928406\n",
      "Iteration 330, Batch: 4, Loss: 0.08874168992042542\n",
      "Iteration 330, Batch: 5, Loss: 0.05895407497882843\n",
      "Iteration 330, Batch: 6, Loss: 0.09673453122377396\n",
      "Iteration 330, Batch: 7, Loss: 0.07323162257671356\n",
      "Iteration 330, Batch: 8, Loss: 0.06067894771695137\n",
      "Iteration 330, Batch: 9, Loss: 0.04920722544193268\n",
      "Iteration 330, Batch: 10, Loss: 0.0565468929708004\n",
      "Iteration 330, Batch: 11, Loss: 0.04641658812761307\n",
      "Iteration 330, Batch: 12, Loss: 0.03674115985631943\n",
      "Iteration 330, Batch: 13, Loss: 0.05678275600075722\n",
      "Iteration 330, Batch: 14, Loss: 0.07103297859430313\n",
      "Iteration 330, Batch: 15, Loss: 0.04854756221175194\n",
      "Iteration 330, Batch: 16, Loss: 0.07490004599094391\n",
      "Iteration 330, Batch: 17, Loss: 0.023215187713503838\n",
      "Iteration 330, Batch: 18, Loss: 0.025929361581802368\n",
      "Iteration 330, Batch: 19, Loss: 0.09040366113185883\n",
      "Iteration 330, Batch: 20, Loss: 0.06107066571712494\n",
      "Iteration 330, Batch: 21, Loss: 0.06252428144216537\n",
      "Iteration 330, Batch: 22, Loss: 0.05277780815958977\n",
      "Iteration 330, Batch: 23, Loss: 0.04880714416503906\n",
      "Iteration 330, Batch: 24, Loss: 0.05302233621478081\n",
      "Iteration 330, Batch: 25, Loss: 0.03569195419549942\n",
      "Iteration 330, Batch: 26, Loss: 0.08577405661344528\n",
      "Iteration 330, Batch: 27, Loss: 0.067955881357193\n",
      "Iteration 330, Batch: 28, Loss: 0.08382689207792282\n",
      "Iteration 330, Batch: 29, Loss: 0.04045126214623451\n",
      "Iteration 330, Batch: 30, Loss: 0.08574344217777252\n",
      "Iteration 330, Batch: 31, Loss: 0.07633017003536224\n",
      "Iteration 330, Batch: 32, Loss: 0.08807390928268433\n",
      "Iteration 330, Batch: 33, Loss: 0.07697836309671402\n",
      "Iteration 330, Batch: 34, Loss: 0.08125411719083786\n",
      "Iteration 330, Batch: 35, Loss: 0.05180226266384125\n",
      "Iteration 330, Batch: 36, Loss: 0.08733831346035004\n",
      "Iteration 330, Batch: 37, Loss: 0.08482786267995834\n",
      "Iteration 330, Batch: 38, Loss: 0.068079374730587\n",
      "Iteration 330, Batch: 39, Loss: 0.08863327652215958\n",
      "Iteration 330, Batch: 40, Loss: 0.060523707419633865\n",
      "Iteration 330, Batch: 41, Loss: 0.08834239840507507\n",
      "Iteration 330, Batch: 42, Loss: 0.049826063215732574\n",
      "Iteration 330, Batch: 43, Loss: 0.028959952294826508\n",
      "Iteration 330, Batch: 44, Loss: 0.07302264124155045\n",
      "Iteration 330, Batch: 45, Loss: 0.08594755083322525\n",
      "Iteration 330, Batch: 46, Loss: 0.04960786551237106\n",
      "Iteration 330, Batch: 47, Loss: 0.04749466851353645\n",
      "Iteration 330, Batch: 48, Loss: 0.05770012363791466\n",
      "Iteration 330, Batch: 49, Loss: 0.06240450590848923\n",
      "Number of layers: 10\n",
      "Iteration 331, Batch: 0, Loss: 0.04525860771536827\n",
      "Iteration 331, Batch: 1, Loss: 0.060627639293670654\n",
      "Iteration 331, Batch: 2, Loss: 0.03181852772831917\n",
      "Iteration 331, Batch: 3, Loss: 0.030673488974571228\n",
      "Iteration 331, Batch: 4, Loss: 0.028070254251360893\n",
      "Iteration 331, Batch: 5, Loss: 0.05975387617945671\n",
      "Iteration 331, Batch: 6, Loss: 0.06212814152240753\n",
      "Iteration 331, Batch: 7, Loss: 0.03610995411872864\n",
      "Iteration 331, Batch: 8, Loss: 0.03515931963920593\n",
      "Iteration 331, Batch: 9, Loss: 0.061246566474437714\n",
      "Iteration 331, Batch: 10, Loss: 0.06022306904196739\n",
      "Iteration 331, Batch: 11, Loss: 0.07370725274085999\n",
      "Iteration 331, Batch: 12, Loss: 0.04489094763994217\n",
      "Iteration 331, Batch: 13, Loss: 0.03924398869276047\n",
      "Iteration 331, Batch: 14, Loss: 0.09626232087612152\n",
      "Iteration 331, Batch: 15, Loss: 0.08453714847564697\n",
      "Iteration 331, Batch: 16, Loss: 0.05045568197965622\n",
      "Iteration 331, Batch: 17, Loss: 0.03928382694721222\n",
      "Iteration 331, Batch: 18, Loss: 0.06779889017343521\n",
      "Iteration 331, Batch: 19, Loss: 0.05833529308438301\n",
      "Iteration 331, Batch: 20, Loss: 0.04458795487880707\n",
      "Iteration 331, Batch: 21, Loss: 0.047891274094581604\n",
      "Iteration 331, Batch: 22, Loss: 0.04576801508665085\n",
      "Iteration 331, Batch: 23, Loss: 0.07197372615337372\n",
      "Iteration 331, Batch: 24, Loss: 0.04456627741456032\n",
      "Iteration 331, Batch: 25, Loss: 0.07329938560724258\n",
      "Iteration 331, Batch: 26, Loss: 0.0868614912033081\n",
      "Iteration 331, Batch: 27, Loss: 0.06935597211122513\n",
      "Iteration 331, Batch: 28, Loss: 0.10159118473529816\n",
      "Iteration 331, Batch: 29, Loss: 0.06481897085905075\n",
      "Iteration 331, Batch: 30, Loss: 0.06866737455129623\n",
      "Iteration 331, Batch: 31, Loss: 0.08104483783245087\n",
      "Iteration 331, Batch: 32, Loss: 0.051830943673849106\n",
      "Iteration 331, Batch: 33, Loss: 0.05199432373046875\n",
      "Iteration 331, Batch: 34, Loss: 0.06310650706291199\n",
      "Iteration 331, Batch: 35, Loss: 0.04040399566292763\n",
      "Iteration 331, Batch: 36, Loss: 0.06617982685565948\n",
      "Iteration 331, Batch: 37, Loss: 0.13726285099983215\n",
      "Iteration 331, Batch: 38, Loss: 0.1643211543560028\n",
      "Iteration 331, Batch: 39, Loss: 0.11858563870191574\n",
      "Iteration 331, Batch: 40, Loss: 0.06928034871816635\n",
      "Iteration 331, Batch: 41, Loss: 0.02849718928337097\n",
      "Iteration 331, Batch: 42, Loss: 0.07007722556591034\n",
      "Iteration 331, Batch: 43, Loss: 0.0855397954583168\n",
      "Iteration 331, Batch: 44, Loss: 0.08769362419843674\n",
      "Iteration 331, Batch: 45, Loss: 0.06049814447760582\n",
      "Iteration 331, Batch: 46, Loss: 0.05402137711644173\n",
      "Iteration 331, Batch: 47, Loss: 0.04714370146393776\n",
      "Iteration 331, Batch: 48, Loss: 0.06932151317596436\n",
      "Iteration 331, Batch: 49, Loss: 0.07362636178731918\n",
      "Number of layers: 10\n",
      "Iteration 332, Batch: 0, Loss: 0.04389961436390877\n",
      "Iteration 332, Batch: 1, Loss: 0.07778825610876083\n",
      "Iteration 332, Batch: 2, Loss: 0.06295120716094971\n",
      "Iteration 332, Batch: 3, Loss: 0.053372666239738464\n",
      "Iteration 332, Batch: 4, Loss: 0.04797527194023132\n",
      "Iteration 332, Batch: 5, Loss: 0.03657438978552818\n",
      "Iteration 332, Batch: 6, Loss: 0.0712692067027092\n",
      "Iteration 332, Batch: 7, Loss: 0.0689023956656456\n",
      "Iteration 332, Batch: 8, Loss: 0.06839249283075333\n",
      "Iteration 332, Batch: 9, Loss: 0.05129808187484741\n",
      "Iteration 332, Batch: 10, Loss: 0.055245332419872284\n",
      "Iteration 332, Batch: 11, Loss: 0.06554663181304932\n",
      "Iteration 332, Batch: 12, Loss: 0.06890947371721268\n",
      "Iteration 332, Batch: 13, Loss: 0.06358641386032104\n",
      "Iteration 332, Batch: 14, Loss: 0.04840255528688431\n",
      "Iteration 332, Batch: 15, Loss: 0.06590712070465088\n",
      "Iteration 332, Batch: 16, Loss: 0.07613126933574677\n",
      "Iteration 332, Batch: 17, Loss: 0.06062214821577072\n",
      "Iteration 332, Batch: 18, Loss: 0.050477035343647\n",
      "Iteration 332, Batch: 19, Loss: 0.05401887372136116\n",
      "Iteration 332, Batch: 20, Loss: 0.031254369765520096\n",
      "Iteration 332, Batch: 21, Loss: 0.05342471972107887\n",
      "Iteration 332, Batch: 22, Loss: 0.04661723971366882\n",
      "Iteration 332, Batch: 23, Loss: 0.055549781769514084\n",
      "Iteration 332, Batch: 24, Loss: 0.04707429185509682\n",
      "Iteration 332, Batch: 25, Loss: 0.07533019781112671\n",
      "Iteration 332, Batch: 26, Loss: 0.05401504784822464\n",
      "Iteration 332, Batch: 27, Loss: 0.0719008520245552\n",
      "Iteration 332, Batch: 28, Loss: 0.04666488617658615\n",
      "Iteration 332, Batch: 29, Loss: 0.059909261763095856\n",
      "Iteration 332, Batch: 30, Loss: 0.06365593522787094\n",
      "Iteration 332, Batch: 31, Loss: 0.06369341909885406\n",
      "Iteration 332, Batch: 32, Loss: 0.06548568606376648\n",
      "Iteration 332, Batch: 33, Loss: 0.08435870707035065\n",
      "Iteration 332, Batch: 34, Loss: 0.07071436196565628\n",
      "Iteration 332, Batch: 35, Loss: 0.04905010014772415\n",
      "Iteration 332, Batch: 36, Loss: 0.03995395451784134\n",
      "Iteration 332, Batch: 37, Loss: 0.05375174805521965\n",
      "Iteration 332, Batch: 38, Loss: 0.06498485058546066\n",
      "Iteration 332, Batch: 39, Loss: 0.0520474798977375\n",
      "Iteration 332, Batch: 40, Loss: 0.06480228900909424\n",
      "Iteration 332, Batch: 41, Loss: 0.047067802399396896\n",
      "Iteration 332, Batch: 42, Loss: 0.07184802740812302\n",
      "Iteration 332, Batch: 43, Loss: 0.08222047984600067\n",
      "Iteration 332, Batch: 44, Loss: 0.09541606903076172\n",
      "Iteration 332, Batch: 45, Loss: 0.07555922120809555\n",
      "Iteration 332, Batch: 46, Loss: 0.08304256200790405\n",
      "Iteration 332, Batch: 47, Loss: 0.12901277840137482\n",
      "Iteration 332, Batch: 48, Loss: 0.10857494175434113\n",
      "Iteration 332, Batch: 49, Loss: 0.10303725302219391\n",
      "Number of layers: 10\n",
      "Iteration 333, Batch: 0, Loss: 0.10229625552892685\n",
      "Iteration 333, Batch: 1, Loss: 0.06020522117614746\n",
      "Iteration 333, Batch: 2, Loss: 0.0634039118885994\n",
      "Iteration 333, Batch: 3, Loss: 0.0682230144739151\n",
      "Iteration 333, Batch: 4, Loss: 0.07581993192434311\n",
      "Iteration 333, Batch: 5, Loss: 0.08914487808942795\n",
      "Iteration 333, Batch: 6, Loss: 0.08311149477958679\n",
      "Iteration 333, Batch: 7, Loss: 0.05283793434500694\n",
      "Iteration 333, Batch: 8, Loss: 0.06385012716054916\n",
      "Iteration 333, Batch: 9, Loss: 0.06119127199053764\n",
      "Iteration 333, Batch: 10, Loss: 0.03649260848760605\n",
      "Iteration 333, Batch: 11, Loss: 0.062117308378219604\n",
      "Iteration 333, Batch: 12, Loss: 0.09327419102191925\n",
      "Iteration 333, Batch: 13, Loss: 0.0645999014377594\n",
      "Iteration 333, Batch: 14, Loss: 0.08299687504768372\n",
      "Iteration 333, Batch: 15, Loss: 0.055248111486434937\n",
      "Iteration 333, Batch: 16, Loss: 0.07835197448730469\n",
      "Iteration 333, Batch: 17, Loss: 0.09022687375545502\n",
      "Iteration 333, Batch: 18, Loss: 0.07144181430339813\n",
      "Iteration 333, Batch: 19, Loss: 0.060003675520420074\n",
      "Iteration 333, Batch: 20, Loss: 0.052180975675582886\n",
      "Iteration 333, Batch: 21, Loss: 0.07520292699337006\n",
      "Iteration 333, Batch: 22, Loss: 0.05408482253551483\n",
      "Iteration 333, Batch: 23, Loss: 0.04691626876592636\n",
      "Iteration 333, Batch: 24, Loss: 0.03290954604744911\n",
      "Iteration 333, Batch: 25, Loss: 0.047325219959020615\n",
      "Iteration 333, Batch: 26, Loss: 0.034831009805202484\n",
      "Iteration 333, Batch: 27, Loss: 0.03860963135957718\n",
      "Iteration 333, Batch: 28, Loss: 0.04227399826049805\n",
      "Iteration 333, Batch: 29, Loss: 0.06105763465166092\n",
      "Iteration 333, Batch: 30, Loss: 0.05087308585643768\n",
      "Iteration 333, Batch: 31, Loss: 0.045952267944812775\n",
      "Iteration 333, Batch: 32, Loss: 0.03762625902891159\n",
      "Iteration 333, Batch: 33, Loss: 0.055967092514038086\n",
      "Iteration 333, Batch: 34, Loss: 0.04299446940422058\n",
      "Iteration 333, Batch: 35, Loss: 0.03766344115138054\n",
      "Iteration 333, Batch: 36, Loss: 0.029219282791018486\n",
      "Iteration 333, Batch: 37, Loss: 0.08019600808620453\n",
      "Iteration 333, Batch: 38, Loss: 0.041609182953834534\n",
      "Iteration 333, Batch: 39, Loss: 0.07557486742734909\n",
      "Iteration 333, Batch: 40, Loss: 0.059119921177625656\n",
      "Iteration 333, Batch: 41, Loss: 0.06420769542455673\n",
      "Iteration 333, Batch: 42, Loss: 0.049252595752477646\n",
      "Iteration 333, Batch: 43, Loss: 0.060460567474365234\n",
      "Iteration 333, Batch: 44, Loss: 0.02930724062025547\n",
      "Iteration 333, Batch: 45, Loss: 0.039961352944374084\n",
      "Iteration 333, Batch: 46, Loss: 0.06809251010417938\n",
      "Iteration 333, Batch: 47, Loss: 0.05718597397208214\n",
      "Iteration 333, Batch: 48, Loss: 0.05185675621032715\n",
      "Iteration 333, Batch: 49, Loss: 0.049009546637535095\n",
      "Number of layers: 10\n",
      "Iteration 334, Batch: 0, Loss: 0.05764433369040489\n",
      "Iteration 334, Batch: 1, Loss: 0.03894488513469696\n",
      "Iteration 334, Batch: 2, Loss: 0.05394459515810013\n",
      "Iteration 334, Batch: 3, Loss: 0.06717415899038315\n",
      "Iteration 334, Batch: 4, Loss: 0.031148258596658707\n",
      "Iteration 334, Batch: 5, Loss: 0.05001911148428917\n",
      "Iteration 334, Batch: 6, Loss: 0.046767812222242355\n",
      "Iteration 334, Batch: 7, Loss: 0.0390251949429512\n",
      "Iteration 334, Batch: 8, Loss: 0.05983433499932289\n",
      "Iteration 334, Batch: 9, Loss: 0.025205392390489578\n",
      "Iteration 334, Batch: 10, Loss: 0.04804597795009613\n",
      "Iteration 334, Batch: 11, Loss: 0.06186269223690033\n",
      "Iteration 334, Batch: 12, Loss: 0.03769954666495323\n",
      "Iteration 334, Batch: 13, Loss: 0.03861666098237038\n",
      "Iteration 334, Batch: 14, Loss: 0.039257872849702835\n",
      "Iteration 334, Batch: 15, Loss: 0.06178615614771843\n",
      "Iteration 334, Batch: 16, Loss: 0.03385963663458824\n",
      "Iteration 334, Batch: 17, Loss: 0.031145373359322548\n",
      "Iteration 334, Batch: 18, Loss: 0.048375204205513\n",
      "Iteration 334, Batch: 19, Loss: 0.022291889414191246\n",
      "Iteration 334, Batch: 20, Loss: 0.05123104900121689\n",
      "Iteration 334, Batch: 21, Loss: 0.052793506532907486\n",
      "Iteration 334, Batch: 22, Loss: 0.02925131656229496\n",
      "Iteration 334, Batch: 23, Loss: 0.03556934744119644\n",
      "Iteration 334, Batch: 24, Loss: 0.04292655736207962\n",
      "Iteration 334, Batch: 25, Loss: 0.031899794936180115\n",
      "Iteration 334, Batch: 26, Loss: 0.03320580720901489\n",
      "Iteration 334, Batch: 27, Loss: 0.05145616456866264\n",
      "Iteration 334, Batch: 28, Loss: 0.041133664548397064\n",
      "Iteration 334, Batch: 29, Loss: 0.03532537817955017\n",
      "Iteration 334, Batch: 30, Loss: 0.06040508300065994\n",
      "Iteration 334, Batch: 31, Loss: 0.04180033504962921\n",
      "Iteration 334, Batch: 32, Loss: 0.04387003928422928\n",
      "Iteration 334, Batch: 33, Loss: 0.06278181821107864\n",
      "Iteration 334, Batch: 34, Loss: 0.03911104425787926\n",
      "Iteration 334, Batch: 35, Loss: 0.051239993423223495\n",
      "Iteration 334, Batch: 36, Loss: 0.022224271669983864\n",
      "Iteration 334, Batch: 37, Loss: 0.05843479558825493\n",
      "Iteration 334, Batch: 38, Loss: 0.030313409864902496\n",
      "Iteration 334, Batch: 39, Loss: 0.024255523458123207\n",
      "Iteration 334, Batch: 40, Loss: 0.03313054144382477\n",
      "Iteration 334, Batch: 41, Loss: 0.051836900413036346\n",
      "Iteration 334, Batch: 42, Loss: 0.039873335510492325\n",
      "Iteration 334, Batch: 43, Loss: 0.04273715987801552\n",
      "Iteration 334, Batch: 44, Loss: 0.04928453266620636\n",
      "Iteration 334, Batch: 45, Loss: 0.04789051413536072\n",
      "Iteration 334, Batch: 46, Loss: 0.024507619440555573\n",
      "Iteration 334, Batch: 47, Loss: 0.05824298784136772\n",
      "Iteration 334, Batch: 48, Loss: 0.07098022103309631\n",
      "Iteration 334, Batch: 49, Loss: 0.028932994231581688\n",
      "Number of layers: 10\n",
      "Iteration 335, Batch: 0, Loss: 0.03712397813796997\n",
      "Iteration 335, Batch: 1, Loss: 0.057184986770153046\n",
      "Iteration 335, Batch: 2, Loss: 0.03250749036669731\n",
      "Iteration 335, Batch: 3, Loss: 0.055051106959581375\n",
      "Iteration 335, Batch: 4, Loss: 0.0215718075633049\n",
      "Iteration 335, Batch: 5, Loss: 0.04328552633523941\n",
      "Iteration 335, Batch: 6, Loss: 0.03839579224586487\n",
      "Iteration 335, Batch: 7, Loss: 0.03290236368775368\n",
      "Iteration 335, Batch: 8, Loss: 0.04995567351579666\n",
      "Iteration 335, Batch: 9, Loss: 0.04438804090023041\n",
      "Iteration 335, Batch: 10, Loss: 0.05229928717017174\n",
      "Iteration 335, Batch: 11, Loss: 0.04073294252157211\n",
      "Iteration 335, Batch: 12, Loss: 0.036901164799928665\n",
      "Iteration 335, Batch: 13, Loss: 0.03703915327787399\n",
      "Iteration 335, Batch: 14, Loss: 0.031117191538214684\n",
      "Iteration 335, Batch: 15, Loss: 0.05470162630081177\n",
      "Iteration 335, Batch: 16, Loss: 0.04178832098841667\n",
      "Iteration 335, Batch: 17, Loss: 0.05552562698721886\n",
      "Iteration 335, Batch: 18, Loss: 0.048419978469610214\n",
      "Iteration 335, Batch: 19, Loss: 0.042088866233825684\n",
      "Iteration 335, Batch: 20, Loss: 0.056905779987573624\n",
      "Iteration 335, Batch: 21, Loss: 0.032455772161483765\n",
      "Iteration 335, Batch: 22, Loss: 0.03356856852769852\n",
      "Iteration 335, Batch: 23, Loss: 0.07385098934173584\n",
      "Iteration 335, Batch: 24, Loss: 0.06183778494596481\n",
      "Iteration 335, Batch: 25, Loss: 0.03034142032265663\n",
      "Iteration 335, Batch: 26, Loss: 0.05539683252573013\n",
      "Iteration 335, Batch: 27, Loss: 0.046261098235845566\n",
      "Iteration 335, Batch: 28, Loss: 0.027975434437394142\n",
      "Iteration 335, Batch: 29, Loss: 0.07097019255161285\n",
      "Iteration 335, Batch: 30, Loss: 0.05865974351763725\n",
      "Iteration 335, Batch: 31, Loss: 0.048926256597042084\n",
      "Iteration 335, Batch: 32, Loss: 0.029418043792247772\n",
      "Iteration 335, Batch: 33, Loss: 0.03944607079029083\n",
      "Iteration 335, Batch: 34, Loss: 0.06657291948795319\n",
      "Iteration 335, Batch: 35, Loss: 0.057985808700323105\n",
      "Iteration 335, Batch: 36, Loss: 0.021680321544408798\n",
      "Iteration 335, Batch: 37, Loss: 0.08553154021501541\n",
      "Iteration 335, Batch: 38, Loss: 0.048640429973602295\n",
      "Iteration 335, Batch: 39, Loss: 0.05456171929836273\n",
      "Iteration 335, Batch: 40, Loss: 0.06925949454307556\n",
      "Iteration 335, Batch: 41, Loss: 0.033334825187921524\n",
      "Iteration 335, Batch: 42, Loss: 0.06675972789525986\n",
      "Iteration 335, Batch: 43, Loss: 0.021934380754828453\n",
      "Iteration 335, Batch: 44, Loss: 0.03710785135626793\n",
      "Iteration 335, Batch: 45, Loss: 0.06676880270242691\n",
      "Iteration 335, Batch: 46, Loss: 0.07426891475915909\n",
      "Iteration 335, Batch: 47, Loss: 0.03227696940302849\n",
      "Iteration 335, Batch: 48, Loss: 0.04982636496424675\n",
      "Iteration 335, Batch: 49, Loss: 0.08065421134233475\n",
      "Number of layers: 10\n",
      "Iteration 336, Batch: 0, Loss: 0.06798601895570755\n",
      "Iteration 336, Batch: 1, Loss: 0.043485213071107864\n",
      "Iteration 336, Batch: 2, Loss: 0.028451910242438316\n",
      "Iteration 336, Batch: 3, Loss: 0.046763379126787186\n",
      "Iteration 336, Batch: 4, Loss: 0.031292326748371124\n",
      "Iteration 336, Batch: 5, Loss: 0.04067109152674675\n",
      "Iteration 336, Batch: 6, Loss: 0.05303041264414787\n",
      "Iteration 336, Batch: 7, Loss: 0.062469691038131714\n",
      "Iteration 336, Batch: 8, Loss: 0.03175665810704231\n",
      "Iteration 336, Batch: 9, Loss: 0.05738779529929161\n",
      "Iteration 336, Batch: 10, Loss: 0.023992447182536125\n",
      "Iteration 336, Batch: 11, Loss: 0.06953513622283936\n",
      "Iteration 336, Batch: 12, Loss: 0.02877192012965679\n",
      "Iteration 336, Batch: 13, Loss: 0.05554487928748131\n",
      "Iteration 336, Batch: 14, Loss: 0.036221545189619064\n",
      "Iteration 336, Batch: 15, Loss: 0.04312510788440704\n",
      "Iteration 336, Batch: 16, Loss: 0.039964936673641205\n",
      "Iteration 336, Batch: 17, Loss: 0.037765663117170334\n",
      "Iteration 336, Batch: 18, Loss: 0.023933708667755127\n",
      "Iteration 336, Batch: 19, Loss: 0.045725345611572266\n",
      "Iteration 336, Batch: 20, Loss: 0.0477340929210186\n",
      "Iteration 336, Batch: 21, Loss: 0.057272110134363174\n",
      "Iteration 336, Batch: 22, Loss: 0.04474681615829468\n",
      "Iteration 336, Batch: 23, Loss: 0.06825098395347595\n",
      "Iteration 336, Batch: 24, Loss: 0.054080333560705185\n",
      "Iteration 336, Batch: 25, Loss: 0.03636886179447174\n",
      "Iteration 336, Batch: 26, Loss: 0.05700905621051788\n",
      "Iteration 336, Batch: 27, Loss: 0.06898559629917145\n",
      "Iteration 336, Batch: 28, Loss: 0.08689241111278534\n",
      "Iteration 336, Batch: 29, Loss: 0.0850193127989769\n",
      "Iteration 336, Batch: 30, Loss: 0.08174478262662888\n",
      "Iteration 336, Batch: 31, Loss: 0.09421434998512268\n",
      "Iteration 336, Batch: 32, Loss: 0.08750305324792862\n",
      "Iteration 336, Batch: 33, Loss: 0.052827175706624985\n",
      "Iteration 336, Batch: 34, Loss: 0.06460768729448318\n",
      "Iteration 336, Batch: 35, Loss: 0.06295142322778702\n",
      "Iteration 336, Batch: 36, Loss: 0.04947371408343315\n",
      "Iteration 336, Batch: 37, Loss: 0.07469846308231354\n",
      "Iteration 336, Batch: 38, Loss: 0.043153852224349976\n",
      "Iteration 336, Batch: 39, Loss: 0.03232923895120621\n",
      "Iteration 336, Batch: 40, Loss: 0.04733017832040787\n",
      "Iteration 336, Batch: 41, Loss: 0.040609825402498245\n",
      "Iteration 336, Batch: 42, Loss: 0.05000758916139603\n",
      "Iteration 336, Batch: 43, Loss: 0.05227314680814743\n",
      "Iteration 336, Batch: 44, Loss: 0.03458354249596596\n",
      "Iteration 336, Batch: 45, Loss: 0.028632473200559616\n",
      "Iteration 336, Batch: 46, Loss: 0.033044349402189255\n",
      "Iteration 336, Batch: 47, Loss: 0.0464031957089901\n",
      "Iteration 336, Batch: 48, Loss: 0.02927054837346077\n",
      "Iteration 336, Batch: 49, Loss: 0.04994422197341919\n",
      "Number of layers: 10\n",
      "Iteration 337, Batch: 0, Loss: 0.060495518147945404\n",
      "Iteration 337, Batch: 1, Loss: 0.031669121235609055\n",
      "Iteration 337, Batch: 2, Loss: 0.03644343838095665\n",
      "Iteration 337, Batch: 3, Loss: 0.04889824241399765\n",
      "Iteration 337, Batch: 4, Loss: 0.05453571677207947\n",
      "Iteration 337, Batch: 5, Loss: 0.06927261501550674\n",
      "Iteration 337, Batch: 6, Loss: 0.04089517146348953\n",
      "Iteration 337, Batch: 7, Loss: 0.04382915794849396\n",
      "Iteration 337, Batch: 8, Loss: 0.04059131443500519\n",
      "Iteration 337, Batch: 9, Loss: 0.06410981714725494\n",
      "Iteration 337, Batch: 10, Loss: 0.055169668048620224\n",
      "Iteration 337, Batch: 11, Loss: 0.04689137265086174\n",
      "Iteration 337, Batch: 12, Loss: 0.04036082327365875\n",
      "Iteration 337, Batch: 13, Loss: 0.04422639310359955\n",
      "Iteration 337, Batch: 14, Loss: 0.04415742680430412\n",
      "Iteration 337, Batch: 15, Loss: 0.03817322477698326\n",
      "Iteration 337, Batch: 16, Loss: 0.04486401379108429\n",
      "Iteration 337, Batch: 17, Loss: 0.03923087567090988\n",
      "Iteration 337, Batch: 18, Loss: 0.08423268795013428\n",
      "Iteration 337, Batch: 19, Loss: 0.03523373231291771\n",
      "Iteration 337, Batch: 20, Loss: 0.04631228372454643\n",
      "Iteration 337, Batch: 21, Loss: 0.04105636104941368\n",
      "Iteration 337, Batch: 22, Loss: 0.05280245840549469\n",
      "Iteration 337, Batch: 23, Loss: 0.04090975970029831\n",
      "Iteration 337, Batch: 24, Loss: 0.06261985003948212\n",
      "Iteration 337, Batch: 25, Loss: 0.05166250467300415\n",
      "Iteration 337, Batch: 26, Loss: 0.05268189683556557\n",
      "Iteration 337, Batch: 27, Loss: 0.04330458492040634\n",
      "Iteration 337, Batch: 28, Loss: 0.05956178531050682\n",
      "Iteration 337, Batch: 29, Loss: 0.034005921334028244\n",
      "Iteration 337, Batch: 30, Loss: 0.06347423046827316\n",
      "Iteration 337, Batch: 31, Loss: 0.0653749480843544\n",
      "Iteration 337, Batch: 32, Loss: 0.04790972173213959\n",
      "Iteration 337, Batch: 33, Loss: 0.06607214361429214\n",
      "Iteration 337, Batch: 34, Loss: 0.03694792091846466\n",
      "Iteration 337, Batch: 35, Loss: 0.04390116408467293\n",
      "Iteration 337, Batch: 36, Loss: 0.045416831970214844\n",
      "Iteration 337, Batch: 37, Loss: 0.03294956684112549\n",
      "Iteration 337, Batch: 38, Loss: 0.05604669451713562\n",
      "Iteration 337, Batch: 39, Loss: 0.06125578656792641\n",
      "Iteration 337, Batch: 40, Loss: 0.03206577152013779\n",
      "Iteration 337, Batch: 41, Loss: 0.03950156271457672\n",
      "Iteration 337, Batch: 42, Loss: 0.05223336070775986\n",
      "Iteration 337, Batch: 43, Loss: 0.03580380231142044\n",
      "Iteration 337, Batch: 44, Loss: 0.05378474295139313\n",
      "Iteration 337, Batch: 45, Loss: 0.045445021241903305\n",
      "Iteration 337, Batch: 46, Loss: 0.040603891015052795\n",
      "Iteration 337, Batch: 47, Loss: 0.04700571298599243\n",
      "Iteration 337, Batch: 48, Loss: 0.04849861189723015\n",
      "Iteration 337, Batch: 49, Loss: 0.06002034991979599\n",
      "Number of layers: 10\n",
      "Iteration 338, Batch: 0, Loss: 0.0580272376537323\n",
      "Iteration 338, Batch: 1, Loss: 0.043916258960962296\n",
      "Iteration 338, Batch: 2, Loss: 0.026814093813300133\n",
      "Iteration 338, Batch: 3, Loss: 0.059883516281843185\n",
      "Iteration 338, Batch: 4, Loss: 0.08643224835395813\n",
      "Iteration 338, Batch: 5, Loss: 0.04104098305106163\n",
      "Iteration 338, Batch: 6, Loss: 0.03866664320230484\n",
      "Iteration 338, Batch: 7, Loss: 0.03192615881562233\n",
      "Iteration 338, Batch: 8, Loss: 0.0250740647315979\n",
      "Iteration 338, Batch: 9, Loss: 0.04579662159085274\n",
      "Iteration 338, Batch: 10, Loss: 0.042983971536159515\n",
      "Iteration 338, Batch: 11, Loss: 0.057061899453401566\n",
      "Iteration 338, Batch: 12, Loss: 0.06598085910081863\n",
      "Iteration 338, Batch: 13, Loss: 0.06424996256828308\n",
      "Iteration 338, Batch: 14, Loss: 0.048795755952596664\n",
      "Iteration 338, Batch: 15, Loss: 0.036890286952257156\n",
      "Iteration 338, Batch: 16, Loss: 0.056598205119371414\n",
      "Iteration 338, Batch: 17, Loss: 0.06915795803070068\n",
      "Iteration 338, Batch: 18, Loss: 0.04120370373129845\n",
      "Iteration 338, Batch: 19, Loss: 0.046471089124679565\n",
      "Iteration 338, Batch: 20, Loss: 0.02956482768058777\n",
      "Iteration 338, Batch: 21, Loss: 0.052393589168787\n",
      "Iteration 338, Batch: 22, Loss: 0.03813343867659569\n",
      "Iteration 338, Batch: 23, Loss: 0.03822385147213936\n",
      "Iteration 338, Batch: 24, Loss: 0.048552256077528\n",
      "Iteration 338, Batch: 25, Loss: 0.027870096266269684\n",
      "Iteration 338, Batch: 26, Loss: 0.08079670369625092\n",
      "Iteration 338, Batch: 27, Loss: 0.0418243445456028\n",
      "Iteration 338, Batch: 28, Loss: 0.083116814494133\n",
      "Iteration 338, Batch: 29, Loss: 0.050573255866765976\n",
      "Iteration 338, Batch: 30, Loss: 0.039756566286087036\n",
      "Iteration 338, Batch: 31, Loss: 0.04611511901021004\n",
      "Iteration 338, Batch: 32, Loss: 0.03059973195195198\n",
      "Iteration 338, Batch: 33, Loss: 0.059272024780511856\n",
      "Iteration 338, Batch: 34, Loss: 0.058721959590911865\n",
      "Iteration 338, Batch: 35, Loss: 0.06351835280656815\n",
      "Iteration 338, Batch: 36, Loss: 0.06265205889940262\n",
      "Iteration 338, Batch: 37, Loss: 0.06482996046543121\n",
      "Iteration 338, Batch: 38, Loss: 0.0830741673707962\n",
      "Iteration 338, Batch: 39, Loss: 0.04533310607075691\n",
      "Iteration 338, Batch: 40, Loss: 0.07804091274738312\n",
      "Iteration 338, Batch: 41, Loss: 0.09690091013908386\n",
      "Iteration 338, Batch: 42, Loss: 0.09789788722991943\n",
      "Iteration 338, Batch: 43, Loss: 0.06001710146665573\n",
      "Iteration 338, Batch: 44, Loss: 0.10024970769882202\n",
      "Iteration 338, Batch: 45, Loss: 0.06025321036577225\n",
      "Iteration 338, Batch: 46, Loss: 0.08138946443796158\n",
      "Iteration 338, Batch: 47, Loss: 0.08767177164554596\n",
      "Iteration 338, Batch: 48, Loss: 0.08243472874164581\n",
      "Iteration 338, Batch: 49, Loss: 0.04748111590743065\n",
      "Number of layers: 10\n",
      "Iteration 339, Batch: 0, Loss: 0.03947748616337776\n",
      "Iteration 339, Batch: 1, Loss: 0.0877225399017334\n",
      "Iteration 339, Batch: 2, Loss: 0.05837984010577202\n",
      "Iteration 339, Batch: 3, Loss: 0.04944482073187828\n",
      "Iteration 339, Batch: 4, Loss: 0.08778849989175797\n",
      "Iteration 339, Batch: 5, Loss: 0.0790671706199646\n",
      "Iteration 339, Batch: 6, Loss: 0.050397202372550964\n",
      "Iteration 339, Batch: 7, Loss: 0.08265861123800278\n",
      "Iteration 339, Batch: 8, Loss: 0.058897942304611206\n",
      "Iteration 339, Batch: 9, Loss: 0.07804295420646667\n",
      "Iteration 339, Batch: 10, Loss: 0.06760572642087936\n",
      "Iteration 339, Batch: 11, Loss: 0.05141840875148773\n",
      "Iteration 339, Batch: 12, Loss: 0.056120365858078\n",
      "Iteration 339, Batch: 13, Loss: 0.039314646273851395\n",
      "Iteration 339, Batch: 14, Loss: 0.0672551840543747\n",
      "Iteration 339, Batch: 15, Loss: 0.05047941952943802\n",
      "Iteration 339, Batch: 16, Loss: 0.05154922232031822\n",
      "Iteration 339, Batch: 17, Loss: 0.05957372486591339\n",
      "Iteration 339, Batch: 18, Loss: 0.05945518985390663\n",
      "Iteration 339, Batch: 19, Loss: 0.06189540773630142\n",
      "Iteration 339, Batch: 20, Loss: 0.07998236268758774\n",
      "Iteration 339, Batch: 21, Loss: 0.08760643750429153\n",
      "Iteration 339, Batch: 22, Loss: 0.06318725645542145\n",
      "Iteration 339, Batch: 23, Loss: 0.07889726758003235\n",
      "Iteration 339, Batch: 24, Loss: 0.054028186947107315\n",
      "Iteration 339, Batch: 25, Loss: 0.041341617703437805\n",
      "Iteration 339, Batch: 26, Loss: 0.05171755328774452\n",
      "Iteration 339, Batch: 27, Loss: 0.035332903265953064\n",
      "Iteration 339, Batch: 28, Loss: 0.036402516067028046\n",
      "Iteration 339, Batch: 29, Loss: 0.017497213557362556\n",
      "Iteration 339, Batch: 30, Loss: 0.04461734741926193\n",
      "Iteration 339, Batch: 31, Loss: 0.05532814934849739\n",
      "Iteration 339, Batch: 32, Loss: 0.04746127873659134\n",
      "Iteration 339, Batch: 33, Loss: 0.03304626792669296\n",
      "Iteration 339, Batch: 34, Loss: 0.06185496225953102\n",
      "Iteration 339, Batch: 35, Loss: 0.060567907989025116\n",
      "Iteration 339, Batch: 36, Loss: 0.07008321583271027\n",
      "Iteration 339, Batch: 37, Loss: 0.034136634320020676\n",
      "Iteration 339, Batch: 38, Loss: 0.032029736787080765\n",
      "Iteration 339, Batch: 39, Loss: 0.055114954710006714\n",
      "Iteration 339, Batch: 40, Loss: 0.05180467665195465\n",
      "Iteration 339, Batch: 41, Loss: 0.05666687712073326\n",
      "Iteration 339, Batch: 42, Loss: 0.05025655776262283\n",
      "Iteration 339, Batch: 43, Loss: 0.06127746403217316\n",
      "Iteration 339, Batch: 44, Loss: 0.05178914964199066\n",
      "Iteration 339, Batch: 45, Loss: 0.04079490154981613\n",
      "Iteration 339, Batch: 46, Loss: 0.04905886575579643\n",
      "Iteration 339, Batch: 47, Loss: 0.05746970698237419\n",
      "Iteration 339, Batch: 48, Loss: 0.04873277246952057\n",
      "Iteration 339, Batch: 49, Loss: 0.0640203207731247\n",
      "Number of layers: 10\n",
      "Iteration 340, Batch: 0, Loss: 0.04972115531563759\n",
      "Iteration 340, Batch: 1, Loss: 0.04622020572423935\n",
      "Iteration 340, Batch: 2, Loss: 0.051036279648542404\n",
      "Iteration 340, Batch: 3, Loss: 0.05513615906238556\n",
      "Iteration 340, Batch: 4, Loss: 0.04113514721393585\n",
      "Iteration 340, Batch: 5, Loss: 0.04961640387773514\n",
      "Iteration 340, Batch: 6, Loss: 0.033891137689352036\n",
      "Iteration 340, Batch: 7, Loss: 0.033183563500642776\n",
      "Iteration 340, Batch: 8, Loss: 0.06637348979711533\n",
      "Iteration 340, Batch: 9, Loss: 0.044759977608919144\n",
      "Iteration 340, Batch: 10, Loss: 0.05128658562898636\n",
      "Iteration 340, Batch: 11, Loss: 0.05947795882821083\n",
      "Iteration 340, Batch: 12, Loss: 0.032663505524396896\n",
      "Iteration 340, Batch: 13, Loss: 0.017000898718833923\n",
      "Iteration 340, Batch: 14, Loss: 0.052252549678087234\n",
      "Iteration 340, Batch: 15, Loss: 0.03118976019322872\n",
      "Iteration 340, Batch: 16, Loss: 0.024508466944098473\n",
      "Iteration 340, Batch: 17, Loss: 0.027611063793301582\n",
      "Iteration 340, Batch: 18, Loss: 0.024356713518500328\n",
      "Iteration 340, Batch: 19, Loss: 0.04340341314673424\n",
      "Iteration 340, Batch: 20, Loss: 0.04076575115323067\n",
      "Iteration 340, Batch: 21, Loss: 0.03872217610478401\n",
      "Iteration 340, Batch: 22, Loss: 0.03843804448843002\n",
      "Iteration 340, Batch: 23, Loss: 0.05124890059232712\n",
      "Iteration 340, Batch: 24, Loss: 0.054388053715229034\n",
      "Iteration 340, Batch: 25, Loss: 0.04181735962629318\n",
      "Iteration 340, Batch: 26, Loss: 0.03740968927741051\n",
      "Iteration 340, Batch: 27, Loss: 0.02148691564798355\n",
      "Iteration 340, Batch: 28, Loss: 0.03237886726856232\n",
      "Iteration 340, Batch: 29, Loss: 0.0315249040722847\n",
      "Iteration 340, Batch: 30, Loss: 0.03685184568166733\n",
      "Iteration 340, Batch: 31, Loss: 0.024467255920171738\n",
      "Iteration 340, Batch: 32, Loss: 0.044825535267591476\n",
      "Iteration 340, Batch: 33, Loss: 0.021944096311926842\n",
      "Iteration 340, Batch: 34, Loss: 0.038647715002298355\n",
      "Iteration 340, Batch: 35, Loss: 0.025654582306742668\n",
      "Iteration 340, Batch: 36, Loss: 0.04670613259077072\n",
      "Iteration 340, Batch: 37, Loss: 0.046109747141599655\n",
      "Iteration 340, Batch: 38, Loss: 0.048660192638635635\n",
      "Iteration 340, Batch: 39, Loss: 0.046634405851364136\n",
      "Iteration 340, Batch: 40, Loss: 0.06675019860267639\n",
      "Iteration 340, Batch: 41, Loss: 0.0440959632396698\n",
      "Iteration 340, Batch: 42, Loss: 0.03249504789710045\n",
      "Iteration 340, Batch: 43, Loss: 0.03800537809729576\n",
      "Iteration 340, Batch: 44, Loss: 0.059417180716991425\n",
      "Iteration 340, Batch: 45, Loss: 0.052964430302381516\n",
      "Iteration 340, Batch: 46, Loss: 0.05936016887426376\n",
      "Iteration 340, Batch: 47, Loss: 0.04055951163172722\n",
      "Iteration 340, Batch: 48, Loss: 0.04440280795097351\n",
      "Iteration 340, Batch: 49, Loss: 0.04942255839705467\n",
      "Number of layers: 10\n",
      "Iteration 341, Batch: 0, Loss: 0.0684501975774765\n",
      "Iteration 341, Batch: 1, Loss: 0.05224468186497688\n",
      "Iteration 341, Batch: 2, Loss: 0.06078158691525459\n",
      "Iteration 341, Batch: 3, Loss: 0.03782859072089195\n",
      "Iteration 341, Batch: 4, Loss: 0.04585439711809158\n",
      "Iteration 341, Batch: 5, Loss: 0.026510318741202354\n",
      "Iteration 341, Batch: 6, Loss: 0.07165224105119705\n",
      "Iteration 341, Batch: 7, Loss: 0.06620223075151443\n",
      "Iteration 341, Batch: 8, Loss: 0.0397530198097229\n",
      "Iteration 341, Batch: 9, Loss: 0.05781448259949684\n",
      "Iteration 341, Batch: 10, Loss: 0.030929330736398697\n",
      "Iteration 341, Batch: 11, Loss: 0.03680461645126343\n",
      "Iteration 341, Batch: 12, Loss: 0.045529745519161224\n",
      "Iteration 341, Batch: 13, Loss: 0.06705915927886963\n",
      "Iteration 341, Batch: 14, Loss: 0.06776385009288788\n",
      "Iteration 341, Batch: 15, Loss: 0.056972965598106384\n",
      "Iteration 341, Batch: 16, Loss: 0.04464386776089668\n",
      "Iteration 341, Batch: 17, Loss: 0.054200343787670135\n",
      "Iteration 341, Batch: 18, Loss: 0.03987448662519455\n",
      "Iteration 341, Batch: 19, Loss: 0.05059412866830826\n",
      "Iteration 341, Batch: 20, Loss: 0.036286361515522\n",
      "Iteration 341, Batch: 21, Loss: 0.06637480854988098\n",
      "Iteration 341, Batch: 22, Loss: 0.041260767728090286\n",
      "Iteration 341, Batch: 23, Loss: 0.06282582879066467\n",
      "Iteration 341, Batch: 24, Loss: 0.0493592731654644\n",
      "Iteration 341, Batch: 25, Loss: 0.050897788256406784\n",
      "Iteration 341, Batch: 26, Loss: 0.03865543007850647\n",
      "Iteration 341, Batch: 27, Loss: 0.05265287682414055\n",
      "Iteration 341, Batch: 28, Loss: 0.0393008217215538\n",
      "Iteration 341, Batch: 29, Loss: 0.03669877350330353\n",
      "Iteration 341, Batch: 30, Loss: 0.056650131940841675\n",
      "Iteration 341, Batch: 31, Loss: 0.026054956018924713\n",
      "Iteration 341, Batch: 32, Loss: 0.03579188138246536\n",
      "Iteration 341, Batch: 33, Loss: 0.04446544498205185\n",
      "Iteration 341, Batch: 34, Loss: 0.061002135276794434\n",
      "Iteration 341, Batch: 35, Loss: 0.05776397883892059\n",
      "Iteration 341, Batch: 36, Loss: 0.04701229929924011\n",
      "Iteration 341, Batch: 37, Loss: 0.05631520226597786\n",
      "Iteration 341, Batch: 38, Loss: 0.05515165254473686\n",
      "Iteration 341, Batch: 39, Loss: 0.06009400635957718\n",
      "Iteration 341, Batch: 40, Loss: 0.06173287332057953\n",
      "Iteration 341, Batch: 41, Loss: 0.059303902089595795\n",
      "Iteration 341, Batch: 42, Loss: 0.07600128650665283\n",
      "Iteration 341, Batch: 43, Loss: 0.046847715973854065\n",
      "Iteration 341, Batch: 44, Loss: 0.046907808631658554\n",
      "Iteration 341, Batch: 45, Loss: 0.04053666815161705\n",
      "Iteration 341, Batch: 46, Loss: 0.04160404950380325\n",
      "Iteration 341, Batch: 47, Loss: 0.062466204166412354\n",
      "Iteration 341, Batch: 48, Loss: 0.05021047592163086\n",
      "Iteration 341, Batch: 49, Loss: 0.07802725583314896\n",
      "Number of layers: 10\n",
      "Iteration 342, Batch: 0, Loss: 0.03565722331404686\n",
      "Iteration 342, Batch: 1, Loss: 0.06598597019910812\n",
      "Iteration 342, Batch: 2, Loss: 0.03712945803999901\n",
      "Iteration 342, Batch: 3, Loss: 0.032740652561187744\n",
      "Iteration 342, Batch: 4, Loss: 0.03725118190050125\n",
      "Iteration 342, Batch: 5, Loss: 0.05055949464440346\n",
      "Iteration 342, Batch: 6, Loss: 0.040035706013441086\n",
      "Iteration 342, Batch: 7, Loss: 0.0515027716755867\n",
      "Iteration 342, Batch: 8, Loss: 0.0491497740149498\n",
      "Iteration 342, Batch: 9, Loss: 0.042955730110406876\n",
      "Iteration 342, Batch: 10, Loss: 0.0271647647023201\n",
      "Iteration 342, Batch: 11, Loss: 0.03568314015865326\n",
      "Iteration 342, Batch: 12, Loss: 0.035930085927248\n",
      "Iteration 342, Batch: 13, Loss: 0.06991276890039444\n",
      "Iteration 342, Batch: 14, Loss: 0.015907984226942062\n",
      "Iteration 342, Batch: 15, Loss: 0.0573698990046978\n",
      "Iteration 342, Batch: 16, Loss: 0.06337279081344604\n",
      "Iteration 342, Batch: 17, Loss: 0.027912141755223274\n",
      "Iteration 342, Batch: 18, Loss: 0.04258676618337631\n",
      "Iteration 342, Batch: 19, Loss: 0.061973050236701965\n",
      "Iteration 342, Batch: 20, Loss: 0.05650646984577179\n",
      "Iteration 342, Batch: 21, Loss: 0.038709308952093124\n",
      "Iteration 342, Batch: 22, Loss: 0.052809953689575195\n",
      "Iteration 342, Batch: 23, Loss: 0.05309798941016197\n",
      "Iteration 342, Batch: 24, Loss: 0.03290089964866638\n",
      "Iteration 342, Batch: 25, Loss: 0.04026208817958832\n",
      "Iteration 342, Batch: 26, Loss: 0.041130322962999344\n",
      "Iteration 342, Batch: 27, Loss: 0.052795879542827606\n",
      "Iteration 342, Batch: 28, Loss: 0.01979067549109459\n",
      "Iteration 342, Batch: 29, Loss: 0.04749529808759689\n",
      "Iteration 342, Batch: 30, Loss: 0.05102415010333061\n",
      "Iteration 342, Batch: 31, Loss: 0.031543150544166565\n",
      "Iteration 342, Batch: 32, Loss: 0.04217468947172165\n",
      "Iteration 342, Batch: 33, Loss: 0.03800186887383461\n",
      "Iteration 342, Batch: 34, Loss: 0.038660336285829544\n",
      "Iteration 342, Batch: 35, Loss: 0.06574521213769913\n",
      "Iteration 342, Batch: 36, Loss: 0.03420525789260864\n",
      "Iteration 342, Batch: 37, Loss: 0.05710434168577194\n",
      "Iteration 342, Batch: 38, Loss: 0.06301800906658173\n",
      "Iteration 342, Batch: 39, Loss: 0.04885801300406456\n",
      "Iteration 342, Batch: 40, Loss: 0.029067248106002808\n",
      "Iteration 342, Batch: 41, Loss: 0.04863309860229492\n",
      "Iteration 342, Batch: 42, Loss: 0.06412851065397263\n",
      "Iteration 342, Batch: 43, Loss: 0.053259070962667465\n",
      "Iteration 342, Batch: 44, Loss: 0.04072216898202896\n",
      "Iteration 342, Batch: 45, Loss: 0.03842621669173241\n",
      "Iteration 342, Batch: 46, Loss: 0.052287571132183075\n",
      "Iteration 342, Batch: 47, Loss: 0.04255092144012451\n",
      "Iteration 342, Batch: 48, Loss: 0.07062187790870667\n",
      "Iteration 342, Batch: 49, Loss: 0.0543903149664402\n",
      "Number of layers: 10\n",
      "Iteration 343, Batch: 0, Loss: 0.12038779258728027\n",
      "Iteration 343, Batch: 1, Loss: 0.10605264455080032\n",
      "Iteration 343, Batch: 2, Loss: 0.03562197461724281\n",
      "Iteration 343, Batch: 3, Loss: 0.03971777483820915\n",
      "Iteration 343, Batch: 4, Loss: 0.054593220353126526\n",
      "Iteration 343, Batch: 5, Loss: 0.059203848242759705\n",
      "Iteration 343, Batch: 6, Loss: 0.08387897908687592\n",
      "Iteration 343, Batch: 7, Loss: 0.09225685149431229\n",
      "Iteration 343, Batch: 8, Loss: 0.06575409322977066\n",
      "Iteration 343, Batch: 9, Loss: 0.07826250791549683\n",
      "Iteration 343, Batch: 10, Loss: 0.05548310652375221\n",
      "Iteration 343, Batch: 11, Loss: 0.07697948068380356\n",
      "Iteration 343, Batch: 12, Loss: 0.06732606142759323\n",
      "Iteration 343, Batch: 13, Loss: 0.046388283371925354\n",
      "Iteration 343, Batch: 14, Loss: 0.0556706041097641\n",
      "Iteration 343, Batch: 15, Loss: 0.07180207967758179\n",
      "Iteration 343, Batch: 16, Loss: 0.05284656584262848\n",
      "Iteration 343, Batch: 17, Loss: 0.06178484484553337\n",
      "Iteration 343, Batch: 18, Loss: 0.05023420229554176\n",
      "Iteration 343, Batch: 19, Loss: 0.05156975984573364\n",
      "Iteration 343, Batch: 20, Loss: 0.03584155812859535\n",
      "Iteration 343, Batch: 21, Loss: 0.06430279463529587\n",
      "Iteration 343, Batch: 22, Loss: 0.05205121263861656\n",
      "Iteration 343, Batch: 23, Loss: 0.028401758521795273\n",
      "Iteration 343, Batch: 24, Loss: 0.04599900171160698\n",
      "Iteration 343, Batch: 25, Loss: 0.04117505997419357\n",
      "Iteration 343, Batch: 26, Loss: 0.049669623374938965\n",
      "Iteration 343, Batch: 27, Loss: 0.04492005333304405\n",
      "Iteration 343, Batch: 28, Loss: 0.06648174673318863\n",
      "Iteration 343, Batch: 29, Loss: 0.04966050013899803\n",
      "Iteration 343, Batch: 30, Loss: 0.03192894533276558\n",
      "Iteration 343, Batch: 31, Loss: 0.051126688718795776\n",
      "Iteration 343, Batch: 32, Loss: 0.04990138113498688\n",
      "Iteration 343, Batch: 33, Loss: 0.06179188936948776\n",
      "Iteration 343, Batch: 34, Loss: 0.04146904870867729\n",
      "Iteration 343, Batch: 35, Loss: 0.05473377928137779\n",
      "Iteration 343, Batch: 36, Loss: 0.049381691962480545\n",
      "Iteration 343, Batch: 37, Loss: 0.06328713893890381\n",
      "Iteration 343, Batch: 38, Loss: 0.05438297241926193\n",
      "Iteration 343, Batch: 39, Loss: 0.04170927777886391\n",
      "Iteration 343, Batch: 40, Loss: 0.025499608367681503\n",
      "Iteration 343, Batch: 41, Loss: 0.055466730147600174\n",
      "Iteration 343, Batch: 42, Loss: 0.04472418501973152\n",
      "Iteration 343, Batch: 43, Loss: 0.06651024520397186\n",
      "Iteration 343, Batch: 44, Loss: 0.04526356980204582\n",
      "Iteration 343, Batch: 45, Loss: 0.04607696086168289\n",
      "Iteration 343, Batch: 46, Loss: 0.058162789791822433\n",
      "Iteration 343, Batch: 47, Loss: 0.0585569329559803\n",
      "Iteration 343, Batch: 48, Loss: 0.03799990192055702\n",
      "Iteration 343, Batch: 49, Loss: 0.05209086462855339\n",
      "Number of layers: 10\n",
      "Iteration 344, Batch: 0, Loss: 0.05892448127269745\n",
      "Iteration 344, Batch: 1, Loss: 0.06572752445936203\n",
      "Iteration 344, Batch: 2, Loss: 0.030093060806393623\n",
      "Iteration 344, Batch: 3, Loss: 0.04520966485142708\n",
      "Iteration 344, Batch: 4, Loss: 0.050382453948259354\n",
      "Iteration 344, Batch: 5, Loss: 0.042519353330135345\n",
      "Iteration 344, Batch: 6, Loss: 0.04742971435189247\n",
      "Iteration 344, Batch: 7, Loss: 0.058058977127075195\n",
      "Iteration 344, Batch: 8, Loss: 0.05788427218794823\n",
      "Iteration 344, Batch: 9, Loss: 0.03694726899266243\n",
      "Iteration 344, Batch: 10, Loss: 0.047811295837163925\n",
      "Iteration 344, Batch: 11, Loss: 0.0529971569776535\n",
      "Iteration 344, Batch: 12, Loss: 0.055166080594062805\n",
      "Iteration 344, Batch: 13, Loss: 0.04379790276288986\n",
      "Iteration 344, Batch: 14, Loss: 0.04140954837203026\n",
      "Iteration 344, Batch: 15, Loss: 0.06972131878137589\n",
      "Iteration 344, Batch: 16, Loss: 0.05921490862965584\n",
      "Iteration 344, Batch: 17, Loss: 0.04925086349248886\n",
      "Iteration 344, Batch: 18, Loss: 0.058563362807035446\n",
      "Iteration 344, Batch: 19, Loss: 0.05251942202448845\n",
      "Iteration 344, Batch: 20, Loss: 0.07216528803110123\n",
      "Iteration 344, Batch: 21, Loss: 0.08369246125221252\n",
      "Iteration 344, Batch: 22, Loss: 0.09645042568445206\n",
      "Iteration 344, Batch: 23, Loss: 0.0793108195066452\n",
      "Iteration 344, Batch: 24, Loss: 0.06434528529644012\n",
      "Iteration 344, Batch: 25, Loss: 0.08608884364366531\n",
      "Iteration 344, Batch: 26, Loss: 0.05770981311798096\n",
      "Iteration 344, Batch: 27, Loss: 0.059295546263456345\n",
      "Iteration 344, Batch: 28, Loss: 0.05288250744342804\n",
      "Iteration 344, Batch: 29, Loss: 0.03734166547656059\n",
      "Iteration 344, Batch: 30, Loss: 0.04910371080040932\n",
      "Iteration 344, Batch: 31, Loss: 0.03973614051938057\n",
      "Iteration 344, Batch: 32, Loss: 0.056187838315963745\n",
      "Iteration 344, Batch: 33, Loss: 0.0485243946313858\n",
      "Iteration 344, Batch: 34, Loss: 0.04183315113186836\n",
      "Iteration 344, Batch: 35, Loss: 0.042911309748888016\n",
      "Iteration 344, Batch: 36, Loss: 0.058778032660484314\n",
      "Iteration 344, Batch: 37, Loss: 0.04877070337533951\n",
      "Iteration 344, Batch: 38, Loss: 0.030686477199196815\n",
      "Iteration 344, Batch: 39, Loss: 0.05408021807670593\n",
      "Iteration 344, Batch: 40, Loss: 0.05247427895665169\n",
      "Iteration 344, Batch: 41, Loss: 0.04732251912355423\n",
      "Iteration 344, Batch: 42, Loss: 0.04632142558693886\n",
      "Iteration 344, Batch: 43, Loss: 0.05679304525256157\n",
      "Iteration 344, Batch: 44, Loss: 0.06707419455051422\n",
      "Iteration 344, Batch: 45, Loss: 0.05523154139518738\n",
      "Iteration 344, Batch: 46, Loss: 0.03275763615965843\n",
      "Iteration 344, Batch: 47, Loss: 0.04901822283864021\n",
      "Iteration 344, Batch: 48, Loss: 0.05775155872106552\n",
      "Iteration 344, Batch: 49, Loss: 0.04128468036651611\n",
      "Number of layers: 10\n",
      "Iteration 345, Batch: 0, Loss: 0.040454231202602386\n",
      "Iteration 345, Batch: 1, Loss: 0.04276316240429878\n",
      "Iteration 345, Batch: 2, Loss: 0.037374358624219894\n",
      "Iteration 345, Batch: 3, Loss: 0.030666952952742577\n",
      "Iteration 345, Batch: 4, Loss: 0.04784223437309265\n",
      "Iteration 345, Batch: 5, Loss: 0.03859352320432663\n",
      "Iteration 345, Batch: 6, Loss: 0.05583488941192627\n",
      "Iteration 345, Batch: 7, Loss: 0.09399410337209702\n",
      "Iteration 345, Batch: 8, Loss: 0.049685876816511154\n",
      "Iteration 345, Batch: 9, Loss: 0.07138840109109879\n",
      "Iteration 345, Batch: 10, Loss: 0.08970270305871964\n",
      "Iteration 345, Batch: 11, Loss: 0.09746184945106506\n",
      "Iteration 345, Batch: 12, Loss: 0.10504883527755737\n",
      "Iteration 345, Batch: 13, Loss: 0.08442585915327072\n",
      "Iteration 345, Batch: 14, Loss: 0.046255435794591904\n",
      "Iteration 345, Batch: 15, Loss: 0.0703498050570488\n",
      "Iteration 345, Batch: 16, Loss: 0.10464110970497131\n",
      "Iteration 345, Batch: 17, Loss: 0.0975278839468956\n",
      "Iteration 345, Batch: 18, Loss: 0.09103778749704361\n",
      "Iteration 345, Batch: 19, Loss: 0.09443822503089905\n",
      "Iteration 345, Batch: 20, Loss: 0.053483132272958755\n",
      "Iteration 345, Batch: 21, Loss: 0.05622865632176399\n",
      "Iteration 345, Batch: 22, Loss: 0.08133149892091751\n",
      "Iteration 345, Batch: 23, Loss: 0.0561365932226181\n",
      "Iteration 345, Batch: 24, Loss: 0.061256978660821915\n",
      "Iteration 345, Batch: 25, Loss: 0.06607489287853241\n",
      "Iteration 345, Batch: 26, Loss: 0.0528319776058197\n",
      "Iteration 345, Batch: 27, Loss: 0.02998424880206585\n",
      "Iteration 345, Batch: 28, Loss: 0.0631476640701294\n",
      "Iteration 345, Batch: 29, Loss: 0.025993069633841515\n",
      "Iteration 345, Batch: 30, Loss: 0.03163512796163559\n",
      "Iteration 345, Batch: 31, Loss: 0.05284819379448891\n",
      "Iteration 345, Batch: 32, Loss: 0.03560251370072365\n",
      "Iteration 345, Batch: 33, Loss: 0.042976729571819305\n",
      "Iteration 345, Batch: 34, Loss: 0.033719487488269806\n",
      "Iteration 345, Batch: 35, Loss: 0.1271725744009018\n",
      "Iteration 345, Batch: 36, Loss: 0.3061573803424835\n",
      "Iteration 345, Batch: 37, Loss: 0.2077174186706543\n",
      "Iteration 345, Batch: 38, Loss: 0.10160764306783676\n",
      "Iteration 345, Batch: 39, Loss: 0.057445552200078964\n",
      "Iteration 345, Batch: 40, Loss: 0.03826771304011345\n",
      "Iteration 345, Batch: 41, Loss: 0.053079795092344284\n",
      "Iteration 345, Batch: 42, Loss: 0.047615278512239456\n",
      "Iteration 345, Batch: 43, Loss: 0.08366899937391281\n",
      "Iteration 345, Batch: 44, Loss: 0.12554903328418732\n",
      "Iteration 345, Batch: 45, Loss: 0.11375093460083008\n",
      "Iteration 345, Batch: 46, Loss: 0.10275782644748688\n",
      "Iteration 345, Batch: 47, Loss: 0.09491201490163803\n",
      "Iteration 345, Batch: 48, Loss: 0.02617000602185726\n",
      "Iteration 345, Batch: 49, Loss: 0.01688150316476822\n",
      "Number of layers: 10\n",
      "Iteration 346, Batch: 0, Loss: 0.02940402366220951\n",
      "Iteration 346, Batch: 1, Loss: 0.04304279386997223\n",
      "Iteration 346, Batch: 2, Loss: 0.0328083373606205\n",
      "Iteration 346, Batch: 3, Loss: 0.03726991266012192\n",
      "Iteration 346, Batch: 4, Loss: 0.03816716745495796\n",
      "Iteration 346, Batch: 5, Loss: 0.02874988503754139\n",
      "Iteration 346, Batch: 6, Loss: 0.04858633875846863\n",
      "Iteration 346, Batch: 7, Loss: 0.04671279713511467\n",
      "Iteration 346, Batch: 8, Loss: 0.03354651480913162\n",
      "Iteration 346, Batch: 9, Loss: 0.06337461620569229\n",
      "Iteration 346, Batch: 10, Loss: 0.04007760435342789\n",
      "Iteration 346, Batch: 11, Loss: 0.047857291996479034\n",
      "Iteration 346, Batch: 12, Loss: 0.03460000455379486\n",
      "Iteration 346, Batch: 13, Loss: 0.041282810270786285\n",
      "Iteration 346, Batch: 14, Loss: 0.03222762420773506\n",
      "Iteration 346, Batch: 15, Loss: 0.03277403488755226\n",
      "Iteration 346, Batch: 16, Loss: 0.054604753851890564\n",
      "Iteration 346, Batch: 17, Loss: 0.03451341390609741\n",
      "Iteration 346, Batch: 18, Loss: 0.047461479902267456\n",
      "Iteration 346, Batch: 19, Loss: 0.04225963354110718\n",
      "Iteration 346, Batch: 20, Loss: 0.020100731402635574\n",
      "Iteration 346, Batch: 21, Loss: 0.03284940496087074\n",
      "Iteration 346, Batch: 22, Loss: 0.046039991080760956\n",
      "Iteration 346, Batch: 23, Loss: 0.048956479877233505\n",
      "Iteration 346, Batch: 24, Loss: 0.02400772087275982\n",
      "Iteration 346, Batch: 25, Loss: 0.0513104610145092\n",
      "Iteration 346, Batch: 26, Loss: 0.043457869440317154\n",
      "Iteration 346, Batch: 27, Loss: 0.050321463495492935\n",
      "Iteration 346, Batch: 28, Loss: 0.057503592222929\n",
      "Iteration 346, Batch: 29, Loss: 0.04177626594901085\n",
      "Iteration 346, Batch: 30, Loss: 0.05279358848929405\n",
      "Iteration 346, Batch: 31, Loss: 0.056458838284015656\n",
      "Iteration 346, Batch: 32, Loss: 0.05806618556380272\n",
      "Iteration 346, Batch: 33, Loss: 0.021215194836258888\n",
      "Iteration 346, Batch: 34, Loss: 0.050204262137413025\n",
      "Iteration 346, Batch: 35, Loss: 0.07021822780370712\n",
      "Iteration 346, Batch: 36, Loss: 0.051204994320869446\n",
      "Iteration 346, Batch: 37, Loss: 0.06749448925256729\n",
      "Iteration 346, Batch: 38, Loss: 0.04609130322933197\n",
      "Iteration 346, Batch: 39, Loss: 0.06602318584918976\n",
      "Iteration 346, Batch: 40, Loss: 0.06689170747995377\n",
      "Iteration 346, Batch: 41, Loss: 0.05767381936311722\n",
      "Iteration 346, Batch: 42, Loss: 0.062362659722566605\n",
      "Iteration 346, Batch: 43, Loss: 0.06674446165561676\n",
      "Iteration 346, Batch: 44, Loss: 0.058549318462610245\n",
      "Iteration 346, Batch: 45, Loss: 0.07703479379415512\n",
      "Iteration 346, Batch: 46, Loss: 0.054771408438682556\n",
      "Iteration 346, Batch: 47, Loss: 0.05036448314785957\n",
      "Iteration 346, Batch: 48, Loss: 0.044739194214344025\n",
      "Iteration 346, Batch: 49, Loss: 0.03559252247214317\n",
      "Number of layers: 10\n",
      "Iteration 347, Batch: 0, Loss: 0.06899566203355789\n",
      "Iteration 347, Batch: 1, Loss: 0.02906911075115204\n",
      "Iteration 347, Batch: 2, Loss: 0.0452810563147068\n",
      "Iteration 347, Batch: 3, Loss: 0.04304119572043419\n",
      "Iteration 347, Batch: 4, Loss: 0.044860437512397766\n",
      "Iteration 347, Batch: 5, Loss: 0.029018491506576538\n",
      "Iteration 347, Batch: 6, Loss: 0.05018095299601555\n",
      "Iteration 347, Batch: 7, Loss: 0.06336555629968643\n",
      "Iteration 347, Batch: 8, Loss: 0.0416349396109581\n",
      "Iteration 347, Batch: 9, Loss: 0.06146039441227913\n",
      "Iteration 347, Batch: 10, Loss: 0.04869510605931282\n",
      "Iteration 347, Batch: 11, Loss: 0.04302462562918663\n",
      "Iteration 347, Batch: 12, Loss: 0.02307451143860817\n",
      "Iteration 347, Batch: 13, Loss: 0.04785247519612312\n",
      "Iteration 347, Batch: 14, Loss: 0.04030182585120201\n",
      "Iteration 347, Batch: 15, Loss: 0.03808444365859032\n",
      "Iteration 347, Batch: 16, Loss: 0.041490163654088974\n",
      "Iteration 347, Batch: 17, Loss: 0.03706732764840126\n",
      "Iteration 347, Batch: 18, Loss: 0.04115975275635719\n",
      "Iteration 347, Batch: 19, Loss: 0.05968332290649414\n",
      "Iteration 347, Batch: 20, Loss: 0.02631833404302597\n",
      "Iteration 347, Batch: 21, Loss: 0.05440923944115639\n",
      "Iteration 347, Batch: 22, Loss: 0.04801657423377037\n",
      "Iteration 347, Batch: 23, Loss: 0.035720400512218475\n",
      "Iteration 347, Batch: 24, Loss: 0.057169780135154724\n",
      "Iteration 347, Batch: 25, Loss: 0.024462291970849037\n",
      "Iteration 347, Batch: 26, Loss: 0.04070639610290527\n",
      "Iteration 347, Batch: 27, Loss: 0.06361868977546692\n",
      "Iteration 347, Batch: 28, Loss: 0.03498649224638939\n",
      "Iteration 347, Batch: 29, Loss: 0.015226896852254868\n",
      "Iteration 347, Batch: 30, Loss: 0.04069584608078003\n",
      "Iteration 347, Batch: 31, Loss: 0.029267575591802597\n",
      "Iteration 347, Batch: 32, Loss: 0.02746504731476307\n",
      "Iteration 347, Batch: 33, Loss: 0.030049044638872147\n",
      "Iteration 347, Batch: 34, Loss: 0.020991718396544456\n",
      "Iteration 347, Batch: 35, Loss: 0.04046320170164108\n",
      "Iteration 347, Batch: 36, Loss: 0.05836008861660957\n",
      "Iteration 347, Batch: 37, Loss: 0.041608989238739014\n",
      "Iteration 347, Batch: 38, Loss: 0.028136547654867172\n",
      "Iteration 347, Batch: 39, Loss: 0.04156138747930527\n",
      "Iteration 347, Batch: 40, Loss: 0.04521365091204643\n",
      "Iteration 347, Batch: 41, Loss: 0.03572486713528633\n",
      "Iteration 347, Batch: 42, Loss: 0.04714898392558098\n",
      "Iteration 347, Batch: 43, Loss: 0.02707548625767231\n",
      "Iteration 347, Batch: 44, Loss: 0.029153479263186455\n",
      "Iteration 347, Batch: 45, Loss: 0.026696929708123207\n",
      "Iteration 347, Batch: 46, Loss: 0.027700895443558693\n",
      "Iteration 347, Batch: 47, Loss: 0.03128768503665924\n",
      "Iteration 347, Batch: 48, Loss: 0.055481597781181335\n",
      "Iteration 347, Batch: 49, Loss: 0.06304016709327698\n",
      "Number of layers: 10\n",
      "Iteration 348, Batch: 0, Loss: 0.06271447986364365\n",
      "Iteration 348, Batch: 1, Loss: 0.04439208284020424\n",
      "Iteration 348, Batch: 2, Loss: 0.0550687350332737\n",
      "Iteration 348, Batch: 3, Loss: 0.05406147241592407\n",
      "Iteration 348, Batch: 4, Loss: 0.0649302527308464\n",
      "Iteration 348, Batch: 5, Loss: 0.06899583339691162\n",
      "Iteration 348, Batch: 6, Loss: 0.030263518914580345\n",
      "Iteration 348, Batch: 7, Loss: 0.06115652993321419\n",
      "Iteration 348, Batch: 8, Loss: 0.03658030927181244\n",
      "Iteration 348, Batch: 9, Loss: 0.030117463320493698\n",
      "Iteration 348, Batch: 10, Loss: 0.054999615997076035\n",
      "Iteration 348, Batch: 11, Loss: 0.05911847949028015\n",
      "Iteration 348, Batch: 12, Loss: 0.03513069450855255\n",
      "Iteration 348, Batch: 13, Loss: 0.03376196324825287\n",
      "Iteration 348, Batch: 14, Loss: 0.02841249667108059\n",
      "Iteration 348, Batch: 15, Loss: 0.046613819897174835\n",
      "Iteration 348, Batch: 16, Loss: 0.020804090425372124\n",
      "Iteration 348, Batch: 17, Loss: 0.03542312607169151\n",
      "Iteration 348, Batch: 18, Loss: 0.03525586053729057\n",
      "Iteration 348, Batch: 19, Loss: 0.030274523422122\n",
      "Iteration 348, Batch: 20, Loss: 0.04996683821082115\n",
      "Iteration 348, Batch: 21, Loss: 0.05676540732383728\n",
      "Iteration 348, Batch: 22, Loss: 0.07011216878890991\n",
      "Iteration 348, Batch: 23, Loss: 0.07182885706424713\n",
      "Iteration 348, Batch: 24, Loss: 0.04439540579915047\n",
      "Iteration 348, Batch: 25, Loss: 0.040259502828121185\n",
      "Iteration 348, Batch: 26, Loss: 0.035116568207740784\n",
      "Iteration 348, Batch: 27, Loss: 0.05025486275553703\n",
      "Iteration 348, Batch: 28, Loss: 0.057006653398275375\n",
      "Iteration 348, Batch: 29, Loss: 0.042197417467832565\n",
      "Iteration 348, Batch: 30, Loss: 0.044436462223529816\n",
      "Iteration 348, Batch: 31, Loss: 0.04821094870567322\n",
      "Iteration 348, Batch: 32, Loss: 0.0370967872440815\n",
      "Iteration 348, Batch: 33, Loss: 0.049334567040205\n",
      "Iteration 348, Batch: 34, Loss: 0.03310307487845421\n",
      "Iteration 348, Batch: 35, Loss: 0.039079632610082626\n",
      "Iteration 348, Batch: 36, Loss: 0.04414758086204529\n",
      "Iteration 348, Batch: 37, Loss: 0.061150211840867996\n",
      "Iteration 348, Batch: 38, Loss: 0.054332345724105835\n",
      "Iteration 348, Batch: 39, Loss: 0.052141234278678894\n",
      "Iteration 348, Batch: 40, Loss: 0.03200490027666092\n",
      "Iteration 348, Batch: 41, Loss: 0.04293394088745117\n",
      "Iteration 348, Batch: 42, Loss: 0.05361364409327507\n",
      "Iteration 348, Batch: 43, Loss: 0.03019719570875168\n",
      "Iteration 348, Batch: 44, Loss: 0.04250866919755936\n",
      "Iteration 348, Batch: 45, Loss: 0.04831636697053909\n",
      "Iteration 348, Batch: 46, Loss: 0.04995961859822273\n",
      "Iteration 348, Batch: 47, Loss: 0.04256543889641762\n",
      "Iteration 348, Batch: 48, Loss: 0.04634789377450943\n",
      "Iteration 348, Batch: 49, Loss: 0.054831426590681076\n",
      "Number of layers: 10\n",
      "Iteration 349, Batch: 0, Loss: 0.05049119517207146\n",
      "Iteration 349, Batch: 1, Loss: 0.0775894746184349\n",
      "Iteration 349, Batch: 2, Loss: 0.04951541870832443\n",
      "Iteration 349, Batch: 3, Loss: 0.05191902071237564\n",
      "Iteration 349, Batch: 4, Loss: 0.02991657890379429\n",
      "Iteration 349, Batch: 5, Loss: 0.04722358658909798\n",
      "Iteration 349, Batch: 6, Loss: 0.051650531589984894\n",
      "Iteration 349, Batch: 7, Loss: 0.035151802003383636\n",
      "Iteration 349, Batch: 8, Loss: 0.02702205628156662\n",
      "Iteration 349, Batch: 9, Loss: 0.02960815653204918\n",
      "Iteration 349, Batch: 10, Loss: 0.03179922327399254\n",
      "Iteration 349, Batch: 11, Loss: 0.020080741494894028\n",
      "Iteration 349, Batch: 12, Loss: 0.05353543162345886\n",
      "Iteration 349, Batch: 13, Loss: 0.04843778535723686\n",
      "Iteration 349, Batch: 14, Loss: 0.06979440897703171\n",
      "Iteration 349, Batch: 15, Loss: 0.04891234263777733\n",
      "Iteration 349, Batch: 16, Loss: 0.040619466453790665\n",
      "Iteration 349, Batch: 17, Loss: 0.04146559163928032\n",
      "Iteration 349, Batch: 18, Loss: 0.025048863142728806\n",
      "Iteration 349, Batch: 19, Loss: 0.04097548499703407\n",
      "Iteration 349, Batch: 20, Loss: 0.05376218631863594\n",
      "Iteration 349, Batch: 21, Loss: 0.03962527960538864\n",
      "Iteration 349, Batch: 22, Loss: 0.05018623545765877\n",
      "Iteration 349, Batch: 23, Loss: 0.05501232668757439\n",
      "Iteration 349, Batch: 24, Loss: 0.06091899797320366\n",
      "Iteration 349, Batch: 25, Loss: 0.034939661622047424\n",
      "Iteration 349, Batch: 26, Loss: 0.027181051671504974\n",
      "Iteration 349, Batch: 27, Loss: 0.03653700649738312\n",
      "Iteration 349, Batch: 28, Loss: 0.028254253789782524\n",
      "Iteration 349, Batch: 29, Loss: 0.046764571219682693\n",
      "Iteration 349, Batch: 30, Loss: 0.0415094830095768\n",
      "Iteration 349, Batch: 31, Loss: 0.042798060923814774\n",
      "Iteration 349, Batch: 32, Loss: 0.05208930745720863\n",
      "Iteration 349, Batch: 33, Loss: 0.038656461983919144\n",
      "Iteration 349, Batch: 34, Loss: 0.03415106609463692\n",
      "Iteration 349, Batch: 35, Loss: 0.04340340197086334\n",
      "Iteration 349, Batch: 36, Loss: 0.04123424366116524\n",
      "Iteration 349, Batch: 37, Loss: 0.018973087891936302\n",
      "Iteration 349, Batch: 38, Loss: 0.04036038741469383\n",
      "Iteration 349, Batch: 39, Loss: 0.0633254200220108\n",
      "Iteration 349, Batch: 40, Loss: 0.048240210860967636\n",
      "Iteration 349, Batch: 41, Loss: 0.02062353305518627\n",
      "Iteration 349, Batch: 42, Loss: 0.047137338668107986\n",
      "Iteration 349, Batch: 43, Loss: 0.057666976004838943\n",
      "Iteration 349, Batch: 44, Loss: 0.028107937425374985\n",
      "Iteration 349, Batch: 45, Loss: 0.03991677612066269\n",
      "Iteration 349, Batch: 46, Loss: 0.031393542885780334\n",
      "Iteration 349, Batch: 47, Loss: 0.05144404619932175\n",
      "Iteration 349, Batch: 48, Loss: 0.04574280604720116\n",
      "Iteration 349, Batch: 49, Loss: 0.050824232399463654\n",
      "Number of layers: 10\n",
      "Iteration 350, Batch: 0, Loss: 0.037521980702877045\n",
      "Iteration 350, Batch: 1, Loss: 0.03714466467499733\n",
      "Iteration 350, Batch: 2, Loss: 0.040302734822034836\n",
      "Iteration 350, Batch: 3, Loss: 0.05714111030101776\n",
      "Iteration 350, Batch: 4, Loss: 0.052331723272800446\n",
      "Iteration 350, Batch: 5, Loss: 0.04659297689795494\n",
      "Iteration 350, Batch: 6, Loss: 0.022233912721276283\n",
      "Iteration 350, Batch: 7, Loss: 0.038763001561164856\n",
      "Iteration 350, Batch: 8, Loss: 0.02333511784672737\n",
      "Iteration 350, Batch: 9, Loss: 0.029231242835521698\n",
      "Iteration 350, Batch: 10, Loss: 0.045976851135492325\n",
      "Iteration 350, Batch: 11, Loss: 0.03704311326146126\n",
      "Iteration 350, Batch: 12, Loss: 0.03461818769574165\n",
      "Iteration 350, Batch: 13, Loss: 0.041404303163290024\n",
      "Iteration 350, Batch: 14, Loss: 0.03698255121707916\n",
      "Iteration 350, Batch: 15, Loss: 0.05948954448103905\n",
      "Iteration 350, Batch: 16, Loss: 0.04452444985508919\n",
      "Iteration 350, Batch: 17, Loss: 0.051107220351696014\n",
      "Iteration 350, Batch: 18, Loss: 0.05278214439749718\n",
      "Iteration 350, Batch: 19, Loss: 0.057971809059381485\n",
      "Iteration 350, Batch: 20, Loss: 0.042859334498643875\n",
      "Iteration 350, Batch: 21, Loss: 0.04242216795682907\n",
      "Iteration 350, Batch: 22, Loss: 0.05220037326216698\n",
      "Iteration 350, Batch: 23, Loss: 0.04499015584588051\n",
      "Iteration 350, Batch: 24, Loss: 0.050950005650520325\n",
      "Iteration 350, Batch: 25, Loss: 0.07422899454832077\n",
      "Iteration 350, Batch: 26, Loss: 0.08940606564283371\n",
      "Iteration 350, Batch: 27, Loss: 0.06375691294670105\n",
      "Iteration 350, Batch: 28, Loss: 0.041527051478624344\n",
      "Iteration 350, Batch: 29, Loss: 0.0498456247150898\n",
      "Iteration 350, Batch: 30, Loss: 0.05306948721408844\n",
      "Iteration 350, Batch: 31, Loss: 0.03274029120802879\n",
      "Iteration 350, Batch: 32, Loss: 0.03289303928613663\n",
      "Iteration 350, Batch: 33, Loss: 0.032621219754219055\n",
      "Iteration 350, Batch: 34, Loss: 0.027203261852264404\n",
      "Iteration 350, Batch: 35, Loss: 0.03353356570005417\n",
      "Iteration 350, Batch: 36, Loss: 0.06989601254463196\n",
      "Iteration 350, Batch: 37, Loss: 0.0371425524353981\n",
      "Iteration 350, Batch: 38, Loss: 0.034394770860672\n",
      "Iteration 350, Batch: 39, Loss: 0.06086735427379608\n",
      "Iteration 350, Batch: 40, Loss: 0.027954360470175743\n",
      "Iteration 350, Batch: 41, Loss: 0.01797644980251789\n",
      "Iteration 350, Batch: 42, Loss: 0.03966667875647545\n",
      "Iteration 350, Batch: 43, Loss: 0.06390036642551422\n",
      "Iteration 350, Batch: 44, Loss: 0.04017101600766182\n",
      "Iteration 350, Batch: 45, Loss: 0.040179334580898285\n",
      "Iteration 350, Batch: 46, Loss: 0.02989453636109829\n",
      "Iteration 350, Batch: 47, Loss: 0.040682725608348846\n",
      "Iteration 350, Batch: 48, Loss: 0.03152414411306381\n",
      "Iteration 350, Batch: 49, Loss: 0.0262693390250206\n",
      "Number of layers: 10\n",
      "Iteration 351, Batch: 0, Loss: 0.053705018013715744\n",
      "Iteration 351, Batch: 1, Loss: 0.02760815992951393\n",
      "Iteration 351, Batch: 2, Loss: 0.05410684645175934\n",
      "Iteration 351, Batch: 3, Loss: 0.07165359705686569\n",
      "Iteration 351, Batch: 4, Loss: 0.028819775208830833\n",
      "Iteration 351, Batch: 5, Loss: 0.05361979454755783\n",
      "Iteration 351, Batch: 6, Loss: 0.04344797134399414\n",
      "Iteration 351, Batch: 7, Loss: 0.048443134874105453\n",
      "Iteration 351, Batch: 8, Loss: 0.06139086186885834\n",
      "Iteration 351, Batch: 9, Loss: 0.06140054389834404\n",
      "Iteration 351, Batch: 10, Loss: 0.027336429804563522\n",
      "Iteration 351, Batch: 11, Loss: 0.030983632430434227\n",
      "Iteration 351, Batch: 12, Loss: 0.04118787497282028\n",
      "Iteration 351, Batch: 13, Loss: 0.08279240131378174\n",
      "Iteration 351, Batch: 14, Loss: 0.04660145565867424\n",
      "Iteration 351, Batch: 15, Loss: 0.05948023125529289\n",
      "Iteration 351, Batch: 16, Loss: 0.053090132772922516\n",
      "Iteration 351, Batch: 17, Loss: 0.04753248021006584\n",
      "Iteration 351, Batch: 18, Loss: 0.06167616695165634\n",
      "Iteration 351, Batch: 19, Loss: 0.07370000332593918\n",
      "Iteration 351, Batch: 20, Loss: 0.05708756297826767\n",
      "Iteration 351, Batch: 21, Loss: 0.03941959887742996\n",
      "Iteration 351, Batch: 22, Loss: 0.03068951703608036\n",
      "Iteration 351, Batch: 23, Loss: 0.028737489134073257\n",
      "Iteration 351, Batch: 24, Loss: 0.04247628524899483\n",
      "Iteration 351, Batch: 25, Loss: 0.03485766425728798\n",
      "Iteration 351, Batch: 26, Loss: 0.05115089192986488\n",
      "Iteration 351, Batch: 27, Loss: 0.06898874789476395\n",
      "Iteration 351, Batch: 28, Loss: 0.057365722954273224\n",
      "Iteration 351, Batch: 29, Loss: 0.06774958223104477\n",
      "Iteration 351, Batch: 30, Loss: 0.0426449291408062\n",
      "Iteration 351, Batch: 31, Loss: 0.019974226132035255\n",
      "Iteration 351, Batch: 32, Loss: 0.05940541252493858\n",
      "Iteration 351, Batch: 33, Loss: 0.03907132148742676\n",
      "Iteration 351, Batch: 34, Loss: 0.040046777576208115\n",
      "Iteration 351, Batch: 35, Loss: 0.05833010375499725\n",
      "Iteration 351, Batch: 36, Loss: 0.05563366785645485\n",
      "Iteration 351, Batch: 37, Loss: 0.01916392706334591\n",
      "Iteration 351, Batch: 38, Loss: 0.02260914258658886\n",
      "Iteration 351, Batch: 39, Loss: 0.03270608186721802\n",
      "Iteration 351, Batch: 40, Loss: 0.04004581272602081\n",
      "Iteration 351, Batch: 41, Loss: 0.03147808834910393\n",
      "Iteration 351, Batch: 42, Loss: 0.07307859510183334\n",
      "Iteration 351, Batch: 43, Loss: 0.05378486588597298\n",
      "Iteration 351, Batch: 44, Loss: 0.0342843160033226\n",
      "Iteration 351, Batch: 45, Loss: 0.06598442792892456\n",
      "Iteration 351, Batch: 46, Loss: 0.020867297425866127\n",
      "Iteration 351, Batch: 47, Loss: 0.04317198693752289\n",
      "Iteration 351, Batch: 48, Loss: 0.03968387097120285\n",
      "Iteration 351, Batch: 49, Loss: 0.051698487251996994\n",
      "Number of layers: 10\n",
      "Iteration 352, Batch: 0, Loss: 0.05975242331624031\n",
      "Iteration 352, Batch: 1, Loss: 0.052905257791280746\n",
      "Iteration 352, Batch: 2, Loss: 0.03184996172785759\n",
      "Iteration 352, Batch: 3, Loss: 0.06636960804462433\n",
      "Iteration 352, Batch: 4, Loss: 0.029395515099167824\n",
      "Iteration 352, Batch: 5, Loss: 0.07983707636594772\n",
      "Iteration 352, Batch: 6, Loss: 0.052282579243183136\n",
      "Iteration 352, Batch: 7, Loss: 0.05710012465715408\n",
      "Iteration 352, Batch: 8, Loss: 0.029552528634667397\n",
      "Iteration 352, Batch: 9, Loss: 0.03445687144994736\n",
      "Iteration 352, Batch: 10, Loss: 0.04853561148047447\n",
      "Iteration 352, Batch: 11, Loss: 0.034663911908864975\n",
      "Iteration 352, Batch: 12, Loss: 0.04013082757592201\n",
      "Iteration 352, Batch: 13, Loss: 0.036100562661886215\n",
      "Iteration 352, Batch: 14, Loss: 0.042383164167404175\n",
      "Iteration 352, Batch: 15, Loss: 0.051902540028095245\n",
      "Iteration 352, Batch: 16, Loss: 0.05388065055012703\n",
      "Iteration 352, Batch: 17, Loss: 0.027403397485613823\n",
      "Iteration 352, Batch: 18, Loss: 0.05122711509466171\n",
      "Iteration 352, Batch: 19, Loss: 0.03230912610888481\n",
      "Iteration 352, Batch: 20, Loss: 0.026850657537579536\n",
      "Iteration 352, Batch: 21, Loss: 0.03780984506011009\n",
      "Iteration 352, Batch: 22, Loss: 0.057030580937862396\n",
      "Iteration 352, Batch: 23, Loss: 0.040401577949523926\n",
      "Iteration 352, Batch: 24, Loss: 0.026939881965517998\n",
      "Iteration 352, Batch: 25, Loss: 0.04640018939971924\n",
      "Iteration 352, Batch: 26, Loss: 0.03520267456769943\n",
      "Iteration 352, Batch: 27, Loss: 0.04959425702691078\n",
      "Iteration 352, Batch: 28, Loss: 0.03440423309803009\n",
      "Iteration 352, Batch: 29, Loss: 0.052190378308296204\n",
      "Iteration 352, Batch: 30, Loss: 0.04762062802910805\n",
      "Iteration 352, Batch: 31, Loss: 0.034929197281599045\n",
      "Iteration 352, Batch: 32, Loss: 0.01744581200182438\n",
      "Iteration 352, Batch: 33, Loss: 0.029248055070638657\n",
      "Iteration 352, Batch: 34, Loss: 0.04928519204258919\n",
      "Iteration 352, Batch: 35, Loss: 0.04463925212621689\n",
      "Iteration 352, Batch: 36, Loss: 0.03314930945634842\n",
      "Iteration 352, Batch: 37, Loss: 0.05043334886431694\n",
      "Iteration 352, Batch: 38, Loss: 0.023270215839147568\n",
      "Iteration 352, Batch: 39, Loss: 0.042476434260606766\n",
      "Iteration 352, Batch: 40, Loss: 0.04726957157254219\n",
      "Iteration 352, Batch: 41, Loss: 0.04641026258468628\n",
      "Iteration 352, Batch: 42, Loss: 0.029839174821972847\n",
      "Iteration 352, Batch: 43, Loss: 0.026542458683252335\n",
      "Iteration 352, Batch: 44, Loss: 0.03186419978737831\n",
      "Iteration 352, Batch: 45, Loss: 0.061879146844148636\n",
      "Iteration 352, Batch: 46, Loss: 0.026581963524222374\n",
      "Iteration 352, Batch: 47, Loss: 0.03911738097667694\n",
      "Iteration 352, Batch: 48, Loss: 0.03153853490948677\n",
      "Iteration 352, Batch: 49, Loss: 0.0471748448908329\n",
      "Number of layers: 10\n",
      "Iteration 353, Batch: 0, Loss: 0.024723244830965996\n",
      "Iteration 353, Batch: 1, Loss: 0.0751270279288292\n",
      "Iteration 353, Batch: 2, Loss: 0.06034664064645767\n",
      "Iteration 353, Batch: 3, Loss: 0.06724941730499268\n",
      "Iteration 353, Batch: 4, Loss: 0.04086916893720627\n",
      "Iteration 353, Batch: 5, Loss: 0.03412520885467529\n",
      "Iteration 353, Batch: 6, Loss: 0.031921129673719406\n",
      "Iteration 353, Batch: 7, Loss: 0.047722019255161285\n",
      "Iteration 353, Batch: 8, Loss: 0.10623332858085632\n",
      "Iteration 353, Batch: 9, Loss: 0.15492090582847595\n",
      "Iteration 353, Batch: 10, Loss: 0.1225728765130043\n",
      "Iteration 353, Batch: 11, Loss: 0.07727500796318054\n",
      "Iteration 353, Batch: 12, Loss: 0.1173316165804863\n",
      "Iteration 353, Batch: 13, Loss: 0.035331424325704575\n",
      "Iteration 353, Batch: 14, Loss: 0.07399165630340576\n",
      "Iteration 353, Batch: 15, Loss: 0.036397021263837814\n",
      "Iteration 353, Batch: 16, Loss: 0.05876449868083\n",
      "Iteration 353, Batch: 17, Loss: 0.05028630048036575\n",
      "Iteration 353, Batch: 18, Loss: 0.056109718978405\n",
      "Iteration 353, Batch: 19, Loss: 0.05107811838388443\n",
      "Iteration 353, Batch: 20, Loss: 0.049017131328582764\n",
      "Iteration 353, Batch: 21, Loss: 0.06347429007291794\n",
      "Iteration 353, Batch: 22, Loss: 0.06846597790718079\n",
      "Iteration 353, Batch: 23, Loss: 0.059084851294755936\n",
      "Iteration 353, Batch: 24, Loss: 0.03676272928714752\n",
      "Iteration 353, Batch: 25, Loss: 0.021016813814640045\n",
      "Iteration 353, Batch: 26, Loss: 0.027719737961888313\n",
      "Iteration 353, Batch: 27, Loss: 0.06649325042963028\n",
      "Iteration 353, Batch: 28, Loss: 0.030747419223189354\n",
      "Iteration 353, Batch: 29, Loss: 0.042972564697265625\n",
      "Iteration 353, Batch: 30, Loss: 0.04082416370511055\n",
      "Iteration 353, Batch: 31, Loss: 0.056386783719062805\n",
      "Iteration 353, Batch: 32, Loss: 0.06381335854530334\n",
      "Iteration 353, Batch: 33, Loss: 0.07286744564771652\n",
      "Iteration 353, Batch: 34, Loss: 0.05934145674109459\n",
      "Iteration 353, Batch: 35, Loss: 0.044233132153749466\n",
      "Iteration 353, Batch: 36, Loss: 0.05392313748598099\n",
      "Iteration 353, Batch: 37, Loss: 0.04731769114732742\n",
      "Iteration 353, Batch: 38, Loss: 0.06276370584964752\n",
      "Iteration 353, Batch: 39, Loss: 0.053151149302721024\n",
      "Iteration 353, Batch: 40, Loss: 0.07180826365947723\n",
      "Iteration 353, Batch: 41, Loss: 0.04730471596121788\n",
      "Iteration 353, Batch: 42, Loss: 0.06576786190271378\n",
      "Iteration 353, Batch: 43, Loss: 0.036460503935813904\n",
      "Iteration 353, Batch: 44, Loss: 0.04709593579173088\n",
      "Iteration 353, Batch: 45, Loss: 0.04613817110657692\n",
      "Iteration 353, Batch: 46, Loss: 0.02537591941654682\n",
      "Iteration 353, Batch: 47, Loss: 0.04790377616882324\n",
      "Iteration 353, Batch: 48, Loss: 0.037941861897706985\n",
      "Iteration 353, Batch: 49, Loss: 0.058373384177684784\n",
      "Number of layers: 10\n",
      "Iteration 354, Batch: 0, Loss: 0.04530075564980507\n",
      "Iteration 354, Batch: 1, Loss: 0.03234045207500458\n",
      "Iteration 354, Batch: 2, Loss: 0.029350876808166504\n",
      "Iteration 354, Batch: 3, Loss: 0.02648359164595604\n",
      "Iteration 354, Batch: 4, Loss: 0.06104850023984909\n",
      "Iteration 354, Batch: 5, Loss: 0.0621773786842823\n",
      "Iteration 354, Batch: 6, Loss: 0.046512436121702194\n",
      "Iteration 354, Batch: 7, Loss: 0.02323113940656185\n",
      "Iteration 354, Batch: 8, Loss: 0.02423316426575184\n",
      "Iteration 354, Batch: 9, Loss: 0.049543123692274094\n",
      "Iteration 354, Batch: 10, Loss: 0.04755592718720436\n",
      "Iteration 354, Batch: 11, Loss: 0.03463967889547348\n",
      "Iteration 354, Batch: 12, Loss: 0.043091271072626114\n",
      "Iteration 354, Batch: 13, Loss: 0.06843104958534241\n",
      "Iteration 354, Batch: 14, Loss: 0.06762143224477768\n",
      "Iteration 354, Batch: 15, Loss: 0.09108134359121323\n",
      "Iteration 354, Batch: 16, Loss: 0.12006420642137527\n",
      "Iteration 354, Batch: 17, Loss: 0.10863189399242401\n",
      "Iteration 354, Batch: 18, Loss: 0.08401119709014893\n",
      "Iteration 354, Batch: 19, Loss: 0.09984113276004791\n",
      "Iteration 354, Batch: 20, Loss: 0.08270464092493057\n",
      "Iteration 354, Batch: 21, Loss: 0.055070798844099045\n",
      "Iteration 354, Batch: 22, Loss: 0.061997201293706894\n",
      "Iteration 354, Batch: 23, Loss: 0.0570659302175045\n",
      "Iteration 354, Batch: 24, Loss: 0.09448989480733871\n",
      "Iteration 354, Batch: 25, Loss: 0.048327285796403885\n",
      "Iteration 354, Batch: 26, Loss: 0.05245097354054451\n",
      "Iteration 354, Batch: 27, Loss: 0.030242400243878365\n",
      "Iteration 354, Batch: 28, Loss: 0.061050452291965485\n",
      "Iteration 354, Batch: 29, Loss: 0.025634897872805595\n",
      "Iteration 354, Batch: 30, Loss: 0.07352398335933685\n",
      "Iteration 354, Batch: 31, Loss: 0.0908154621720314\n",
      "Iteration 354, Batch: 32, Loss: 0.043239615857601166\n",
      "Iteration 354, Batch: 33, Loss: 0.03166064992547035\n",
      "Iteration 354, Batch: 34, Loss: 0.037594884634017944\n",
      "Iteration 354, Batch: 35, Loss: 0.06796110421419144\n",
      "Iteration 354, Batch: 36, Loss: 0.034798868000507355\n",
      "Iteration 354, Batch: 37, Loss: 0.016707146540284157\n",
      "Iteration 354, Batch: 38, Loss: 0.05025012791156769\n",
      "Iteration 354, Batch: 39, Loss: 0.05165163055062294\n",
      "Iteration 354, Batch: 40, Loss: 0.027691705152392387\n",
      "Iteration 354, Batch: 41, Loss: 0.04179888591170311\n",
      "Iteration 354, Batch: 42, Loss: 0.042387645691633224\n",
      "Iteration 354, Batch: 43, Loss: 0.057314250618219376\n",
      "Iteration 354, Batch: 44, Loss: 0.0602789968252182\n",
      "Iteration 354, Batch: 45, Loss: 0.041514359414577484\n",
      "Iteration 354, Batch: 46, Loss: 0.046926889568567276\n",
      "Iteration 354, Batch: 47, Loss: 0.03650427609682083\n",
      "Iteration 354, Batch: 48, Loss: 0.03896118327975273\n",
      "Iteration 354, Batch: 49, Loss: 0.06418517231941223\n",
      "Number of layers: 10\n",
      "Iteration 355, Batch: 0, Loss: 0.05733674019575119\n",
      "Iteration 355, Batch: 1, Loss: 0.03726104646921158\n",
      "Iteration 355, Batch: 2, Loss: 0.04424101859331131\n",
      "Iteration 355, Batch: 3, Loss: 0.019774584099650383\n",
      "Iteration 355, Batch: 4, Loss: 0.06664276123046875\n",
      "Iteration 355, Batch: 5, Loss: 0.06302368640899658\n",
      "Iteration 355, Batch: 6, Loss: 0.0702967420220375\n",
      "Iteration 355, Batch: 7, Loss: 0.04644937068223953\n",
      "Iteration 355, Batch: 8, Loss: 0.02266182377934456\n",
      "Iteration 355, Batch: 9, Loss: 0.046179402619600296\n",
      "Iteration 355, Batch: 10, Loss: 0.04194410890340805\n",
      "Iteration 355, Batch: 11, Loss: 0.03452136367559433\n",
      "Iteration 355, Batch: 12, Loss: 0.042443208396434784\n",
      "Iteration 355, Batch: 13, Loss: 0.03863530606031418\n",
      "Iteration 355, Batch: 14, Loss: 0.04062751680612564\n",
      "Iteration 355, Batch: 15, Loss: 0.031132405623793602\n",
      "Iteration 355, Batch: 16, Loss: 0.03322635963559151\n",
      "Iteration 355, Batch: 17, Loss: 0.02129274234175682\n",
      "Iteration 355, Batch: 18, Loss: 0.0352691113948822\n",
      "Iteration 355, Batch: 19, Loss: 0.055281467735767365\n",
      "Iteration 355, Batch: 20, Loss: 0.052308518439531326\n",
      "Iteration 355, Batch: 21, Loss: 0.025675153359770775\n",
      "Iteration 355, Batch: 22, Loss: 0.05221852287650108\n",
      "Iteration 355, Batch: 23, Loss: 0.05618608370423317\n",
      "Iteration 355, Batch: 24, Loss: 0.04499313235282898\n",
      "Iteration 355, Batch: 25, Loss: 0.042833440005779266\n",
      "Iteration 355, Batch: 26, Loss: 0.03090408258140087\n",
      "Iteration 355, Batch: 27, Loss: 0.02912428043782711\n",
      "Iteration 355, Batch: 28, Loss: 0.04973384365439415\n",
      "Iteration 355, Batch: 29, Loss: 0.030962752178311348\n",
      "Iteration 355, Batch: 30, Loss: 0.06201599910855293\n",
      "Iteration 355, Batch: 31, Loss: 0.023128792643547058\n",
      "Iteration 355, Batch: 32, Loss: 0.05507351830601692\n",
      "Iteration 355, Batch: 33, Loss: 0.034062061458826065\n",
      "Iteration 355, Batch: 34, Loss: 0.021736573427915573\n",
      "Iteration 355, Batch: 35, Loss: 0.05293215811252594\n",
      "Iteration 355, Batch: 36, Loss: 0.024359997361898422\n",
      "Iteration 355, Batch: 37, Loss: 0.05407092347741127\n",
      "Iteration 355, Batch: 38, Loss: 0.03567015752196312\n",
      "Iteration 355, Batch: 39, Loss: 0.05086237192153931\n",
      "Iteration 355, Batch: 40, Loss: 0.04662231355905533\n",
      "Iteration 355, Batch: 41, Loss: 0.07231166213750839\n",
      "Iteration 355, Batch: 42, Loss: 0.03589535132050514\n",
      "Iteration 355, Batch: 43, Loss: 0.032206106930971146\n",
      "Iteration 355, Batch: 44, Loss: 0.04327588155865669\n",
      "Iteration 355, Batch: 45, Loss: 0.056309714913368225\n",
      "Iteration 355, Batch: 46, Loss: 0.045652084052562714\n",
      "Iteration 355, Batch: 47, Loss: 0.04750781133770943\n",
      "Iteration 355, Batch: 48, Loss: 0.04878021031618118\n",
      "Iteration 355, Batch: 49, Loss: 0.05494179576635361\n",
      "Number of layers: 10\n",
      "Iteration 356, Batch: 0, Loss: 0.034859851002693176\n",
      "Iteration 356, Batch: 1, Loss: 0.0539279542863369\n",
      "Iteration 356, Batch: 2, Loss: 0.036120738834142685\n",
      "Iteration 356, Batch: 3, Loss: 0.03649335354566574\n",
      "Iteration 356, Batch: 4, Loss: 0.05492449551820755\n",
      "Iteration 356, Batch: 5, Loss: 0.04132314771413803\n",
      "Iteration 356, Batch: 6, Loss: 0.034492503851652145\n",
      "Iteration 356, Batch: 7, Loss: 0.04238881915807724\n",
      "Iteration 356, Batch: 8, Loss: 0.030284304171800613\n",
      "Iteration 356, Batch: 9, Loss: 0.05368649214506149\n",
      "Iteration 356, Batch: 10, Loss: 0.03579435124993324\n",
      "Iteration 356, Batch: 11, Loss: 0.048229292035102844\n",
      "Iteration 356, Batch: 12, Loss: 0.038058049976825714\n",
      "Iteration 356, Batch: 13, Loss: 0.047322433441877365\n",
      "Iteration 356, Batch: 14, Loss: 0.0534755140542984\n",
      "Iteration 356, Batch: 15, Loss: 0.04393846541643143\n",
      "Iteration 356, Batch: 16, Loss: 0.040739983320236206\n",
      "Iteration 356, Batch: 17, Loss: 0.044237229973077774\n",
      "Iteration 356, Batch: 18, Loss: 0.028252143412828445\n",
      "Iteration 356, Batch: 19, Loss: 0.04410986974835396\n",
      "Iteration 356, Batch: 20, Loss: 0.05339910089969635\n",
      "Iteration 356, Batch: 21, Loss: 0.033249758183956146\n",
      "Iteration 356, Batch: 22, Loss: 0.03531826287508011\n",
      "Iteration 356, Batch: 23, Loss: 0.033495161682367325\n",
      "Iteration 356, Batch: 24, Loss: 0.03488374501466751\n",
      "Iteration 356, Batch: 25, Loss: 0.03856491297483444\n",
      "Iteration 356, Batch: 26, Loss: 0.07923023402690887\n",
      "Iteration 356, Batch: 27, Loss: 0.06485401093959808\n",
      "Iteration 356, Batch: 28, Loss: 0.0630994662642479\n",
      "Iteration 356, Batch: 29, Loss: 0.04858359694480896\n",
      "Iteration 356, Batch: 30, Loss: 0.0489230677485466\n",
      "Iteration 356, Batch: 31, Loss: 0.03604843467473984\n",
      "Iteration 356, Batch: 32, Loss: 0.05319768935441971\n",
      "Iteration 356, Batch: 33, Loss: 0.04730905964970589\n",
      "Iteration 356, Batch: 34, Loss: 0.04155098274350166\n",
      "Iteration 356, Batch: 35, Loss: 0.03996797651052475\n",
      "Iteration 356, Batch: 36, Loss: 0.03211352229118347\n",
      "Iteration 356, Batch: 37, Loss: 0.04503440856933594\n",
      "Iteration 356, Batch: 38, Loss: 0.04175889864563942\n",
      "Iteration 356, Batch: 39, Loss: 0.043381813913583755\n",
      "Iteration 356, Batch: 40, Loss: 0.05191692337393761\n",
      "Iteration 356, Batch: 41, Loss: 0.047677651047706604\n",
      "Iteration 356, Batch: 42, Loss: 0.03474762663245201\n",
      "Iteration 356, Batch: 43, Loss: 0.04827280715107918\n",
      "Iteration 356, Batch: 44, Loss: 0.06126713752746582\n",
      "Iteration 356, Batch: 45, Loss: 0.0389932245016098\n",
      "Iteration 356, Batch: 46, Loss: 0.04992740973830223\n",
      "Iteration 356, Batch: 47, Loss: 0.0758238211274147\n",
      "Iteration 356, Batch: 48, Loss: 0.04815081134438515\n",
      "Iteration 356, Batch: 49, Loss: 0.031122392043471336\n",
      "Number of layers: 10\n",
      "Iteration 357, Batch: 0, Loss: 0.043917007744312286\n",
      "Iteration 357, Batch: 1, Loss: 0.037751760333776474\n",
      "Iteration 357, Batch: 2, Loss: 0.06645528227090836\n",
      "Iteration 357, Batch: 3, Loss: 0.06448786705732346\n",
      "Iteration 357, Batch: 4, Loss: 0.05009705573320389\n",
      "Iteration 357, Batch: 5, Loss: 0.04618983343243599\n",
      "Iteration 357, Batch: 6, Loss: 0.057146456092596054\n",
      "Iteration 357, Batch: 7, Loss: 0.035716235637664795\n",
      "Iteration 357, Batch: 8, Loss: 0.0543179027736187\n",
      "Iteration 357, Batch: 9, Loss: 0.05747704207897186\n",
      "Iteration 357, Batch: 10, Loss: 0.06950459629297256\n",
      "Iteration 357, Batch: 11, Loss: 0.0556422621011734\n",
      "Iteration 357, Batch: 12, Loss: 0.03707227110862732\n",
      "Iteration 357, Batch: 13, Loss: 0.04872966557741165\n",
      "Iteration 357, Batch: 14, Loss: 0.05680195614695549\n",
      "Iteration 357, Batch: 15, Loss: 0.04741143062710762\n",
      "Iteration 357, Batch: 16, Loss: 0.05762043967843056\n",
      "Iteration 357, Batch: 17, Loss: 0.040838394314050674\n",
      "Iteration 357, Batch: 18, Loss: 0.04350527748465538\n",
      "Iteration 357, Batch: 19, Loss: 0.04116508364677429\n",
      "Iteration 357, Batch: 20, Loss: 0.045835819095373154\n",
      "Iteration 357, Batch: 21, Loss: 0.034576840698719025\n",
      "Iteration 357, Batch: 22, Loss: 0.03778964281082153\n",
      "Iteration 357, Batch: 23, Loss: 0.037925537675619125\n",
      "Iteration 357, Batch: 24, Loss: 0.07522115111351013\n",
      "Iteration 357, Batch: 25, Loss: 0.049029383808374405\n",
      "Iteration 357, Batch: 26, Loss: 0.03520863130688667\n",
      "Iteration 357, Batch: 27, Loss: 0.05023391172289848\n",
      "Iteration 357, Batch: 28, Loss: 0.030160412192344666\n",
      "Iteration 357, Batch: 29, Loss: 0.022509923204779625\n",
      "Iteration 357, Batch: 30, Loss: 0.057568300515413284\n",
      "Iteration 357, Batch: 31, Loss: 0.055598486214876175\n",
      "Iteration 357, Batch: 32, Loss: 0.0339660681784153\n",
      "Iteration 357, Batch: 33, Loss: 0.043466176837682724\n",
      "Iteration 357, Batch: 34, Loss: 0.041200943291187286\n",
      "Iteration 357, Batch: 35, Loss: 0.043674152344465256\n",
      "Iteration 357, Batch: 36, Loss: 0.03820275515317917\n",
      "Iteration 357, Batch: 37, Loss: 0.046463318169116974\n",
      "Iteration 357, Batch: 38, Loss: 0.04180661216378212\n",
      "Iteration 357, Batch: 39, Loss: 0.07570987194776535\n",
      "Iteration 357, Batch: 40, Loss: 0.05336082726716995\n",
      "Iteration 357, Batch: 41, Loss: 0.05894229933619499\n",
      "Iteration 357, Batch: 42, Loss: 0.07865838706493378\n",
      "Iteration 357, Batch: 43, Loss: 0.062449950724840164\n",
      "Iteration 357, Batch: 44, Loss: 0.05904475226998329\n",
      "Iteration 357, Batch: 45, Loss: 0.0534968264400959\n",
      "Iteration 357, Batch: 46, Loss: 0.05594383180141449\n",
      "Iteration 357, Batch: 47, Loss: 0.021390670910477638\n",
      "Iteration 357, Batch: 48, Loss: 0.0624823234975338\n",
      "Iteration 357, Batch: 49, Loss: 0.07499323040246964\n",
      "Number of layers: 10\n",
      "Iteration 358, Batch: 0, Loss: 0.06029766798019409\n",
      "Iteration 358, Batch: 1, Loss: 0.028647223487496376\n",
      "Iteration 358, Batch: 2, Loss: 0.027214355766773224\n",
      "Iteration 358, Batch: 3, Loss: 0.07215749472379684\n",
      "Iteration 358, Batch: 4, Loss: 0.042031463235616684\n",
      "Iteration 358, Batch: 5, Loss: 0.038506459444761276\n",
      "Iteration 358, Batch: 6, Loss: 0.040902771055698395\n",
      "Iteration 358, Batch: 7, Loss: 0.04164442792534828\n",
      "Iteration 358, Batch: 8, Loss: 0.049076031893491745\n",
      "Iteration 358, Batch: 9, Loss: 0.05272361636161804\n",
      "Iteration 358, Batch: 10, Loss: 0.06945347040891647\n",
      "Iteration 358, Batch: 11, Loss: 0.04456501081585884\n",
      "Iteration 358, Batch: 12, Loss: 0.03352324664592743\n",
      "Iteration 358, Batch: 13, Loss: 0.04908003658056259\n",
      "Iteration 358, Batch: 14, Loss: 0.038098640739917755\n",
      "Iteration 358, Batch: 15, Loss: 0.05965873599052429\n",
      "Iteration 358, Batch: 16, Loss: 0.05240173265337944\n",
      "Iteration 358, Batch: 17, Loss: 0.08007030934095383\n",
      "Iteration 358, Batch: 18, Loss: 0.05692705512046814\n",
      "Iteration 358, Batch: 19, Loss: 0.04531969130039215\n",
      "Iteration 358, Batch: 20, Loss: 0.03648023679852486\n",
      "Iteration 358, Batch: 21, Loss: 0.05531662702560425\n",
      "Iteration 358, Batch: 22, Loss: 0.046463802456855774\n",
      "Iteration 358, Batch: 23, Loss: 0.03105933591723442\n",
      "Iteration 358, Batch: 24, Loss: 0.06538109481334686\n",
      "Iteration 358, Batch: 25, Loss: 0.07462690025568008\n",
      "Iteration 358, Batch: 26, Loss: 0.03883503004908562\n",
      "Iteration 358, Batch: 27, Loss: 0.04408680647611618\n",
      "Iteration 358, Batch: 28, Loss: 0.023938588798046112\n",
      "Iteration 358, Batch: 29, Loss: 0.022557584568858147\n",
      "Iteration 358, Batch: 30, Loss: 0.03813788667321205\n",
      "Iteration 358, Batch: 31, Loss: 0.05239996686577797\n",
      "Iteration 358, Batch: 32, Loss: 0.02505491115152836\n",
      "Iteration 358, Batch: 33, Loss: 0.04572734609246254\n",
      "Iteration 358, Batch: 34, Loss: 0.042814791202545166\n",
      "Iteration 358, Batch: 35, Loss: 0.04613419249653816\n",
      "Iteration 358, Batch: 36, Loss: 0.038158826529979706\n",
      "Iteration 358, Batch: 37, Loss: 0.0323498509824276\n",
      "Iteration 358, Batch: 38, Loss: 0.04462737962603569\n",
      "Iteration 358, Batch: 39, Loss: 0.04567441716790199\n",
      "Iteration 358, Batch: 40, Loss: 0.02339211478829384\n",
      "Iteration 358, Batch: 41, Loss: 0.05099479481577873\n",
      "Iteration 358, Batch: 42, Loss: 0.041651640087366104\n",
      "Iteration 358, Batch: 43, Loss: 0.024030698463320732\n",
      "Iteration 358, Batch: 44, Loss: 0.04349527135491371\n",
      "Iteration 358, Batch: 45, Loss: 0.0500449538230896\n",
      "Iteration 358, Batch: 46, Loss: 0.05066029727458954\n",
      "Iteration 358, Batch: 47, Loss: 0.05173731595277786\n",
      "Iteration 358, Batch: 48, Loss: 0.06312558799982071\n",
      "Iteration 358, Batch: 49, Loss: 0.03665107116103172\n",
      "Number of layers: 10\n",
      "Iteration 359, Batch: 0, Loss: 0.035880010575056076\n",
      "Iteration 359, Batch: 1, Loss: 0.03751089423894882\n",
      "Iteration 359, Batch: 2, Loss: 0.05027893930673599\n",
      "Iteration 359, Batch: 3, Loss: 0.0434703528881073\n",
      "Iteration 359, Batch: 4, Loss: 0.04848406836390495\n",
      "Iteration 359, Batch: 5, Loss: 0.045336075127124786\n",
      "Iteration 359, Batch: 6, Loss: 0.04630262404680252\n",
      "Iteration 359, Batch: 7, Loss: 0.04072514548897743\n",
      "Iteration 359, Batch: 8, Loss: 0.08007349073886871\n",
      "Iteration 359, Batch: 9, Loss: 0.08843500167131424\n",
      "Iteration 359, Batch: 10, Loss: 0.06291677802801132\n",
      "Iteration 359, Batch: 11, Loss: 0.08853522688150406\n",
      "Iteration 359, Batch: 12, Loss: 0.07497742027044296\n",
      "Iteration 359, Batch: 13, Loss: 0.022769249975681305\n",
      "Iteration 359, Batch: 14, Loss: 0.03540662303566933\n",
      "Iteration 359, Batch: 15, Loss: 0.0669124573469162\n",
      "Iteration 359, Batch: 16, Loss: 0.037882160395383835\n",
      "Iteration 359, Batch: 17, Loss: 0.04444807767868042\n",
      "Iteration 359, Batch: 18, Loss: 0.056201234459877014\n",
      "Iteration 359, Batch: 19, Loss: 0.04889722540974617\n",
      "Iteration 359, Batch: 20, Loss: 0.04799744114279747\n",
      "Iteration 359, Batch: 21, Loss: 0.03706210479140282\n",
      "Iteration 359, Batch: 22, Loss: 0.05934969708323479\n",
      "Iteration 359, Batch: 23, Loss: 0.051755085587501526\n",
      "Iteration 359, Batch: 24, Loss: 0.05747896060347557\n",
      "Iteration 359, Batch: 25, Loss: 0.05534142255783081\n",
      "Iteration 359, Batch: 26, Loss: 0.07167527824640274\n",
      "Iteration 359, Batch: 27, Loss: 0.042144086211919785\n",
      "Iteration 359, Batch: 28, Loss: 0.031852755695581436\n",
      "Iteration 359, Batch: 29, Loss: 0.07604276388883591\n",
      "Iteration 359, Batch: 30, Loss: 0.024158988147974014\n",
      "Iteration 359, Batch: 31, Loss: 0.018229907378554344\n",
      "Iteration 359, Batch: 32, Loss: 0.047757185995578766\n",
      "Iteration 359, Batch: 33, Loss: 0.04020671173930168\n",
      "Iteration 359, Batch: 34, Loss: 0.031500935554504395\n",
      "Iteration 359, Batch: 35, Loss: 0.05903236195445061\n",
      "Iteration 359, Batch: 36, Loss: 0.053607530891895294\n",
      "Iteration 359, Batch: 37, Loss: 0.04598703607916832\n",
      "Iteration 359, Batch: 38, Loss: 0.05296429619193077\n",
      "Iteration 359, Batch: 39, Loss: 0.05205397307872772\n",
      "Iteration 359, Batch: 40, Loss: 0.043178632855415344\n",
      "Iteration 359, Batch: 41, Loss: 0.046981118619441986\n",
      "Iteration 359, Batch: 42, Loss: 0.04557269811630249\n",
      "Iteration 359, Batch: 43, Loss: 0.050340212881565094\n",
      "Iteration 359, Batch: 44, Loss: 0.030293099582195282\n",
      "Iteration 359, Batch: 45, Loss: 0.07198833674192429\n",
      "Iteration 359, Batch: 46, Loss: 0.028553463518619537\n",
      "Iteration 359, Batch: 47, Loss: 0.0526537224650383\n",
      "Iteration 359, Batch: 48, Loss: 0.05371899530291557\n",
      "Iteration 359, Batch: 49, Loss: 0.03078019618988037\n",
      "Number of layers: 10\n",
      "Iteration 360, Batch: 0, Loss: 0.023418942466378212\n",
      "Iteration 360, Batch: 1, Loss: 0.020598793402314186\n",
      "Iteration 360, Batch: 2, Loss: 0.050273820757865906\n",
      "Iteration 360, Batch: 3, Loss: 0.04916951060295105\n",
      "Iteration 360, Batch: 4, Loss: 0.05190179869532585\n",
      "Iteration 360, Batch: 5, Loss: 0.012362675741314888\n",
      "Iteration 360, Batch: 6, Loss: 0.04520699381828308\n",
      "Iteration 360, Batch: 7, Loss: 0.033604033291339874\n",
      "Iteration 360, Batch: 8, Loss: 0.04580254480242729\n",
      "Iteration 360, Batch: 9, Loss: 0.03984145447611809\n",
      "Iteration 360, Batch: 10, Loss: 0.03856603428721428\n",
      "Iteration 360, Batch: 11, Loss: 0.06214875727891922\n",
      "Iteration 360, Batch: 12, Loss: 0.027390846982598305\n",
      "Iteration 360, Batch: 13, Loss: 0.04632201045751572\n",
      "Iteration 360, Batch: 14, Loss: 0.06101403012871742\n",
      "Iteration 360, Batch: 15, Loss: 0.10715757310390472\n",
      "Iteration 360, Batch: 16, Loss: 0.11138048022985458\n",
      "Iteration 360, Batch: 17, Loss: 0.11250293254852295\n",
      "Iteration 360, Batch: 18, Loss: 0.09132186323404312\n",
      "Iteration 360, Batch: 19, Loss: 0.06889712810516357\n",
      "Iteration 360, Batch: 20, Loss: 0.039252690970897675\n",
      "Iteration 360, Batch: 21, Loss: 0.0422990582883358\n",
      "Iteration 360, Batch: 22, Loss: 0.04542333632707596\n",
      "Iteration 360, Batch: 23, Loss: 0.045534636825323105\n",
      "Iteration 360, Batch: 24, Loss: 0.05270659178495407\n",
      "Iteration 360, Batch: 25, Loss: 0.04711272940039635\n",
      "Iteration 360, Batch: 26, Loss: 0.031292624771595\n",
      "Iteration 360, Batch: 27, Loss: 0.028792565688490868\n",
      "Iteration 360, Batch: 28, Loss: 0.06065066531300545\n",
      "Iteration 360, Batch: 29, Loss: 0.04660769924521446\n",
      "Iteration 360, Batch: 30, Loss: 0.028696026653051376\n",
      "Iteration 360, Batch: 31, Loss: 0.03947189822793007\n",
      "Iteration 360, Batch: 32, Loss: 0.03866707161068916\n",
      "Iteration 360, Batch: 33, Loss: 0.05086013674736023\n",
      "Iteration 360, Batch: 34, Loss: 0.035494085401296616\n",
      "Iteration 360, Batch: 35, Loss: 0.026317238807678223\n",
      "Iteration 360, Batch: 36, Loss: 0.03553866595029831\n",
      "Iteration 360, Batch: 37, Loss: 0.021653424948453903\n",
      "Iteration 360, Batch: 38, Loss: 0.04155291989445686\n",
      "Iteration 360, Batch: 39, Loss: 0.04133811220526695\n",
      "Iteration 360, Batch: 40, Loss: 0.047074273228645325\n",
      "Iteration 360, Batch: 41, Loss: 0.07777395099401474\n",
      "Iteration 360, Batch: 42, Loss: 0.039323557168245316\n",
      "Iteration 360, Batch: 43, Loss: 0.07780127972364426\n",
      "Iteration 360, Batch: 44, Loss: 0.050522223114967346\n",
      "Iteration 360, Batch: 45, Loss: 0.025302954018115997\n",
      "Iteration 360, Batch: 46, Loss: 0.03789747133851051\n",
      "Iteration 360, Batch: 47, Loss: 0.0312679298222065\n",
      "Iteration 360, Batch: 48, Loss: 0.028679784387350082\n",
      "Iteration 360, Batch: 49, Loss: 0.03160027042031288\n",
      "Number of layers: 10\n",
      "Iteration 361, Batch: 0, Loss: 0.056111354380846024\n",
      "Iteration 361, Batch: 1, Loss: 0.05774232745170593\n",
      "Iteration 361, Batch: 2, Loss: 0.049100592732429504\n",
      "Iteration 361, Batch: 3, Loss: 0.06509801000356674\n",
      "Iteration 361, Batch: 4, Loss: 0.0492187961935997\n",
      "Iteration 361, Batch: 5, Loss: 0.05551011115312576\n",
      "Iteration 361, Batch: 6, Loss: 0.027697324752807617\n",
      "Iteration 361, Batch: 7, Loss: 0.040094271302223206\n",
      "Iteration 361, Batch: 8, Loss: 0.04597919061779976\n",
      "Iteration 361, Batch: 9, Loss: 0.05367857962846756\n",
      "Iteration 361, Batch: 10, Loss: 0.03330434858798981\n",
      "Iteration 361, Batch: 11, Loss: 0.06851296871900558\n",
      "Iteration 361, Batch: 12, Loss: 0.025954827666282654\n",
      "Iteration 361, Batch: 13, Loss: 0.04861215129494667\n",
      "Iteration 361, Batch: 14, Loss: 0.026209406554698944\n",
      "Iteration 361, Batch: 15, Loss: 0.02049238607287407\n",
      "Iteration 361, Batch: 16, Loss: 0.06106693670153618\n",
      "Iteration 361, Batch: 17, Loss: 0.05587136000394821\n",
      "Iteration 361, Batch: 18, Loss: 0.04234796389937401\n",
      "Iteration 361, Batch: 19, Loss: 0.04065549373626709\n",
      "Iteration 361, Batch: 20, Loss: 0.05157256871461868\n",
      "Iteration 361, Batch: 21, Loss: 0.016498122364282608\n",
      "Iteration 361, Batch: 22, Loss: 0.034626565873622894\n",
      "Iteration 361, Batch: 23, Loss: 0.020980391651391983\n",
      "Iteration 361, Batch: 24, Loss: 0.05493750423192978\n",
      "Iteration 361, Batch: 25, Loss: 0.03695317730307579\n",
      "Iteration 361, Batch: 26, Loss: 0.054753564298152924\n",
      "Iteration 361, Batch: 27, Loss: 0.032359905540943146\n",
      "Iteration 361, Batch: 28, Loss: 0.03839864209294319\n",
      "Iteration 361, Batch: 29, Loss: 0.036271318793296814\n",
      "Iteration 361, Batch: 30, Loss: 0.043034471571445465\n",
      "Iteration 361, Batch: 31, Loss: 0.04130404442548752\n",
      "Iteration 361, Batch: 32, Loss: 0.03621912747621536\n",
      "Iteration 361, Batch: 33, Loss: 0.028742648661136627\n",
      "Iteration 361, Batch: 34, Loss: 0.03621867671608925\n",
      "Iteration 361, Batch: 35, Loss: 0.05004817992448807\n",
      "Iteration 361, Batch: 36, Loss: 0.03863649442791939\n",
      "Iteration 361, Batch: 37, Loss: 0.05855280160903931\n",
      "Iteration 361, Batch: 38, Loss: 0.023282498121261597\n",
      "Iteration 361, Batch: 39, Loss: 0.022316379472613335\n",
      "Iteration 361, Batch: 40, Loss: 0.037022750824689865\n",
      "Iteration 361, Batch: 41, Loss: 0.05122414603829384\n",
      "Iteration 361, Batch: 42, Loss: 0.04363479092717171\n",
      "Iteration 361, Batch: 43, Loss: 0.05158058926463127\n",
      "Iteration 361, Batch: 44, Loss: 0.039302244782447815\n",
      "Iteration 361, Batch: 45, Loss: 0.037919871509075165\n",
      "Iteration 361, Batch: 46, Loss: 0.04146056994795799\n",
      "Iteration 361, Batch: 47, Loss: 0.014860205352306366\n",
      "Iteration 361, Batch: 48, Loss: 0.017984163016080856\n",
      "Iteration 361, Batch: 49, Loss: 0.056223392486572266\n",
      "Number of layers: 10\n",
      "Iteration 362, Batch: 0, Loss: 0.029598291963338852\n",
      "Iteration 362, Batch: 1, Loss: 0.050535544753074646\n",
      "Iteration 362, Batch: 2, Loss: 0.052138909697532654\n",
      "Iteration 362, Batch: 3, Loss: 0.028799084946513176\n",
      "Iteration 362, Batch: 4, Loss: 0.033490970730781555\n",
      "Iteration 362, Batch: 5, Loss: 0.05225113779306412\n",
      "Iteration 362, Batch: 6, Loss: 0.057224396616220474\n",
      "Iteration 362, Batch: 7, Loss: 0.053726714104413986\n",
      "Iteration 362, Batch: 8, Loss: 0.04949616268277168\n",
      "Iteration 362, Batch: 9, Loss: 0.03512512519955635\n",
      "Iteration 362, Batch: 10, Loss: 0.045583438128232956\n",
      "Iteration 362, Batch: 11, Loss: 0.07284965366125107\n",
      "Iteration 362, Batch: 12, Loss: 0.06462625414133072\n",
      "Iteration 362, Batch: 13, Loss: 0.040689799934625626\n",
      "Iteration 362, Batch: 14, Loss: 0.06447390466928482\n",
      "Iteration 362, Batch: 15, Loss: 0.04590022936463356\n",
      "Iteration 362, Batch: 16, Loss: 0.03787223994731903\n",
      "Iteration 362, Batch: 17, Loss: 0.05736616998910904\n",
      "Iteration 362, Batch: 18, Loss: 0.05263771861791611\n",
      "Iteration 362, Batch: 19, Loss: 0.031209101900458336\n",
      "Iteration 362, Batch: 20, Loss: 0.046446360647678375\n",
      "Iteration 362, Batch: 21, Loss: 0.04776438698172569\n",
      "Iteration 362, Batch: 22, Loss: 0.039105676114559174\n",
      "Iteration 362, Batch: 23, Loss: 0.03703714534640312\n",
      "Iteration 362, Batch: 24, Loss: 0.040600262582302094\n",
      "Iteration 362, Batch: 25, Loss: 0.04824623838067055\n",
      "Iteration 362, Batch: 26, Loss: 0.04036957770586014\n",
      "Iteration 362, Batch: 27, Loss: 0.05749198794364929\n",
      "Iteration 362, Batch: 28, Loss: 0.05797619745135307\n",
      "Iteration 362, Batch: 29, Loss: 0.05206114798784256\n",
      "Iteration 362, Batch: 30, Loss: 0.06513576954603195\n",
      "Iteration 362, Batch: 31, Loss: 0.040740516036748886\n",
      "Iteration 362, Batch: 32, Loss: 0.0296164583414793\n",
      "Iteration 362, Batch: 33, Loss: 0.05814525485038757\n",
      "Iteration 362, Batch: 34, Loss: 0.052794765681028366\n",
      "Iteration 362, Batch: 35, Loss: 0.028329065069556236\n",
      "Iteration 362, Batch: 36, Loss: 0.02699669823050499\n",
      "Iteration 362, Batch: 37, Loss: 0.022672031074762344\n",
      "Iteration 362, Batch: 38, Loss: 0.052377890795469284\n",
      "Iteration 362, Batch: 39, Loss: 0.028130169957876205\n",
      "Iteration 362, Batch: 40, Loss: 0.048935387283563614\n",
      "Iteration 362, Batch: 41, Loss: 0.053527019917964935\n",
      "Iteration 362, Batch: 42, Loss: 0.030865449458360672\n",
      "Iteration 362, Batch: 43, Loss: 0.05437158793210983\n",
      "Iteration 362, Batch: 44, Loss: 0.04823950678110123\n",
      "Iteration 362, Batch: 45, Loss: 0.0331689715385437\n",
      "Iteration 362, Batch: 46, Loss: 0.0366855226457119\n",
      "Iteration 362, Batch: 47, Loss: 0.04501151293516159\n",
      "Iteration 362, Batch: 48, Loss: 0.026011500507593155\n",
      "Iteration 362, Batch: 49, Loss: 0.036857809871435165\n",
      "Number of layers: 10\n",
      "Iteration 363, Batch: 0, Loss: 0.03326529264450073\n",
      "Iteration 363, Batch: 1, Loss: 0.04368983581662178\n",
      "Iteration 363, Batch: 2, Loss: 0.04366906359791756\n",
      "Iteration 363, Batch: 3, Loss: 0.03657761588692665\n",
      "Iteration 363, Batch: 4, Loss: 0.021835479885339737\n",
      "Iteration 363, Batch: 5, Loss: 0.02802516333758831\n",
      "Iteration 363, Batch: 6, Loss: 0.03931533917784691\n",
      "Iteration 363, Batch: 7, Loss: 0.01841779798269272\n",
      "Iteration 363, Batch: 8, Loss: 0.03689083829522133\n",
      "Iteration 363, Batch: 9, Loss: 0.03651043772697449\n",
      "Iteration 363, Batch: 10, Loss: 0.03189220279455185\n",
      "Iteration 363, Batch: 11, Loss: 0.037681944668293\n",
      "Iteration 363, Batch: 12, Loss: 0.04612762853503227\n",
      "Iteration 363, Batch: 13, Loss: 0.028034916147589684\n",
      "Iteration 363, Batch: 14, Loss: 0.031266435980796814\n",
      "Iteration 363, Batch: 15, Loss: 0.03664861246943474\n",
      "Iteration 363, Batch: 16, Loss: 0.034736767411231995\n",
      "Iteration 363, Batch: 17, Loss: 0.043881483376026154\n",
      "Iteration 363, Batch: 18, Loss: 0.04358962923288345\n",
      "Iteration 363, Batch: 19, Loss: 0.025911517441272736\n",
      "Iteration 363, Batch: 20, Loss: 0.026465406641364098\n",
      "Iteration 363, Batch: 21, Loss: 0.04480744153261185\n",
      "Iteration 363, Batch: 22, Loss: 0.061967261135578156\n",
      "Iteration 363, Batch: 23, Loss: 0.05315472185611725\n",
      "Iteration 363, Batch: 24, Loss: 0.06252458691596985\n",
      "Iteration 363, Batch: 25, Loss: 0.04081002250313759\n",
      "Iteration 363, Batch: 26, Loss: 0.029458526521921158\n",
      "Iteration 363, Batch: 27, Loss: 0.051352642476558685\n",
      "Iteration 363, Batch: 28, Loss: 0.05158450827002525\n",
      "Iteration 363, Batch: 29, Loss: 0.0540587455034256\n",
      "Iteration 363, Batch: 30, Loss: 0.04732612520456314\n",
      "Iteration 363, Batch: 31, Loss: 0.04517388343811035\n",
      "Iteration 363, Batch: 32, Loss: 0.05004910007119179\n",
      "Iteration 363, Batch: 33, Loss: 0.031497661024332047\n",
      "Iteration 363, Batch: 34, Loss: 0.038894712924957275\n",
      "Iteration 363, Batch: 35, Loss: 0.03411780297756195\n",
      "Iteration 363, Batch: 36, Loss: 0.03504013642668724\n",
      "Iteration 363, Batch: 37, Loss: 0.05656149610877037\n",
      "Iteration 363, Batch: 38, Loss: 0.05420501157641411\n",
      "Iteration 363, Batch: 39, Loss: 0.052550844848155975\n",
      "Iteration 363, Batch: 40, Loss: 0.061079706996679306\n",
      "Iteration 363, Batch: 41, Loss: 0.03694738820195198\n",
      "Iteration 363, Batch: 42, Loss: 0.05441240966320038\n",
      "Iteration 363, Batch: 43, Loss: 0.02756807953119278\n",
      "Iteration 363, Batch: 44, Loss: 0.025167997926473618\n",
      "Iteration 363, Batch: 45, Loss: 0.023011041805148125\n",
      "Iteration 363, Batch: 46, Loss: 0.05087628215551376\n",
      "Iteration 363, Batch: 47, Loss: 0.029149850830435753\n",
      "Iteration 363, Batch: 48, Loss: 0.032942067831754684\n",
      "Iteration 363, Batch: 49, Loss: 0.02966197021305561\n",
      "Number of layers: 10\n",
      "Iteration 364, Batch: 0, Loss: 0.03347514942288399\n",
      "Iteration 364, Batch: 1, Loss: 0.050037138164043427\n",
      "Iteration 364, Batch: 2, Loss: 0.060547612607479095\n",
      "Iteration 364, Batch: 3, Loss: 0.02153901755809784\n",
      "Iteration 364, Batch: 4, Loss: 0.04745611175894737\n",
      "Iteration 364, Batch: 5, Loss: 0.03908657282590866\n",
      "Iteration 364, Batch: 6, Loss: 0.035513736307621\n",
      "Iteration 364, Batch: 7, Loss: 0.061191245913505554\n",
      "Iteration 364, Batch: 8, Loss: 0.04123552143573761\n",
      "Iteration 364, Batch: 9, Loss: 0.03755771368741989\n",
      "Iteration 364, Batch: 10, Loss: 0.03770202398300171\n",
      "Iteration 364, Batch: 11, Loss: 0.021997861564159393\n",
      "Iteration 364, Batch: 12, Loss: 0.03756309300661087\n",
      "Iteration 364, Batch: 13, Loss: 0.05256931111216545\n",
      "Iteration 364, Batch: 14, Loss: 0.042752690613269806\n",
      "Iteration 364, Batch: 15, Loss: 0.034191928803920746\n",
      "Iteration 364, Batch: 16, Loss: 0.05297346040606499\n",
      "Iteration 364, Batch: 17, Loss: 0.05430089309811592\n",
      "Iteration 364, Batch: 18, Loss: 0.0916244387626648\n",
      "Iteration 364, Batch: 19, Loss: 0.09049931913614273\n",
      "Iteration 364, Batch: 20, Loss: 0.07645475119352341\n",
      "Iteration 364, Batch: 21, Loss: 0.045442093163728714\n",
      "Iteration 364, Batch: 22, Loss: 0.034941814839839935\n",
      "Iteration 364, Batch: 23, Loss: 0.053942397236824036\n",
      "Iteration 364, Batch: 24, Loss: 0.055042851716279984\n",
      "Iteration 364, Batch: 25, Loss: 0.052418068051338196\n",
      "Iteration 364, Batch: 26, Loss: 0.0584019273519516\n",
      "Iteration 364, Batch: 27, Loss: 0.04756831005215645\n",
      "Iteration 364, Batch: 28, Loss: 0.056994665414094925\n",
      "Iteration 364, Batch: 29, Loss: 0.0453469455242157\n",
      "Iteration 364, Batch: 30, Loss: 0.07341501861810684\n",
      "Iteration 364, Batch: 31, Loss: 0.019977325573563576\n",
      "Iteration 364, Batch: 32, Loss: 0.025054916739463806\n",
      "Iteration 364, Batch: 33, Loss: 0.040484026074409485\n",
      "Iteration 364, Batch: 34, Loss: 0.04255206137895584\n",
      "Iteration 364, Batch: 35, Loss: 0.040969137102365494\n",
      "Iteration 364, Batch: 36, Loss: 0.027967819944024086\n",
      "Iteration 364, Batch: 37, Loss: 0.0348515659570694\n",
      "Iteration 364, Batch: 38, Loss: 0.07449975609779358\n",
      "Iteration 364, Batch: 39, Loss: 0.05562412738800049\n",
      "Iteration 364, Batch: 40, Loss: 0.05193460360169411\n",
      "Iteration 364, Batch: 41, Loss: 0.0684644803404808\n",
      "Iteration 364, Batch: 42, Loss: 0.03158946335315704\n",
      "Iteration 364, Batch: 43, Loss: 0.031127218157052994\n",
      "Iteration 364, Batch: 44, Loss: 0.050683144479990005\n",
      "Iteration 364, Batch: 45, Loss: 0.06855921447277069\n",
      "Iteration 364, Batch: 46, Loss: 0.05892346426844597\n",
      "Iteration 364, Batch: 47, Loss: 0.04945998638868332\n",
      "Iteration 364, Batch: 48, Loss: 0.04556954652070999\n",
      "Iteration 364, Batch: 49, Loss: 0.04143418371677399\n",
      "Number of layers: 10\n",
      "Iteration 365, Batch: 0, Loss: 0.08119716495275497\n",
      "Iteration 365, Batch: 1, Loss: 0.06389546394348145\n",
      "Iteration 365, Batch: 2, Loss: 0.07010815292596817\n",
      "Iteration 365, Batch: 3, Loss: 0.06942068040370941\n",
      "Iteration 365, Batch: 4, Loss: 0.07127082347869873\n",
      "Iteration 365, Batch: 5, Loss: 0.056842122226953506\n",
      "Iteration 365, Batch: 6, Loss: 0.03122914396226406\n",
      "Iteration 365, Batch: 7, Loss: 0.02959328144788742\n",
      "Iteration 365, Batch: 8, Loss: 0.06029720976948738\n",
      "Iteration 365, Batch: 9, Loss: 0.05906560644507408\n",
      "Iteration 365, Batch: 10, Loss: 0.053190916776657104\n",
      "Iteration 365, Batch: 11, Loss: 0.07286790013313293\n",
      "Iteration 365, Batch: 12, Loss: 0.073372483253479\n",
      "Iteration 365, Batch: 13, Loss: 0.06137986108660698\n",
      "Iteration 365, Batch: 14, Loss: 0.03303137794137001\n",
      "Iteration 365, Batch: 15, Loss: 0.09764288365840912\n",
      "Iteration 365, Batch: 16, Loss: 0.08320159465074539\n",
      "Iteration 365, Batch: 17, Loss: 0.042956408113241196\n",
      "Iteration 365, Batch: 18, Loss: 0.06763353198766708\n",
      "Iteration 365, Batch: 19, Loss: 0.04019620642066002\n",
      "Iteration 365, Batch: 20, Loss: 0.037472233176231384\n",
      "Iteration 365, Batch: 21, Loss: 0.0754721462726593\n",
      "Iteration 365, Batch: 22, Loss: 0.05532737448811531\n",
      "Iteration 365, Batch: 23, Loss: 0.04088710620999336\n",
      "Iteration 365, Batch: 24, Loss: 0.0516253300011158\n",
      "Iteration 365, Batch: 25, Loss: 0.05602633208036423\n",
      "Iteration 365, Batch: 26, Loss: 0.02810562588274479\n",
      "Iteration 365, Batch: 27, Loss: 0.05802449584007263\n",
      "Iteration 365, Batch: 28, Loss: 0.05515459552407265\n",
      "Iteration 365, Batch: 29, Loss: 0.06961695849895477\n",
      "Iteration 365, Batch: 30, Loss: 0.05693540349602699\n",
      "Iteration 365, Batch: 31, Loss: 0.0456845760345459\n",
      "Iteration 365, Batch: 32, Loss: 0.042923085391521454\n",
      "Iteration 365, Batch: 33, Loss: 0.021612780168652534\n",
      "Iteration 365, Batch: 34, Loss: 0.02640567533671856\n",
      "Iteration 365, Batch: 35, Loss: 0.03830096870660782\n",
      "Iteration 365, Batch: 36, Loss: 0.044423799961805344\n",
      "Iteration 365, Batch: 37, Loss: 0.05388764664530754\n",
      "Iteration 365, Batch: 38, Loss: 0.02574126236140728\n",
      "Iteration 365, Batch: 39, Loss: 0.04583950713276863\n",
      "Iteration 365, Batch: 40, Loss: 0.03238162770867348\n",
      "Iteration 365, Batch: 41, Loss: 0.044437225908041\n",
      "Iteration 365, Batch: 42, Loss: 0.044320717453956604\n",
      "Iteration 365, Batch: 43, Loss: 0.05689622089266777\n",
      "Iteration 365, Batch: 44, Loss: 0.04398993402719498\n",
      "Iteration 365, Batch: 45, Loss: 0.04549885541200638\n",
      "Iteration 365, Batch: 46, Loss: 0.05478419363498688\n",
      "Iteration 365, Batch: 47, Loss: 0.05023510754108429\n",
      "Iteration 365, Batch: 48, Loss: 0.041058242321014404\n",
      "Iteration 365, Batch: 49, Loss: 0.04410720616579056\n",
      "Number of layers: 10\n",
      "Iteration 366, Batch: 0, Loss: 0.05144205316901207\n",
      "Iteration 366, Batch: 1, Loss: 0.06401162594556808\n",
      "Iteration 366, Batch: 2, Loss: 0.046111688017845154\n",
      "Iteration 366, Batch: 3, Loss: 0.06708787381649017\n",
      "Iteration 366, Batch: 4, Loss: 0.07715070992708206\n",
      "Iteration 366, Batch: 5, Loss: 0.045119863003492355\n",
      "Iteration 366, Batch: 6, Loss: 0.03432139381766319\n",
      "Iteration 366, Batch: 7, Loss: 0.0682380199432373\n",
      "Iteration 366, Batch: 8, Loss: 0.05432576686143875\n",
      "Iteration 366, Batch: 9, Loss: 0.0639471784234047\n",
      "Iteration 366, Batch: 10, Loss: 0.06274048984050751\n",
      "Iteration 366, Batch: 11, Loss: 0.060672733932733536\n",
      "Iteration 366, Batch: 12, Loss: 0.0672507956624031\n",
      "Iteration 366, Batch: 13, Loss: 0.0732836052775383\n",
      "Iteration 366, Batch: 14, Loss: 0.053668078035116196\n",
      "Iteration 366, Batch: 15, Loss: 0.0752781555056572\n",
      "Iteration 366, Batch: 16, Loss: 0.10385704040527344\n",
      "Iteration 366, Batch: 17, Loss: 0.08865892142057419\n",
      "Iteration 366, Batch: 18, Loss: 0.01927357353270054\n",
      "Iteration 366, Batch: 19, Loss: 0.06570415198802948\n",
      "Iteration 366, Batch: 20, Loss: 0.0384063720703125\n",
      "Iteration 366, Batch: 21, Loss: 0.05209983140230179\n",
      "Iteration 366, Batch: 22, Loss: 0.04948186129331589\n",
      "Iteration 366, Batch: 23, Loss: 0.0383218415081501\n",
      "Iteration 366, Batch: 24, Loss: 0.021662423387169838\n",
      "Iteration 366, Batch: 25, Loss: 0.053175248205661774\n",
      "Iteration 366, Batch: 26, Loss: 0.024903740733861923\n",
      "Iteration 366, Batch: 27, Loss: 0.035000815987586975\n",
      "Iteration 366, Batch: 28, Loss: 0.0473979189991951\n",
      "Iteration 366, Batch: 29, Loss: 0.06525126844644547\n",
      "Iteration 366, Batch: 30, Loss: 0.06083701178431511\n",
      "Iteration 366, Batch: 31, Loss: 0.052304934710264206\n",
      "Iteration 366, Batch: 32, Loss: 0.04685378819704056\n",
      "Iteration 366, Batch: 33, Loss: 0.0324774831533432\n",
      "Iteration 366, Batch: 34, Loss: 0.03481234982609749\n",
      "Iteration 366, Batch: 35, Loss: 0.07461919635534286\n",
      "Iteration 366, Batch: 36, Loss: 0.0755862146615982\n",
      "Iteration 366, Batch: 37, Loss: 0.032013241201639175\n",
      "Iteration 366, Batch: 38, Loss: 0.07008527219295502\n",
      "Iteration 366, Batch: 39, Loss: 0.043844420462846756\n",
      "Iteration 366, Batch: 40, Loss: 0.0385708250105381\n",
      "Iteration 366, Batch: 41, Loss: 0.030598176643252373\n",
      "Iteration 366, Batch: 42, Loss: 0.04155057668685913\n",
      "Iteration 366, Batch: 43, Loss: 0.06282887607812881\n",
      "Iteration 366, Batch: 44, Loss: 0.027799148112535477\n",
      "Iteration 366, Batch: 45, Loss: 0.07429972290992737\n",
      "Iteration 366, Batch: 46, Loss: 0.05868682265281677\n",
      "Iteration 366, Batch: 47, Loss: 0.07260885089635849\n",
      "Iteration 366, Batch: 48, Loss: 0.054150838404893875\n",
      "Iteration 366, Batch: 49, Loss: 0.15874351561069489\n",
      "Number of layers: 10\n",
      "Iteration 367, Batch: 0, Loss: 0.2693478465080261\n",
      "Iteration 367, Batch: 1, Loss: 0.18451017141342163\n",
      "Iteration 367, Batch: 2, Loss: 0.08916637301445007\n",
      "Iteration 367, Batch: 3, Loss: 0.0955822765827179\n",
      "Iteration 367, Batch: 4, Loss: 0.1264418214559555\n",
      "Iteration 367, Batch: 5, Loss: 0.09722509980201721\n",
      "Iteration 367, Batch: 6, Loss: 0.09419942647218704\n",
      "Iteration 367, Batch: 7, Loss: 0.07129564136266708\n",
      "Iteration 367, Batch: 8, Loss: 0.06600246578454971\n",
      "Iteration 367, Batch: 9, Loss: 0.09934365749359131\n",
      "Iteration 367, Batch: 10, Loss: 0.07410144805908203\n",
      "Iteration 367, Batch: 11, Loss: 0.06904485821723938\n",
      "Iteration 367, Batch: 12, Loss: 0.08123211562633514\n",
      "Iteration 367, Batch: 13, Loss: 0.06612934172153473\n",
      "Iteration 367, Batch: 14, Loss: 0.0936235785484314\n",
      "Iteration 367, Batch: 15, Loss: 0.0526452474296093\n",
      "Iteration 367, Batch: 16, Loss: 0.08871910721063614\n",
      "Iteration 367, Batch: 17, Loss: 0.06513983756303787\n",
      "Iteration 367, Batch: 18, Loss: 0.04378919675946236\n",
      "Iteration 367, Batch: 19, Loss: 0.05619599297642708\n",
      "Iteration 367, Batch: 20, Loss: 0.0669606551527977\n",
      "Iteration 367, Batch: 21, Loss: 0.09425705671310425\n",
      "Iteration 367, Batch: 22, Loss: 0.08848464488983154\n",
      "Iteration 367, Batch: 23, Loss: 0.08002033829689026\n",
      "Iteration 367, Batch: 24, Loss: 0.06625985354185104\n",
      "Iteration 367, Batch: 25, Loss: 0.04136950895190239\n",
      "Iteration 367, Batch: 26, Loss: 0.06405794620513916\n",
      "Iteration 367, Batch: 27, Loss: 0.06364355236291885\n",
      "Iteration 367, Batch: 28, Loss: 0.05591703951358795\n",
      "Iteration 367, Batch: 29, Loss: 0.054133202880620956\n",
      "Iteration 367, Batch: 30, Loss: 0.049922604113817215\n",
      "Iteration 367, Batch: 31, Loss: 0.05320614576339722\n",
      "Iteration 367, Batch: 32, Loss: 0.09295027703046799\n",
      "Iteration 367, Batch: 33, Loss: 0.061325378715991974\n",
      "Iteration 367, Batch: 34, Loss: 0.0424044169485569\n",
      "Iteration 367, Batch: 35, Loss: 0.06437891721725464\n",
      "Iteration 367, Batch: 36, Loss: 0.0663379579782486\n",
      "Iteration 367, Batch: 37, Loss: 0.05355563014745712\n",
      "Iteration 367, Batch: 38, Loss: 0.09535790979862213\n",
      "Iteration 367, Batch: 39, Loss: 0.09322835505008698\n",
      "Iteration 367, Batch: 40, Loss: 0.11209914833307266\n",
      "Iteration 367, Batch: 41, Loss: 0.12333909422159195\n",
      "Iteration 367, Batch: 42, Loss: 0.13861501216888428\n",
      "Iteration 367, Batch: 43, Loss: 0.09521450102329254\n",
      "Iteration 367, Batch: 44, Loss: 0.09158269315958023\n",
      "Iteration 367, Batch: 45, Loss: 0.11164121329784393\n",
      "Iteration 367, Batch: 46, Loss: 0.07509560883045197\n",
      "Iteration 367, Batch: 47, Loss: 0.07083232700824738\n",
      "Iteration 367, Batch: 48, Loss: 0.06496808677911758\n",
      "Iteration 367, Batch: 49, Loss: 0.056951288133859634\n",
      "Number of layers: 10\n",
      "Iteration 368, Batch: 0, Loss: 0.07528756558895111\n",
      "Iteration 368, Batch: 1, Loss: 0.06704021990299225\n",
      "Iteration 368, Batch: 2, Loss: 0.06719185411930084\n",
      "Iteration 368, Batch: 3, Loss: 0.04098976030945778\n",
      "Iteration 368, Batch: 4, Loss: 0.05486306920647621\n",
      "Iteration 368, Batch: 5, Loss: 0.030384663492441177\n",
      "Iteration 368, Batch: 6, Loss: 0.03284464031457901\n",
      "Iteration 368, Batch: 7, Loss: 0.06259665638208389\n",
      "Iteration 368, Batch: 8, Loss: 0.0651153102517128\n",
      "Iteration 368, Batch: 9, Loss: 0.06831252574920654\n",
      "Iteration 368, Batch: 10, Loss: 0.04357495158910751\n",
      "Iteration 368, Batch: 11, Loss: 0.04929863661527634\n",
      "Iteration 368, Batch: 12, Loss: 0.04771675169467926\n",
      "Iteration 368, Batch: 13, Loss: 0.06939589977264404\n",
      "Iteration 368, Batch: 14, Loss: 0.10889751464128494\n",
      "Iteration 368, Batch: 15, Loss: 0.03353964909911156\n",
      "Iteration 368, Batch: 16, Loss: 0.055068809539079666\n",
      "Iteration 368, Batch: 17, Loss: 0.05009313300251961\n",
      "Iteration 368, Batch: 18, Loss: 0.03699348121881485\n",
      "Iteration 368, Batch: 19, Loss: 0.032216206192970276\n",
      "Iteration 368, Batch: 20, Loss: 0.03704272210597992\n",
      "Iteration 368, Batch: 21, Loss: 0.056338243186473846\n",
      "Iteration 368, Batch: 22, Loss: 0.060769837349653244\n",
      "Iteration 368, Batch: 23, Loss: 0.09154403209686279\n",
      "Iteration 368, Batch: 24, Loss: 0.04705432802438736\n",
      "Iteration 368, Batch: 25, Loss: 0.04072060063481331\n",
      "Iteration 368, Batch: 26, Loss: 0.05736250430345535\n",
      "Iteration 368, Batch: 27, Loss: 0.04288431257009506\n",
      "Iteration 368, Batch: 28, Loss: 0.06251254677772522\n",
      "Iteration 368, Batch: 29, Loss: 0.041680220514535904\n",
      "Iteration 368, Batch: 30, Loss: 0.057202279567718506\n",
      "Iteration 368, Batch: 31, Loss: 0.0576460175216198\n",
      "Iteration 368, Batch: 32, Loss: 0.074515700340271\n",
      "Iteration 368, Batch: 33, Loss: 0.08357597142457962\n",
      "Iteration 368, Batch: 34, Loss: 0.029909079894423485\n",
      "Iteration 368, Batch: 35, Loss: 0.06052135303616524\n",
      "Iteration 368, Batch: 36, Loss: 0.069420225918293\n",
      "Iteration 368, Batch: 37, Loss: 0.0508841872215271\n",
      "Iteration 368, Batch: 38, Loss: 0.05356157198548317\n",
      "Iteration 368, Batch: 39, Loss: 0.04498734697699547\n",
      "Iteration 368, Batch: 40, Loss: 0.040681350976228714\n",
      "Iteration 368, Batch: 41, Loss: 0.06813864409923553\n",
      "Iteration 368, Batch: 42, Loss: 0.07600754499435425\n",
      "Iteration 368, Batch: 43, Loss: 0.06495945155620575\n",
      "Iteration 368, Batch: 44, Loss: 0.08119049668312073\n",
      "Iteration 368, Batch: 45, Loss: 0.057739291340112686\n",
      "Iteration 368, Batch: 46, Loss: 0.05057446286082268\n",
      "Iteration 368, Batch: 47, Loss: 0.0374162383377552\n",
      "Iteration 368, Batch: 48, Loss: 0.05497566983103752\n",
      "Iteration 368, Batch: 49, Loss: 0.025187956169247627\n",
      "Number of layers: 10\n",
      "Iteration 369, Batch: 0, Loss: 0.04138077050447464\n",
      "Iteration 369, Batch: 1, Loss: 0.031181687489151955\n",
      "Iteration 369, Batch: 2, Loss: 0.059741418808698654\n",
      "Iteration 369, Batch: 3, Loss: 0.04476751387119293\n",
      "Iteration 369, Batch: 4, Loss: 0.0651065930724144\n",
      "Iteration 369, Batch: 5, Loss: 0.0331394299864769\n",
      "Iteration 369, Batch: 6, Loss: 0.06477545946836472\n",
      "Iteration 369, Batch: 7, Loss: 0.05897907167673111\n",
      "Iteration 369, Batch: 8, Loss: 0.061961449682712555\n",
      "Iteration 369, Batch: 9, Loss: 0.08382091671228409\n",
      "Iteration 369, Batch: 10, Loss: 0.056117944419384\n",
      "Iteration 369, Batch: 11, Loss: 0.051838234066963196\n",
      "Iteration 369, Batch: 12, Loss: 0.07602076977491379\n",
      "Iteration 369, Batch: 13, Loss: 0.06750009208917618\n",
      "Iteration 369, Batch: 14, Loss: 0.03588779270648956\n",
      "Iteration 369, Batch: 15, Loss: 0.051896002143621445\n",
      "Iteration 369, Batch: 16, Loss: 0.04906637221574783\n",
      "Iteration 369, Batch: 17, Loss: 0.04632164537906647\n",
      "Iteration 369, Batch: 18, Loss: 0.0739813819527626\n",
      "Iteration 369, Batch: 19, Loss: 0.03753752261400223\n",
      "Iteration 369, Batch: 20, Loss: 0.054266441613435745\n",
      "Iteration 369, Batch: 21, Loss: 0.059101685881614685\n",
      "Iteration 369, Batch: 22, Loss: 0.023251280188560486\n",
      "Iteration 369, Batch: 23, Loss: 0.017008453607559204\n",
      "Iteration 369, Batch: 24, Loss: 0.03989594057202339\n",
      "Iteration 369, Batch: 25, Loss: 0.040415018796920776\n",
      "Iteration 369, Batch: 26, Loss: 0.049841538071632385\n",
      "Iteration 369, Batch: 27, Loss: 0.045538853853940964\n",
      "Iteration 369, Batch: 28, Loss: 0.08316852152347565\n",
      "Iteration 369, Batch: 29, Loss: 0.1334095299243927\n",
      "Iteration 369, Batch: 30, Loss: 0.06348272413015366\n",
      "Iteration 369, Batch: 31, Loss: 0.0759260430932045\n",
      "Iteration 369, Batch: 32, Loss: 0.052606791257858276\n",
      "Iteration 369, Batch: 33, Loss: 0.0709395483136177\n",
      "Iteration 369, Batch: 34, Loss: 0.07175898551940918\n",
      "Iteration 369, Batch: 35, Loss: 0.048109427094459534\n",
      "Iteration 369, Batch: 36, Loss: 0.05759125202894211\n",
      "Iteration 369, Batch: 37, Loss: 0.04616747796535492\n",
      "Iteration 369, Batch: 38, Loss: 0.02913072519004345\n",
      "Iteration 369, Batch: 39, Loss: 0.018196139484643936\n",
      "Iteration 369, Batch: 40, Loss: 0.03376072272658348\n",
      "Iteration 369, Batch: 41, Loss: 0.05608230456709862\n",
      "Iteration 369, Batch: 42, Loss: 0.04376431182026863\n",
      "Iteration 369, Batch: 43, Loss: 0.04258796200156212\n",
      "Iteration 369, Batch: 44, Loss: 0.03353187069296837\n",
      "Iteration 369, Batch: 45, Loss: 0.04392427206039429\n",
      "Iteration 369, Batch: 46, Loss: 0.04799555614590645\n",
      "Iteration 369, Batch: 47, Loss: 0.036938317120075226\n",
      "Iteration 369, Batch: 48, Loss: 0.05662838742136955\n",
      "Iteration 369, Batch: 49, Loss: 0.0457434318959713\n",
      "Number of layers: 10\n",
      "Iteration 370, Batch: 0, Loss: 0.03161545470356941\n",
      "Iteration 370, Batch: 1, Loss: 0.06619752198457718\n",
      "Iteration 370, Batch: 2, Loss: 0.04311646521091461\n",
      "Iteration 370, Batch: 3, Loss: 0.05728033930063248\n",
      "Iteration 370, Batch: 4, Loss: 0.04168064147233963\n",
      "Iteration 370, Batch: 5, Loss: 0.033999569714069366\n",
      "Iteration 370, Batch: 6, Loss: 0.04156922176480293\n",
      "Iteration 370, Batch: 7, Loss: 0.06503064185380936\n",
      "Iteration 370, Batch: 8, Loss: 0.037779808044433594\n",
      "Iteration 370, Batch: 9, Loss: 0.06822522729635239\n",
      "Iteration 370, Batch: 10, Loss: 0.04914028197526932\n",
      "Iteration 370, Batch: 11, Loss: 0.07005435228347778\n",
      "Iteration 370, Batch: 12, Loss: 0.055377185344696045\n",
      "Iteration 370, Batch: 13, Loss: 0.04964783415198326\n",
      "Iteration 370, Batch: 14, Loss: 0.050021637231111526\n",
      "Iteration 370, Batch: 15, Loss: 0.057636793702840805\n",
      "Iteration 370, Batch: 16, Loss: 0.04082426056265831\n",
      "Iteration 370, Batch: 17, Loss: 0.0429856963455677\n",
      "Iteration 370, Batch: 18, Loss: 0.110369473695755\n",
      "Iteration 370, Batch: 19, Loss: 0.07569257915019989\n",
      "Iteration 370, Batch: 20, Loss: 0.1012549102306366\n",
      "Iteration 370, Batch: 21, Loss: 0.06668811291456223\n",
      "Iteration 370, Batch: 22, Loss: 0.058323029428720474\n",
      "Iteration 370, Batch: 23, Loss: 0.061044346541166306\n",
      "Iteration 370, Batch: 24, Loss: 0.0421949103474617\n",
      "Iteration 370, Batch: 25, Loss: 0.08466513454914093\n",
      "Iteration 370, Batch: 26, Loss: 0.07494916021823883\n",
      "Iteration 370, Batch: 27, Loss: 0.07553992420434952\n",
      "Iteration 370, Batch: 28, Loss: 0.05038207396864891\n",
      "Iteration 370, Batch: 29, Loss: 0.0708981528878212\n",
      "Iteration 370, Batch: 30, Loss: 0.0653315857052803\n",
      "Iteration 370, Batch: 31, Loss: 0.03461688384413719\n",
      "Iteration 370, Batch: 32, Loss: 0.054721344262361526\n",
      "Iteration 370, Batch: 33, Loss: 0.057546891272068024\n",
      "Iteration 370, Batch: 34, Loss: 0.032928548753261566\n",
      "Iteration 370, Batch: 35, Loss: 0.0621701218187809\n",
      "Iteration 370, Batch: 36, Loss: 0.06238367035984993\n",
      "Iteration 370, Batch: 37, Loss: 0.049326758831739426\n",
      "Iteration 370, Batch: 38, Loss: 0.056624121963977814\n",
      "Iteration 370, Batch: 39, Loss: 0.08933372050523758\n",
      "Iteration 370, Batch: 40, Loss: 0.04080949351191521\n",
      "Iteration 370, Batch: 41, Loss: 0.03753820061683655\n",
      "Iteration 370, Batch: 42, Loss: 0.05921497195959091\n",
      "Iteration 370, Batch: 43, Loss: 0.06839608401060104\n",
      "Iteration 370, Batch: 44, Loss: 0.08096031099557877\n",
      "Iteration 370, Batch: 45, Loss: 0.06854260712862015\n",
      "Iteration 370, Batch: 46, Loss: 0.06481622904539108\n",
      "Iteration 370, Batch: 47, Loss: 0.04568206146359444\n",
      "Iteration 370, Batch: 48, Loss: 0.051677316427230835\n",
      "Iteration 370, Batch: 49, Loss: 0.07050483673810959\n",
      "Number of layers: 10\n",
      "Iteration 371, Batch: 0, Loss: 0.06317885965108871\n",
      "Iteration 371, Batch: 1, Loss: 0.09448104351758957\n",
      "Iteration 371, Batch: 2, Loss: 0.05878908187150955\n",
      "Iteration 371, Batch: 3, Loss: 0.05862812697887421\n",
      "Iteration 371, Batch: 4, Loss: 0.05695423483848572\n",
      "Iteration 371, Batch: 5, Loss: 0.057038936764001846\n",
      "Iteration 371, Batch: 6, Loss: 0.05381074175238609\n",
      "Iteration 371, Batch: 7, Loss: 0.04081211984157562\n",
      "Iteration 371, Batch: 8, Loss: 0.028520839288830757\n",
      "Iteration 371, Batch: 9, Loss: 0.05040114372968674\n",
      "Iteration 371, Batch: 10, Loss: 0.05301225930452347\n",
      "Iteration 371, Batch: 11, Loss: 0.03719278424978256\n",
      "Iteration 371, Batch: 12, Loss: 0.02547612413764\n",
      "Iteration 371, Batch: 13, Loss: 0.03317560255527496\n",
      "Iteration 371, Batch: 14, Loss: 0.05337009206414223\n",
      "Iteration 371, Batch: 15, Loss: 0.03330988809466362\n",
      "Iteration 371, Batch: 16, Loss: 0.04550963640213013\n",
      "Iteration 371, Batch: 17, Loss: 0.07578811794519424\n",
      "Iteration 371, Batch: 18, Loss: 0.06648667901754379\n",
      "Iteration 371, Batch: 19, Loss: 0.05835438147187233\n",
      "Iteration 371, Batch: 20, Loss: 0.05629061535000801\n",
      "Iteration 371, Batch: 21, Loss: 0.03557726368308067\n",
      "Iteration 371, Batch: 22, Loss: 0.059999559074640274\n",
      "Iteration 371, Batch: 23, Loss: 0.037287939339876175\n",
      "Iteration 371, Batch: 24, Loss: 0.05318637564778328\n",
      "Iteration 371, Batch: 25, Loss: 0.02326560951769352\n",
      "Iteration 371, Batch: 26, Loss: 0.03164424002170563\n",
      "Iteration 371, Batch: 27, Loss: 0.040387123823165894\n",
      "Iteration 371, Batch: 28, Loss: 0.048609305173158646\n",
      "Iteration 371, Batch: 29, Loss: 0.05496211722493172\n",
      "Iteration 371, Batch: 30, Loss: 0.04639691114425659\n",
      "Iteration 371, Batch: 31, Loss: 0.03520769998431206\n",
      "Iteration 371, Batch: 32, Loss: 0.035652413964271545\n",
      "Iteration 371, Batch: 33, Loss: 0.05190841853618622\n",
      "Iteration 371, Batch: 34, Loss: 0.04113694280385971\n",
      "Iteration 371, Batch: 35, Loss: 0.05232479050755501\n",
      "Iteration 371, Batch: 36, Loss: 0.06100315600633621\n",
      "Iteration 371, Batch: 37, Loss: 0.03194321691989899\n",
      "Iteration 371, Batch: 38, Loss: 0.03151547908782959\n",
      "Iteration 371, Batch: 39, Loss: 0.047007493674755096\n",
      "Iteration 371, Batch: 40, Loss: 0.03752148151397705\n",
      "Iteration 371, Batch: 41, Loss: 0.047720327973365784\n",
      "Iteration 371, Batch: 42, Loss: 0.040295131504535675\n",
      "Iteration 371, Batch: 43, Loss: 0.040105756372213364\n",
      "Iteration 371, Batch: 44, Loss: 0.04001857340335846\n",
      "Iteration 371, Batch: 45, Loss: 0.05736284703016281\n",
      "Iteration 371, Batch: 46, Loss: 0.04461466521024704\n",
      "Iteration 371, Batch: 47, Loss: 0.022908473387360573\n",
      "Iteration 371, Batch: 48, Loss: 0.038474615663290024\n",
      "Iteration 371, Batch: 49, Loss: 0.04569111764431\n",
      "Number of layers: 10\n",
      "Iteration 372, Batch: 0, Loss: 0.05839208886027336\n",
      "Iteration 372, Batch: 1, Loss: 0.06578239053487778\n",
      "Iteration 372, Batch: 2, Loss: 0.07096239179372787\n",
      "Iteration 372, Batch: 3, Loss: 0.04166009649634361\n",
      "Iteration 372, Batch: 4, Loss: 0.04629679396748543\n",
      "Iteration 372, Batch: 5, Loss: 0.04495161771774292\n",
      "Iteration 372, Batch: 6, Loss: 0.04858751595020294\n",
      "Iteration 372, Batch: 7, Loss: 0.05811541527509689\n",
      "Iteration 372, Batch: 8, Loss: 0.03973562270402908\n",
      "Iteration 372, Batch: 9, Loss: 0.034820809960365295\n",
      "Iteration 372, Batch: 10, Loss: 0.0323338657617569\n",
      "Iteration 372, Batch: 11, Loss: 0.026135677471756935\n",
      "Iteration 372, Batch: 12, Loss: 0.05762066692113876\n",
      "Iteration 372, Batch: 13, Loss: 0.05308983847498894\n",
      "Iteration 372, Batch: 14, Loss: 0.04589628428220749\n",
      "Iteration 372, Batch: 15, Loss: 0.06419514864683151\n",
      "Iteration 372, Batch: 16, Loss: 0.019435852766036987\n",
      "Iteration 372, Batch: 17, Loss: 0.04714251309633255\n",
      "Iteration 372, Batch: 18, Loss: 0.03926638141274452\n",
      "Iteration 372, Batch: 19, Loss: 0.02752630226314068\n",
      "Iteration 372, Batch: 20, Loss: 0.05293332785367966\n",
      "Iteration 372, Batch: 21, Loss: 0.03180793300271034\n",
      "Iteration 372, Batch: 22, Loss: 0.04467438906431198\n",
      "Iteration 372, Batch: 23, Loss: 0.04481353610754013\n",
      "Iteration 372, Batch: 24, Loss: 0.021173743531107903\n",
      "Iteration 372, Batch: 25, Loss: 0.06491098552942276\n",
      "Iteration 372, Batch: 26, Loss: 0.055928830057382584\n",
      "Iteration 372, Batch: 27, Loss: 0.03338104113936424\n",
      "Iteration 372, Batch: 28, Loss: 0.05243958532810211\n",
      "Iteration 372, Batch: 29, Loss: 0.048684995621442795\n",
      "Iteration 372, Batch: 30, Loss: 0.04105396196246147\n",
      "Iteration 372, Batch: 31, Loss: 0.0381435826420784\n",
      "Iteration 372, Batch: 32, Loss: 0.04371660202741623\n",
      "Iteration 372, Batch: 33, Loss: 0.05531312897801399\n",
      "Iteration 372, Batch: 34, Loss: 0.061074528843164444\n",
      "Iteration 372, Batch: 35, Loss: 0.04459377005696297\n",
      "Iteration 372, Batch: 36, Loss: 0.04649337753653526\n",
      "Iteration 372, Batch: 37, Loss: 0.03427884355187416\n",
      "Iteration 372, Batch: 38, Loss: 0.02562124654650688\n",
      "Iteration 372, Batch: 39, Loss: 0.015024621970951557\n",
      "Iteration 372, Batch: 40, Loss: 0.008224483579397202\n",
      "Iteration 372, Batch: 41, Loss: 0.035818763077259064\n",
      "Iteration 372, Batch: 42, Loss: 0.04121928662061691\n",
      "Iteration 372, Batch: 43, Loss: 0.032906439155340195\n",
      "Iteration 372, Batch: 44, Loss: 0.051616523414850235\n",
      "Iteration 372, Batch: 45, Loss: 0.03693046793341637\n",
      "Iteration 372, Batch: 46, Loss: 0.051726073026657104\n",
      "Iteration 372, Batch: 47, Loss: 0.06535415351390839\n",
      "Iteration 372, Batch: 48, Loss: 0.03298509865999222\n",
      "Iteration 372, Batch: 49, Loss: 0.03854174166917801\n",
      "Number of layers: 10\n",
      "Iteration 373, Batch: 0, Loss: 0.021538829430937767\n",
      "Iteration 373, Batch: 1, Loss: 0.060031335800886154\n",
      "Iteration 373, Batch: 2, Loss: 0.04238055273890495\n",
      "Iteration 373, Batch: 3, Loss: 0.025931162759661674\n",
      "Iteration 373, Batch: 4, Loss: 0.05226830765604973\n",
      "Iteration 373, Batch: 5, Loss: 0.043925005942583084\n",
      "Iteration 373, Batch: 6, Loss: 0.028842469677329063\n",
      "Iteration 373, Batch: 7, Loss: 0.03317970409989357\n",
      "Iteration 373, Batch: 8, Loss: 0.00496830977499485\n",
      "Iteration 373, Batch: 9, Loss: 0.05447732284665108\n",
      "Iteration 373, Batch: 10, Loss: 0.045365504920482635\n",
      "Iteration 373, Batch: 11, Loss: 0.0280163511633873\n",
      "Iteration 373, Batch: 12, Loss: 0.023619865998625755\n",
      "Iteration 373, Batch: 13, Loss: 0.039737384766340256\n",
      "Iteration 373, Batch: 14, Loss: 0.02516135945916176\n",
      "Iteration 373, Batch: 15, Loss: 0.04539244994521141\n",
      "Iteration 373, Batch: 16, Loss: 0.03971465677022934\n",
      "Iteration 373, Batch: 17, Loss: 0.03466946259140968\n",
      "Iteration 373, Batch: 18, Loss: 0.035389672964811325\n",
      "Iteration 373, Batch: 19, Loss: 0.03716164827346802\n",
      "Iteration 373, Batch: 20, Loss: 0.05337020754814148\n",
      "Iteration 373, Batch: 21, Loss: 0.024446234107017517\n",
      "Iteration 373, Batch: 22, Loss: 0.029700612649321556\n",
      "Iteration 373, Batch: 23, Loss: 0.049586404114961624\n",
      "Iteration 373, Batch: 24, Loss: 0.03355017676949501\n",
      "Iteration 373, Batch: 25, Loss: 0.0169084370136261\n",
      "Iteration 373, Batch: 26, Loss: 0.031076477840542793\n",
      "Iteration 373, Batch: 27, Loss: 0.02635841816663742\n",
      "Iteration 373, Batch: 28, Loss: 0.03933404013514519\n",
      "Iteration 373, Batch: 29, Loss: 0.05296294018626213\n",
      "Iteration 373, Batch: 30, Loss: 0.025751212611794472\n",
      "Iteration 373, Batch: 31, Loss: 0.05518215894699097\n",
      "Iteration 373, Batch: 32, Loss: 0.04152344912290573\n",
      "Iteration 373, Batch: 33, Loss: 0.036171574145555496\n",
      "Iteration 373, Batch: 34, Loss: 0.043545402586460114\n",
      "Iteration 373, Batch: 35, Loss: 0.051715150475502014\n",
      "Iteration 373, Batch: 36, Loss: 0.04094910994172096\n",
      "Iteration 373, Batch: 37, Loss: 0.040249619632959366\n",
      "Iteration 373, Batch: 38, Loss: 0.08153986930847168\n",
      "Iteration 373, Batch: 39, Loss: 0.0826558917760849\n",
      "Iteration 373, Batch: 40, Loss: 0.14440464973449707\n",
      "Iteration 373, Batch: 41, Loss: 0.04099549353122711\n",
      "Iteration 373, Batch: 42, Loss: 0.06686610728502274\n",
      "Iteration 373, Batch: 43, Loss: 0.0532289557158947\n",
      "Iteration 373, Batch: 44, Loss: 0.04413238540291786\n",
      "Iteration 373, Batch: 45, Loss: 0.041911736130714417\n",
      "Iteration 373, Batch: 46, Loss: 0.03699788823723793\n",
      "Iteration 373, Batch: 47, Loss: 0.03369196504354477\n",
      "Iteration 373, Batch: 48, Loss: 0.05399710685014725\n",
      "Iteration 373, Batch: 49, Loss: 0.04410451650619507\n",
      "Number of layers: 10\n",
      "Iteration 374, Batch: 0, Loss: 0.03839074447751045\n",
      "Iteration 374, Batch: 1, Loss: 0.05376913398504257\n",
      "Iteration 374, Batch: 2, Loss: 0.07496993988752365\n",
      "Iteration 374, Batch: 3, Loss: 0.04388236254453659\n",
      "Iteration 374, Batch: 4, Loss: 0.05349018797278404\n",
      "Iteration 374, Batch: 5, Loss: 0.03855770453810692\n",
      "Iteration 374, Batch: 6, Loss: 0.04667621850967407\n",
      "Iteration 374, Batch: 7, Loss: 0.044586945325136185\n",
      "Iteration 374, Batch: 8, Loss: 0.040847957134246826\n",
      "Iteration 374, Batch: 9, Loss: 0.05597221106290817\n",
      "Iteration 374, Batch: 10, Loss: 0.016397623345255852\n",
      "Iteration 374, Batch: 11, Loss: 0.04868829622864723\n",
      "Iteration 374, Batch: 12, Loss: 0.026304299011826515\n",
      "Iteration 374, Batch: 13, Loss: 0.04070216417312622\n",
      "Iteration 374, Batch: 14, Loss: 0.03991381824016571\n",
      "Iteration 374, Batch: 15, Loss: 0.06064312160015106\n",
      "Iteration 374, Batch: 16, Loss: 0.01928747445344925\n",
      "Iteration 374, Batch: 17, Loss: 0.05842767655849457\n",
      "Iteration 374, Batch: 18, Loss: 0.0467194989323616\n",
      "Iteration 374, Batch: 19, Loss: 0.05676842853426933\n",
      "Iteration 374, Batch: 20, Loss: 0.04120083898305893\n",
      "Iteration 374, Batch: 21, Loss: 0.04813842102885246\n",
      "Iteration 374, Batch: 22, Loss: 0.05194613337516785\n",
      "Iteration 374, Batch: 23, Loss: 0.03938900679349899\n",
      "Iteration 374, Batch: 24, Loss: 0.06074860319495201\n",
      "Iteration 374, Batch: 25, Loss: 0.06548944115638733\n",
      "Iteration 374, Batch: 26, Loss: 0.048010680824518204\n",
      "Iteration 374, Batch: 27, Loss: 0.03243091329932213\n",
      "Iteration 374, Batch: 28, Loss: 0.05069886893033981\n",
      "Iteration 374, Batch: 29, Loss: 0.05407704412937164\n",
      "Iteration 374, Batch: 30, Loss: 0.021611640229821205\n",
      "Iteration 374, Batch: 31, Loss: 0.054689984768629074\n",
      "Iteration 374, Batch: 32, Loss: 0.04391040280461311\n",
      "Iteration 374, Batch: 33, Loss: 0.030326535925269127\n",
      "Iteration 374, Batch: 34, Loss: 0.05091369152069092\n",
      "Iteration 374, Batch: 35, Loss: 0.036650385707616806\n",
      "Iteration 374, Batch: 36, Loss: 0.03373555466532707\n",
      "Iteration 374, Batch: 37, Loss: 0.03692896291613579\n",
      "Iteration 374, Batch: 38, Loss: 0.048297666013240814\n",
      "Iteration 374, Batch: 39, Loss: 0.06076304242014885\n",
      "Iteration 374, Batch: 40, Loss: 0.022556500509381294\n",
      "Iteration 374, Batch: 41, Loss: 0.04100437089800835\n",
      "Iteration 374, Batch: 42, Loss: 0.05519775301218033\n",
      "Iteration 374, Batch: 43, Loss: 0.04630052298307419\n",
      "Iteration 374, Batch: 44, Loss: 0.054165083914995193\n",
      "Iteration 374, Batch: 45, Loss: 0.056801050901412964\n",
      "Iteration 374, Batch: 46, Loss: 0.034394800662994385\n",
      "Iteration 374, Batch: 47, Loss: 0.046447865664958954\n",
      "Iteration 374, Batch: 48, Loss: 0.03847636282444\n",
      "Iteration 374, Batch: 49, Loss: 0.053936030715703964\n",
      "Number of layers: 10\n",
      "Iteration 375, Batch: 0, Loss: 0.029960718005895615\n",
      "Iteration 375, Batch: 1, Loss: 0.04877994954586029\n",
      "Iteration 375, Batch: 2, Loss: 0.06495354324579239\n",
      "Iteration 375, Batch: 3, Loss: 0.053359631448984146\n",
      "Iteration 375, Batch: 4, Loss: 0.032472480088472366\n",
      "Iteration 375, Batch: 5, Loss: 0.04158890247344971\n",
      "Iteration 375, Batch: 6, Loss: 0.05957670882344246\n",
      "Iteration 375, Batch: 7, Loss: 0.028211453929543495\n",
      "Iteration 375, Batch: 8, Loss: 0.06082608178257942\n",
      "Iteration 375, Batch: 9, Loss: 0.03965501859784126\n",
      "Iteration 375, Batch: 10, Loss: 0.06145969405770302\n",
      "Iteration 375, Batch: 11, Loss: 0.06188992038369179\n",
      "Iteration 375, Batch: 12, Loss: 0.05328671261668205\n",
      "Iteration 375, Batch: 13, Loss: 0.033284757286310196\n",
      "Iteration 375, Batch: 14, Loss: 0.06126432120800018\n",
      "Iteration 375, Batch: 15, Loss: 0.05281771719455719\n",
      "Iteration 375, Batch: 16, Loss: 0.05553143471479416\n",
      "Iteration 375, Batch: 17, Loss: 0.0746975988149643\n",
      "Iteration 375, Batch: 18, Loss: 0.08203320950269699\n",
      "Iteration 375, Batch: 19, Loss: 0.07618080079555511\n",
      "Iteration 375, Batch: 20, Loss: 0.04264634847640991\n",
      "Iteration 375, Batch: 21, Loss: 0.06054820865392685\n",
      "Iteration 375, Batch: 22, Loss: 0.055852074176073074\n",
      "Iteration 375, Batch: 23, Loss: 0.047869451344013214\n",
      "Iteration 375, Batch: 24, Loss: 0.054839007556438446\n",
      "Iteration 375, Batch: 25, Loss: 0.05452287569642067\n",
      "Iteration 375, Batch: 26, Loss: 0.056318070739507675\n",
      "Iteration 375, Batch: 27, Loss: 0.043726224452257156\n",
      "Iteration 375, Batch: 28, Loss: 0.03292125463485718\n",
      "Iteration 375, Batch: 29, Loss: 0.07052931189537048\n",
      "Iteration 375, Batch: 30, Loss: 0.036172736436128616\n",
      "Iteration 375, Batch: 31, Loss: 0.06795921176671982\n",
      "Iteration 375, Batch: 32, Loss: 0.045837707817554474\n",
      "Iteration 375, Batch: 33, Loss: 0.05060657858848572\n",
      "Iteration 375, Batch: 34, Loss: 0.033197131007909775\n",
      "Iteration 375, Batch: 35, Loss: 0.04923003166913986\n",
      "Iteration 375, Batch: 36, Loss: 0.053471870720386505\n",
      "Iteration 375, Batch: 37, Loss: 0.036233365535736084\n",
      "Iteration 375, Batch: 38, Loss: 0.03574783354997635\n",
      "Iteration 375, Batch: 39, Loss: 0.04572154954075813\n",
      "Iteration 375, Batch: 40, Loss: 0.05420694872736931\n",
      "Iteration 375, Batch: 41, Loss: 0.043837498873472214\n",
      "Iteration 375, Batch: 42, Loss: 0.08088910579681396\n",
      "Iteration 375, Batch: 43, Loss: 0.04579875245690346\n",
      "Iteration 375, Batch: 44, Loss: 0.04427267983555794\n",
      "Iteration 375, Batch: 45, Loss: 0.07109825313091278\n",
      "Iteration 375, Batch: 46, Loss: 0.0937073603272438\n",
      "Iteration 375, Batch: 47, Loss: 0.05391930788755417\n",
      "Iteration 375, Batch: 48, Loss: 0.04541247338056564\n",
      "Iteration 375, Batch: 49, Loss: 0.056941185146570206\n",
      "Number of layers: 10\n",
      "Iteration 376, Batch: 0, Loss: 0.04728718101978302\n",
      "Iteration 376, Batch: 1, Loss: 0.036865197122097015\n",
      "Iteration 376, Batch: 2, Loss: 0.0328054316341877\n",
      "Iteration 376, Batch: 3, Loss: 0.05404923856258392\n",
      "Iteration 376, Batch: 4, Loss: 0.025710105895996094\n",
      "Iteration 376, Batch: 5, Loss: 0.05485796183347702\n",
      "Iteration 376, Batch: 6, Loss: 0.015065876767039299\n",
      "Iteration 376, Batch: 7, Loss: 0.02239849604666233\n",
      "Iteration 376, Batch: 8, Loss: 0.06624168902635574\n",
      "Iteration 376, Batch: 9, Loss: 0.04055221006274223\n",
      "Iteration 376, Batch: 10, Loss: 0.06296203285455704\n",
      "Iteration 376, Batch: 11, Loss: 0.0529169924557209\n",
      "Iteration 376, Batch: 12, Loss: 0.04780922830104828\n",
      "Iteration 376, Batch: 13, Loss: 0.0650421679019928\n",
      "Iteration 376, Batch: 14, Loss: 0.04573729261755943\n",
      "Iteration 376, Batch: 15, Loss: 0.05540672317147255\n",
      "Iteration 376, Batch: 16, Loss: 0.015716947615146637\n",
      "Iteration 376, Batch: 17, Loss: 0.03379319608211517\n",
      "Iteration 376, Batch: 18, Loss: 0.02702251821756363\n",
      "Iteration 376, Batch: 19, Loss: 0.026427382603287697\n",
      "Iteration 376, Batch: 20, Loss: 0.035862501710653305\n",
      "Iteration 376, Batch: 21, Loss: 0.0314922071993351\n",
      "Iteration 376, Batch: 22, Loss: 0.04856645315885544\n",
      "Iteration 376, Batch: 23, Loss: 0.03198092803359032\n",
      "Iteration 376, Batch: 24, Loss: 0.02162076346576214\n",
      "Iteration 376, Batch: 25, Loss: 0.051029376685619354\n",
      "Iteration 376, Batch: 26, Loss: 0.05084637552499771\n",
      "Iteration 376, Batch: 27, Loss: 0.0562388114631176\n",
      "Iteration 376, Batch: 28, Loss: 0.04611732438206673\n",
      "Iteration 376, Batch: 29, Loss: 0.032206933945417404\n",
      "Iteration 376, Batch: 30, Loss: 0.04127451404929161\n",
      "Iteration 376, Batch: 31, Loss: 0.03436233848333359\n",
      "Iteration 376, Batch: 32, Loss: 0.04595179483294487\n",
      "Iteration 376, Batch: 33, Loss: 0.04519825428724289\n",
      "Iteration 376, Batch: 34, Loss: 0.035662826150655746\n",
      "Iteration 376, Batch: 35, Loss: 0.04778674989938736\n",
      "Iteration 376, Batch: 36, Loss: 0.046278584748506546\n",
      "Iteration 376, Batch: 37, Loss: 0.030566668137907982\n",
      "Iteration 376, Batch: 38, Loss: 0.032389163970947266\n",
      "Iteration 376, Batch: 39, Loss: 0.037057869136333466\n",
      "Iteration 376, Batch: 40, Loss: 0.060792338103055954\n",
      "Iteration 376, Batch: 41, Loss: 0.03790465369820595\n",
      "Iteration 376, Batch: 42, Loss: 0.031512301415205\n",
      "Iteration 376, Batch: 43, Loss: 0.02125592902302742\n",
      "Iteration 376, Batch: 44, Loss: 0.02848776988685131\n",
      "Iteration 376, Batch: 45, Loss: 0.0379788838326931\n",
      "Iteration 376, Batch: 46, Loss: 0.0688164085149765\n",
      "Iteration 376, Batch: 47, Loss: 0.03692092373967171\n",
      "Iteration 376, Batch: 48, Loss: 0.03297605365514755\n",
      "Iteration 376, Batch: 49, Loss: 0.05402354151010513\n",
      "Number of layers: 10\n",
      "Iteration 377, Batch: 0, Loss: 0.04204520955681801\n",
      "Iteration 377, Batch: 1, Loss: 0.035705581307411194\n",
      "Iteration 377, Batch: 2, Loss: 0.03763612359762192\n",
      "Iteration 377, Batch: 3, Loss: 0.03651849552989006\n",
      "Iteration 377, Batch: 4, Loss: 0.0448845811188221\n",
      "Iteration 377, Batch: 5, Loss: 0.0580766387283802\n",
      "Iteration 377, Batch: 6, Loss: 0.032211631536483765\n",
      "Iteration 377, Batch: 7, Loss: 0.030853284522891045\n",
      "Iteration 377, Batch: 8, Loss: 0.04620110243558884\n",
      "Iteration 377, Batch: 9, Loss: 0.053261298686265945\n",
      "Iteration 377, Batch: 10, Loss: 0.04626942425966263\n",
      "Iteration 377, Batch: 11, Loss: 0.03327014297246933\n",
      "Iteration 377, Batch: 12, Loss: 0.03122754581272602\n",
      "Iteration 377, Batch: 13, Loss: 0.028574401512742043\n",
      "Iteration 377, Batch: 14, Loss: 0.02989855781197548\n",
      "Iteration 377, Batch: 15, Loss: 0.04100370407104492\n",
      "Iteration 377, Batch: 16, Loss: 0.036600492894649506\n",
      "Iteration 377, Batch: 17, Loss: 0.04622550308704376\n",
      "Iteration 377, Batch: 18, Loss: 0.041654180735349655\n",
      "Iteration 377, Batch: 19, Loss: 0.04092031717300415\n",
      "Iteration 377, Batch: 20, Loss: 0.020866530016064644\n",
      "Iteration 377, Batch: 21, Loss: 0.02331593446433544\n",
      "Iteration 377, Batch: 22, Loss: 0.05852443724870682\n",
      "Iteration 377, Batch: 23, Loss: 0.05258810892701149\n",
      "Iteration 377, Batch: 24, Loss: 0.020766189321875572\n",
      "Iteration 377, Batch: 25, Loss: 0.0618087463080883\n",
      "Iteration 377, Batch: 26, Loss: 0.04450424015522003\n",
      "Iteration 377, Batch: 27, Loss: 0.03345782309770584\n",
      "Iteration 377, Batch: 28, Loss: 0.014807322062551975\n",
      "Iteration 377, Batch: 29, Loss: 0.019112737849354744\n",
      "Iteration 377, Batch: 30, Loss: 0.050714775919914246\n",
      "Iteration 377, Batch: 31, Loss: 0.040274400264024734\n",
      "Iteration 377, Batch: 32, Loss: 0.04377449303865433\n",
      "Iteration 377, Batch: 33, Loss: 0.03549615293741226\n",
      "Iteration 377, Batch: 34, Loss: 0.029368272051215172\n",
      "Iteration 377, Batch: 35, Loss: 0.05226476863026619\n",
      "Iteration 377, Batch: 36, Loss: 0.028619004413485527\n",
      "Iteration 377, Batch: 37, Loss: 0.05716845765709877\n",
      "Iteration 377, Batch: 38, Loss: 0.032375138252973557\n",
      "Iteration 377, Batch: 39, Loss: 0.027333226054906845\n",
      "Iteration 377, Batch: 40, Loss: 0.01889445260167122\n",
      "Iteration 377, Batch: 41, Loss: 0.014132765121757984\n",
      "Iteration 377, Batch: 42, Loss: 0.044485919177532196\n",
      "Iteration 377, Batch: 43, Loss: 0.03916037827730179\n",
      "Iteration 377, Batch: 44, Loss: 0.0284466240555048\n",
      "Iteration 377, Batch: 45, Loss: 0.05725117027759552\n",
      "Iteration 377, Batch: 46, Loss: 0.05068926513195038\n",
      "Iteration 377, Batch: 47, Loss: 0.06169116497039795\n",
      "Iteration 377, Batch: 48, Loss: 0.05912665277719498\n",
      "Iteration 377, Batch: 49, Loss: 0.06060146167874336\n",
      "Number of layers: 10\n",
      "Iteration 378, Batch: 0, Loss: 0.034611545503139496\n",
      "Iteration 378, Batch: 1, Loss: 0.03803345188498497\n",
      "Iteration 378, Batch: 2, Loss: 0.053106069564819336\n",
      "Iteration 378, Batch: 3, Loss: 0.07770131528377533\n",
      "Iteration 378, Batch: 4, Loss: 0.06190713867545128\n",
      "Iteration 378, Batch: 5, Loss: 0.11000386625528336\n",
      "Iteration 378, Batch: 6, Loss: 0.12580880522727966\n",
      "Iteration 378, Batch: 7, Loss: 0.05649097263813019\n",
      "Iteration 378, Batch: 8, Loss: 0.04681667312979698\n",
      "Iteration 378, Batch: 9, Loss: 0.03945876657962799\n",
      "Iteration 378, Batch: 10, Loss: 0.03093769960105419\n",
      "Iteration 378, Batch: 11, Loss: 0.049256790429353714\n",
      "Iteration 378, Batch: 12, Loss: 0.02803051471710205\n",
      "Iteration 378, Batch: 13, Loss: 0.04991579055786133\n",
      "Iteration 378, Batch: 14, Loss: 0.04966754838824272\n",
      "Iteration 378, Batch: 15, Loss: 0.026833057403564453\n",
      "Iteration 378, Batch: 16, Loss: 0.025994814932346344\n",
      "Iteration 378, Batch: 17, Loss: 0.047607019543647766\n",
      "Iteration 378, Batch: 18, Loss: 0.027158252894878387\n",
      "Iteration 378, Batch: 19, Loss: 0.04853584244847298\n",
      "Iteration 378, Batch: 20, Loss: 0.04319746792316437\n",
      "Iteration 378, Batch: 21, Loss: 0.04181786999106407\n",
      "Iteration 378, Batch: 22, Loss: 0.03481663763523102\n",
      "Iteration 378, Batch: 23, Loss: 0.039261870086193085\n",
      "Iteration 378, Batch: 24, Loss: 0.01947399601340294\n",
      "Iteration 378, Batch: 25, Loss: 0.036240871995687485\n",
      "Iteration 378, Batch: 26, Loss: 0.023406079038977623\n",
      "Iteration 378, Batch: 27, Loss: 0.056274157017469406\n",
      "Iteration 378, Batch: 28, Loss: 0.054918210953474045\n",
      "Iteration 378, Batch: 29, Loss: 0.026547228917479515\n",
      "Iteration 378, Batch: 30, Loss: 0.041010964661836624\n",
      "Iteration 378, Batch: 31, Loss: 0.05052342638373375\n",
      "Iteration 378, Batch: 32, Loss: 0.04647123068571091\n",
      "Iteration 378, Batch: 33, Loss: 0.02294744923710823\n",
      "Iteration 378, Batch: 34, Loss: 0.054108332842588425\n",
      "Iteration 378, Batch: 35, Loss: 0.04744589328765869\n",
      "Iteration 378, Batch: 36, Loss: 0.047771111130714417\n",
      "Iteration 378, Batch: 37, Loss: 0.045215919613838196\n",
      "Iteration 378, Batch: 38, Loss: 0.035267751663923264\n",
      "Iteration 378, Batch: 39, Loss: 0.01841004006564617\n",
      "Iteration 378, Batch: 40, Loss: 0.04172747582197189\n",
      "Iteration 378, Batch: 41, Loss: 0.06282541155815125\n",
      "Iteration 378, Batch: 42, Loss: 0.035840604454278946\n",
      "Iteration 378, Batch: 43, Loss: 0.04197072982788086\n",
      "Iteration 378, Batch: 44, Loss: 0.02791052684187889\n",
      "Iteration 378, Batch: 45, Loss: 0.06180361285805702\n",
      "Iteration 378, Batch: 46, Loss: 0.02842254564166069\n",
      "Iteration 378, Batch: 47, Loss: 0.03368046507239342\n",
      "Iteration 378, Batch: 48, Loss: 0.0434463769197464\n",
      "Iteration 378, Batch: 49, Loss: 0.06557866930961609\n",
      "Number of layers: 10\n",
      "Iteration 379, Batch: 0, Loss: 0.06209743395447731\n",
      "Iteration 379, Batch: 1, Loss: 0.030501309782266617\n",
      "Iteration 379, Batch: 2, Loss: 0.08588545024394989\n",
      "Iteration 379, Batch: 3, Loss: 0.04733188450336456\n",
      "Iteration 379, Batch: 4, Loss: 0.06505973637104034\n",
      "Iteration 379, Batch: 5, Loss: 0.07623934745788574\n",
      "Iteration 379, Batch: 6, Loss: 0.0703640803694725\n",
      "Iteration 379, Batch: 7, Loss: 0.0799763947725296\n",
      "Iteration 379, Batch: 8, Loss: 0.07730121165513992\n",
      "Iteration 379, Batch: 9, Loss: 0.08615123480558395\n",
      "Iteration 379, Batch: 10, Loss: 0.050916966050863266\n",
      "Iteration 379, Batch: 11, Loss: 0.048736609518527985\n",
      "Iteration 379, Batch: 12, Loss: 0.08886541426181793\n",
      "Iteration 379, Batch: 13, Loss: 0.053171366453170776\n",
      "Iteration 379, Batch: 14, Loss: 0.051775481551885605\n",
      "Iteration 379, Batch: 15, Loss: 0.07155150920152664\n",
      "Iteration 379, Batch: 16, Loss: 0.07219212502241135\n",
      "Iteration 379, Batch: 17, Loss: 0.07264022529125214\n",
      "Iteration 379, Batch: 18, Loss: 0.06593245267868042\n",
      "Iteration 379, Batch: 19, Loss: 0.07374093681573868\n",
      "Iteration 379, Batch: 20, Loss: 0.09111005812883377\n",
      "Iteration 379, Batch: 21, Loss: 0.0802663043141365\n",
      "Iteration 379, Batch: 22, Loss: 0.0706934705376625\n",
      "Iteration 379, Batch: 23, Loss: 0.0593898668885231\n",
      "Iteration 379, Batch: 24, Loss: 0.06309834122657776\n",
      "Iteration 379, Batch: 25, Loss: 0.0627782866358757\n",
      "Iteration 379, Batch: 26, Loss: 0.04348159581422806\n",
      "Iteration 379, Batch: 27, Loss: 0.07412409782409668\n",
      "Iteration 379, Batch: 28, Loss: 0.04908473789691925\n",
      "Iteration 379, Batch: 29, Loss: 0.04932808876037598\n",
      "Iteration 379, Batch: 30, Loss: 0.03575097396969795\n",
      "Iteration 379, Batch: 31, Loss: 0.04556071013212204\n",
      "Iteration 379, Batch: 32, Loss: 0.061739590018987656\n",
      "Iteration 379, Batch: 33, Loss: 0.05089019984006882\n",
      "Iteration 379, Batch: 34, Loss: 0.05786225199699402\n",
      "Iteration 379, Batch: 35, Loss: 0.0484851635992527\n",
      "Iteration 379, Batch: 36, Loss: 0.06342819333076477\n",
      "Iteration 379, Batch: 37, Loss: 0.031047115102410316\n",
      "Iteration 379, Batch: 38, Loss: 0.05258740484714508\n",
      "Iteration 379, Batch: 39, Loss: 0.048921726644039154\n",
      "Iteration 379, Batch: 40, Loss: 0.05585792288184166\n",
      "Iteration 379, Batch: 41, Loss: 0.046108826994895935\n",
      "Iteration 379, Batch: 42, Loss: 0.04182147979736328\n",
      "Iteration 379, Batch: 43, Loss: 0.022950761020183563\n",
      "Iteration 379, Batch: 44, Loss: 0.02525949478149414\n",
      "Iteration 379, Batch: 45, Loss: 0.015327365137636662\n",
      "Iteration 379, Batch: 46, Loss: 0.03979777172207832\n",
      "Iteration 379, Batch: 47, Loss: 0.05002221092581749\n",
      "Iteration 379, Batch: 48, Loss: 0.01680300198495388\n",
      "Iteration 379, Batch: 49, Loss: 0.04650600627064705\n",
      "Number of layers: 10\n",
      "Iteration 380, Batch: 0, Loss: 0.07457135617733002\n",
      "Iteration 380, Batch: 1, Loss: 0.048123788088560104\n",
      "Iteration 380, Batch: 2, Loss: 0.04266386106610298\n",
      "Iteration 380, Batch: 3, Loss: 0.041122760623693466\n",
      "Iteration 380, Batch: 4, Loss: 0.05944642052054405\n",
      "Iteration 380, Batch: 5, Loss: 0.04854008927941322\n",
      "Iteration 380, Batch: 6, Loss: 0.03642543405294418\n",
      "Iteration 380, Batch: 7, Loss: 0.043243616819381714\n",
      "Iteration 380, Batch: 8, Loss: 0.06245630234479904\n",
      "Iteration 380, Batch: 9, Loss: 0.024394897744059563\n",
      "Iteration 380, Batch: 10, Loss: 0.03675585240125656\n",
      "Iteration 380, Batch: 11, Loss: 0.03300345689058304\n",
      "Iteration 380, Batch: 12, Loss: 0.02368433214724064\n",
      "Iteration 380, Batch: 13, Loss: 0.035099636763334274\n",
      "Iteration 380, Batch: 14, Loss: 0.028798632323741913\n",
      "Iteration 380, Batch: 15, Loss: 0.041326854377985\n",
      "Iteration 380, Batch: 16, Loss: 0.04060077667236328\n",
      "Iteration 380, Batch: 17, Loss: 0.04718135669827461\n",
      "Iteration 380, Batch: 18, Loss: 0.050359610468149185\n",
      "Iteration 380, Batch: 19, Loss: 0.03821037337183952\n",
      "Iteration 380, Batch: 20, Loss: 0.06164908409118652\n",
      "Iteration 380, Batch: 21, Loss: 0.03188514709472656\n",
      "Iteration 380, Batch: 22, Loss: 0.04498528689146042\n",
      "Iteration 380, Batch: 23, Loss: 0.057574111968278885\n",
      "Iteration 380, Batch: 24, Loss: 0.037651579827070236\n",
      "Iteration 380, Batch: 25, Loss: 0.04389480501413345\n",
      "Iteration 380, Batch: 26, Loss: 0.041077226400375366\n",
      "Iteration 380, Batch: 27, Loss: 0.03611466661095619\n",
      "Iteration 380, Batch: 28, Loss: 0.04686807841062546\n",
      "Iteration 380, Batch: 29, Loss: 0.0428132638335228\n",
      "Iteration 380, Batch: 30, Loss: 0.04165993630886078\n",
      "Iteration 380, Batch: 31, Loss: 0.04261844605207443\n",
      "Iteration 380, Batch: 32, Loss: 0.039639923721551895\n",
      "Iteration 380, Batch: 33, Loss: 0.05362241715192795\n",
      "Iteration 380, Batch: 34, Loss: 0.06136133894324303\n",
      "Iteration 380, Batch: 35, Loss: 0.043967023491859436\n",
      "Iteration 380, Batch: 36, Loss: 0.03306622803211212\n",
      "Iteration 380, Batch: 37, Loss: 0.025880184024572372\n",
      "Iteration 380, Batch: 38, Loss: 0.03497955948114395\n",
      "Iteration 380, Batch: 39, Loss: 0.04609769582748413\n",
      "Iteration 380, Batch: 40, Loss: 0.041639599949121475\n",
      "Iteration 380, Batch: 41, Loss: 0.048865411430597305\n",
      "Iteration 380, Batch: 42, Loss: 0.050091493874788284\n",
      "Iteration 380, Batch: 43, Loss: 0.04973713681101799\n",
      "Iteration 380, Batch: 44, Loss: 0.03294997289776802\n",
      "Iteration 380, Batch: 45, Loss: 0.026938028633594513\n",
      "Iteration 380, Batch: 46, Loss: 0.05163851007819176\n",
      "Iteration 380, Batch: 47, Loss: 0.03239836543798447\n",
      "Iteration 380, Batch: 48, Loss: 0.044933583587408066\n",
      "Iteration 380, Batch: 49, Loss: 0.067679762840271\n",
      "Number of layers: 10\n",
      "Iteration 381, Batch: 0, Loss: 0.06881792843341827\n",
      "Iteration 381, Batch: 1, Loss: 0.06965865939855576\n",
      "Iteration 381, Batch: 2, Loss: 0.08219148218631744\n",
      "Iteration 381, Batch: 3, Loss: 0.059084318578243256\n",
      "Iteration 381, Batch: 4, Loss: 0.05181269347667694\n",
      "Iteration 381, Batch: 5, Loss: 0.0583043098449707\n",
      "Iteration 381, Batch: 6, Loss: 0.0885472297668457\n",
      "Iteration 381, Batch: 7, Loss: 0.05504610762000084\n",
      "Iteration 381, Batch: 8, Loss: 0.057710494846105576\n",
      "Iteration 381, Batch: 9, Loss: 0.04177647456526756\n",
      "Iteration 381, Batch: 10, Loss: 0.05976495519280434\n",
      "Iteration 381, Batch: 11, Loss: 0.038840893656015396\n",
      "Iteration 381, Batch: 12, Loss: 0.06720852106809616\n",
      "Iteration 381, Batch: 13, Loss: 0.05502055957913399\n",
      "Iteration 381, Batch: 14, Loss: 0.04292429983615875\n",
      "Iteration 381, Batch: 15, Loss: 0.047092780470848083\n",
      "Iteration 381, Batch: 16, Loss: 0.05541066825389862\n",
      "Iteration 381, Batch: 17, Loss: 0.03805302083492279\n",
      "Iteration 381, Batch: 18, Loss: 0.03533589467406273\n",
      "Iteration 381, Batch: 19, Loss: 0.04075755178928375\n",
      "Iteration 381, Batch: 20, Loss: 0.06183263659477234\n",
      "Iteration 381, Batch: 21, Loss: 0.02449619770050049\n",
      "Iteration 381, Batch: 22, Loss: 0.028802841901779175\n",
      "Iteration 381, Batch: 23, Loss: 0.020936867222189903\n",
      "Iteration 381, Batch: 24, Loss: 0.04644066095352173\n",
      "Iteration 381, Batch: 25, Loss: 0.04274885728955269\n",
      "Iteration 381, Batch: 26, Loss: 0.05196544528007507\n",
      "Iteration 381, Batch: 27, Loss: 0.046558309346437454\n",
      "Iteration 381, Batch: 28, Loss: 0.060659706592559814\n",
      "Iteration 381, Batch: 29, Loss: 0.07323069870471954\n",
      "Iteration 381, Batch: 30, Loss: 0.06538436561822891\n",
      "Iteration 381, Batch: 31, Loss: 0.039743028581142426\n",
      "Iteration 381, Batch: 32, Loss: 0.050561223179101944\n",
      "Iteration 381, Batch: 33, Loss: 0.033403605222702026\n",
      "Iteration 381, Batch: 34, Loss: 0.0374365858733654\n",
      "Iteration 381, Batch: 35, Loss: 0.03412356600165367\n",
      "Iteration 381, Batch: 36, Loss: 0.05269717052578926\n",
      "Iteration 381, Batch: 37, Loss: 0.04533804953098297\n",
      "Iteration 381, Batch: 38, Loss: 0.037036217749118805\n",
      "Iteration 381, Batch: 39, Loss: 0.05713490769267082\n",
      "Iteration 381, Batch: 40, Loss: 0.058347444981336594\n",
      "Iteration 381, Batch: 41, Loss: 0.05973724648356438\n",
      "Iteration 381, Batch: 42, Loss: 0.03696257621049881\n",
      "Iteration 381, Batch: 43, Loss: 0.043925438076257706\n",
      "Iteration 381, Batch: 44, Loss: 0.04115863889455795\n",
      "Iteration 381, Batch: 45, Loss: 0.05597729608416557\n",
      "Iteration 381, Batch: 46, Loss: 0.03351068124175072\n",
      "Iteration 381, Batch: 47, Loss: 0.03444298729300499\n",
      "Iteration 381, Batch: 48, Loss: 0.06933777034282684\n",
      "Iteration 381, Batch: 49, Loss: 0.056043319404125214\n",
      "Number of layers: 10\n",
      "Iteration 382, Batch: 0, Loss: 0.03606376051902771\n",
      "Iteration 382, Batch: 1, Loss: 0.034606531262397766\n",
      "Iteration 382, Batch: 2, Loss: 0.039590973407030106\n",
      "Iteration 382, Batch: 3, Loss: 0.03724033385515213\n",
      "Iteration 382, Batch: 4, Loss: 0.03975188359618187\n",
      "Iteration 382, Batch: 5, Loss: 0.03670486435294151\n",
      "Iteration 382, Batch: 6, Loss: 0.04320624843239784\n",
      "Iteration 382, Batch: 7, Loss: 0.042537108063697815\n",
      "Iteration 382, Batch: 8, Loss: 0.04530702531337738\n",
      "Iteration 382, Batch: 9, Loss: 0.061319831758737564\n",
      "Iteration 382, Batch: 10, Loss: 0.023302070796489716\n",
      "Iteration 382, Batch: 11, Loss: 0.029159216210246086\n",
      "Iteration 382, Batch: 12, Loss: 0.03685341402888298\n",
      "Iteration 382, Batch: 13, Loss: 0.02983405999839306\n",
      "Iteration 382, Batch: 14, Loss: 0.030087508261203766\n",
      "Iteration 382, Batch: 15, Loss: 0.03220409154891968\n",
      "Iteration 382, Batch: 16, Loss: 0.03438090905547142\n",
      "Iteration 382, Batch: 17, Loss: 0.02499622292816639\n",
      "Iteration 382, Batch: 18, Loss: 0.02743731439113617\n",
      "Iteration 382, Batch: 19, Loss: 0.053843285888433456\n",
      "Iteration 382, Batch: 20, Loss: 0.0335104763507843\n",
      "Iteration 382, Batch: 21, Loss: 0.04099132493138313\n",
      "Iteration 382, Batch: 22, Loss: 0.042265672236680984\n",
      "Iteration 382, Batch: 23, Loss: 0.06010787934064865\n",
      "Iteration 382, Batch: 24, Loss: 0.04105527326464653\n",
      "Iteration 382, Batch: 25, Loss: 0.05376298725605011\n",
      "Iteration 382, Batch: 26, Loss: 0.03441290557384491\n",
      "Iteration 382, Batch: 27, Loss: 0.03044739179313183\n",
      "Iteration 382, Batch: 28, Loss: 0.02377944439649582\n",
      "Iteration 382, Batch: 29, Loss: 0.023300213739275932\n",
      "Iteration 382, Batch: 30, Loss: 0.02970454841852188\n",
      "Iteration 382, Batch: 31, Loss: 0.05082854628562927\n",
      "Iteration 382, Batch: 32, Loss: 0.03706089407205582\n",
      "Iteration 382, Batch: 33, Loss: 0.04106980189681053\n",
      "Iteration 382, Batch: 34, Loss: 0.03842516243457794\n",
      "Iteration 382, Batch: 35, Loss: 0.0380256325006485\n",
      "Iteration 382, Batch: 36, Loss: 0.03372921794652939\n",
      "Iteration 382, Batch: 37, Loss: 0.04503009840846062\n",
      "Iteration 382, Batch: 38, Loss: 0.027599884197115898\n",
      "Iteration 382, Batch: 39, Loss: 0.041205938905477524\n",
      "Iteration 382, Batch: 40, Loss: 0.03099123015999794\n",
      "Iteration 382, Batch: 41, Loss: 0.04697011783719063\n",
      "Iteration 382, Batch: 42, Loss: 0.026119984686374664\n",
      "Iteration 382, Batch: 43, Loss: 0.040366314351558685\n",
      "Iteration 382, Batch: 44, Loss: 0.0447186715900898\n",
      "Iteration 382, Batch: 45, Loss: 0.015410427935421467\n",
      "Iteration 382, Batch: 46, Loss: 0.04420124366879463\n",
      "Iteration 382, Batch: 47, Loss: 0.01628158986568451\n",
      "Iteration 382, Batch: 48, Loss: 0.04595620185136795\n",
      "Iteration 382, Batch: 49, Loss: 0.04063498228788376\n",
      "Number of layers: 10\n",
      "Iteration 383, Batch: 0, Loss: 0.03485381230711937\n",
      "Iteration 383, Batch: 1, Loss: 0.04772394150495529\n",
      "Iteration 383, Batch: 2, Loss: 0.0236098300665617\n",
      "Iteration 383, Batch: 3, Loss: 0.04175188019871712\n",
      "Iteration 383, Batch: 4, Loss: 0.05805240571498871\n",
      "Iteration 383, Batch: 5, Loss: 0.04752441868185997\n",
      "Iteration 383, Batch: 6, Loss: 0.027237240225076675\n",
      "Iteration 383, Batch: 7, Loss: 0.03645900636911392\n",
      "Iteration 383, Batch: 8, Loss: 0.03962493687868118\n",
      "Iteration 383, Batch: 9, Loss: 0.02175525575876236\n",
      "Iteration 383, Batch: 10, Loss: 0.027846354991197586\n",
      "Iteration 383, Batch: 11, Loss: 0.044147390872240067\n",
      "Iteration 383, Batch: 12, Loss: 0.03719264641404152\n",
      "Iteration 383, Batch: 13, Loss: 0.0443207211792469\n",
      "Iteration 383, Batch: 14, Loss: 0.0605863556265831\n",
      "Iteration 383, Batch: 15, Loss: 0.04255881533026695\n",
      "Iteration 383, Batch: 16, Loss: 0.020654430612921715\n",
      "Iteration 383, Batch: 17, Loss: 0.024720534682273865\n",
      "Iteration 383, Batch: 18, Loss: 0.050417717546224594\n",
      "Iteration 383, Batch: 19, Loss: 0.03766448050737381\n",
      "Iteration 383, Batch: 20, Loss: 0.06309395283460617\n",
      "Iteration 383, Batch: 21, Loss: 0.0542801171541214\n",
      "Iteration 383, Batch: 22, Loss: 0.05794506520032883\n",
      "Iteration 383, Batch: 23, Loss: 0.034212879836559296\n",
      "Iteration 383, Batch: 24, Loss: 0.0147860711440444\n",
      "Iteration 383, Batch: 25, Loss: 0.036797281354665756\n",
      "Iteration 383, Batch: 26, Loss: 0.040256526321172714\n",
      "Iteration 383, Batch: 27, Loss: 0.0415068045258522\n",
      "Iteration 383, Batch: 28, Loss: 0.03627895936369896\n",
      "Iteration 383, Batch: 29, Loss: 0.034085143357515335\n",
      "Iteration 383, Batch: 30, Loss: 0.04029376804828644\n",
      "Iteration 383, Batch: 31, Loss: 0.04125744849443436\n",
      "Iteration 383, Batch: 32, Loss: 0.04169628396630287\n",
      "Iteration 383, Batch: 33, Loss: 0.05885830894112587\n",
      "Iteration 383, Batch: 34, Loss: 0.04856068640947342\n",
      "Iteration 383, Batch: 35, Loss: 0.0390302799642086\n",
      "Iteration 383, Batch: 36, Loss: 0.025949783623218536\n",
      "Iteration 383, Batch: 37, Loss: 0.044849056750535965\n",
      "Iteration 383, Batch: 38, Loss: 0.03108847513794899\n",
      "Iteration 383, Batch: 39, Loss: 0.03770118206739426\n",
      "Iteration 383, Batch: 40, Loss: 0.03314559906721115\n",
      "Iteration 383, Batch: 41, Loss: 0.0478217639029026\n",
      "Iteration 383, Batch: 42, Loss: 0.055798668414354324\n",
      "Iteration 383, Batch: 43, Loss: 0.03683377057313919\n",
      "Iteration 383, Batch: 44, Loss: 0.0314522348344326\n",
      "Iteration 383, Batch: 45, Loss: 0.020762013271450996\n",
      "Iteration 383, Batch: 46, Loss: 0.030673904344439507\n",
      "Iteration 383, Batch: 47, Loss: 0.030922846868634224\n",
      "Iteration 383, Batch: 48, Loss: 0.04885135218501091\n",
      "Iteration 383, Batch: 49, Loss: 0.046592168509960175\n",
      "Number of layers: 10\n",
      "Iteration 384, Batch: 0, Loss: 0.030275287106633186\n",
      "Iteration 384, Batch: 1, Loss: 0.03510309010744095\n",
      "Iteration 384, Batch: 2, Loss: 0.05784435197710991\n",
      "Iteration 384, Batch: 3, Loss: 0.034647129476070404\n",
      "Iteration 384, Batch: 4, Loss: 0.039897408336400986\n",
      "Iteration 384, Batch: 5, Loss: 0.03196822479367256\n",
      "Iteration 384, Batch: 6, Loss: 0.04511537775397301\n",
      "Iteration 384, Batch: 7, Loss: 0.023194389417767525\n",
      "Iteration 384, Batch: 8, Loss: 0.02541925385594368\n",
      "Iteration 384, Batch: 9, Loss: 0.04703960195183754\n",
      "Iteration 384, Batch: 10, Loss: 0.05325784906744957\n",
      "Iteration 384, Batch: 11, Loss: 0.02837238647043705\n",
      "Iteration 384, Batch: 12, Loss: 0.06446737796068192\n",
      "Iteration 384, Batch: 13, Loss: 0.0201509278267622\n",
      "Iteration 384, Batch: 14, Loss: 0.031536467373371124\n",
      "Iteration 384, Batch: 15, Loss: 0.055694062262773514\n",
      "Iteration 384, Batch: 16, Loss: 0.028257058933377266\n",
      "Iteration 384, Batch: 17, Loss: 0.031116269528865814\n",
      "Iteration 384, Batch: 18, Loss: 0.059193093329668045\n",
      "Iteration 384, Batch: 19, Loss: 0.06276632845401764\n",
      "Iteration 384, Batch: 20, Loss: 0.06059185788035393\n",
      "Iteration 384, Batch: 21, Loss: 0.024691665545105934\n",
      "Iteration 384, Batch: 22, Loss: 0.04350081458687782\n",
      "Iteration 384, Batch: 23, Loss: 0.05560610070824623\n",
      "Iteration 384, Batch: 24, Loss: 0.05170739069581032\n",
      "Iteration 384, Batch: 25, Loss: 0.026378072798252106\n",
      "Iteration 384, Batch: 26, Loss: 0.06339861452579498\n",
      "Iteration 384, Batch: 27, Loss: 0.04621363431215286\n",
      "Iteration 384, Batch: 28, Loss: 0.03366627171635628\n",
      "Iteration 384, Batch: 29, Loss: 0.02563084475696087\n",
      "Iteration 384, Batch: 30, Loss: 0.04046761244535446\n",
      "Iteration 384, Batch: 31, Loss: 0.05265972018241882\n",
      "Iteration 384, Batch: 32, Loss: 0.0696602314710617\n",
      "Iteration 384, Batch: 33, Loss: 0.0401478074491024\n",
      "Iteration 384, Batch: 34, Loss: 0.05987303704023361\n",
      "Iteration 384, Batch: 35, Loss: 0.028656816110014915\n",
      "Iteration 384, Batch: 36, Loss: 0.052671849727630615\n",
      "Iteration 384, Batch: 37, Loss: 0.040896978229284286\n",
      "Iteration 384, Batch: 38, Loss: 0.053203284740448\n",
      "Iteration 384, Batch: 39, Loss: 0.03522216156125069\n",
      "Iteration 384, Batch: 40, Loss: 0.025532063096761703\n",
      "Iteration 384, Batch: 41, Loss: 0.036184024065732956\n",
      "Iteration 384, Batch: 42, Loss: 0.051360443234443665\n",
      "Iteration 384, Batch: 43, Loss: 0.038391102105379105\n",
      "Iteration 384, Batch: 44, Loss: 0.05185308679938316\n",
      "Iteration 384, Batch: 45, Loss: 0.07041913270950317\n",
      "Iteration 384, Batch: 46, Loss: 0.04952431470155716\n",
      "Iteration 384, Batch: 47, Loss: 0.055297937244176865\n",
      "Iteration 384, Batch: 48, Loss: 0.06878302246332169\n",
      "Iteration 384, Batch: 49, Loss: 0.04691242054104805\n",
      "Number of layers: 10\n",
      "Iteration 385, Batch: 0, Loss: 0.03222629055380821\n",
      "Iteration 385, Batch: 1, Loss: 0.052675239741802216\n",
      "Iteration 385, Batch: 2, Loss: 0.06395328044891357\n",
      "Iteration 385, Batch: 3, Loss: 0.024295499548316002\n",
      "Iteration 385, Batch: 4, Loss: 0.030705051496624947\n",
      "Iteration 385, Batch: 5, Loss: 0.03379145637154579\n",
      "Iteration 385, Batch: 6, Loss: 0.04055320471525192\n",
      "Iteration 385, Batch: 7, Loss: 0.051701467484235764\n",
      "Iteration 385, Batch: 8, Loss: 0.05016004294157028\n",
      "Iteration 385, Batch: 9, Loss: 0.0541786327958107\n",
      "Iteration 385, Batch: 10, Loss: 0.06199454143643379\n",
      "Iteration 385, Batch: 11, Loss: 0.027607111260294914\n",
      "Iteration 385, Batch: 12, Loss: 0.04935804381966591\n",
      "Iteration 385, Batch: 13, Loss: 0.04022891819477081\n",
      "Iteration 385, Batch: 14, Loss: 0.05576697364449501\n",
      "Iteration 385, Batch: 15, Loss: 0.06193041801452637\n",
      "Iteration 385, Batch: 16, Loss: 0.06290139257907867\n",
      "Iteration 385, Batch: 17, Loss: 0.04527547210454941\n",
      "Iteration 385, Batch: 18, Loss: 0.056867606937885284\n",
      "Iteration 385, Batch: 19, Loss: 0.02663395181298256\n",
      "Iteration 385, Batch: 20, Loss: 0.02990006096661091\n",
      "Iteration 385, Batch: 21, Loss: 0.03972010314464569\n",
      "Iteration 385, Batch: 22, Loss: 0.04746929928660393\n",
      "Iteration 385, Batch: 23, Loss: 0.035501446574926376\n",
      "Iteration 385, Batch: 24, Loss: 0.03279862925410271\n",
      "Iteration 385, Batch: 25, Loss: 0.0417608842253685\n",
      "Iteration 385, Batch: 26, Loss: 0.024304799735546112\n",
      "Iteration 385, Batch: 27, Loss: 0.04083593934774399\n",
      "Iteration 385, Batch: 28, Loss: 0.03503786027431488\n",
      "Iteration 385, Batch: 29, Loss: 0.027116267010569572\n",
      "Iteration 385, Batch: 30, Loss: 0.03318561241030693\n",
      "Iteration 385, Batch: 31, Loss: 0.028593143448233604\n",
      "Iteration 385, Batch: 32, Loss: 0.03416777774691582\n",
      "Iteration 385, Batch: 33, Loss: 0.045462340116500854\n",
      "Iteration 385, Batch: 34, Loss: 0.05087658762931824\n",
      "Iteration 385, Batch: 35, Loss: 0.041272085160017014\n",
      "Iteration 385, Batch: 36, Loss: 0.03510348126292229\n",
      "Iteration 385, Batch: 37, Loss: 0.050275880843400955\n",
      "Iteration 385, Batch: 38, Loss: 0.06581876426935196\n",
      "Iteration 385, Batch: 39, Loss: 0.08352197706699371\n",
      "Iteration 385, Batch: 40, Loss: 0.23967345058918\n",
      "Iteration 385, Batch: 41, Loss: 0.14972057938575745\n",
      "Iteration 385, Batch: 42, Loss: 0.10490016639232635\n",
      "Iteration 385, Batch: 43, Loss: 0.094927579164505\n",
      "Iteration 385, Batch: 44, Loss: 0.07427431643009186\n",
      "Iteration 385, Batch: 45, Loss: 0.05274226889014244\n",
      "Iteration 385, Batch: 46, Loss: 0.17124637961387634\n",
      "Iteration 385, Batch: 47, Loss: 0.09026742726564407\n",
      "Iteration 385, Batch: 48, Loss: 0.04162823408842087\n",
      "Iteration 385, Batch: 49, Loss: 0.08502370864152908\n",
      "Number of layers: 10\n",
      "Iteration 386, Batch: 0, Loss: 0.04697420820593834\n",
      "Iteration 386, Batch: 1, Loss: 0.09706477075815201\n",
      "Iteration 386, Batch: 2, Loss: 0.04569997638463974\n",
      "Iteration 386, Batch: 3, Loss: 0.08051413297653198\n",
      "Iteration 386, Batch: 4, Loss: 0.04433503374457359\n",
      "Iteration 386, Batch: 5, Loss: 0.06169770285487175\n",
      "Iteration 386, Batch: 6, Loss: 0.03650243207812309\n",
      "Iteration 386, Batch: 7, Loss: 0.05247800797224045\n",
      "Iteration 386, Batch: 8, Loss: 0.06358391046524048\n",
      "Iteration 386, Batch: 9, Loss: 0.0766051784157753\n",
      "Iteration 386, Batch: 10, Loss: 0.0792875587940216\n",
      "Iteration 386, Batch: 11, Loss: 0.07017876952886581\n",
      "Iteration 386, Batch: 12, Loss: 0.06614954024553299\n",
      "Iteration 386, Batch: 13, Loss: 0.07374672591686249\n",
      "Iteration 386, Batch: 14, Loss: 0.08093136548995972\n",
      "Iteration 386, Batch: 15, Loss: 0.07551949471235275\n",
      "Iteration 386, Batch: 16, Loss: 0.022756602615118027\n",
      "Iteration 386, Batch: 17, Loss: 0.05317740887403488\n",
      "Iteration 386, Batch: 18, Loss: 0.07964971661567688\n",
      "Iteration 386, Batch: 19, Loss: 0.03497668728232384\n",
      "Iteration 386, Batch: 20, Loss: 0.06164016202092171\n",
      "Iteration 386, Batch: 21, Loss: 0.03811371698975563\n",
      "Iteration 386, Batch: 22, Loss: 0.03726658970117569\n",
      "Iteration 386, Batch: 23, Loss: 0.03223248943686485\n",
      "Iteration 386, Batch: 24, Loss: 0.04859967157244682\n",
      "Iteration 386, Batch: 25, Loss: 0.032695744186639786\n",
      "Iteration 386, Batch: 26, Loss: 0.06057906895875931\n",
      "Iteration 386, Batch: 27, Loss: 0.0318453274667263\n",
      "Iteration 386, Batch: 28, Loss: 0.04096711426973343\n",
      "Iteration 386, Batch: 29, Loss: 0.0327417328953743\n",
      "Iteration 386, Batch: 30, Loss: 0.05106092989444733\n",
      "Iteration 386, Batch: 31, Loss: 0.03622639179229736\n",
      "Iteration 386, Batch: 32, Loss: 0.03639698401093483\n",
      "Iteration 386, Batch: 33, Loss: 0.046761929988861084\n",
      "Iteration 386, Batch: 34, Loss: 0.05743858218193054\n",
      "Iteration 386, Batch: 35, Loss: 0.05709407851099968\n",
      "Iteration 386, Batch: 36, Loss: 0.027273211628198624\n",
      "Iteration 386, Batch: 37, Loss: 0.026217661798000336\n",
      "Iteration 386, Batch: 38, Loss: 0.04745295271277428\n",
      "Iteration 386, Batch: 39, Loss: 0.02402310259640217\n",
      "Iteration 386, Batch: 40, Loss: 0.051719892770051956\n",
      "Iteration 386, Batch: 41, Loss: 0.044297464191913605\n",
      "Iteration 386, Batch: 42, Loss: 0.059925954788923264\n",
      "Iteration 386, Batch: 43, Loss: 0.03711802512407303\n",
      "Iteration 386, Batch: 44, Loss: 0.041296184062957764\n",
      "Iteration 386, Batch: 45, Loss: 0.04664677381515503\n",
      "Iteration 386, Batch: 46, Loss: 0.0792359933257103\n",
      "Iteration 386, Batch: 47, Loss: 0.05557801574468613\n",
      "Iteration 386, Batch: 48, Loss: 0.05608702450990677\n",
      "Iteration 386, Batch: 49, Loss: 0.034729622304439545\n",
      "Number of layers: 10\n",
      "Iteration 387, Batch: 0, Loss: 0.029379568994045258\n",
      "Iteration 387, Batch: 1, Loss: 0.032213713973760605\n",
      "Iteration 387, Batch: 2, Loss: 0.05723497271537781\n",
      "Iteration 387, Batch: 3, Loss: 0.0411575585603714\n",
      "Iteration 387, Batch: 4, Loss: 0.04668610543012619\n",
      "Iteration 387, Batch: 5, Loss: 0.053987130522727966\n",
      "Iteration 387, Batch: 6, Loss: 0.08878694474697113\n",
      "Iteration 387, Batch: 7, Loss: 0.04539920762181282\n",
      "Iteration 387, Batch: 8, Loss: 0.05888651683926582\n",
      "Iteration 387, Batch: 9, Loss: 0.053991787135601044\n",
      "Iteration 387, Batch: 10, Loss: 0.05835218355059624\n",
      "Iteration 387, Batch: 11, Loss: 0.050172582268714905\n",
      "Iteration 387, Batch: 12, Loss: 0.04219190403819084\n",
      "Iteration 387, Batch: 13, Loss: 0.053661804646253586\n",
      "Iteration 387, Batch: 14, Loss: 0.05873893201351166\n",
      "Iteration 387, Batch: 15, Loss: 0.03494122251868248\n",
      "Iteration 387, Batch: 16, Loss: 0.04664834961295128\n",
      "Iteration 387, Batch: 17, Loss: 0.049032535403966904\n",
      "Iteration 387, Batch: 18, Loss: 0.05006227269768715\n",
      "Iteration 387, Batch: 19, Loss: 0.07885130494832993\n",
      "Iteration 387, Batch: 20, Loss: 0.048301246017217636\n",
      "Iteration 387, Batch: 21, Loss: 0.04273610562086105\n",
      "Iteration 387, Batch: 22, Loss: 0.037504054605960846\n",
      "Iteration 387, Batch: 23, Loss: 0.05429588630795479\n",
      "Iteration 387, Batch: 24, Loss: 0.06109859421849251\n",
      "Iteration 387, Batch: 25, Loss: 0.05031988024711609\n",
      "Iteration 387, Batch: 26, Loss: 0.05091019347310066\n",
      "Iteration 387, Batch: 27, Loss: 0.04497499391436577\n",
      "Iteration 387, Batch: 28, Loss: 0.037711672484874725\n",
      "Iteration 387, Batch: 29, Loss: 0.04071934148669243\n",
      "Iteration 387, Batch: 30, Loss: 0.05909489467740059\n",
      "Iteration 387, Batch: 31, Loss: 0.052251242101192474\n",
      "Iteration 387, Batch: 32, Loss: 0.0473971851170063\n",
      "Iteration 387, Batch: 33, Loss: 0.04416985437273979\n",
      "Iteration 387, Batch: 34, Loss: 0.014269503764808178\n",
      "Iteration 387, Batch: 35, Loss: 0.05027614161372185\n",
      "Iteration 387, Batch: 36, Loss: 0.031148945912718773\n",
      "Iteration 387, Batch: 37, Loss: 0.03412400558590889\n",
      "Iteration 387, Batch: 38, Loss: 0.044463444501161575\n",
      "Iteration 387, Batch: 39, Loss: 0.022477464750409126\n",
      "Iteration 387, Batch: 40, Loss: 0.03648051992058754\n",
      "Iteration 387, Batch: 41, Loss: 0.03083530068397522\n",
      "Iteration 387, Batch: 42, Loss: 0.035106875002384186\n",
      "Iteration 387, Batch: 43, Loss: 0.035507261753082275\n",
      "Iteration 387, Batch: 44, Loss: 0.038365524262189865\n",
      "Iteration 387, Batch: 45, Loss: 0.03822862356901169\n",
      "Iteration 387, Batch: 46, Loss: 0.04193272814154625\n",
      "Iteration 387, Batch: 47, Loss: 0.0383465476334095\n",
      "Iteration 387, Batch: 48, Loss: 0.029298577457666397\n",
      "Iteration 387, Batch: 49, Loss: 0.03710903227329254\n",
      "Number of layers: 10\n",
      "Iteration 388, Batch: 0, Loss: 0.026682009920477867\n",
      "Iteration 388, Batch: 1, Loss: 0.03646695613861084\n",
      "Iteration 388, Batch: 2, Loss: 0.029699577018618584\n",
      "Iteration 388, Batch: 3, Loss: 0.03452242910861969\n",
      "Iteration 388, Batch: 4, Loss: 0.061807677149772644\n",
      "Iteration 388, Batch: 5, Loss: 0.024311937391757965\n",
      "Iteration 388, Batch: 6, Loss: 0.06684304773807526\n",
      "Iteration 388, Batch: 7, Loss: 0.028200367465615273\n",
      "Iteration 388, Batch: 8, Loss: 0.02650238387286663\n",
      "Iteration 388, Batch: 9, Loss: 0.04535844177007675\n",
      "Iteration 388, Batch: 10, Loss: 0.059557363390922546\n",
      "Iteration 388, Batch: 11, Loss: 0.050229303538799286\n",
      "Iteration 388, Batch: 12, Loss: 0.04062559828162193\n",
      "Iteration 388, Batch: 13, Loss: 0.04073498770594597\n",
      "Iteration 388, Batch: 14, Loss: 0.05194311961531639\n",
      "Iteration 388, Batch: 15, Loss: 0.02453402243554592\n",
      "Iteration 388, Batch: 16, Loss: 0.030770301818847656\n",
      "Iteration 388, Batch: 17, Loss: 0.028467625379562378\n",
      "Iteration 388, Batch: 18, Loss: 0.014856266789138317\n",
      "Iteration 388, Batch: 19, Loss: 0.04305999353528023\n",
      "Iteration 388, Batch: 20, Loss: 0.05243680253624916\n",
      "Iteration 388, Batch: 21, Loss: 0.04100275784730911\n",
      "Iteration 388, Batch: 22, Loss: 0.020603951066732407\n",
      "Iteration 388, Batch: 23, Loss: 0.045519083738327026\n",
      "Iteration 388, Batch: 24, Loss: 0.0287159476429224\n",
      "Iteration 388, Batch: 25, Loss: 0.04382651299238205\n",
      "Iteration 388, Batch: 26, Loss: 0.03395276144146919\n",
      "Iteration 388, Batch: 27, Loss: 0.02063116431236267\n",
      "Iteration 388, Batch: 28, Loss: 0.024933934211730957\n",
      "Iteration 388, Batch: 29, Loss: 0.03469707444310188\n",
      "Iteration 388, Batch: 30, Loss: 0.04644370079040527\n",
      "Iteration 388, Batch: 31, Loss: 0.02787056937813759\n",
      "Iteration 388, Batch: 32, Loss: 0.011026758700609207\n",
      "Iteration 388, Batch: 33, Loss: 0.03382090851664543\n",
      "Iteration 388, Batch: 34, Loss: 0.03198627382516861\n",
      "Iteration 388, Batch: 35, Loss: 0.01935586705803871\n",
      "Iteration 388, Batch: 36, Loss: 0.03938058391213417\n",
      "Iteration 388, Batch: 37, Loss: 0.015448281541466713\n",
      "Iteration 388, Batch: 38, Loss: 0.026157917454838753\n",
      "Iteration 388, Batch: 39, Loss: 0.036316607147455215\n",
      "Iteration 388, Batch: 40, Loss: 0.04160716384649277\n",
      "Iteration 388, Batch: 41, Loss: 0.008769501000642776\n",
      "Iteration 388, Batch: 42, Loss: 0.029141541570425034\n",
      "Iteration 388, Batch: 43, Loss: 0.012706897221505642\n",
      "Iteration 388, Batch: 44, Loss: 0.029387015849351883\n",
      "Iteration 388, Batch: 45, Loss: 0.055755358189344406\n",
      "Iteration 388, Batch: 46, Loss: 0.04770384356379509\n",
      "Iteration 388, Batch: 47, Loss: 0.16489312052726746\n",
      "Iteration 388, Batch: 48, Loss: 0.11114668846130371\n",
      "Iteration 388, Batch: 49, Loss: 0.048134010285139084\n",
      "Number of layers: 10\n",
      "Iteration 389, Batch: 0, Loss: 0.04312454164028168\n",
      "Iteration 389, Batch: 1, Loss: 0.0509016253054142\n",
      "Iteration 389, Batch: 2, Loss: 0.04233996942639351\n",
      "Iteration 389, Batch: 3, Loss: 0.02095145732164383\n",
      "Iteration 389, Batch: 4, Loss: 0.02912559174001217\n",
      "Iteration 389, Batch: 5, Loss: 0.04479599371552467\n",
      "Iteration 389, Batch: 6, Loss: 0.05227285623550415\n",
      "Iteration 389, Batch: 7, Loss: 0.03895553573966026\n",
      "Iteration 389, Batch: 8, Loss: 0.04391209036111832\n",
      "Iteration 389, Batch: 9, Loss: 0.029627520591020584\n",
      "Iteration 389, Batch: 10, Loss: 0.04957788437604904\n",
      "Iteration 389, Batch: 11, Loss: 0.020149175077676773\n",
      "Iteration 389, Batch: 12, Loss: 0.01729259453713894\n",
      "Iteration 389, Batch: 13, Loss: 0.030633987858891487\n",
      "Iteration 389, Batch: 14, Loss: 0.04310046136379242\n",
      "Iteration 389, Batch: 15, Loss: 0.06919857114553452\n",
      "Iteration 389, Batch: 16, Loss: 0.07278027385473251\n",
      "Iteration 389, Batch: 17, Loss: 0.06699233502149582\n",
      "Iteration 389, Batch: 18, Loss: 0.05756237357854843\n",
      "Iteration 389, Batch: 19, Loss: 0.05364812910556793\n",
      "Iteration 389, Batch: 20, Loss: 0.04736794903874397\n",
      "Iteration 389, Batch: 21, Loss: 0.03852692246437073\n",
      "Iteration 389, Batch: 22, Loss: 0.034996163100004196\n",
      "Iteration 389, Batch: 23, Loss: 0.060624413192272186\n",
      "Iteration 389, Batch: 24, Loss: 0.03405871242284775\n",
      "Iteration 389, Batch: 25, Loss: 0.0701441839337349\n",
      "Iteration 389, Batch: 26, Loss: 0.022019309923052788\n",
      "Iteration 389, Batch: 27, Loss: 0.04551208019256592\n",
      "Iteration 389, Batch: 28, Loss: 0.05731906741857529\n",
      "Iteration 389, Batch: 29, Loss: 0.04407570883631706\n",
      "Iteration 389, Batch: 30, Loss: 0.04840322211384773\n",
      "Iteration 389, Batch: 31, Loss: 0.0622963085770607\n",
      "Iteration 389, Batch: 32, Loss: 0.06257697939872742\n",
      "Iteration 389, Batch: 33, Loss: 0.066929392516613\n",
      "Iteration 389, Batch: 34, Loss: 0.026861228048801422\n",
      "Iteration 389, Batch: 35, Loss: 0.047444503754377365\n",
      "Iteration 389, Batch: 36, Loss: 0.02871081791818142\n",
      "Iteration 389, Batch: 37, Loss: 0.032438069581985474\n",
      "Iteration 389, Batch: 38, Loss: 0.04393324628472328\n",
      "Iteration 389, Batch: 39, Loss: 0.05212923884391785\n",
      "Iteration 389, Batch: 40, Loss: 0.05150166526436806\n",
      "Iteration 389, Batch: 41, Loss: 0.04395567625761032\n",
      "Iteration 389, Batch: 42, Loss: 0.04156794026494026\n",
      "Iteration 389, Batch: 43, Loss: 0.04965266212821007\n",
      "Iteration 389, Batch: 44, Loss: 0.0800890177488327\n",
      "Iteration 389, Batch: 45, Loss: 0.04278820753097534\n",
      "Iteration 389, Batch: 46, Loss: 0.05531170591711998\n",
      "Iteration 389, Batch: 47, Loss: 0.06426461040973663\n",
      "Iteration 389, Batch: 48, Loss: 0.041812729090452194\n",
      "Iteration 389, Batch: 49, Loss: 0.027298225089907646\n",
      "Number of layers: 10\n",
      "Iteration 390, Batch: 0, Loss: 0.03631729260087013\n",
      "Iteration 390, Batch: 1, Loss: 0.026215024292469025\n",
      "Iteration 390, Batch: 2, Loss: 0.05737150087952614\n",
      "Iteration 390, Batch: 3, Loss: 0.024371923878788948\n",
      "Iteration 390, Batch: 4, Loss: 0.04782319813966751\n",
      "Iteration 390, Batch: 5, Loss: 0.0607595331966877\n",
      "Iteration 390, Batch: 6, Loss: 0.0335698276758194\n",
      "Iteration 390, Batch: 7, Loss: 0.040031157433986664\n",
      "Iteration 390, Batch: 8, Loss: 0.04273772984743118\n",
      "Iteration 390, Batch: 9, Loss: 0.033172305673360825\n",
      "Iteration 390, Batch: 10, Loss: 0.016855912283062935\n",
      "Iteration 390, Batch: 11, Loss: 0.06041351705789566\n",
      "Iteration 390, Batch: 12, Loss: 0.037968724966049194\n",
      "Iteration 390, Batch: 13, Loss: 0.030724840238690376\n",
      "Iteration 390, Batch: 14, Loss: 0.021622303873300552\n",
      "Iteration 390, Batch: 15, Loss: 0.038336437195539474\n",
      "Iteration 390, Batch: 16, Loss: 0.03567754849791527\n",
      "Iteration 390, Batch: 17, Loss: 0.042481664568185806\n",
      "Iteration 390, Batch: 18, Loss: 0.03383559733629227\n",
      "Iteration 390, Batch: 19, Loss: 0.04433780536055565\n",
      "Iteration 390, Batch: 20, Loss: 0.030451668426394463\n",
      "Iteration 390, Batch: 21, Loss: 0.039457645267248154\n",
      "Iteration 390, Batch: 22, Loss: 0.029537158086895943\n",
      "Iteration 390, Batch: 23, Loss: 0.02275429666042328\n",
      "Iteration 390, Batch: 24, Loss: 0.041684284806251526\n",
      "Iteration 390, Batch: 25, Loss: 0.03346594423055649\n",
      "Iteration 390, Batch: 26, Loss: 0.023792065680027008\n",
      "Iteration 390, Batch: 27, Loss: 0.03638555854558945\n",
      "Iteration 390, Batch: 28, Loss: 0.054626625031232834\n",
      "Iteration 390, Batch: 29, Loss: 0.046493738889694214\n",
      "Iteration 390, Batch: 30, Loss: 0.026795728132128716\n",
      "Iteration 390, Batch: 31, Loss: 0.035961199551820755\n",
      "Iteration 390, Batch: 32, Loss: 0.035100340843200684\n",
      "Iteration 390, Batch: 33, Loss: 0.033858221024274826\n",
      "Iteration 390, Batch: 34, Loss: 0.04465296119451523\n",
      "Iteration 390, Batch: 35, Loss: 0.03402148559689522\n",
      "Iteration 390, Batch: 36, Loss: 0.022780397906899452\n",
      "Iteration 390, Batch: 37, Loss: 0.041450135409832\n",
      "Iteration 390, Batch: 38, Loss: 0.03862408921122551\n",
      "Iteration 390, Batch: 39, Loss: 0.045714933425188065\n",
      "Iteration 390, Batch: 40, Loss: 0.037456054240465164\n",
      "Iteration 390, Batch: 41, Loss: 0.06334324926137924\n",
      "Iteration 390, Batch: 42, Loss: 0.041587378829717636\n",
      "Iteration 390, Batch: 43, Loss: 0.013489941135048866\n",
      "Iteration 390, Batch: 44, Loss: 0.03910398855805397\n",
      "Iteration 390, Batch: 45, Loss: 0.03668125346302986\n",
      "Iteration 390, Batch: 46, Loss: 0.03691622242331505\n",
      "Iteration 390, Batch: 47, Loss: 0.03347444161772728\n",
      "Iteration 390, Batch: 48, Loss: 0.039998896420001984\n",
      "Iteration 390, Batch: 49, Loss: 0.05361160263419151\n",
      "Number of layers: 10\n",
      "Iteration 391, Batch: 0, Loss: 0.03731662034988403\n",
      "Iteration 391, Batch: 1, Loss: 0.0467815063893795\n",
      "Iteration 391, Batch: 2, Loss: 0.046611785888671875\n",
      "Iteration 391, Batch: 3, Loss: 0.04144810512661934\n",
      "Iteration 391, Batch: 4, Loss: 0.03607486933469772\n",
      "Iteration 391, Batch: 5, Loss: 0.02296323888003826\n",
      "Iteration 391, Batch: 6, Loss: 0.022975916042923927\n",
      "Iteration 391, Batch: 7, Loss: 0.0433172807097435\n",
      "Iteration 391, Batch: 8, Loss: 0.023480309173464775\n",
      "Iteration 391, Batch: 9, Loss: 0.0429844930768013\n",
      "Iteration 391, Batch: 10, Loss: 0.049003779888153076\n",
      "Iteration 391, Batch: 11, Loss: 0.028691839426755905\n",
      "Iteration 391, Batch: 12, Loss: 0.05862076207995415\n",
      "Iteration 391, Batch: 13, Loss: 0.049073390662670135\n",
      "Iteration 391, Batch: 14, Loss: 0.057639215141534805\n",
      "Iteration 391, Batch: 15, Loss: 0.04450660198926926\n",
      "Iteration 391, Batch: 16, Loss: 0.037099432200193405\n",
      "Iteration 391, Batch: 17, Loss: 0.040307626128196716\n",
      "Iteration 391, Batch: 18, Loss: 0.03731706365942955\n",
      "Iteration 391, Batch: 19, Loss: 0.05158110335469246\n",
      "Iteration 391, Batch: 20, Loss: 0.042519692331552505\n",
      "Iteration 391, Batch: 21, Loss: 0.03471631556749344\n",
      "Iteration 391, Batch: 22, Loss: 0.023334123194217682\n",
      "Iteration 391, Batch: 23, Loss: 0.03209792077541351\n",
      "Iteration 391, Batch: 24, Loss: 0.04191505163908005\n",
      "Iteration 391, Batch: 25, Loss: 0.027684727683663368\n",
      "Iteration 391, Batch: 26, Loss: 0.03677917271852493\n",
      "Iteration 391, Batch: 27, Loss: 0.038942232728004456\n",
      "Iteration 391, Batch: 28, Loss: 0.04197085648775101\n",
      "Iteration 391, Batch: 29, Loss: 0.029932793229818344\n",
      "Iteration 391, Batch: 30, Loss: 0.05920251086354256\n",
      "Iteration 391, Batch: 31, Loss: 0.04925953596830368\n",
      "Iteration 391, Batch: 32, Loss: 0.04578040912747383\n",
      "Iteration 391, Batch: 33, Loss: 0.057949017733335495\n",
      "Iteration 391, Batch: 34, Loss: 0.039558328688144684\n",
      "Iteration 391, Batch: 35, Loss: 0.05543134734034538\n",
      "Iteration 391, Batch: 36, Loss: 0.061851758509874344\n",
      "Iteration 391, Batch: 37, Loss: 0.09944944083690643\n",
      "Iteration 391, Batch: 38, Loss: 0.0418783575296402\n",
      "Iteration 391, Batch: 39, Loss: 0.05366786941885948\n",
      "Iteration 391, Batch: 40, Loss: 0.06033582612872124\n",
      "Iteration 391, Batch: 41, Loss: 0.06526827067136765\n",
      "Iteration 391, Batch: 42, Loss: 0.08636484295129776\n",
      "Iteration 391, Batch: 43, Loss: 0.05991903319954872\n",
      "Iteration 391, Batch: 44, Loss: 0.07197625190019608\n",
      "Iteration 391, Batch: 45, Loss: 0.048830173909664154\n",
      "Iteration 391, Batch: 46, Loss: 0.05441853776574135\n",
      "Iteration 391, Batch: 47, Loss: 0.07855753600597382\n",
      "Iteration 391, Batch: 48, Loss: 0.055536068975925446\n",
      "Iteration 391, Batch: 49, Loss: 0.04627570882439613\n",
      "Number of layers: 10\n",
      "Iteration 392, Batch: 0, Loss: 0.056486375629901886\n",
      "Iteration 392, Batch: 1, Loss: 0.04847552254796028\n",
      "Iteration 392, Batch: 2, Loss: 0.05000148341059685\n",
      "Iteration 392, Batch: 3, Loss: 0.04928942397236824\n",
      "Iteration 392, Batch: 4, Loss: 0.024365562945604324\n",
      "Iteration 392, Batch: 5, Loss: 0.025868333876132965\n",
      "Iteration 392, Batch: 6, Loss: 0.06695941835641861\n",
      "Iteration 392, Batch: 7, Loss: 0.046556394547224045\n",
      "Iteration 392, Batch: 8, Loss: 0.04650510847568512\n",
      "Iteration 392, Batch: 9, Loss: 0.03559298440814018\n",
      "Iteration 392, Batch: 10, Loss: 0.05567849054932594\n",
      "Iteration 392, Batch: 11, Loss: 0.016216672956943512\n",
      "Iteration 392, Batch: 12, Loss: 0.02366776019334793\n",
      "Iteration 392, Batch: 13, Loss: 0.039458952844142914\n",
      "Iteration 392, Batch: 14, Loss: 0.031234011054039\n",
      "Iteration 392, Batch: 15, Loss: 0.04616782069206238\n",
      "Iteration 392, Batch: 16, Loss: 0.051992740482091904\n",
      "Iteration 392, Batch: 17, Loss: 0.04616736248135567\n",
      "Iteration 392, Batch: 18, Loss: 0.04263775795698166\n",
      "Iteration 392, Batch: 19, Loss: 0.04648316279053688\n",
      "Iteration 392, Batch: 20, Loss: 0.04245365411043167\n",
      "Iteration 392, Batch: 21, Loss: 0.04171919822692871\n",
      "Iteration 392, Batch: 22, Loss: 0.011618556454777718\n",
      "Iteration 392, Batch: 23, Loss: 0.05896497517824173\n",
      "Iteration 392, Batch: 24, Loss: 0.04979430511593819\n",
      "Iteration 392, Batch: 25, Loss: 0.0794619768857956\n",
      "Iteration 392, Batch: 26, Loss: 0.05858228728175163\n",
      "Iteration 392, Batch: 27, Loss: 0.048083022236824036\n",
      "Iteration 392, Batch: 28, Loss: 0.04012700915336609\n",
      "Iteration 392, Batch: 29, Loss: 0.05437728390097618\n",
      "Iteration 392, Batch: 30, Loss: 0.03781956061720848\n",
      "Iteration 392, Batch: 31, Loss: 0.06030732020735741\n",
      "Iteration 392, Batch: 32, Loss: 0.04854276403784752\n",
      "Iteration 392, Batch: 33, Loss: 0.03436795994639397\n",
      "Iteration 392, Batch: 34, Loss: 0.043563202023506165\n",
      "Iteration 392, Batch: 35, Loss: 0.03715038672089577\n",
      "Iteration 392, Batch: 36, Loss: 0.04709543287754059\n",
      "Iteration 392, Batch: 37, Loss: 0.032914936542510986\n",
      "Iteration 392, Batch: 38, Loss: 0.04053131863474846\n",
      "Iteration 392, Batch: 39, Loss: 0.06731265038251877\n",
      "Iteration 392, Batch: 40, Loss: 0.016880277544260025\n",
      "Iteration 392, Batch: 41, Loss: 0.06825730949640274\n",
      "Iteration 392, Batch: 42, Loss: 0.030965670943260193\n",
      "Iteration 392, Batch: 43, Loss: 0.03988165408372879\n",
      "Iteration 392, Batch: 44, Loss: 0.03391719609498978\n",
      "Iteration 392, Batch: 45, Loss: 0.027810797095298767\n",
      "Iteration 392, Batch: 46, Loss: 0.04049677774310112\n",
      "Iteration 392, Batch: 47, Loss: 0.044947508722543716\n",
      "Iteration 392, Batch: 48, Loss: 0.025092730298638344\n",
      "Iteration 392, Batch: 49, Loss: 0.04682903364300728\n",
      "Number of layers: 10\n",
      "Iteration 393, Batch: 0, Loss: 0.07747318595647812\n",
      "Iteration 393, Batch: 1, Loss: 0.03337230905890465\n",
      "Iteration 393, Batch: 2, Loss: 0.0488220676779747\n",
      "Iteration 393, Batch: 3, Loss: 0.05633693188428879\n",
      "Iteration 393, Batch: 4, Loss: 0.03289683163166046\n",
      "Iteration 393, Batch: 5, Loss: 0.03622951731085777\n",
      "Iteration 393, Batch: 6, Loss: 0.05924472585320473\n",
      "Iteration 393, Batch: 7, Loss: 0.04339306429028511\n",
      "Iteration 393, Batch: 8, Loss: 0.046991728246212006\n",
      "Iteration 393, Batch: 9, Loss: 0.051877010613679886\n",
      "Iteration 393, Batch: 10, Loss: 0.03845475986599922\n",
      "Iteration 393, Batch: 11, Loss: 0.04523594677448273\n",
      "Iteration 393, Batch: 12, Loss: 0.05911428481340408\n",
      "Iteration 393, Batch: 13, Loss: 0.0579800009727478\n",
      "Iteration 393, Batch: 14, Loss: 0.042108144611120224\n",
      "Iteration 393, Batch: 15, Loss: 0.030294572934508324\n",
      "Iteration 393, Batch: 16, Loss: 0.04613620787858963\n",
      "Iteration 393, Batch: 17, Loss: 0.07399243861436844\n",
      "Iteration 393, Batch: 18, Loss: 0.05059266462922096\n",
      "Iteration 393, Batch: 19, Loss: 0.060979507863521576\n",
      "Iteration 393, Batch: 20, Loss: 0.05373629555106163\n",
      "Iteration 393, Batch: 21, Loss: 0.045784223824739456\n",
      "Iteration 393, Batch: 22, Loss: 0.0494190938770771\n",
      "Iteration 393, Batch: 23, Loss: 0.03640845790505409\n",
      "Iteration 393, Batch: 24, Loss: 0.025894125923514366\n",
      "Iteration 393, Batch: 25, Loss: 0.039124004542827606\n",
      "Iteration 393, Batch: 26, Loss: 0.023315876722335815\n",
      "Iteration 393, Batch: 27, Loss: 0.04503459855914116\n",
      "Iteration 393, Batch: 28, Loss: 0.06695324182510376\n",
      "Iteration 393, Batch: 29, Loss: 0.04250587522983551\n",
      "Iteration 393, Batch: 30, Loss: 0.024486489593982697\n",
      "Iteration 393, Batch: 31, Loss: 0.03619496896862984\n",
      "Iteration 393, Batch: 32, Loss: 0.028841717168688774\n",
      "Iteration 393, Batch: 33, Loss: 0.04342253506183624\n",
      "Iteration 393, Batch: 34, Loss: 0.04550634324550629\n",
      "Iteration 393, Batch: 35, Loss: 0.03092454932630062\n",
      "Iteration 393, Batch: 36, Loss: 0.05342598631978035\n",
      "Iteration 393, Batch: 37, Loss: 0.05717233195900917\n",
      "Iteration 393, Batch: 38, Loss: 0.06966694444417953\n",
      "Iteration 393, Batch: 39, Loss: 0.06738436222076416\n",
      "Iteration 393, Batch: 40, Loss: 0.049315229058265686\n",
      "Iteration 393, Batch: 41, Loss: 0.05769798532128334\n",
      "Iteration 393, Batch: 42, Loss: 0.11971021443605423\n",
      "Iteration 393, Batch: 43, Loss: 0.15825630724430084\n",
      "Iteration 393, Batch: 44, Loss: 0.22981098294258118\n",
      "Iteration 393, Batch: 45, Loss: 0.20609050989151\n",
      "Iteration 393, Batch: 46, Loss: 0.26904475688934326\n",
      "Iteration 393, Batch: 47, Loss: 0.2232789546251297\n",
      "Iteration 393, Batch: 48, Loss: 0.15732724964618683\n",
      "Iteration 393, Batch: 49, Loss: 0.1997617483139038\n",
      "Number of layers: 10\n",
      "Iteration 394, Batch: 0, Loss: 0.15484794974327087\n",
      "Iteration 394, Batch: 1, Loss: 0.14857295155525208\n",
      "Iteration 394, Batch: 2, Loss: 0.14810405671596527\n",
      "Iteration 394, Batch: 3, Loss: 0.15296633541584015\n",
      "Iteration 394, Batch: 4, Loss: 0.10986243188381195\n",
      "Iteration 394, Batch: 5, Loss: 0.12146677076816559\n",
      "Iteration 394, Batch: 6, Loss: 0.16941039264202118\n",
      "Iteration 394, Batch: 7, Loss: 0.16395877301692963\n",
      "Iteration 394, Batch: 8, Loss: 0.15806707739830017\n",
      "Iteration 394, Batch: 9, Loss: 0.13570617139339447\n",
      "Iteration 394, Batch: 10, Loss: 0.15799519419670105\n",
      "Iteration 394, Batch: 11, Loss: 0.08414453268051147\n",
      "Iteration 394, Batch: 12, Loss: 0.14402678608894348\n",
      "Iteration 394, Batch: 13, Loss: 0.11301930993795395\n",
      "Iteration 394, Batch: 14, Loss: 0.1335654854774475\n",
      "Iteration 394, Batch: 15, Loss: 0.11539081484079361\n",
      "Iteration 394, Batch: 16, Loss: 0.0663117989897728\n",
      "Iteration 394, Batch: 17, Loss: 0.11108795553445816\n",
      "Iteration 394, Batch: 18, Loss: 0.15079426765441895\n",
      "Iteration 394, Batch: 19, Loss: 0.1424971967935562\n",
      "Iteration 394, Batch: 20, Loss: 0.1265055388212204\n",
      "Iteration 394, Batch: 21, Loss: 0.09593413770198822\n",
      "Iteration 394, Batch: 22, Loss: 0.08123312890529633\n",
      "Iteration 394, Batch: 23, Loss: 0.0666050910949707\n",
      "Iteration 394, Batch: 24, Loss: 0.06216532737016678\n",
      "Iteration 394, Batch: 25, Loss: 0.12023129314184189\n",
      "Iteration 394, Batch: 26, Loss: 0.11373472213745117\n",
      "Iteration 394, Batch: 27, Loss: 0.08405511826276779\n",
      "Iteration 394, Batch: 28, Loss: 0.07998675107955933\n",
      "Iteration 394, Batch: 29, Loss: 0.06478731334209442\n",
      "Iteration 394, Batch: 30, Loss: 0.09984593838453293\n",
      "Iteration 394, Batch: 31, Loss: 0.07501235604286194\n",
      "Iteration 394, Batch: 32, Loss: 0.11202220618724823\n",
      "Iteration 394, Batch: 33, Loss: 0.0922599509358406\n",
      "Iteration 394, Batch: 34, Loss: 0.05861896276473999\n",
      "Iteration 394, Batch: 35, Loss: 0.06937076896429062\n",
      "Iteration 394, Batch: 36, Loss: 0.06268539279699326\n",
      "Iteration 394, Batch: 37, Loss: 0.03775200992822647\n",
      "Iteration 394, Batch: 38, Loss: 0.07756703346967697\n",
      "Iteration 394, Batch: 39, Loss: 0.06775996088981628\n",
      "Iteration 394, Batch: 40, Loss: 0.05057264119386673\n",
      "Iteration 394, Batch: 41, Loss: 0.037741392850875854\n",
      "Iteration 394, Batch: 42, Loss: 0.06276079267263412\n",
      "Iteration 394, Batch: 43, Loss: 0.05594700574874878\n",
      "Iteration 394, Batch: 44, Loss: 0.04575509950518608\n",
      "Iteration 394, Batch: 45, Loss: 0.08922717720270157\n",
      "Iteration 394, Batch: 46, Loss: 0.08951995521783829\n",
      "Iteration 394, Batch: 47, Loss: 0.049967698752880096\n",
      "Iteration 394, Batch: 48, Loss: 0.04877731204032898\n",
      "Iteration 394, Batch: 49, Loss: 0.07540871202945709\n",
      "Number of layers: 10\n",
      "Iteration 395, Batch: 0, Loss: 0.08257532119750977\n",
      "Iteration 395, Batch: 1, Loss: 0.0875590518116951\n",
      "Iteration 395, Batch: 2, Loss: 0.07767661660909653\n",
      "Iteration 395, Batch: 3, Loss: 0.07211277633905411\n",
      "Iteration 395, Batch: 4, Loss: 0.07538340985774994\n",
      "Iteration 395, Batch: 5, Loss: 0.08529030531644821\n",
      "Iteration 395, Batch: 6, Loss: 0.06997298449277878\n",
      "Iteration 395, Batch: 7, Loss: 0.08629199117422104\n",
      "Iteration 395, Batch: 8, Loss: 0.07405836880207062\n",
      "Iteration 395, Batch: 9, Loss: 0.0794706642627716\n",
      "Iteration 395, Batch: 10, Loss: 0.0756225734949112\n",
      "Iteration 395, Batch: 11, Loss: 0.04508264362812042\n",
      "Iteration 395, Batch: 12, Loss: 0.07836746424436569\n",
      "Iteration 395, Batch: 13, Loss: 0.04884091764688492\n",
      "Iteration 395, Batch: 14, Loss: 0.04290979728102684\n",
      "Iteration 395, Batch: 15, Loss: 0.06526713818311691\n",
      "Iteration 395, Batch: 16, Loss: 0.04971673712134361\n",
      "Iteration 395, Batch: 17, Loss: 0.04820379242300987\n",
      "Iteration 395, Batch: 18, Loss: 0.0905306413769722\n",
      "Iteration 395, Batch: 19, Loss: 0.06721071153879166\n",
      "Iteration 395, Batch: 20, Loss: 0.07163643836975098\n",
      "Iteration 395, Batch: 21, Loss: 0.07167569547891617\n",
      "Iteration 395, Batch: 22, Loss: 0.06540098041296005\n",
      "Iteration 395, Batch: 23, Loss: 0.04995325580239296\n",
      "Iteration 395, Batch: 24, Loss: 0.05243793502449989\n",
      "Iteration 395, Batch: 25, Loss: 0.05119183287024498\n",
      "Iteration 395, Batch: 26, Loss: 0.054024677723646164\n",
      "Iteration 395, Batch: 27, Loss: 0.06860332936048508\n",
      "Iteration 395, Batch: 28, Loss: 0.0743613988161087\n",
      "Iteration 395, Batch: 29, Loss: 0.06518010050058365\n",
      "Iteration 395, Batch: 30, Loss: 0.05812116339802742\n",
      "Iteration 395, Batch: 31, Loss: 0.055664632469415665\n",
      "Iteration 395, Batch: 32, Loss: 0.06493997573852539\n",
      "Iteration 395, Batch: 33, Loss: 0.048025913536548615\n",
      "Iteration 395, Batch: 34, Loss: 0.06149233505129814\n",
      "Iteration 395, Batch: 35, Loss: 0.03542431443929672\n",
      "Iteration 395, Batch: 36, Loss: 0.062409598380327225\n",
      "Iteration 395, Batch: 37, Loss: 0.05906523019075394\n",
      "Iteration 395, Batch: 38, Loss: 0.07706628739833832\n",
      "Iteration 395, Batch: 39, Loss: 0.06334663927555084\n",
      "Iteration 395, Batch: 40, Loss: 0.05514359474182129\n",
      "Iteration 395, Batch: 41, Loss: 0.056290194392204285\n",
      "Iteration 395, Batch: 42, Loss: 0.044792644679546356\n",
      "Iteration 395, Batch: 43, Loss: 0.04330803453922272\n",
      "Iteration 395, Batch: 44, Loss: 0.07260715961456299\n",
      "Iteration 395, Batch: 45, Loss: 0.05096959322690964\n",
      "Iteration 395, Batch: 46, Loss: 0.05231483280658722\n",
      "Iteration 395, Batch: 47, Loss: 0.05396585166454315\n",
      "Iteration 395, Batch: 48, Loss: 0.03715567663311958\n",
      "Iteration 395, Batch: 49, Loss: 0.06636099517345428\n",
      "Number of layers: 10\n",
      "Iteration 396, Batch: 0, Loss: 0.060293883085250854\n",
      "Iteration 396, Batch: 1, Loss: 0.05826598033308983\n",
      "Iteration 396, Batch: 2, Loss: 0.04136345162987709\n",
      "Iteration 396, Batch: 3, Loss: 0.07001785188913345\n",
      "Iteration 396, Batch: 4, Loss: 0.03931029886007309\n",
      "Iteration 396, Batch: 5, Loss: 0.049048688262701035\n",
      "Iteration 396, Batch: 6, Loss: 0.08092240989208221\n",
      "Iteration 396, Batch: 7, Loss: 0.06847745925188065\n",
      "Iteration 396, Batch: 8, Loss: 0.046094540506601334\n",
      "Iteration 396, Batch: 9, Loss: 0.051221054047346115\n",
      "Iteration 396, Batch: 10, Loss: 0.037326496094465256\n",
      "Iteration 396, Batch: 11, Loss: 0.05049413442611694\n",
      "Iteration 396, Batch: 12, Loss: 0.04829801619052887\n",
      "Iteration 396, Batch: 13, Loss: 0.058866068720817566\n",
      "Iteration 396, Batch: 14, Loss: 0.09356111288070679\n",
      "Iteration 396, Batch: 15, Loss: 0.030081283301115036\n",
      "Iteration 396, Batch: 16, Loss: 0.023196199908852577\n",
      "Iteration 396, Batch: 17, Loss: 0.04489672929048538\n",
      "Iteration 396, Batch: 18, Loss: 0.05118449032306671\n",
      "Iteration 396, Batch: 19, Loss: 0.05452682822942734\n",
      "Iteration 396, Batch: 20, Loss: 0.06542130559682846\n",
      "Iteration 396, Batch: 21, Loss: 0.05648011714220047\n",
      "Iteration 396, Batch: 22, Loss: 0.059784889221191406\n",
      "Iteration 396, Batch: 23, Loss: 0.03318075090646744\n",
      "Iteration 396, Batch: 24, Loss: 0.05543017387390137\n",
      "Iteration 396, Batch: 25, Loss: 0.04664577543735504\n",
      "Iteration 396, Batch: 26, Loss: 0.061470068991184235\n",
      "Iteration 396, Batch: 27, Loss: 0.0488501600921154\n",
      "Iteration 396, Batch: 28, Loss: 0.06949906796216965\n",
      "Iteration 396, Batch: 29, Loss: 0.047973018139600754\n",
      "Iteration 396, Batch: 30, Loss: 0.033461712300777435\n",
      "Iteration 396, Batch: 31, Loss: 0.05072348937392235\n",
      "Iteration 396, Batch: 32, Loss: 0.06018012389540672\n",
      "Iteration 396, Batch: 33, Loss: 0.05117538943886757\n",
      "Iteration 396, Batch: 34, Loss: 0.034828439354896545\n",
      "Iteration 396, Batch: 35, Loss: 0.03469450771808624\n",
      "Iteration 396, Batch: 36, Loss: 0.057028092443943024\n",
      "Iteration 396, Batch: 37, Loss: 0.060014769434928894\n",
      "Iteration 396, Batch: 38, Loss: 0.04151938855648041\n",
      "Iteration 396, Batch: 39, Loss: 0.08454973995685577\n",
      "Iteration 396, Batch: 40, Loss: 0.09061545878648758\n",
      "Iteration 396, Batch: 41, Loss: 0.07649054378271103\n",
      "Iteration 396, Batch: 42, Loss: 0.05733911320567131\n",
      "Iteration 396, Batch: 43, Loss: 0.03967874497175217\n",
      "Iteration 396, Batch: 44, Loss: 0.05090333893895149\n",
      "Iteration 396, Batch: 45, Loss: 0.04498889669775963\n",
      "Iteration 396, Batch: 46, Loss: 0.061512190848588943\n",
      "Iteration 396, Batch: 47, Loss: 0.04301517829298973\n",
      "Iteration 396, Batch: 48, Loss: 0.056035201996564865\n",
      "Iteration 396, Batch: 49, Loss: 0.06788114458322525\n",
      "Number of layers: 10\n",
      "Iteration 397, Batch: 0, Loss: 0.07423331588506699\n",
      "Iteration 397, Batch: 1, Loss: 0.06041310355067253\n",
      "Iteration 397, Batch: 2, Loss: 0.060322780162096024\n",
      "Iteration 397, Batch: 3, Loss: 0.06343665719032288\n",
      "Iteration 397, Batch: 4, Loss: 0.030710944905877113\n",
      "Iteration 397, Batch: 5, Loss: 0.04591652378439903\n",
      "Iteration 397, Batch: 6, Loss: 0.04328150674700737\n",
      "Iteration 397, Batch: 7, Loss: 0.03555995598435402\n",
      "Iteration 397, Batch: 8, Loss: 0.03012523613870144\n",
      "Iteration 397, Batch: 9, Loss: 0.042899731546640396\n",
      "Iteration 397, Batch: 10, Loss: 0.040766503661870956\n",
      "Iteration 397, Batch: 11, Loss: 0.05610119551420212\n",
      "Iteration 397, Batch: 12, Loss: 0.027507305145263672\n",
      "Iteration 397, Batch: 13, Loss: 0.031757015734910965\n",
      "Iteration 397, Batch: 14, Loss: 0.04020161181688309\n",
      "Iteration 397, Batch: 15, Loss: 0.06812798231840134\n",
      "Iteration 397, Batch: 16, Loss: 0.05329811945557594\n",
      "Iteration 397, Batch: 17, Loss: 0.06351630389690399\n",
      "Iteration 397, Batch: 18, Loss: 0.07189743965864182\n",
      "Iteration 397, Batch: 19, Loss: 0.06428201496601105\n",
      "Iteration 397, Batch: 20, Loss: 0.05179167538881302\n",
      "Iteration 397, Batch: 21, Loss: 0.04176686331629753\n",
      "Iteration 397, Batch: 22, Loss: 0.035906996577978134\n",
      "Iteration 397, Batch: 23, Loss: 0.044434208422899246\n",
      "Iteration 397, Batch: 24, Loss: 0.03959383815526962\n",
      "Iteration 397, Batch: 25, Loss: 0.04835205152630806\n",
      "Iteration 397, Batch: 26, Loss: 0.05504075810313225\n",
      "Iteration 397, Batch: 27, Loss: 0.027972526848316193\n",
      "Iteration 397, Batch: 28, Loss: 0.05101844668388367\n",
      "Iteration 397, Batch: 29, Loss: 0.04920979216694832\n",
      "Iteration 397, Batch: 30, Loss: 0.029408510774374008\n",
      "Iteration 397, Batch: 31, Loss: 0.056965429335832596\n",
      "Iteration 397, Batch: 32, Loss: 0.08616951107978821\n",
      "Iteration 397, Batch: 33, Loss: 0.05800411105155945\n",
      "Iteration 397, Batch: 34, Loss: 0.08022657036781311\n",
      "Iteration 397, Batch: 35, Loss: 0.05463261157274246\n",
      "Iteration 397, Batch: 36, Loss: 0.043889664113521576\n",
      "Iteration 397, Batch: 37, Loss: 0.0647546723484993\n",
      "Iteration 397, Batch: 38, Loss: 0.03455632925033569\n",
      "Iteration 397, Batch: 39, Loss: 0.06222618743777275\n",
      "Iteration 397, Batch: 40, Loss: 0.05296872556209564\n",
      "Iteration 397, Batch: 41, Loss: 0.042493369430303574\n",
      "Iteration 397, Batch: 42, Loss: 0.09984403103590012\n",
      "Iteration 397, Batch: 43, Loss: 0.033177852630615234\n",
      "Iteration 397, Batch: 44, Loss: 0.05992298945784569\n",
      "Iteration 397, Batch: 45, Loss: 0.05317533016204834\n",
      "Iteration 397, Batch: 46, Loss: 0.03334176540374756\n",
      "Iteration 397, Batch: 47, Loss: 0.0685400515794754\n",
      "Iteration 397, Batch: 48, Loss: 0.06645414978265762\n",
      "Iteration 397, Batch: 49, Loss: 0.057041462510824203\n",
      "Number of layers: 10\n",
      "Iteration 398, Batch: 0, Loss: 0.08006631582975388\n",
      "Iteration 398, Batch: 1, Loss: 0.08950816839933395\n",
      "Iteration 398, Batch: 2, Loss: 0.046058159321546555\n",
      "Iteration 398, Batch: 3, Loss: 0.05925761163234711\n",
      "Iteration 398, Batch: 4, Loss: 0.06771921366453171\n",
      "Iteration 398, Batch: 5, Loss: 0.05760648474097252\n",
      "Iteration 398, Batch: 6, Loss: 0.04194515198469162\n",
      "Iteration 398, Batch: 7, Loss: 0.0643380880355835\n",
      "Iteration 398, Batch: 8, Loss: 0.05664820224046707\n",
      "Iteration 398, Batch: 9, Loss: 0.04662790149450302\n",
      "Iteration 398, Batch: 10, Loss: 0.03793332725763321\n",
      "Iteration 398, Batch: 11, Loss: 0.06223468482494354\n",
      "Iteration 398, Batch: 12, Loss: 0.07208188623189926\n",
      "Iteration 398, Batch: 13, Loss: 0.05429543927311897\n",
      "Iteration 398, Batch: 14, Loss: 0.04757865145802498\n",
      "Iteration 398, Batch: 15, Loss: 0.04796381667256355\n",
      "Iteration 398, Batch: 16, Loss: 0.06779558956623077\n",
      "Iteration 398, Batch: 17, Loss: 0.060203228145837784\n",
      "Iteration 398, Batch: 18, Loss: 0.032521963119506836\n",
      "Iteration 398, Batch: 19, Loss: 0.05004807934165001\n",
      "Iteration 398, Batch: 20, Loss: 0.027424713596701622\n",
      "Iteration 398, Batch: 21, Loss: 0.043010976165533066\n",
      "Iteration 398, Batch: 22, Loss: 0.03169507533311844\n",
      "Iteration 398, Batch: 23, Loss: 0.035592325031757355\n",
      "Iteration 398, Batch: 24, Loss: 0.0450025349855423\n",
      "Iteration 398, Batch: 25, Loss: 0.040036074817180634\n",
      "Iteration 398, Batch: 26, Loss: 0.04686284437775612\n",
      "Iteration 398, Batch: 27, Loss: 0.05592846870422363\n",
      "Iteration 398, Batch: 28, Loss: 0.040043335407972336\n",
      "Iteration 398, Batch: 29, Loss: 0.024439379572868347\n",
      "Iteration 398, Batch: 30, Loss: 0.05752768740057945\n",
      "Iteration 398, Batch: 31, Loss: 0.04011765494942665\n",
      "Iteration 398, Batch: 32, Loss: 0.04407137632369995\n",
      "Iteration 398, Batch: 33, Loss: 0.0494229719042778\n",
      "Iteration 398, Batch: 34, Loss: 0.046763647347688675\n",
      "Iteration 398, Batch: 35, Loss: 0.030967295169830322\n",
      "Iteration 398, Batch: 36, Loss: 0.05665987357497215\n",
      "Iteration 398, Batch: 37, Loss: 0.04232964292168617\n",
      "Iteration 398, Batch: 38, Loss: 0.0314302071928978\n",
      "Iteration 398, Batch: 39, Loss: 0.05113164708018303\n",
      "Iteration 398, Batch: 40, Loss: 0.03942016884684563\n",
      "Iteration 398, Batch: 41, Loss: 0.0566524863243103\n",
      "Iteration 398, Batch: 42, Loss: 0.044713154435157776\n",
      "Iteration 398, Batch: 43, Loss: 0.028981372714042664\n",
      "Iteration 398, Batch: 44, Loss: 0.025706421583890915\n",
      "Iteration 398, Batch: 45, Loss: 0.040838804095983505\n",
      "Iteration 398, Batch: 46, Loss: 0.03327763453125954\n",
      "Iteration 398, Batch: 47, Loss: 0.058567922562360764\n",
      "Iteration 398, Batch: 48, Loss: 0.06301666051149368\n",
      "Iteration 398, Batch: 49, Loss: 0.04861428216099739\n",
      "Number of layers: 10\n",
      "Iteration 399, Batch: 0, Loss: 0.03865678235888481\n",
      "Iteration 399, Batch: 1, Loss: 0.06000233441591263\n",
      "Iteration 399, Batch: 2, Loss: 0.03620855137705803\n",
      "Iteration 399, Batch: 3, Loss: 0.09090522676706314\n",
      "Iteration 399, Batch: 4, Loss: 0.05234282836318016\n",
      "Iteration 399, Batch: 5, Loss: 0.05586187168955803\n",
      "Iteration 399, Batch: 6, Loss: 0.053273387253284454\n",
      "Iteration 399, Batch: 7, Loss: 0.038651127368211746\n",
      "Iteration 399, Batch: 8, Loss: 0.051842302083969116\n",
      "Iteration 399, Batch: 9, Loss: 0.03604799136519432\n",
      "Iteration 399, Batch: 10, Loss: 0.06169458106160164\n",
      "Iteration 399, Batch: 11, Loss: 0.08539806306362152\n",
      "Iteration 399, Batch: 12, Loss: 0.0845675840973854\n",
      "Iteration 399, Batch: 13, Loss: 0.045904841274023056\n",
      "Iteration 399, Batch: 14, Loss: 0.04845137149095535\n",
      "Iteration 399, Batch: 15, Loss: 0.027212880551815033\n",
      "Iteration 399, Batch: 16, Loss: 0.060043495148420334\n",
      "Iteration 399, Batch: 17, Loss: 0.04667168855667114\n",
      "Iteration 399, Batch: 18, Loss: 0.0912555679678917\n",
      "Iteration 399, Batch: 19, Loss: 0.0823114886879921\n",
      "Iteration 399, Batch: 20, Loss: 0.06781841814517975\n",
      "Iteration 399, Batch: 21, Loss: 0.06411618739366531\n",
      "Iteration 399, Batch: 22, Loss: 0.06601302325725555\n",
      "Iteration 399, Batch: 23, Loss: 0.057777442038059235\n",
      "Iteration 399, Batch: 24, Loss: 0.054815106093883514\n",
      "Iteration 399, Batch: 25, Loss: 0.03749524801969528\n",
      "Iteration 399, Batch: 26, Loss: 0.04813120886683464\n",
      "Iteration 399, Batch: 27, Loss: 0.07199245691299438\n",
      "Iteration 399, Batch: 28, Loss: 0.05885301157832146\n",
      "Iteration 399, Batch: 29, Loss: 0.06419367343187332\n",
      "Iteration 399, Batch: 30, Loss: 0.05758143216371536\n",
      "Iteration 399, Batch: 31, Loss: 0.06778759509325027\n",
      "Iteration 399, Batch: 32, Loss: 0.07748604565858841\n",
      "Iteration 399, Batch: 33, Loss: 0.10735476016998291\n",
      "Iteration 399, Batch: 34, Loss: 0.05707559362053871\n",
      "Iteration 399, Batch: 35, Loss: 0.05300012603402138\n",
      "Iteration 399, Batch: 36, Loss: 0.05949431657791138\n",
      "Iteration 399, Batch: 37, Loss: 0.08792899549007416\n",
      "Iteration 399, Batch: 38, Loss: 0.08044734597206116\n",
      "Iteration 399, Batch: 39, Loss: 0.06921838968992233\n",
      "Iteration 399, Batch: 40, Loss: 0.05949162319302559\n",
      "Iteration 399, Batch: 41, Loss: 0.08242395520210266\n",
      "Iteration 399, Batch: 42, Loss: 0.09724733978509903\n",
      "Iteration 399, Batch: 43, Loss: 0.07427266240119934\n",
      "Iteration 399, Batch: 44, Loss: 0.0409032441675663\n",
      "Iteration 399, Batch: 45, Loss: 0.05787748098373413\n",
      "Iteration 399, Batch: 46, Loss: 0.06893408298492432\n",
      "Iteration 399, Batch: 47, Loss: 0.040524180978536606\n",
      "Iteration 399, Batch: 48, Loss: 0.06129217520356178\n",
      "Iteration 399, Batch: 49, Loss: 0.03819844126701355\n"
     ]
    }
   ],
   "source": [
    "# Specify parameters\n",
    "N = 10000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "train_data = sample_data(N, n, k)\n",
    "train_dataset = SubmatrixDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "# Specify parameters\n",
    "num_epochs = 400\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "\n",
    "# Training routine\n",
    "model = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10, alpha=0.5)\n",
    "model.to(device)\n",
    "\n",
    "# Store optimal model\n",
    "model_opt = deepcopy(model)\n",
    "min_loss = float(\"inf\")\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-3)\n",
    "\"\"\"\n",
    "for it in range(236, num_epochs):\n",
    "    counter = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    # If iteration is small, train with 9 layers\n",
    "    if it <= num_epochs // 3:\n",
    "        model.num_layers = 7\n",
    "    elif it <= 2 * num_epochs // 3:\n",
    "        model.num_layers = 9\n",
    "    else:\n",
    "        model.num_layers = 10\n",
    "\n",
    "    print(\"Number of layers: {}\".format(model.num_layers))\n",
    "    # Iterate through batches\n",
    "    for batch in train_dataloader:\n",
    "        # Get loss from model\n",
    "        loss = mc_loss_batch_simul(model, batch, time_array, n, k, num_steps=None, time_threshold=400, p_bad=0.05)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "        # Backpropagate\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Iteration {}, Batch: {}, Loss: {}\".format(it, counter, loss.item()))\n",
    "\n",
    "        # Update counter\n",
    "        counter += 1\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    # Update best model\n",
    "    with torch.no_grad():\n",
    "        if loss_total < min_loss:\n",
    "            min_loss = loss_total\n",
    "            model_opt = deepcopy(model)\n",
    "\n",
    "# Save optimal model to save time for generation\n",
    "torch.save(model_opt.state_dict(), \"model_weights_intermediate_1.pth\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c1b14be-1232-4b43-b1f9-510e30bc1db2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 228, Batch: 0, Loss: 0.05089670419692993\n",
      "Iteration 228, Batch: 1, Loss: 0.06180037185549736\n",
      "Iteration 228, Batch: 2, Loss: 0.06241530179977417\n",
      "Iteration 228, Batch: 3, Loss: 0.0414377860724926\n",
      "Iteration 228, Batch: 4, Loss: 0.045566849410533905\n",
      "Iteration 228, Batch: 5, Loss: 0.06437168270349503\n",
      "Iteration 228, Batch: 6, Loss: 0.059408895671367645\n",
      "Iteration 228, Batch: 7, Loss: 0.08264163881540298\n",
      "Iteration 228, Batch: 8, Loss: 0.06695272028446198\n",
      "Iteration 228, Batch: 9, Loss: 0.05737430602312088\n",
      "Iteration 228, Batch: 10, Loss: 0.0625535175204277\n",
      "Iteration 228, Batch: 11, Loss: 0.04457927495241165\n",
      "Iteration 228, Batch: 12, Loss: 0.0691099688410759\n",
      "Iteration 228, Batch: 13, Loss: 0.056269239634275436\n",
      "Iteration 228, Batch: 14, Loss: 0.03668055310845375\n",
      "Iteration 228, Batch: 15, Loss: 0.08798881620168686\n",
      "Iteration 228, Batch: 16, Loss: 0.06727524101734161\n",
      "Iteration 228, Batch: 17, Loss: 0.04403548315167427\n",
      "Iteration 228, Batch: 18, Loss: 0.051988858729600906\n",
      "Iteration 228, Batch: 19, Loss: 0.05948662757873535\n",
      "Iteration 228, Batch: 20, Loss: 0.07153475284576416\n",
      "Iteration 228, Batch: 21, Loss: 0.027869515120983124\n",
      "Iteration 228, Batch: 22, Loss: 0.04897768422961235\n",
      "Iteration 228, Batch: 23, Loss: 0.05039278790354729\n",
      "Iteration 228, Batch: 24, Loss: 0.058159541338682175\n",
      "Iteration 228, Batch: 25, Loss: 0.06419934332370758\n",
      "Iteration 228, Batch: 26, Loss: 0.07601529359817505\n",
      "Iteration 228, Batch: 27, Loss: 0.051303036510944366\n",
      "Iteration 228, Batch: 28, Loss: 0.08150248974561691\n",
      "Iteration 228, Batch: 29, Loss: 0.06895626336336136\n",
      "Iteration 228, Batch: 30, Loss: 0.050538260489702225\n",
      "Iteration 228, Batch: 31, Loss: 0.05262516066431999\n",
      "Iteration 228, Batch: 32, Loss: 0.044037483632564545\n",
      "Iteration 228, Batch: 33, Loss: 0.05534757673740387\n",
      "Iteration 228, Batch: 34, Loss: 0.05741992965340614\n",
      "Iteration 228, Batch: 35, Loss: 0.047020893543958664\n",
      "Iteration 228, Batch: 36, Loss: 0.04503008350729942\n",
      "Iteration 228, Batch: 37, Loss: 0.050976019352674484\n",
      "Iteration 228, Batch: 38, Loss: 0.06542056798934937\n",
      "Iteration 228, Batch: 39, Loss: 0.054622989147901535\n",
      "Iteration 228, Batch: 40, Loss: 0.028773711994290352\n",
      "Iteration 228, Batch: 41, Loss: 0.05664795637130737\n",
      "Iteration 228, Batch: 42, Loss: 0.04605480656027794\n",
      "Iteration 228, Batch: 43, Loss: 0.05781806632876396\n",
      "Iteration 228, Batch: 44, Loss: 0.05650338903069496\n",
      "Iteration 228, Batch: 45, Loss: 0.06353973597288132\n",
      "Iteration 228, Batch: 46, Loss: 0.08108313381671906\n",
      "Iteration 228, Batch: 47, Loss: 0.06296341121196747\n",
      "Iteration 228, Batch: 48, Loss: 0.024068264290690422\n",
      "Iteration 228, Batch: 49, Loss: 0.0510062500834465\n",
      "Iteration 229, Batch: 0, Loss: 0.03527018427848816\n",
      "Iteration 229, Batch: 1, Loss: 0.03192770108580589\n",
      "Iteration 229, Batch: 2, Loss: 0.045089635998010635\n",
      "Iteration 229, Batch: 3, Loss: 0.046903446316719055\n",
      "Iteration 229, Batch: 4, Loss: 0.07293269038200378\n",
      "Iteration 229, Batch: 5, Loss: 0.055185526609420776\n",
      "Iteration 229, Batch: 6, Loss: 0.056528594344854355\n",
      "Iteration 229, Batch: 7, Loss: 0.03379175439476967\n",
      "Iteration 229, Batch: 8, Loss: 0.05384216830134392\n",
      "Iteration 229, Batch: 9, Loss: 0.04868152365088463\n",
      "Iteration 229, Batch: 10, Loss: 0.051171403378248215\n",
      "Iteration 229, Batch: 11, Loss: 0.050016894936561584\n",
      "Iteration 229, Batch: 12, Loss: 0.03281611576676369\n",
      "Iteration 229, Batch: 13, Loss: 0.07245586067438126\n",
      "Iteration 229, Batch: 14, Loss: 0.037868209183216095\n",
      "Iteration 229, Batch: 15, Loss: 0.05842636153101921\n",
      "Iteration 229, Batch: 16, Loss: 0.05705836042761803\n",
      "Iteration 229, Batch: 17, Loss: 0.06268127262592316\n",
      "Iteration 229, Batch: 18, Loss: 0.0732632428407669\n",
      "Iteration 229, Batch: 19, Loss: 0.059607528150081635\n",
      "Iteration 229, Batch: 20, Loss: 0.031088508665561676\n",
      "Iteration 229, Batch: 21, Loss: 0.05080479383468628\n",
      "Iteration 229, Batch: 22, Loss: 0.042357187718153\n",
      "Iteration 229, Batch: 23, Loss: 0.06448201090097427\n",
      "Iteration 229, Batch: 24, Loss: 0.07917250692844391\n",
      "Iteration 229, Batch: 25, Loss: 0.0730227380990982\n",
      "Iteration 229, Batch: 26, Loss: 0.03182356432080269\n",
      "Iteration 229, Batch: 27, Loss: 0.05065805837512016\n",
      "Iteration 229, Batch: 28, Loss: 0.04170307517051697\n",
      "Iteration 229, Batch: 29, Loss: 0.04341518133878708\n",
      "Iteration 229, Batch: 30, Loss: 0.0446602888405323\n",
      "Iteration 229, Batch: 31, Loss: 0.06564562767744064\n",
      "Iteration 229, Batch: 32, Loss: 0.04649611935019493\n",
      "Iteration 229, Batch: 33, Loss: 0.06683458387851715\n",
      "Iteration 229, Batch: 34, Loss: 0.04542453587055206\n",
      "Iteration 229, Batch: 35, Loss: 0.05224592983722687\n",
      "Iteration 229, Batch: 36, Loss: 0.04169927537441254\n",
      "Iteration 229, Batch: 37, Loss: 0.05176528915762901\n",
      "Iteration 229, Batch: 38, Loss: 0.054276350885629654\n",
      "Iteration 229, Batch: 39, Loss: 0.04489583522081375\n",
      "Iteration 229, Batch: 40, Loss: 0.03188347443938255\n",
      "Iteration 229, Batch: 41, Loss: 0.030610574409365654\n",
      "Iteration 229, Batch: 42, Loss: 0.05529408156871796\n",
      "Iteration 229, Batch: 43, Loss: 0.055835455656051636\n",
      "Iteration 229, Batch: 44, Loss: 0.03728923574090004\n",
      "Iteration 229, Batch: 45, Loss: 0.037699922919273376\n",
      "Iteration 229, Batch: 46, Loss: 0.0767628625035286\n",
      "Iteration 229, Batch: 47, Loss: 0.06443174928426743\n",
      "Iteration 229, Batch: 48, Loss: 0.04256291314959526\n",
      "Iteration 229, Batch: 49, Loss: 0.038770999759435654\n",
      "Iteration 230, Batch: 0, Loss: 0.05852048844099045\n",
      "Iteration 230, Batch: 1, Loss: 0.05510517954826355\n",
      "Iteration 230, Batch: 2, Loss: 0.05949116498231888\n",
      "Iteration 230, Batch: 3, Loss: 0.04773189127445221\n",
      "Iteration 230, Batch: 4, Loss: 0.059971485286951065\n",
      "Iteration 230, Batch: 5, Loss: 0.06486485153436661\n",
      "Iteration 230, Batch: 6, Loss: 0.05004891753196716\n",
      "Iteration 230, Batch: 7, Loss: 0.07972215116024017\n",
      "Iteration 230, Batch: 8, Loss: 0.0786648839712143\n",
      "Iteration 230, Batch: 9, Loss: 0.056544288992881775\n",
      "Iteration 230, Batch: 10, Loss: 0.04963518679141998\n",
      "Iteration 230, Batch: 11, Loss: 0.05372770130634308\n",
      "Iteration 230, Batch: 12, Loss: 0.06211305409669876\n",
      "Iteration 230, Batch: 13, Loss: 0.04141000658273697\n",
      "Iteration 230, Batch: 14, Loss: 0.059221990406513214\n",
      "Iteration 230, Batch: 15, Loss: 0.05605842545628548\n",
      "Iteration 230, Batch: 16, Loss: 0.04239727556705475\n",
      "Iteration 230, Batch: 17, Loss: 0.07360689342021942\n",
      "Iteration 230, Batch: 18, Loss: 0.06049105152487755\n",
      "Iteration 230, Batch: 19, Loss: 0.057336192578077316\n",
      "Iteration 230, Batch: 20, Loss: 0.06383947283029556\n",
      "Iteration 230, Batch: 21, Loss: 0.036150600761175156\n",
      "Iteration 230, Batch: 22, Loss: 0.05182306095957756\n",
      "Iteration 230, Batch: 23, Loss: 0.055397551506757736\n",
      "Iteration 230, Batch: 24, Loss: 0.06013046205043793\n",
      "Iteration 230, Batch: 25, Loss: 0.044350896030664444\n",
      "Iteration 230, Batch: 26, Loss: 0.05483602359890938\n",
      "Iteration 230, Batch: 27, Loss: 0.039152953773736954\n",
      "Iteration 230, Batch: 28, Loss: 0.08677936345338821\n",
      "Iteration 230, Batch: 29, Loss: 0.0824187844991684\n",
      "Iteration 230, Batch: 30, Loss: 0.04198469594120979\n",
      "Iteration 230, Batch: 31, Loss: 0.05157386511564255\n",
      "Iteration 230, Batch: 32, Loss: 0.045806024223566055\n",
      "Iteration 230, Batch: 33, Loss: 0.04933767393231392\n",
      "Iteration 230, Batch: 34, Loss: 0.040562953799963\n",
      "Iteration 230, Batch: 35, Loss: 0.0453205443918705\n",
      "Iteration 230, Batch: 36, Loss: 0.044219110161066055\n",
      "Iteration 230, Batch: 37, Loss: 0.03814355283975601\n",
      "Iteration 230, Batch: 38, Loss: 0.032228026539087296\n",
      "Iteration 230, Batch: 39, Loss: 0.042227886617183685\n",
      "Iteration 230, Batch: 40, Loss: 0.05384020134806633\n",
      "Iteration 230, Batch: 41, Loss: 0.06491325050592422\n",
      "Iteration 230, Batch: 42, Loss: 0.02711406722664833\n",
      "Iteration 230, Batch: 43, Loss: 0.04206095263361931\n",
      "Iteration 230, Batch: 44, Loss: 0.03985881805419922\n",
      "Iteration 230, Batch: 45, Loss: 0.05817153677344322\n",
      "Iteration 230, Batch: 46, Loss: 0.04486153647303581\n",
      "Iteration 230, Batch: 47, Loss: 0.04798122122883797\n",
      "Iteration 230, Batch: 48, Loss: 0.03404483571648598\n",
      "Iteration 230, Batch: 49, Loss: 0.025871872901916504\n",
      "Iteration 231, Batch: 0, Loss: 0.04509876295924187\n",
      "Iteration 231, Batch: 1, Loss: 0.061717718839645386\n",
      "Iteration 231, Batch: 2, Loss: 0.07078304886817932\n",
      "Iteration 231, Batch: 3, Loss: 0.0558205209672451\n",
      "Iteration 231, Batch: 4, Loss: 0.05257852375507355\n",
      "Iteration 231, Batch: 5, Loss: 0.06504885852336884\n",
      "Iteration 231, Batch: 6, Loss: 0.04408802092075348\n",
      "Iteration 231, Batch: 7, Loss: 0.03493257984519005\n",
      "Iteration 231, Batch: 8, Loss: 0.032948195934295654\n",
      "Iteration 231, Batch: 9, Loss: 0.050324033945798874\n",
      "Iteration 231, Batch: 10, Loss: 0.04546983912587166\n",
      "Iteration 231, Batch: 11, Loss: 0.02537425048649311\n",
      "Iteration 231, Batch: 12, Loss: 0.053047869354486465\n",
      "Iteration 231, Batch: 13, Loss: 0.043161652982234955\n",
      "Iteration 231, Batch: 14, Loss: 0.05042150989174843\n",
      "Iteration 231, Batch: 15, Loss: 0.033044684678316116\n",
      "Iteration 231, Batch: 16, Loss: 0.05618962273001671\n",
      "Iteration 231, Batch: 17, Loss: 0.060301512479782104\n",
      "Iteration 231, Batch: 18, Loss: 0.03962797671556473\n",
      "Iteration 231, Batch: 19, Loss: 0.05326128005981445\n",
      "Iteration 231, Batch: 20, Loss: 0.045828238129615784\n",
      "Iteration 231, Batch: 21, Loss: 0.05550769716501236\n",
      "Iteration 231, Batch: 22, Loss: 0.06855110824108124\n",
      "Iteration 231, Batch: 23, Loss: 0.047185782343149185\n",
      "Iteration 231, Batch: 24, Loss: 0.04163811728358269\n",
      "Iteration 231, Batch: 25, Loss: 0.06983451545238495\n",
      "Iteration 231, Batch: 26, Loss: 0.06274835765361786\n",
      "Iteration 231, Batch: 27, Loss: 0.030697902664542198\n",
      "Iteration 231, Batch: 28, Loss: 0.044455308467149734\n",
      "Iteration 231, Batch: 29, Loss: 0.06408105045557022\n",
      "Iteration 231, Batch: 30, Loss: 0.049338892102241516\n",
      "Iteration 231, Batch: 31, Loss: 0.05443619191646576\n",
      "Iteration 231, Batch: 32, Loss: 0.043803758919239044\n",
      "Iteration 231, Batch: 33, Loss: 0.036258596926927567\n",
      "Iteration 231, Batch: 34, Loss: 0.05074387416243553\n",
      "Iteration 231, Batch: 35, Loss: 0.0474194772541523\n",
      "Iteration 231, Batch: 36, Loss: 0.04552570357918739\n",
      "Iteration 231, Batch: 37, Loss: 0.04523066058754921\n",
      "Iteration 231, Batch: 38, Loss: 0.053774889558553696\n",
      "Iteration 231, Batch: 39, Loss: 0.03249020874500275\n",
      "Iteration 231, Batch: 40, Loss: 0.05542545020580292\n",
      "Iteration 231, Batch: 41, Loss: 0.03475058451294899\n",
      "Iteration 231, Batch: 42, Loss: 0.04199319705367088\n",
      "Iteration 231, Batch: 43, Loss: 0.07547182589769363\n",
      "Iteration 231, Batch: 44, Loss: 0.047824926674366\n",
      "Iteration 231, Batch: 45, Loss: 0.04635236784815788\n",
      "Iteration 231, Batch: 46, Loss: 0.050253976136446\n",
      "Iteration 231, Batch: 47, Loss: 0.05218520388007164\n",
      "Iteration 231, Batch: 48, Loss: 0.04097757861018181\n",
      "Iteration 231, Batch: 49, Loss: 0.04362545907497406\n",
      "Iteration 232, Batch: 0, Loss: 0.06123441085219383\n",
      "Iteration 232, Batch: 1, Loss: 0.021532749757170677\n",
      "Iteration 232, Batch: 2, Loss: 0.06531146168708801\n",
      "Iteration 232, Batch: 3, Loss: 0.036146845668554306\n",
      "Iteration 232, Batch: 4, Loss: 0.027525562793016434\n",
      "Iteration 232, Batch: 5, Loss: 0.03621329739689827\n",
      "Iteration 232, Batch: 6, Loss: 0.05745232477784157\n",
      "Iteration 232, Batch: 7, Loss: 0.05664663016796112\n",
      "Iteration 232, Batch: 8, Loss: 0.06061658635735512\n",
      "Iteration 232, Batch: 9, Loss: 0.07175978273153305\n",
      "Iteration 232, Batch: 10, Loss: 0.04622536525130272\n",
      "Iteration 232, Batch: 11, Loss: 0.06253629177808762\n",
      "Iteration 232, Batch: 12, Loss: 0.06131391227245331\n",
      "Iteration 232, Batch: 13, Loss: 0.05963807925581932\n",
      "Iteration 232, Batch: 14, Loss: 0.05952269583940506\n",
      "Iteration 232, Batch: 15, Loss: 0.04289843887090683\n",
      "Iteration 232, Batch: 16, Loss: 0.04157450422644615\n",
      "Iteration 232, Batch: 17, Loss: 0.062016043812036514\n",
      "Iteration 232, Batch: 18, Loss: 0.05523733049631119\n",
      "Iteration 232, Batch: 19, Loss: 0.07345018535852432\n",
      "Iteration 232, Batch: 20, Loss: 0.08595753461122513\n",
      "Iteration 232, Batch: 21, Loss: 0.033951058983802795\n",
      "Iteration 232, Batch: 22, Loss: 0.03327382355928421\n",
      "Iteration 232, Batch: 23, Loss: 0.07108733057975769\n",
      "Iteration 232, Batch: 24, Loss: 0.053394801914691925\n",
      "Iteration 232, Batch: 25, Loss: 0.054411742836236954\n",
      "Iteration 232, Batch: 26, Loss: 0.07709085941314697\n",
      "Iteration 232, Batch: 27, Loss: 0.05698586255311966\n",
      "Iteration 232, Batch: 28, Loss: 0.07524662464857101\n",
      "Iteration 232, Batch: 29, Loss: 0.047889914363622665\n",
      "Iteration 232, Batch: 30, Loss: 0.0456254668533802\n",
      "Iteration 232, Batch: 31, Loss: 0.048755645751953125\n",
      "Iteration 232, Batch: 32, Loss: 0.038206104189157486\n",
      "Iteration 232, Batch: 33, Loss: 0.06167995184659958\n",
      "Iteration 232, Batch: 34, Loss: 0.04229685664176941\n",
      "Iteration 232, Batch: 35, Loss: 0.04991903156042099\n",
      "Iteration 232, Batch: 36, Loss: 0.04768355190753937\n",
      "Iteration 232, Batch: 37, Loss: 0.04783928766846657\n",
      "Iteration 232, Batch: 38, Loss: 0.05684047192335129\n",
      "Iteration 232, Batch: 39, Loss: 0.02524302713572979\n",
      "Iteration 232, Batch: 40, Loss: 0.06385233998298645\n",
      "Iteration 232, Batch: 41, Loss: 0.07627611607313156\n",
      "Iteration 232, Batch: 42, Loss: 0.054539754986763\n",
      "Iteration 232, Batch: 43, Loss: 0.039570603519678116\n",
      "Iteration 232, Batch: 44, Loss: 0.04069078713655472\n",
      "Iteration 232, Batch: 45, Loss: 0.0548325814306736\n",
      "Iteration 232, Batch: 46, Loss: 0.04246458038687706\n",
      "Iteration 232, Batch: 47, Loss: 0.0616215355694294\n",
      "Iteration 232, Batch: 48, Loss: 0.0476837083697319\n",
      "Iteration 232, Batch: 49, Loss: 0.051151979714632034\n",
      "Iteration 233, Batch: 0, Loss: 0.0603112168610096\n",
      "Iteration 233, Batch: 1, Loss: 0.05175728350877762\n",
      "Iteration 233, Batch: 2, Loss: 0.060615360736846924\n",
      "Iteration 233, Batch: 3, Loss: 0.03239022567868233\n",
      "Iteration 233, Batch: 4, Loss: 0.040962353348731995\n",
      "Iteration 233, Batch: 5, Loss: 0.0460013747215271\n",
      "Iteration 233, Batch: 6, Loss: 0.04637313261628151\n",
      "Iteration 233, Batch: 7, Loss: 0.04633794724941254\n",
      "Iteration 233, Batch: 8, Loss: 0.06485522538423538\n",
      "Iteration 233, Batch: 9, Loss: 0.04339578375220299\n",
      "Iteration 233, Batch: 10, Loss: 0.045474909245967865\n",
      "Iteration 233, Batch: 11, Loss: 0.048081111162900925\n",
      "Iteration 233, Batch: 12, Loss: 0.040056753903627396\n",
      "Iteration 233, Batch: 13, Loss: 0.06284536421298981\n",
      "Iteration 233, Batch: 14, Loss: 0.04703181982040405\n",
      "Iteration 233, Batch: 15, Loss: 0.0769457295536995\n",
      "Iteration 233, Batch: 16, Loss: 0.08716052770614624\n",
      "Iteration 233, Batch: 17, Loss: 0.0478469654917717\n",
      "Iteration 233, Batch: 18, Loss: 0.04534079506993294\n",
      "Iteration 233, Batch: 19, Loss: 0.06794310361146927\n",
      "Iteration 233, Batch: 20, Loss: 0.07478500157594681\n",
      "Iteration 233, Batch: 21, Loss: 0.07235075533390045\n",
      "Iteration 233, Batch: 22, Loss: 0.05068066343665123\n",
      "Iteration 233, Batch: 23, Loss: 0.049414098262786865\n",
      "Iteration 233, Batch: 24, Loss: 0.04177829623222351\n",
      "Iteration 233, Batch: 25, Loss: 0.05821560323238373\n",
      "Iteration 233, Batch: 26, Loss: 0.048283595591783524\n",
      "Iteration 233, Batch: 27, Loss: 0.03792806714773178\n",
      "Iteration 233, Batch: 28, Loss: 0.03686537966132164\n",
      "Iteration 233, Batch: 29, Loss: 0.033523838967084885\n",
      "Iteration 233, Batch: 30, Loss: 0.05263148248195648\n",
      "Iteration 233, Batch: 31, Loss: 0.05033285170793533\n",
      "Iteration 233, Batch: 32, Loss: 0.051332052797079086\n",
      "Iteration 233, Batch: 33, Loss: 0.05795527249574661\n",
      "Iteration 233, Batch: 34, Loss: 0.05012153089046478\n",
      "Iteration 233, Batch: 35, Loss: 0.04346560314297676\n",
      "Iteration 233, Batch: 36, Loss: 0.04942282661795616\n",
      "Iteration 233, Batch: 37, Loss: 0.05209948495030403\n",
      "Iteration 233, Batch: 38, Loss: 0.03720065951347351\n",
      "Iteration 233, Batch: 39, Loss: 0.05737391486763954\n",
      "Iteration 233, Batch: 40, Loss: 0.04272841289639473\n",
      "Iteration 233, Batch: 41, Loss: 0.07248036563396454\n",
      "Iteration 233, Batch: 42, Loss: 0.05410172417759895\n",
      "Iteration 233, Batch: 43, Loss: 0.062271781265735626\n",
      "Iteration 233, Batch: 44, Loss: 0.04860977455973625\n",
      "Iteration 233, Batch: 45, Loss: 0.05947142466902733\n",
      "Iteration 233, Batch: 46, Loss: 0.05284546688199043\n",
      "Iteration 233, Batch: 47, Loss: 0.05094614997506142\n",
      "Iteration 233, Batch: 48, Loss: 0.053798284381628036\n",
      "Iteration 233, Batch: 49, Loss: 0.0962476134300232\n",
      "Iteration 234, Batch: 0, Loss: 0.06520652025938034\n",
      "Iteration 234, Batch: 1, Loss: 0.05732935294508934\n",
      "Iteration 234, Batch: 2, Loss: 0.050092361867427826\n",
      "Iteration 234, Batch: 3, Loss: 0.05513927340507507\n",
      "Iteration 234, Batch: 4, Loss: 0.0632610023021698\n",
      "Iteration 234, Batch: 5, Loss: 0.056133050471544266\n",
      "Iteration 234, Batch: 6, Loss: 0.028177618980407715\n",
      "Iteration 234, Batch: 7, Loss: 0.06510359048843384\n",
      "Iteration 234, Batch: 8, Loss: 0.042968157678842545\n",
      "Iteration 234, Batch: 9, Loss: 0.043236713856458664\n",
      "Iteration 234, Batch: 10, Loss: 0.06218875199556351\n",
      "Iteration 234, Batch: 11, Loss: 0.08849376440048218\n",
      "Iteration 234, Batch: 12, Loss: 0.0801800787448883\n",
      "Iteration 234, Batch: 13, Loss: 0.06673252582550049\n",
      "Iteration 234, Batch: 14, Loss: 0.07225007563829422\n",
      "Iteration 234, Batch: 15, Loss: 0.07318268716335297\n",
      "Iteration 234, Batch: 16, Loss: 0.07087580859661102\n",
      "Iteration 234, Batch: 17, Loss: 0.02678120881319046\n",
      "Iteration 234, Batch: 18, Loss: 0.06442908942699432\n",
      "Iteration 234, Batch: 19, Loss: 0.06787936389446259\n",
      "Iteration 234, Batch: 20, Loss: 0.06268271058797836\n",
      "Iteration 234, Batch: 21, Loss: 0.06736930459737778\n",
      "Iteration 234, Batch: 22, Loss: 0.07495150715112686\n",
      "Iteration 234, Batch: 23, Loss: 0.05384797975420952\n",
      "Iteration 234, Batch: 24, Loss: 0.03719787299633026\n",
      "Iteration 234, Batch: 25, Loss: 0.02237372286617756\n",
      "Iteration 234, Batch: 26, Loss: 0.05272771790623665\n",
      "Iteration 234, Batch: 27, Loss: 0.07383707165718079\n",
      "Iteration 234, Batch: 28, Loss: 0.05639095976948738\n",
      "Iteration 234, Batch: 29, Loss: 0.029325461015105247\n",
      "Iteration 234, Batch: 30, Loss: 0.044821374118328094\n",
      "Iteration 234, Batch: 31, Loss: 0.0640300065279007\n",
      "Iteration 234, Batch: 32, Loss: 0.060827672481536865\n",
      "Iteration 234, Batch: 33, Loss: 0.040922291576862335\n",
      "Iteration 234, Batch: 34, Loss: 0.054581012576818466\n",
      "Iteration 234, Batch: 35, Loss: 0.06407996267080307\n",
      "Iteration 234, Batch: 36, Loss: 0.04853842034935951\n",
      "Iteration 234, Batch: 37, Loss: 0.0564592145383358\n",
      "Iteration 234, Batch: 38, Loss: 0.05760931968688965\n",
      "Iteration 234, Batch: 39, Loss: 0.04805548116564751\n",
      "Iteration 234, Batch: 40, Loss: 0.05211551487445831\n",
      "Iteration 234, Batch: 41, Loss: 0.06257731467485428\n",
      "Iteration 234, Batch: 42, Loss: 0.05537394434213638\n",
      "Iteration 234, Batch: 43, Loss: 0.0354892835021019\n",
      "Iteration 234, Batch: 44, Loss: 0.04786207899451256\n",
      "Iteration 234, Batch: 45, Loss: 0.0568079836666584\n",
      "Iteration 234, Batch: 46, Loss: 0.044277358800172806\n",
      "Iteration 234, Batch: 47, Loss: 0.04603472724556923\n",
      "Iteration 234, Batch: 48, Loss: 0.049268048256635666\n",
      "Iteration 234, Batch: 49, Loss: 0.03712206333875656\n",
      "Iteration 235, Batch: 0, Loss: 0.08677667379379272\n",
      "Iteration 235, Batch: 1, Loss: 0.07202142477035522\n",
      "Iteration 235, Batch: 2, Loss: 0.048065461218357086\n",
      "Iteration 235, Batch: 3, Loss: 0.04922464117407799\n",
      "Iteration 235, Batch: 4, Loss: 0.04041154310107231\n",
      "Iteration 235, Batch: 5, Loss: 0.05354612320661545\n",
      "Iteration 235, Batch: 6, Loss: 0.07928118854761124\n",
      "Iteration 235, Batch: 7, Loss: 0.06063112989068031\n",
      "Iteration 235, Batch: 8, Loss: 0.02552465908229351\n",
      "Iteration 235, Batch: 9, Loss: 0.052898604422807693\n",
      "Iteration 235, Batch: 10, Loss: 0.06018875911831856\n",
      "Iteration 235, Batch: 11, Loss: 0.06715704500675201\n",
      "Iteration 235, Batch: 12, Loss: 0.03806750848889351\n",
      "Iteration 235, Batch: 13, Loss: 0.06306616961956024\n",
      "Iteration 235, Batch: 14, Loss: 0.078218974173069\n",
      "Iteration 235, Batch: 15, Loss: 0.09808268398046494\n",
      "Iteration 235, Batch: 16, Loss: 0.06650982052087784\n",
      "Iteration 235, Batch: 17, Loss: 0.04791251942515373\n",
      "Iteration 235, Batch: 18, Loss: 0.05263916403055191\n",
      "Iteration 235, Batch: 19, Loss: 0.07731775939464569\n",
      "Iteration 235, Batch: 20, Loss: 0.09393153339624405\n",
      "Iteration 235, Batch: 21, Loss: 0.0604117289185524\n",
      "Iteration 235, Batch: 22, Loss: 0.058244913816452026\n",
      "Iteration 235, Batch: 23, Loss: 0.05209057778120041\n",
      "Iteration 235, Batch: 24, Loss: 0.04907282814383507\n",
      "Iteration 235, Batch: 25, Loss: 0.049451783299446106\n",
      "Iteration 235, Batch: 26, Loss: 0.051537517458200455\n",
      "Iteration 235, Batch: 27, Loss: 0.04483063146471977\n",
      "Iteration 235, Batch: 28, Loss: 0.08091262727975845\n",
      "Iteration 235, Batch: 29, Loss: 0.044594477862119675\n",
      "Iteration 235, Batch: 30, Loss: 0.0755319744348526\n",
      "Iteration 235, Batch: 31, Loss: 0.04149855673313141\n",
      "Iteration 235, Batch: 32, Loss: 0.062238190323114395\n",
      "Iteration 235, Batch: 33, Loss: 0.03533574193716049\n",
      "Iteration 235, Batch: 34, Loss: 0.04935409128665924\n",
      "Iteration 235, Batch: 35, Loss: 0.029070556163787842\n",
      "Iteration 235, Batch: 36, Loss: 0.049213774502277374\n",
      "Iteration 235, Batch: 37, Loss: 0.05372156202793121\n",
      "Iteration 235, Batch: 38, Loss: 0.04366818442940712\n",
      "Iteration 235, Batch: 39, Loss: 0.055910270661115646\n",
      "Iteration 235, Batch: 40, Loss: 0.054702188819646835\n",
      "Iteration 235, Batch: 41, Loss: 0.03534993156790733\n",
      "Iteration 235, Batch: 42, Loss: 0.0702190175652504\n",
      "Iteration 235, Batch: 43, Loss: 0.04359442740678787\n",
      "Iteration 235, Batch: 44, Loss: 0.04073484241962433\n",
      "Iteration 235, Batch: 45, Loss: 0.05251224711537361\n",
      "Iteration 235, Batch: 46, Loss: 0.0746551975607872\n",
      "Iteration 235, Batch: 47, Loss: 0.06093417853116989\n",
      "Iteration 235, Batch: 48, Loss: 0.05937463790178299\n",
      "Iteration 235, Batch: 49, Loss: 0.05359338968992233\n",
      "Iteration 236, Batch: 0, Loss: 0.05254944786429405\n",
      "Iteration 236, Batch: 1, Loss: 0.03403490409255028\n",
      "Iteration 236, Batch: 2, Loss: 0.04963027685880661\n",
      "Iteration 236, Batch: 3, Loss: 0.037430163472890854\n",
      "Iteration 236, Batch: 4, Loss: 0.043380316346883774\n",
      "Iteration 236, Batch: 5, Loss: 0.06670941412448883\n",
      "Iteration 236, Batch: 6, Loss: 0.06683079153299332\n",
      "Iteration 236, Batch: 7, Loss: 0.06190099939703941\n",
      "Iteration 236, Batch: 8, Loss: 0.04261062666773796\n",
      "Iteration 236, Batch: 9, Loss: 0.03264346346259117\n",
      "Iteration 236, Batch: 10, Loss: 0.06306512653827667\n",
      "Iteration 236, Batch: 11, Loss: 0.05640000104904175\n",
      "Iteration 236, Batch: 12, Loss: 0.06640724837779999\n",
      "Iteration 236, Batch: 13, Loss: 0.046804942190647125\n",
      "Iteration 236, Batch: 14, Loss: 0.07327167689800262\n",
      "Iteration 236, Batch: 15, Loss: 0.07229026407003403\n",
      "Iteration 236, Batch: 16, Loss: 0.051782406866550446\n",
      "Iteration 236, Batch: 17, Loss: 0.07083536684513092\n",
      "Iteration 236, Batch: 18, Loss: 0.05518442019820213\n",
      "Iteration 236, Batch: 19, Loss: 0.05621091648936272\n",
      "Iteration 236, Batch: 20, Loss: 0.05451073497533798\n",
      "Iteration 236, Batch: 21, Loss: 0.05292867496609688\n",
      "Iteration 236, Batch: 22, Loss: 0.06559977680444717\n",
      "Iteration 236, Batch: 23, Loss: 0.06415955722332001\n",
      "Iteration 236, Batch: 24, Loss: 0.05435584485530853\n",
      "Iteration 236, Batch: 25, Loss: 0.05668092519044876\n",
      "Iteration 236, Batch: 26, Loss: 0.03561502695083618\n",
      "Iteration 236, Batch: 27, Loss: 0.047163642942905426\n",
      "Iteration 236, Batch: 28, Loss: 0.019258376210927963\n",
      "Iteration 236, Batch: 29, Loss: 0.05467863753437996\n",
      "Iteration 236, Batch: 30, Loss: 0.056691110134124756\n",
      "Iteration 236, Batch: 31, Loss: 0.0316290520131588\n",
      "Iteration 236, Batch: 32, Loss: 0.06036888808012009\n",
      "Iteration 236, Batch: 33, Loss: 0.03762343153357506\n",
      "Iteration 236, Batch: 34, Loss: 0.05733386054635048\n",
      "Iteration 236, Batch: 35, Loss: 0.05986538156867027\n",
      "Iteration 236, Batch: 36, Loss: 0.04125950112938881\n",
      "Iteration 236, Batch: 37, Loss: 0.06858302652835846\n",
      "Iteration 236, Batch: 38, Loss: 0.02731163240969181\n",
      "Iteration 236, Batch: 39, Loss: 0.0801854208111763\n",
      "Iteration 236, Batch: 40, Loss: 0.04902659356594086\n",
      "Iteration 236, Batch: 41, Loss: 0.059718966484069824\n",
      "Iteration 236, Batch: 42, Loss: 0.09383317083120346\n",
      "Iteration 236, Batch: 43, Loss: 0.06868264824151993\n",
      "Iteration 236, Batch: 44, Loss: 0.07550501823425293\n",
      "Iteration 236, Batch: 45, Loss: 0.07111383229494095\n",
      "Iteration 236, Batch: 46, Loss: 0.04909064248204231\n",
      "Iteration 236, Batch: 47, Loss: 0.07568734139204025\n",
      "Iteration 236, Batch: 48, Loss: 0.06869589537382126\n",
      "Iteration 236, Batch: 49, Loss: 0.0857885554432869\n",
      "Iteration 237, Batch: 0, Loss: 0.050022125244140625\n",
      "Iteration 237, Batch: 1, Loss: 0.07261405140161514\n",
      "Iteration 237, Batch: 2, Loss: 0.05357837677001953\n",
      "Iteration 237, Batch: 3, Loss: 0.05907922610640526\n",
      "Iteration 237, Batch: 4, Loss: 0.03709680959582329\n",
      "Iteration 237, Batch: 5, Loss: 0.06491094082593918\n",
      "Iteration 237, Batch: 6, Loss: 0.06628170609474182\n",
      "Iteration 237, Batch: 7, Loss: 0.04828455299139023\n",
      "Iteration 237, Batch: 8, Loss: 0.03533435985445976\n",
      "Iteration 237, Batch: 9, Loss: 0.023475050926208496\n",
      "Iteration 237, Batch: 10, Loss: 0.04002183675765991\n",
      "Iteration 237, Batch: 11, Loss: 0.044002048671245575\n",
      "Iteration 237, Batch: 12, Loss: 0.05755917355418205\n",
      "Iteration 237, Batch: 13, Loss: 0.04776191711425781\n",
      "Iteration 237, Batch: 14, Loss: 0.050159577280282974\n",
      "Iteration 237, Batch: 15, Loss: 0.0445881150662899\n",
      "Iteration 237, Batch: 16, Loss: 0.04725316911935806\n",
      "Iteration 237, Batch: 17, Loss: 0.03092852234840393\n",
      "Iteration 237, Batch: 18, Loss: 0.04317205399274826\n",
      "Iteration 237, Batch: 19, Loss: 0.04725635424256325\n",
      "Iteration 237, Batch: 20, Loss: 0.08508104830980301\n",
      "Iteration 237, Batch: 21, Loss: 0.04525682330131531\n",
      "Iteration 237, Batch: 22, Loss: 0.04645835608243942\n",
      "Iteration 237, Batch: 23, Loss: 0.045313555747270584\n",
      "Iteration 237, Batch: 24, Loss: 0.052868347615003586\n",
      "Iteration 237, Batch: 25, Loss: 0.05510380491614342\n",
      "Iteration 237, Batch: 26, Loss: 0.038952555507421494\n",
      "Iteration 237, Batch: 27, Loss: 0.044477567076683044\n",
      "Iteration 237, Batch: 28, Loss: 0.08978725224733353\n",
      "Iteration 237, Batch: 29, Loss: 0.047777023166418076\n",
      "Iteration 237, Batch: 30, Loss: 0.031090792268514633\n",
      "Iteration 237, Batch: 31, Loss: 0.06561292707920074\n",
      "Iteration 237, Batch: 32, Loss: 0.057275284081697464\n",
      "Iteration 237, Batch: 33, Loss: 0.08250493556261063\n",
      "Iteration 237, Batch: 34, Loss: 0.05344638600945473\n",
      "Iteration 237, Batch: 35, Loss: 0.05562971904873848\n",
      "Iteration 237, Batch: 36, Loss: 0.05084177479147911\n",
      "Iteration 237, Batch: 37, Loss: 0.04737096652388573\n",
      "Iteration 237, Batch: 38, Loss: 0.05049673840403557\n",
      "Iteration 237, Batch: 39, Loss: 0.03807691112160683\n",
      "Iteration 237, Batch: 40, Loss: 0.028347760438919067\n",
      "Iteration 237, Batch: 41, Loss: 0.05074261873960495\n",
      "Iteration 237, Batch: 42, Loss: 0.03364915773272514\n",
      "Iteration 237, Batch: 43, Loss: 0.06395900249481201\n",
      "Iteration 237, Batch: 44, Loss: 0.05675113573670387\n",
      "Iteration 237, Batch: 45, Loss: 0.055992286652326584\n",
      "Iteration 237, Batch: 46, Loss: 0.038528189063072205\n",
      "Iteration 237, Batch: 47, Loss: 0.053287748247385025\n",
      "Iteration 237, Batch: 48, Loss: 0.05165417492389679\n",
      "Iteration 237, Batch: 49, Loss: 0.07677635550498962\n",
      "Iteration 238, Batch: 0, Loss: 0.05956925079226494\n",
      "Iteration 238, Batch: 1, Loss: 0.05907926335930824\n",
      "Iteration 238, Batch: 2, Loss: 0.05536237731575966\n",
      "Iteration 238, Batch: 3, Loss: 0.03142207860946655\n",
      "Iteration 238, Batch: 4, Loss: 0.05592312663793564\n",
      "Iteration 238, Batch: 5, Loss: 0.03306487575173378\n",
      "Iteration 238, Batch: 6, Loss: 0.05724463239312172\n",
      "Iteration 238, Batch: 7, Loss: 0.05355651676654816\n",
      "Iteration 238, Batch: 8, Loss: 0.043659280985593796\n",
      "Iteration 238, Batch: 9, Loss: 0.042742036283016205\n",
      "Iteration 238, Batch: 10, Loss: 0.04679747298359871\n",
      "Iteration 238, Batch: 11, Loss: 0.05684411898255348\n",
      "Iteration 238, Batch: 12, Loss: 0.044819965958595276\n",
      "Iteration 238, Batch: 13, Loss: 0.022904234007000923\n",
      "Iteration 238, Batch: 14, Loss: 0.04757893458008766\n",
      "Iteration 238, Batch: 15, Loss: 0.06087922304868698\n",
      "Iteration 238, Batch: 16, Loss: 0.069952093064785\n",
      "Iteration 238, Batch: 17, Loss: 0.056991443037986755\n",
      "Iteration 238, Batch: 18, Loss: 0.05852348729968071\n",
      "Iteration 238, Batch: 19, Loss: 0.058456677943468094\n",
      "Iteration 238, Batch: 20, Loss: 0.06290813535451889\n",
      "Iteration 238, Batch: 21, Loss: 0.04037753865122795\n",
      "Iteration 238, Batch: 22, Loss: 0.035168424248695374\n",
      "Iteration 238, Batch: 23, Loss: 0.04391653090715408\n",
      "Iteration 238, Batch: 24, Loss: 0.09117565304040909\n",
      "Iteration 238, Batch: 25, Loss: 0.03419185429811478\n",
      "Iteration 238, Batch: 26, Loss: 0.052347153425216675\n",
      "Iteration 238, Batch: 27, Loss: 0.04144414886832237\n",
      "Iteration 238, Batch: 28, Loss: 0.04435489699244499\n",
      "Iteration 238, Batch: 29, Loss: 0.07086312025785446\n",
      "Iteration 238, Batch: 30, Loss: 0.04571303352713585\n",
      "Iteration 238, Batch: 31, Loss: 0.045957159250974655\n",
      "Iteration 238, Batch: 32, Loss: 0.04300736263394356\n",
      "Iteration 238, Batch: 33, Loss: 0.041641559451818466\n",
      "Iteration 238, Batch: 34, Loss: 0.0627691000699997\n",
      "Iteration 238, Batch: 35, Loss: 0.061392903327941895\n",
      "Iteration 238, Batch: 36, Loss: 0.06259424239397049\n",
      "Iteration 238, Batch: 37, Loss: 0.053997401148080826\n",
      "Iteration 238, Batch: 38, Loss: 0.03262942284345627\n",
      "Iteration 238, Batch: 39, Loss: 0.053384359925985336\n",
      "Iteration 238, Batch: 40, Loss: 0.028107576072216034\n",
      "Iteration 238, Batch: 41, Loss: 0.053640373051166534\n",
      "Iteration 238, Batch: 42, Loss: 0.056547049432992935\n",
      "Iteration 238, Batch: 43, Loss: 0.03974469006061554\n",
      "Iteration 238, Batch: 44, Loss: 0.06491277366876602\n",
      "Iteration 238, Batch: 45, Loss: 0.04156634584069252\n",
      "Iteration 238, Batch: 46, Loss: 0.04085399582982063\n",
      "Iteration 238, Batch: 47, Loss: 0.0515664741396904\n",
      "Iteration 238, Batch: 48, Loss: 0.058965470641851425\n",
      "Iteration 238, Batch: 49, Loss: 0.04364530369639397\n",
      "Iteration 239, Batch: 0, Loss: 0.08589491993188858\n",
      "Iteration 239, Batch: 1, Loss: 0.05955451354384422\n",
      "Iteration 239, Batch: 2, Loss: 0.05780604109168053\n",
      "Iteration 239, Batch: 3, Loss: 0.062265779823064804\n",
      "Iteration 239, Batch: 4, Loss: 0.06430812925100327\n",
      "Iteration 239, Batch: 5, Loss: 0.0883965939283371\n",
      "Iteration 239, Batch: 6, Loss: 0.08811870217323303\n",
      "Iteration 239, Batch: 7, Loss: 0.06280968338251114\n",
      "Iteration 239, Batch: 8, Loss: 0.05695117637515068\n",
      "Iteration 239, Batch: 9, Loss: 0.07456084340810776\n",
      "Iteration 239, Batch: 10, Loss: 0.026815418154001236\n",
      "Iteration 239, Batch: 11, Loss: 0.035178083926439285\n",
      "Iteration 239, Batch: 12, Loss: 0.028744829818606377\n",
      "Iteration 239, Batch: 13, Loss: 0.06263583898544312\n",
      "Iteration 239, Batch: 14, Loss: 0.025797883048653603\n",
      "Iteration 239, Batch: 15, Loss: 0.05113781988620758\n",
      "Iteration 239, Batch: 16, Loss: 0.07715876400470734\n",
      "Iteration 239, Batch: 17, Loss: 0.05476989597082138\n",
      "Iteration 239, Batch: 18, Loss: 0.07040773332118988\n",
      "Iteration 239, Batch: 19, Loss: 0.051644034683704376\n",
      "Iteration 239, Batch: 20, Loss: 0.06355953216552734\n",
      "Iteration 239, Batch: 21, Loss: 0.0510762482881546\n",
      "Iteration 239, Batch: 22, Loss: 0.050383374094963074\n",
      "Iteration 239, Batch: 23, Loss: 0.03955153003334999\n",
      "Iteration 239, Batch: 24, Loss: 0.05178026109933853\n",
      "Iteration 239, Batch: 25, Loss: 0.07425209134817123\n",
      "Iteration 239, Batch: 26, Loss: 0.03625919297337532\n",
      "Iteration 239, Batch: 27, Loss: 0.05611249431967735\n",
      "Iteration 239, Batch: 28, Loss: 0.06184903159737587\n",
      "Iteration 239, Batch: 29, Loss: 0.05051916837692261\n",
      "Iteration 239, Batch: 30, Loss: 0.024557003751397133\n",
      "Iteration 239, Batch: 31, Loss: 0.06216208264231682\n",
      "Iteration 239, Batch: 32, Loss: 0.047907035797834396\n",
      "Iteration 239, Batch: 33, Loss: 0.04457806423306465\n",
      "Iteration 239, Batch: 34, Loss: 0.05813609063625336\n",
      "Iteration 239, Batch: 35, Loss: 0.04787454381585121\n",
      "Iteration 239, Batch: 36, Loss: 0.058340560644865036\n",
      "Iteration 239, Batch: 37, Loss: 0.0329756997525692\n",
      "Iteration 239, Batch: 38, Loss: 0.029528416693210602\n",
      "Iteration 239, Batch: 39, Loss: 0.05733135715126991\n",
      "Iteration 239, Batch: 40, Loss: 0.04877059534192085\n",
      "Iteration 239, Batch: 41, Loss: 0.06478028744459152\n",
      "Iteration 239, Batch: 42, Loss: 0.05140889063477516\n",
      "Iteration 239, Batch: 43, Loss: 0.0695827454328537\n",
      "Iteration 239, Batch: 44, Loss: 0.056261319667100906\n",
      "Iteration 239, Batch: 45, Loss: 0.04602228105068207\n",
      "Iteration 239, Batch: 46, Loss: 0.051926594227552414\n",
      "Iteration 239, Batch: 47, Loss: 0.05027075111865997\n",
      "Iteration 239, Batch: 48, Loss: 0.053374651819467545\n",
      "Iteration 239, Batch: 49, Loss: 0.06918580085039139\n",
      "Iteration 240, Batch: 0, Loss: 0.057888422161340714\n",
      "Iteration 240, Batch: 1, Loss: 0.056952279061079025\n",
      "Iteration 240, Batch: 2, Loss: 0.042278025299310684\n",
      "Iteration 240, Batch: 3, Loss: 0.03516717255115509\n",
      "Iteration 240, Batch: 4, Loss: 0.03577703237533569\n",
      "Iteration 240, Batch: 5, Loss: 0.040627021342515945\n",
      "Iteration 240, Batch: 6, Loss: 0.04312402009963989\n",
      "Iteration 240, Batch: 7, Loss: 0.07152926176786423\n",
      "Iteration 240, Batch: 8, Loss: 0.05344390124082565\n",
      "Iteration 240, Batch: 9, Loss: 0.02676921710371971\n",
      "Iteration 240, Batch: 10, Loss: 0.05240429937839508\n",
      "Iteration 240, Batch: 11, Loss: 0.043741483241319656\n",
      "Iteration 240, Batch: 12, Loss: 0.05753091350197792\n",
      "Iteration 240, Batch: 13, Loss: 0.044961899518966675\n",
      "Iteration 240, Batch: 14, Loss: 0.05765453353524208\n",
      "Iteration 240, Batch: 15, Loss: 0.05580177158117294\n",
      "Iteration 240, Batch: 16, Loss: 0.055818360298871994\n",
      "Iteration 240, Batch: 17, Loss: 0.03561989217996597\n",
      "Iteration 240, Batch: 18, Loss: 0.0398474857211113\n",
      "Iteration 240, Batch: 19, Loss: 0.03768133372068405\n",
      "Iteration 240, Batch: 20, Loss: 0.0706457793712616\n",
      "Iteration 240, Batch: 21, Loss: 0.05312449485063553\n",
      "Iteration 240, Batch: 22, Loss: 0.09106133878231049\n",
      "Iteration 240, Batch: 23, Loss: 0.06609148532152176\n",
      "Iteration 240, Batch: 24, Loss: 0.05103059113025665\n",
      "Iteration 240, Batch: 25, Loss: 0.05834954231977463\n",
      "Iteration 240, Batch: 26, Loss: 0.05229325219988823\n",
      "Iteration 240, Batch: 27, Loss: 0.05146417394280434\n",
      "Iteration 240, Batch: 28, Loss: 0.05710840970277786\n",
      "Iteration 240, Batch: 29, Loss: 0.057332124561071396\n",
      "Iteration 240, Batch: 30, Loss: 0.05814604461193085\n",
      "Iteration 240, Batch: 31, Loss: 0.05058565363287926\n",
      "Iteration 240, Batch: 32, Loss: 0.05188304930925369\n",
      "Iteration 240, Batch: 33, Loss: 0.05213947966694832\n",
      "Iteration 240, Batch: 34, Loss: 0.04900951310992241\n",
      "Iteration 240, Batch: 35, Loss: 0.05071723833680153\n",
      "Iteration 240, Batch: 36, Loss: 0.0562034510076046\n",
      "Iteration 240, Batch: 37, Loss: 0.05033211410045624\n",
      "Iteration 240, Batch: 38, Loss: 0.056199055165052414\n",
      "Iteration 240, Batch: 39, Loss: 0.04649357870221138\n",
      "Iteration 240, Batch: 40, Loss: 0.048791561275720596\n",
      "Iteration 240, Batch: 41, Loss: 0.06325895339250565\n",
      "Iteration 240, Batch: 42, Loss: 0.08319427073001862\n",
      "Iteration 240, Batch: 43, Loss: 0.0465521365404129\n",
      "Iteration 240, Batch: 44, Loss: 0.06315594166517258\n",
      "Iteration 240, Batch: 45, Loss: 0.0838983878493309\n",
      "Iteration 240, Batch: 46, Loss: 0.04844267666339874\n",
      "Iteration 240, Batch: 47, Loss: 0.07364912331104279\n",
      "Iteration 240, Batch: 48, Loss: 0.05309413745999336\n",
      "Iteration 240, Batch: 49, Loss: 0.06383447349071503\n",
      "Iteration 241, Batch: 0, Loss: 0.05240326747298241\n",
      "Iteration 241, Batch: 1, Loss: 0.07503703981637955\n",
      "Iteration 241, Batch: 2, Loss: 0.08454380184412003\n",
      "Iteration 241, Batch: 3, Loss: 0.09143466502428055\n",
      "Iteration 241, Batch: 4, Loss: 0.07330428063869476\n",
      "Iteration 241, Batch: 5, Loss: 0.0740332305431366\n",
      "Iteration 241, Batch: 6, Loss: 0.12247011065483093\n",
      "Iteration 241, Batch: 7, Loss: 0.13463108241558075\n",
      "Iteration 241, Batch: 8, Loss: 0.11277543753385544\n",
      "Iteration 241, Batch: 9, Loss: 0.10386509448289871\n",
      "Iteration 241, Batch: 10, Loss: 0.09353066980838776\n",
      "Iteration 241, Batch: 11, Loss: 0.12322885543107986\n",
      "Iteration 241, Batch: 12, Loss: 0.06302524358034134\n",
      "Iteration 241, Batch: 13, Loss: 0.10373228788375854\n",
      "Iteration 241, Batch: 14, Loss: 0.08244689553976059\n",
      "Iteration 241, Batch: 15, Loss: 0.08127646148204803\n",
      "Iteration 241, Batch: 16, Loss: 0.11336304992437363\n",
      "Iteration 241, Batch: 17, Loss: 0.0762583464384079\n",
      "Iteration 241, Batch: 18, Loss: 0.0892956331372261\n",
      "Iteration 241, Batch: 19, Loss: 0.0570528507232666\n",
      "Iteration 241, Batch: 20, Loss: 0.07357799261808395\n",
      "Iteration 241, Batch: 21, Loss: 0.08893582224845886\n",
      "Iteration 241, Batch: 22, Loss: 0.07507917284965515\n",
      "Iteration 241, Batch: 23, Loss: 0.06884776055812836\n",
      "Iteration 241, Batch: 24, Loss: 0.06792476773262024\n",
      "Iteration 241, Batch: 25, Loss: 0.06309989839792252\n",
      "Iteration 241, Batch: 26, Loss: 0.07849679887294769\n",
      "Iteration 241, Batch: 27, Loss: 0.06115046888589859\n",
      "Iteration 241, Batch: 28, Loss: 0.08916524797677994\n",
      "Iteration 241, Batch: 29, Loss: 0.04945782572031021\n",
      "Iteration 241, Batch: 30, Loss: 0.08362109959125519\n",
      "Iteration 241, Batch: 31, Loss: 0.05313291400671005\n",
      "Iteration 241, Batch: 32, Loss: 0.07146661728620529\n",
      "Iteration 241, Batch: 33, Loss: 0.07733363658189774\n",
      "Iteration 241, Batch: 34, Loss: 0.06301649659872055\n",
      "Iteration 241, Batch: 35, Loss: 0.04034517705440521\n",
      "Iteration 241, Batch: 36, Loss: 0.06840050220489502\n",
      "Iteration 241, Batch: 37, Loss: 0.07217378169298172\n",
      "Iteration 241, Batch: 38, Loss: 0.08678825199604034\n",
      "Iteration 241, Batch: 39, Loss: 0.045871634036302567\n",
      "Iteration 241, Batch: 40, Loss: 0.06753629446029663\n",
      "Iteration 241, Batch: 41, Loss: 0.040427565574645996\n",
      "Iteration 241, Batch: 42, Loss: 0.060046613216400146\n",
      "Iteration 241, Batch: 43, Loss: 0.0738384798169136\n",
      "Iteration 241, Batch: 44, Loss: 0.0675850659608841\n",
      "Iteration 241, Batch: 45, Loss: 0.06406741589307785\n",
      "Iteration 241, Batch: 46, Loss: 0.044602520763874054\n",
      "Iteration 241, Batch: 47, Loss: 0.08269378542900085\n",
      "Iteration 241, Batch: 48, Loss: 0.06260573118925095\n",
      "Iteration 241, Batch: 49, Loss: 0.06672582775354385\n",
      "Iteration 242, Batch: 0, Loss: 0.02861959859728813\n",
      "Iteration 242, Batch: 1, Loss: 0.05633299797773361\n",
      "Iteration 242, Batch: 2, Loss: 0.049514349550008774\n",
      "Iteration 242, Batch: 3, Loss: 0.04388372227549553\n",
      "Iteration 242, Batch: 4, Loss: 0.054019127041101456\n",
      "Iteration 242, Batch: 5, Loss: 0.055868107825517654\n",
      "Iteration 242, Batch: 6, Loss: 0.048111122101545334\n",
      "Iteration 242, Batch: 7, Loss: 0.04691275581717491\n",
      "Iteration 242, Batch: 8, Loss: 0.02970237471163273\n",
      "Iteration 242, Batch: 9, Loss: 0.048326969146728516\n",
      "Iteration 242, Batch: 10, Loss: 0.06629493087530136\n",
      "Iteration 242, Batch: 11, Loss: 0.036931514739990234\n",
      "Iteration 242, Batch: 12, Loss: 0.044683828949928284\n",
      "Iteration 242, Batch: 13, Loss: 0.07203314453363419\n",
      "Iteration 242, Batch: 14, Loss: 0.058447062969207764\n",
      "Iteration 242, Batch: 15, Loss: 0.05448467284440994\n",
      "Iteration 242, Batch: 16, Loss: 0.046259064227342606\n",
      "Iteration 242, Batch: 17, Loss: 0.04462387412786484\n",
      "Iteration 242, Batch: 18, Loss: 0.08341047167778015\n",
      "Iteration 242, Batch: 19, Loss: 0.059460658580064774\n",
      "Iteration 242, Batch: 20, Loss: 0.026810219511389732\n",
      "Iteration 242, Batch: 21, Loss: 0.06319987773895264\n",
      "Iteration 242, Batch: 22, Loss: 0.05351424217224121\n",
      "Iteration 242, Batch: 23, Loss: 0.07518254220485687\n",
      "Iteration 242, Batch: 24, Loss: 0.08309882134199142\n",
      "Iteration 242, Batch: 25, Loss: 0.05224059149622917\n",
      "Iteration 242, Batch: 26, Loss: 0.04786812514066696\n",
      "Iteration 242, Batch: 27, Loss: 0.06271688640117645\n",
      "Iteration 242, Batch: 28, Loss: 0.04784134775400162\n",
      "Iteration 242, Batch: 29, Loss: 0.05460147559642792\n",
      "Iteration 242, Batch: 30, Loss: 0.02889743261039257\n",
      "Iteration 242, Batch: 31, Loss: 0.05259525030851364\n",
      "Iteration 242, Batch: 32, Loss: 0.07923834770917892\n",
      "Iteration 242, Batch: 33, Loss: 0.039127908647060394\n",
      "Iteration 242, Batch: 34, Loss: 0.026416845619678497\n",
      "Iteration 242, Batch: 35, Loss: 0.03206178545951843\n",
      "Iteration 242, Batch: 36, Loss: 0.08082598447799683\n",
      "Iteration 242, Batch: 37, Loss: 0.05824737995862961\n",
      "Iteration 242, Batch: 38, Loss: 0.06015774980187416\n",
      "Iteration 242, Batch: 39, Loss: 0.07445068657398224\n",
      "Iteration 242, Batch: 40, Loss: 0.06354925781488419\n",
      "Iteration 242, Batch: 41, Loss: 0.05964016914367676\n",
      "Iteration 242, Batch: 42, Loss: 0.050949353724718094\n",
      "Iteration 242, Batch: 43, Loss: 0.05629698187112808\n",
      "Iteration 242, Batch: 44, Loss: 0.093675896525383\n",
      "Iteration 242, Batch: 45, Loss: 0.07208871841430664\n",
      "Iteration 242, Batch: 46, Loss: 0.07431378215551376\n",
      "Iteration 242, Batch: 47, Loss: 0.08274337649345398\n",
      "Iteration 242, Batch: 48, Loss: 0.04590995982289314\n",
      "Iteration 242, Batch: 49, Loss: 0.03489644080400467\n",
      "Iteration 243, Batch: 0, Loss: 0.05042194575071335\n",
      "Iteration 243, Batch: 1, Loss: 0.05538756772875786\n",
      "Iteration 243, Batch: 2, Loss: 0.026158340275287628\n",
      "Iteration 243, Batch: 3, Loss: 0.038002751767635345\n",
      "Iteration 243, Batch: 4, Loss: 0.056478388607501984\n",
      "Iteration 243, Batch: 5, Loss: 0.027591647580266\n",
      "Iteration 243, Batch: 6, Loss: 0.05053481087088585\n",
      "Iteration 243, Batch: 7, Loss: 0.05373537912964821\n",
      "Iteration 243, Batch: 8, Loss: 0.059140369296073914\n",
      "Iteration 243, Batch: 9, Loss: 0.05397595465183258\n",
      "Iteration 243, Batch: 10, Loss: 0.04504476115107536\n",
      "Iteration 243, Batch: 11, Loss: 0.05706016719341278\n",
      "Iteration 243, Batch: 12, Loss: 0.061848580837249756\n",
      "Iteration 243, Batch: 13, Loss: 0.07439542561769485\n",
      "Iteration 243, Batch: 14, Loss: 0.08757039159536362\n",
      "Iteration 243, Batch: 15, Loss: 0.049790315330028534\n",
      "Iteration 243, Batch: 16, Loss: 0.042175453156232834\n",
      "Iteration 243, Batch: 17, Loss: 0.04470418393611908\n",
      "Iteration 243, Batch: 18, Loss: 0.058963701128959656\n",
      "Iteration 243, Batch: 19, Loss: 0.06921215355396271\n",
      "Iteration 243, Batch: 20, Loss: 0.03870092332363129\n",
      "Iteration 243, Batch: 21, Loss: 0.036519091576337814\n",
      "Iteration 243, Batch: 22, Loss: 0.046602655202150345\n",
      "Iteration 243, Batch: 23, Loss: 0.05234428867697716\n",
      "Iteration 243, Batch: 24, Loss: 0.05111917480826378\n",
      "Iteration 243, Batch: 25, Loss: 0.04187909886240959\n",
      "Iteration 243, Batch: 26, Loss: 0.07363396137952805\n",
      "Iteration 243, Batch: 27, Loss: 0.05068930611014366\n",
      "Iteration 243, Batch: 28, Loss: 0.03944268077611923\n",
      "Iteration 243, Batch: 29, Loss: 0.061846647411584854\n",
      "Iteration 243, Batch: 30, Loss: 0.05015233904123306\n",
      "Iteration 243, Batch: 31, Loss: 0.036584511399269104\n",
      "Iteration 243, Batch: 32, Loss: 0.04469095170497894\n",
      "Iteration 243, Batch: 33, Loss: 0.046718530356884\n",
      "Iteration 243, Batch: 34, Loss: 0.039006855338811874\n",
      "Iteration 243, Batch: 35, Loss: 0.040718622505664825\n",
      "Iteration 243, Batch: 36, Loss: 0.054807987064123154\n",
      "Iteration 243, Batch: 37, Loss: 0.04114318639039993\n",
      "Iteration 243, Batch: 38, Loss: 0.06569353491067886\n",
      "Iteration 243, Batch: 39, Loss: 0.038715701550245285\n",
      "Iteration 243, Batch: 40, Loss: 0.05073443427681923\n",
      "Iteration 243, Batch: 41, Loss: 0.05609824135899544\n",
      "Iteration 243, Batch: 42, Loss: 0.06569591164588928\n",
      "Iteration 243, Batch: 43, Loss: 0.04679417610168457\n",
      "Iteration 243, Batch: 44, Loss: 0.032466623932123184\n",
      "Iteration 243, Batch: 45, Loss: 0.041543856263160706\n",
      "Iteration 243, Batch: 46, Loss: 0.04079195484519005\n",
      "Iteration 243, Batch: 47, Loss: 0.03416134789586067\n",
      "Iteration 243, Batch: 48, Loss: 0.04613768309354782\n",
      "Iteration 243, Batch: 49, Loss: 0.04839968681335449\n",
      "Iteration 244, Batch: 0, Loss: 0.04678657278418541\n",
      "Iteration 244, Batch: 1, Loss: 0.07127885520458221\n",
      "Iteration 244, Batch: 2, Loss: 0.05782647058367729\n",
      "Iteration 244, Batch: 3, Loss: 0.06953194737434387\n",
      "Iteration 244, Batch: 4, Loss: 0.05535375326871872\n",
      "Iteration 244, Batch: 5, Loss: 0.06428693234920502\n",
      "Iteration 244, Batch: 6, Loss: 0.061391785740852356\n",
      "Iteration 244, Batch: 7, Loss: 0.058347269892692566\n",
      "Iteration 244, Batch: 8, Loss: 0.07212059944868088\n",
      "Iteration 244, Batch: 9, Loss: 0.06195952743291855\n",
      "Iteration 244, Batch: 10, Loss: 0.057921282947063446\n",
      "Iteration 244, Batch: 11, Loss: 0.04847702756524086\n",
      "Iteration 244, Batch: 12, Loss: 0.0709296390414238\n",
      "Iteration 244, Batch: 13, Loss: 0.04713773727416992\n",
      "Iteration 244, Batch: 14, Loss: 0.052050136029720306\n",
      "Iteration 244, Batch: 15, Loss: 0.04080314561724663\n",
      "Iteration 244, Batch: 16, Loss: 0.05071309953927994\n",
      "Iteration 244, Batch: 17, Loss: 0.06328943371772766\n",
      "Iteration 244, Batch: 18, Loss: 0.05741487070918083\n",
      "Iteration 244, Batch: 19, Loss: 0.06944019347429276\n",
      "Iteration 244, Batch: 20, Loss: 0.04784368723630905\n",
      "Iteration 244, Batch: 21, Loss: 0.04761301726102829\n",
      "Iteration 244, Batch: 22, Loss: 0.06085849180817604\n",
      "Iteration 244, Batch: 23, Loss: 0.06308656930923462\n",
      "Iteration 244, Batch: 24, Loss: 0.061908524483442307\n",
      "Iteration 244, Batch: 25, Loss: 0.0535360611975193\n",
      "Iteration 244, Batch: 26, Loss: 0.049777090549468994\n",
      "Iteration 244, Batch: 27, Loss: 0.04237237945199013\n",
      "Iteration 244, Batch: 28, Loss: 0.06999628245830536\n",
      "Iteration 244, Batch: 29, Loss: 0.05403275415301323\n",
      "Iteration 244, Batch: 30, Loss: 0.05897568538784981\n",
      "Iteration 244, Batch: 31, Loss: 0.05777047201991081\n",
      "Iteration 244, Batch: 32, Loss: 0.043824076652526855\n",
      "Iteration 244, Batch: 33, Loss: 0.05193844065070152\n",
      "Iteration 244, Batch: 34, Loss: 0.05073096975684166\n",
      "Iteration 244, Batch: 35, Loss: 0.06107223406434059\n",
      "Iteration 244, Batch: 36, Loss: 0.07336671650409698\n",
      "Iteration 244, Batch: 37, Loss: 0.05733715370297432\n",
      "Iteration 244, Batch: 38, Loss: 0.05687651410698891\n",
      "Iteration 244, Batch: 39, Loss: 0.04413497447967529\n",
      "Iteration 244, Batch: 40, Loss: 0.06521138548851013\n",
      "Iteration 244, Batch: 41, Loss: 0.06397150456905365\n",
      "Iteration 244, Batch: 42, Loss: 0.050568852573633194\n",
      "Iteration 244, Batch: 43, Loss: 0.07091251015663147\n",
      "Iteration 244, Batch: 44, Loss: 0.03498123586177826\n",
      "Iteration 244, Batch: 45, Loss: 0.07143192738294601\n",
      "Iteration 244, Batch: 46, Loss: 0.060271136462688446\n",
      "Iteration 244, Batch: 47, Loss: 0.05924222990870476\n",
      "Iteration 244, Batch: 48, Loss: 0.02987901121377945\n",
      "Iteration 244, Batch: 49, Loss: 0.048656921833753586\n",
      "Iteration 245, Batch: 0, Loss: 0.024005809798836708\n",
      "Iteration 245, Batch: 1, Loss: 0.04447178915143013\n",
      "Iteration 245, Batch: 2, Loss: 0.04763235151767731\n",
      "Iteration 245, Batch: 3, Loss: 0.05585891753435135\n",
      "Iteration 245, Batch: 4, Loss: 0.029374318197369576\n",
      "Iteration 245, Batch: 5, Loss: 0.04633804038167\n",
      "Iteration 245, Batch: 6, Loss: 0.07485605776309967\n",
      "Iteration 245, Batch: 7, Loss: 0.074346624314785\n",
      "Iteration 245, Batch: 8, Loss: 0.04409060254693031\n",
      "Iteration 245, Batch: 9, Loss: 0.041018132120370865\n",
      "Iteration 245, Batch: 10, Loss: 0.037702884525060654\n",
      "Iteration 245, Batch: 11, Loss: 0.05107984319329262\n",
      "Iteration 245, Batch: 12, Loss: 0.0649629458785057\n",
      "Iteration 245, Batch: 13, Loss: 0.08105963468551636\n",
      "Iteration 245, Batch: 14, Loss: 0.0508161336183548\n",
      "Iteration 245, Batch: 15, Loss: 0.05036821588873863\n",
      "Iteration 245, Batch: 16, Loss: 0.04713984206318855\n",
      "Iteration 245, Batch: 17, Loss: 0.03889868035912514\n",
      "Iteration 245, Batch: 18, Loss: 0.04028438776731491\n",
      "Iteration 245, Batch: 19, Loss: 0.0528738871216774\n",
      "Iteration 245, Batch: 20, Loss: 0.07551895081996918\n",
      "Iteration 245, Batch: 21, Loss: 0.05456240475177765\n",
      "Iteration 245, Batch: 22, Loss: 0.07196630537509918\n",
      "Iteration 245, Batch: 23, Loss: 0.07007043808698654\n",
      "Iteration 245, Batch: 24, Loss: 0.0715891420841217\n",
      "Iteration 245, Batch: 25, Loss: 0.053695909678936005\n",
      "Iteration 245, Batch: 26, Loss: 0.05500149726867676\n",
      "Iteration 245, Batch: 27, Loss: 0.04882429912686348\n",
      "Iteration 245, Batch: 28, Loss: 0.07087213546037674\n",
      "Iteration 245, Batch: 29, Loss: 0.07898560166358948\n",
      "Iteration 245, Batch: 30, Loss: 0.03854982927441597\n",
      "Iteration 245, Batch: 31, Loss: 0.07117456197738647\n",
      "Iteration 245, Batch: 32, Loss: 0.026737581938505173\n",
      "Iteration 245, Batch: 33, Loss: 0.08760131895542145\n",
      "Iteration 245, Batch: 34, Loss: 0.06354925036430359\n",
      "Iteration 245, Batch: 35, Loss: 0.06444399058818817\n",
      "Iteration 245, Batch: 36, Loss: 0.06446408480405807\n",
      "Iteration 245, Batch: 37, Loss: 0.04648183658719063\n",
      "Iteration 245, Batch: 38, Loss: 0.04896248131990433\n",
      "Iteration 245, Batch: 39, Loss: 0.06975454837083817\n",
      "Iteration 245, Batch: 40, Loss: 0.072769895195961\n",
      "Iteration 245, Batch: 41, Loss: 0.08215034008026123\n",
      "Iteration 245, Batch: 42, Loss: 0.07305635511875153\n",
      "Iteration 245, Batch: 43, Loss: 0.05638378858566284\n",
      "Iteration 245, Batch: 44, Loss: 0.06554847210645676\n",
      "Iteration 245, Batch: 45, Loss: 0.04700919985771179\n",
      "Iteration 245, Batch: 46, Loss: 0.05139249563217163\n",
      "Iteration 245, Batch: 47, Loss: 0.05874497443437576\n",
      "Iteration 245, Batch: 48, Loss: 0.06595353782176971\n",
      "Iteration 245, Batch: 49, Loss: 0.05255137383937836\n",
      "Iteration 246, Batch: 0, Loss: 0.03478832542896271\n",
      "Iteration 246, Batch: 1, Loss: 0.043216198682785034\n",
      "Iteration 246, Batch: 2, Loss: 0.03806278482079506\n",
      "Iteration 246, Batch: 3, Loss: 0.0670086145401001\n",
      "Iteration 246, Batch: 4, Loss: 0.049854230135679245\n",
      "Iteration 246, Batch: 5, Loss: 0.04769343137741089\n",
      "Iteration 246, Batch: 6, Loss: 0.06422533839941025\n",
      "Iteration 246, Batch: 7, Loss: 0.06458780914545059\n",
      "Iteration 246, Batch: 8, Loss: 0.05874715372920036\n",
      "Iteration 246, Batch: 9, Loss: 0.04189044237136841\n",
      "Iteration 246, Batch: 10, Loss: 0.04065608233213425\n",
      "Iteration 246, Batch: 11, Loss: 0.05561857670545578\n",
      "Iteration 246, Batch: 12, Loss: 0.060268912464380264\n",
      "Iteration 246, Batch: 13, Loss: 0.048585254698991776\n",
      "Iteration 246, Batch: 14, Loss: 0.05395166203379631\n",
      "Iteration 246, Batch: 15, Loss: 0.047357749193906784\n",
      "Iteration 246, Batch: 16, Loss: 0.04925021156668663\n",
      "Iteration 246, Batch: 17, Loss: 0.04842483997344971\n",
      "Iteration 246, Batch: 18, Loss: 0.05101121962070465\n",
      "Iteration 246, Batch: 19, Loss: 0.08877430111169815\n",
      "Iteration 246, Batch: 20, Loss: 0.06013505905866623\n",
      "Iteration 246, Batch: 21, Loss: 0.058016616851091385\n",
      "Iteration 246, Batch: 22, Loss: 0.05960717052221298\n",
      "Iteration 246, Batch: 23, Loss: 0.07612933963537216\n",
      "Iteration 246, Batch: 24, Loss: 0.03690608963370323\n",
      "Iteration 246, Batch: 25, Loss: 0.052262671291828156\n",
      "Iteration 246, Batch: 26, Loss: 0.06527648866176605\n",
      "Iteration 246, Batch: 27, Loss: 0.06308234483003616\n",
      "Iteration 246, Batch: 28, Loss: 0.054818324744701385\n",
      "Iteration 246, Batch: 29, Loss: 0.05084528774023056\n",
      "Iteration 246, Batch: 30, Loss: 0.04884226620197296\n",
      "Iteration 246, Batch: 31, Loss: 0.06044653803110123\n",
      "Iteration 246, Batch: 32, Loss: 0.0696866437792778\n",
      "Iteration 246, Batch: 33, Loss: 0.05221838876605034\n",
      "Iteration 246, Batch: 34, Loss: 0.044126175343990326\n",
      "Iteration 246, Batch: 35, Loss: 0.03487669304013252\n",
      "Iteration 246, Batch: 36, Loss: 0.08963459730148315\n",
      "Iteration 246, Batch: 37, Loss: 0.0648384541273117\n",
      "Iteration 246, Batch: 38, Loss: 0.06403964757919312\n",
      "Iteration 246, Batch: 39, Loss: 0.07363783568143845\n",
      "Iteration 246, Batch: 40, Loss: 0.07158539444208145\n",
      "Iteration 246, Batch: 41, Loss: 0.05611787363886833\n",
      "Iteration 246, Batch: 42, Loss: 0.04664362221956253\n",
      "Iteration 246, Batch: 43, Loss: 0.03979682922363281\n",
      "Iteration 246, Batch: 44, Loss: 0.07163400202989578\n",
      "Iteration 246, Batch: 45, Loss: 0.056022778153419495\n",
      "Iteration 246, Batch: 46, Loss: 0.05670502036809921\n",
      "Iteration 246, Batch: 47, Loss: 0.06288132816553116\n",
      "Iteration 246, Batch: 48, Loss: 0.036578468978405\n",
      "Iteration 246, Batch: 49, Loss: 0.05718771740794182\n",
      "Iteration 247, Batch: 0, Loss: 0.05519755184650421\n",
      "Iteration 247, Batch: 1, Loss: 0.06014086678624153\n",
      "Iteration 247, Batch: 2, Loss: 0.05102352052927017\n",
      "Iteration 247, Batch: 3, Loss: 0.05540616810321808\n",
      "Iteration 247, Batch: 4, Loss: 0.06027405709028244\n",
      "Iteration 247, Batch: 5, Loss: 0.06942892074584961\n",
      "Iteration 247, Batch: 6, Loss: 0.06859071552753448\n",
      "Iteration 247, Batch: 7, Loss: 0.04282055422663689\n",
      "Iteration 247, Batch: 8, Loss: 0.04632437229156494\n",
      "Iteration 247, Batch: 9, Loss: 0.03189316764473915\n",
      "Iteration 247, Batch: 10, Loss: 0.04956629499793053\n",
      "Iteration 247, Batch: 11, Loss: 0.059049367904663086\n",
      "Iteration 247, Batch: 12, Loss: 0.06859611719846725\n",
      "Iteration 247, Batch: 13, Loss: 0.05802402272820473\n",
      "Iteration 247, Batch: 14, Loss: 0.06025451421737671\n",
      "Iteration 247, Batch: 15, Loss: 0.07997404038906097\n",
      "Iteration 247, Batch: 16, Loss: 0.07214784622192383\n",
      "Iteration 247, Batch: 17, Loss: 0.0708170011639595\n",
      "Iteration 247, Batch: 18, Loss: 0.043293531984090805\n",
      "Iteration 247, Batch: 19, Loss: 0.04651951789855957\n",
      "Iteration 247, Batch: 20, Loss: 0.0734061598777771\n",
      "Iteration 247, Batch: 21, Loss: 0.07233338803052902\n",
      "Iteration 247, Batch: 22, Loss: 0.05345814675092697\n",
      "Iteration 247, Batch: 23, Loss: 0.06843103468418121\n",
      "Iteration 247, Batch: 24, Loss: 0.0350835956633091\n",
      "Iteration 247, Batch: 25, Loss: 0.06887859851121902\n",
      "Iteration 247, Batch: 26, Loss: 0.08022104203701019\n",
      "Iteration 247, Batch: 27, Loss: 0.054298609495162964\n",
      "Iteration 247, Batch: 28, Loss: 0.043432608246803284\n",
      "Iteration 247, Batch: 29, Loss: 0.039641402661800385\n",
      "Iteration 247, Batch: 30, Loss: 0.056168556213378906\n",
      "Iteration 247, Batch: 31, Loss: 0.05340852215886116\n",
      "Iteration 247, Batch: 32, Loss: 0.07739976048469543\n",
      "Iteration 247, Batch: 33, Loss: 0.07926283031702042\n",
      "Iteration 247, Batch: 34, Loss: 0.06800572574138641\n",
      "Iteration 247, Batch: 35, Loss: 0.027250410988926888\n",
      "Iteration 247, Batch: 36, Loss: 0.04521932452917099\n",
      "Iteration 247, Batch: 37, Loss: 0.052232950925827026\n",
      "Iteration 247, Batch: 38, Loss: 0.04297845438122749\n",
      "Iteration 247, Batch: 39, Loss: 0.04610365629196167\n",
      "Iteration 247, Batch: 40, Loss: 0.06509073823690414\n",
      "Iteration 247, Batch: 41, Loss: 0.04366055503487587\n",
      "Iteration 247, Batch: 42, Loss: 0.051749102771282196\n",
      "Iteration 247, Batch: 43, Loss: 0.06392233818769455\n",
      "Iteration 247, Batch: 44, Loss: 0.04386715963482857\n",
      "Iteration 247, Batch: 45, Loss: 0.03715471178293228\n",
      "Iteration 247, Batch: 46, Loss: 0.04643529653549194\n",
      "Iteration 247, Batch: 47, Loss: 0.06499909609556198\n",
      "Iteration 247, Batch: 48, Loss: 0.0628606304526329\n",
      "Iteration 247, Batch: 49, Loss: 0.05709819495677948\n",
      "Iteration 248, Batch: 0, Loss: 0.03885069489479065\n",
      "Iteration 248, Batch: 1, Loss: 0.056206703186035156\n",
      "Iteration 248, Batch: 2, Loss: 0.04185963422060013\n",
      "Iteration 248, Batch: 3, Loss: 0.046283166855573654\n",
      "Iteration 248, Batch: 4, Loss: 0.03904196247458458\n",
      "Iteration 248, Batch: 5, Loss: 0.06335132569074631\n",
      "Iteration 248, Batch: 6, Loss: 0.05758611857891083\n",
      "Iteration 248, Batch: 7, Loss: 0.02842593751847744\n",
      "Iteration 248, Batch: 8, Loss: 0.09684158116579056\n",
      "Iteration 248, Batch: 9, Loss: 0.03876715525984764\n",
      "Iteration 248, Batch: 10, Loss: 0.057483479380607605\n",
      "Iteration 248, Batch: 11, Loss: 0.07214558124542236\n",
      "Iteration 248, Batch: 12, Loss: 0.08130776882171631\n",
      "Iteration 248, Batch: 13, Loss: 0.06388948112726212\n",
      "Iteration 248, Batch: 14, Loss: 0.04244748875498772\n",
      "Iteration 248, Batch: 15, Loss: 0.04451310634613037\n",
      "Iteration 248, Batch: 16, Loss: 0.030949940904974937\n",
      "Iteration 248, Batch: 17, Loss: 0.057116974145174026\n",
      "Iteration 248, Batch: 18, Loss: 0.06782139837741852\n",
      "Iteration 248, Batch: 19, Loss: 0.04558774456381798\n",
      "Iteration 248, Batch: 20, Loss: 0.06674510985612869\n",
      "Iteration 248, Batch: 21, Loss: 0.04814235493540764\n",
      "Iteration 248, Batch: 22, Loss: 0.034673061221838\n",
      "Iteration 248, Batch: 23, Loss: 0.03264583274722099\n",
      "Iteration 248, Batch: 24, Loss: 0.057997968047857285\n",
      "Iteration 248, Batch: 25, Loss: 0.06409251689910889\n",
      "Iteration 248, Batch: 26, Loss: 0.060472797602415085\n",
      "Iteration 248, Batch: 27, Loss: 0.07314908504486084\n",
      "Iteration 248, Batch: 28, Loss: 0.04319475591182709\n",
      "Iteration 248, Batch: 29, Loss: 0.045738667249679565\n",
      "Iteration 248, Batch: 30, Loss: 0.0363258458673954\n",
      "Iteration 248, Batch: 31, Loss: 0.041291866451501846\n",
      "Iteration 248, Batch: 32, Loss: 0.06990528106689453\n",
      "Iteration 248, Batch: 33, Loss: 0.03428429737687111\n",
      "Iteration 248, Batch: 34, Loss: 0.06600631773471832\n",
      "Iteration 248, Batch: 35, Loss: 0.07672182470560074\n",
      "Iteration 248, Batch: 36, Loss: 0.05697004124522209\n",
      "Iteration 248, Batch: 37, Loss: 0.05488862842321396\n",
      "Iteration 248, Batch: 38, Loss: 0.05501151829957962\n",
      "Iteration 248, Batch: 39, Loss: 0.04455895721912384\n",
      "Iteration 248, Batch: 40, Loss: 0.05567936599254608\n",
      "Iteration 248, Batch: 41, Loss: 0.07208450138568878\n",
      "Iteration 248, Batch: 42, Loss: 0.04375864565372467\n",
      "Iteration 248, Batch: 43, Loss: 0.03820410743355751\n",
      "Iteration 248, Batch: 44, Loss: 0.06530824303627014\n",
      "Iteration 248, Batch: 45, Loss: 0.05254623293876648\n",
      "Iteration 248, Batch: 46, Loss: 0.038146745413541794\n",
      "Iteration 248, Batch: 47, Loss: 0.03692478686571121\n",
      "Iteration 248, Batch: 48, Loss: 0.011341971345245838\n",
      "Iteration 248, Batch: 49, Loss: 0.05324496701359749\n",
      "Iteration 249, Batch: 0, Loss: 0.06697041541337967\n",
      "Iteration 249, Batch: 1, Loss: 0.049359407275915146\n",
      "Iteration 249, Batch: 2, Loss: 0.05800864100456238\n",
      "Iteration 249, Batch: 3, Loss: 0.07412038743495941\n",
      "Iteration 249, Batch: 4, Loss: 0.03986118733882904\n",
      "Iteration 249, Batch: 5, Loss: 0.0724020004272461\n",
      "Iteration 249, Batch: 6, Loss: 0.03481634706258774\n",
      "Iteration 249, Batch: 7, Loss: 0.048864006996154785\n",
      "Iteration 249, Batch: 8, Loss: 0.06494857370853424\n",
      "Iteration 249, Batch: 9, Loss: 0.019965026527643204\n",
      "Iteration 249, Batch: 10, Loss: 0.03904741629958153\n",
      "Iteration 249, Batch: 11, Loss: 0.031239651143550873\n",
      "Iteration 249, Batch: 12, Loss: 0.045146405696868896\n",
      "Iteration 249, Batch: 13, Loss: 0.06893779337406158\n",
      "Iteration 249, Batch: 14, Loss: 0.053031075745821\n",
      "Iteration 249, Batch: 15, Loss: 0.05385803431272507\n",
      "Iteration 249, Batch: 16, Loss: 0.06682070344686508\n",
      "Iteration 249, Batch: 17, Loss: 0.027762625366449356\n",
      "Iteration 249, Batch: 18, Loss: 0.03522394970059395\n",
      "Iteration 249, Batch: 19, Loss: 0.022751668468117714\n",
      "Iteration 249, Batch: 20, Loss: 0.054001789540052414\n",
      "Iteration 249, Batch: 21, Loss: 0.04574619233608246\n",
      "Iteration 249, Batch: 22, Loss: 0.05430104210972786\n",
      "Iteration 249, Batch: 23, Loss: 0.05247166380286217\n",
      "Iteration 249, Batch: 24, Loss: 0.03911862149834633\n",
      "Iteration 249, Batch: 25, Loss: 0.03728485479950905\n",
      "Iteration 249, Batch: 26, Loss: 0.06204044073820114\n",
      "Iteration 249, Batch: 27, Loss: 0.056430716067552567\n",
      "Iteration 249, Batch: 28, Loss: 0.05900167301297188\n",
      "Iteration 249, Batch: 29, Loss: 0.03538140282034874\n",
      "Iteration 249, Batch: 30, Loss: 0.04181867465376854\n",
      "Iteration 249, Batch: 31, Loss: 0.0493004135787487\n",
      "Iteration 249, Batch: 32, Loss: 0.03878331556916237\n",
      "Iteration 249, Batch: 33, Loss: 0.036658238619565964\n",
      "Iteration 249, Batch: 34, Loss: 0.06823064386844635\n",
      "Iteration 249, Batch: 35, Loss: 0.04332900419831276\n",
      "Iteration 249, Batch: 36, Loss: 0.06372182816267014\n",
      "Iteration 249, Batch: 37, Loss: 0.048633236438035965\n",
      "Iteration 249, Batch: 38, Loss: 0.049630407243967056\n",
      "Iteration 249, Batch: 39, Loss: 0.03758833184838295\n",
      "Iteration 249, Batch: 40, Loss: 0.06328979134559631\n",
      "Iteration 249, Batch: 41, Loss: 0.02842753380537033\n",
      "Iteration 249, Batch: 42, Loss: 0.0702875480055809\n",
      "Iteration 249, Batch: 43, Loss: 0.054380062967538834\n",
      "Iteration 249, Batch: 44, Loss: 0.03403148427605629\n",
      "Iteration 249, Batch: 45, Loss: 0.05107038840651512\n",
      "Iteration 249, Batch: 46, Loss: 0.07935202121734619\n",
      "Iteration 249, Batch: 47, Loss: 0.059700820595026016\n",
      "Iteration 249, Batch: 48, Loss: 0.08373057097196579\n",
      "Iteration 249, Batch: 49, Loss: 0.04124520719051361\n",
      "Iteration 250, Batch: 0, Loss: 0.053604431450366974\n",
      "Iteration 250, Batch: 1, Loss: 0.058109913021326065\n",
      "Iteration 250, Batch: 2, Loss: 0.058820921927690506\n",
      "Iteration 250, Batch: 3, Loss: 0.04955541342496872\n",
      "Iteration 250, Batch: 4, Loss: 0.0346076563000679\n",
      "Iteration 250, Batch: 5, Loss: 0.04751564934849739\n",
      "Iteration 250, Batch: 6, Loss: 0.0285395085811615\n",
      "Iteration 250, Batch: 7, Loss: 0.06167009100317955\n",
      "Iteration 250, Batch: 8, Loss: 0.058204349130392075\n",
      "Iteration 250, Batch: 9, Loss: 0.04523276537656784\n",
      "Iteration 250, Batch: 10, Loss: 0.049151647835969925\n",
      "Iteration 250, Batch: 11, Loss: 0.0520937442779541\n",
      "Iteration 250, Batch: 12, Loss: 0.04938676580786705\n",
      "Iteration 250, Batch: 13, Loss: 0.06146060675382614\n",
      "Iteration 250, Batch: 14, Loss: 0.0634528398513794\n",
      "Iteration 250, Batch: 15, Loss: 0.057573359459638596\n",
      "Iteration 250, Batch: 16, Loss: 0.03294457867741585\n",
      "Iteration 250, Batch: 17, Loss: 0.03865569084882736\n",
      "Iteration 250, Batch: 18, Loss: 0.03821766376495361\n",
      "Iteration 250, Batch: 19, Loss: 0.048239126801490784\n",
      "Iteration 250, Batch: 20, Loss: 0.036880191415548325\n",
      "Iteration 250, Batch: 21, Loss: 0.0674622654914856\n",
      "Iteration 250, Batch: 22, Loss: 0.036457229405641556\n",
      "Iteration 250, Batch: 23, Loss: 0.058044422417879105\n",
      "Iteration 250, Batch: 24, Loss: 0.060440544039011\n",
      "Iteration 250, Batch: 25, Loss: 0.07444846630096436\n",
      "Iteration 250, Batch: 26, Loss: 0.05237668752670288\n",
      "Iteration 250, Batch: 27, Loss: 0.0440712533891201\n",
      "Iteration 250, Batch: 28, Loss: 0.04771769419312477\n",
      "Iteration 250, Batch: 29, Loss: 0.07229781150817871\n",
      "Iteration 250, Batch: 30, Loss: 0.05786537006497383\n",
      "Iteration 250, Batch: 31, Loss: 0.06327233463525772\n",
      "Iteration 250, Batch: 32, Loss: 0.0520806759595871\n",
      "Iteration 250, Batch: 33, Loss: 0.048744190484285355\n",
      "Iteration 250, Batch: 34, Loss: 0.04090723395347595\n",
      "Iteration 250, Batch: 35, Loss: 0.06865634769201279\n",
      "Iteration 250, Batch: 36, Loss: 0.0598905012011528\n",
      "Iteration 250, Batch: 37, Loss: 0.05098002776503563\n",
      "Iteration 250, Batch: 38, Loss: 0.07899754494428635\n",
      "Iteration 250, Batch: 39, Loss: 0.046605031937360764\n",
      "Iteration 250, Batch: 40, Loss: 0.031976696103811264\n",
      "Iteration 250, Batch: 41, Loss: 0.07116492837667465\n",
      "Iteration 250, Batch: 42, Loss: 0.05795563384890556\n",
      "Iteration 250, Batch: 43, Loss: 0.0516788512468338\n",
      "Iteration 250, Batch: 44, Loss: 0.03577200695872307\n",
      "Iteration 250, Batch: 45, Loss: 0.07222644984722137\n",
      "Iteration 250, Batch: 46, Loss: 0.05731750652194023\n",
      "Iteration 250, Batch: 47, Loss: 0.0598461888730526\n",
      "Iteration 250, Batch: 48, Loss: 0.07066332548856735\n",
      "Iteration 250, Batch: 49, Loss: 0.036461323499679565\n",
      "Iteration 251, Batch: 0, Loss: 0.032289303839206696\n",
      "Iteration 251, Batch: 1, Loss: 0.03000725619494915\n",
      "Iteration 251, Batch: 2, Loss: 0.04375097528100014\n",
      "Iteration 251, Batch: 3, Loss: 0.04827815294265747\n",
      "Iteration 251, Batch: 4, Loss: 0.06397724896669388\n",
      "Iteration 251, Batch: 5, Loss: 0.04232492297887802\n",
      "Iteration 251, Batch: 6, Loss: 0.03597323223948479\n",
      "Iteration 251, Batch: 7, Loss: 0.047053202986717224\n",
      "Iteration 251, Batch: 8, Loss: 0.05129344388842583\n",
      "Iteration 251, Batch: 9, Loss: 0.07936351001262665\n",
      "Iteration 251, Batch: 10, Loss: 0.06626372039318085\n",
      "Iteration 251, Batch: 11, Loss: 0.0520339161157608\n",
      "Iteration 251, Batch: 12, Loss: 0.056079089641571045\n",
      "Iteration 251, Batch: 13, Loss: 0.04294533655047417\n",
      "Iteration 251, Batch: 14, Loss: 0.072113037109375\n",
      "Iteration 251, Batch: 15, Loss: 0.05932264029979706\n",
      "Iteration 251, Batch: 16, Loss: 0.05821918323636055\n",
      "Iteration 251, Batch: 17, Loss: 0.04622073471546173\n",
      "Iteration 251, Batch: 18, Loss: 0.06300853192806244\n",
      "Iteration 251, Batch: 19, Loss: 0.07439208030700684\n",
      "Iteration 251, Batch: 20, Loss: 0.04334396868944168\n",
      "Iteration 251, Batch: 21, Loss: 0.07014519721269608\n",
      "Iteration 251, Batch: 22, Loss: 0.057097285985946655\n",
      "Iteration 251, Batch: 23, Loss: 0.07315967231988907\n",
      "Iteration 251, Batch: 24, Loss: 0.06168682873249054\n",
      "Iteration 251, Batch: 25, Loss: 0.07481645792722702\n",
      "Iteration 251, Batch: 26, Loss: 0.08944223076105118\n",
      "Iteration 251, Batch: 27, Loss: 0.08628890663385391\n",
      "Iteration 251, Batch: 28, Loss: 0.06155351549386978\n",
      "Iteration 251, Batch: 29, Loss: 0.05872607231140137\n",
      "Iteration 251, Batch: 30, Loss: 0.04267087206244469\n",
      "Iteration 251, Batch: 31, Loss: 0.01797427237033844\n",
      "Iteration 251, Batch: 32, Loss: 0.04529149830341339\n",
      "Iteration 251, Batch: 33, Loss: 0.04627091437578201\n",
      "Iteration 251, Batch: 34, Loss: 0.04101288691163063\n",
      "Iteration 251, Batch: 35, Loss: 0.04310016706585884\n",
      "Iteration 251, Batch: 36, Loss: 0.06334946304559708\n",
      "Iteration 251, Batch: 37, Loss: 0.055420417338609695\n",
      "Iteration 251, Batch: 38, Loss: 0.03824950009584427\n",
      "Iteration 251, Batch: 39, Loss: 0.03922239691019058\n",
      "Iteration 251, Batch: 40, Loss: 0.044211965054273605\n",
      "Iteration 251, Batch: 41, Loss: 0.05947338789701462\n",
      "Iteration 251, Batch: 42, Loss: 0.05603630095720291\n",
      "Iteration 251, Batch: 43, Loss: 0.0673777163028717\n",
      "Iteration 251, Batch: 44, Loss: 0.04731225222349167\n",
      "Iteration 251, Batch: 45, Loss: 0.04860777407884598\n",
      "Iteration 251, Batch: 46, Loss: 0.0620010644197464\n",
      "Iteration 251, Batch: 47, Loss: 0.082960344851017\n",
      "Iteration 251, Batch: 48, Loss: 0.04030116647481918\n",
      "Iteration 251, Batch: 49, Loss: 0.04679758474230766\n",
      "Iteration 252, Batch: 0, Loss: 0.07154326885938644\n",
      "Iteration 252, Batch: 1, Loss: 0.03370404243469238\n",
      "Iteration 252, Batch: 2, Loss: 0.0484098456799984\n",
      "Iteration 252, Batch: 3, Loss: 0.059580035507678986\n",
      "Iteration 252, Batch: 4, Loss: 0.04952645301818848\n",
      "Iteration 252, Batch: 5, Loss: 0.037024643272161484\n",
      "Iteration 252, Batch: 6, Loss: 0.035736218094825745\n",
      "Iteration 252, Batch: 7, Loss: 0.06072840467095375\n",
      "Iteration 252, Batch: 8, Loss: 0.044924546033144\n",
      "Iteration 252, Batch: 9, Loss: 0.04133790731430054\n",
      "Iteration 252, Batch: 10, Loss: 0.048654064536094666\n",
      "Iteration 252, Batch: 11, Loss: 0.05277320742607117\n",
      "Iteration 252, Batch: 12, Loss: 0.0694112554192543\n",
      "Iteration 252, Batch: 13, Loss: 0.03549068048596382\n",
      "Iteration 252, Batch: 14, Loss: 0.0360058955848217\n",
      "Iteration 252, Batch: 15, Loss: 0.08702009171247482\n",
      "Iteration 252, Batch: 16, Loss: 0.06363712251186371\n",
      "Iteration 252, Batch: 17, Loss: 0.05502861738204956\n",
      "Iteration 252, Batch: 18, Loss: 0.05067439004778862\n",
      "Iteration 252, Batch: 19, Loss: 0.06258276104927063\n",
      "Iteration 252, Batch: 20, Loss: 0.04302666708827019\n",
      "Iteration 252, Batch: 21, Loss: 0.044585566967725754\n",
      "Iteration 252, Batch: 22, Loss: 0.06562276929616928\n",
      "Iteration 252, Batch: 23, Loss: 0.03499915450811386\n",
      "Iteration 252, Batch: 24, Loss: 0.03483203426003456\n",
      "Iteration 252, Batch: 25, Loss: 0.10772296041250229\n",
      "Iteration 252, Batch: 26, Loss: 0.04934665188193321\n",
      "Iteration 252, Batch: 27, Loss: 0.06052688509225845\n",
      "Iteration 252, Batch: 28, Loss: 0.06809387356042862\n",
      "Iteration 252, Batch: 29, Loss: 0.0564294196665287\n",
      "Iteration 252, Batch: 30, Loss: 0.06057344004511833\n",
      "Iteration 252, Batch: 31, Loss: 0.05545289069414139\n",
      "Iteration 252, Batch: 32, Loss: 0.06193023547530174\n",
      "Iteration 252, Batch: 33, Loss: 0.03967050090432167\n",
      "Iteration 252, Batch: 34, Loss: 0.04248422011733055\n",
      "Iteration 252, Batch: 35, Loss: 0.05767284333705902\n",
      "Iteration 252, Batch: 36, Loss: 0.05059247836470604\n",
      "Iteration 252, Batch: 37, Loss: 0.06384629756212234\n",
      "Iteration 252, Batch: 38, Loss: 0.037925805896520615\n",
      "Iteration 252, Batch: 39, Loss: 0.06989343464374542\n",
      "Iteration 252, Batch: 40, Loss: 0.05183786153793335\n",
      "Iteration 252, Batch: 41, Loss: 0.042177628725767136\n",
      "Iteration 252, Batch: 42, Loss: 0.04730118811130524\n",
      "Iteration 252, Batch: 43, Loss: 0.04241054505109787\n",
      "Iteration 252, Batch: 44, Loss: 0.04550878331065178\n",
      "Iteration 252, Batch: 45, Loss: 0.05709977075457573\n",
      "Iteration 252, Batch: 46, Loss: 0.03798427805304527\n",
      "Iteration 252, Batch: 47, Loss: 0.05596839264035225\n",
      "Iteration 252, Batch: 48, Loss: 0.03307389095425606\n",
      "Iteration 252, Batch: 49, Loss: 0.05948479473590851\n",
      "Iteration 253, Batch: 0, Loss: 0.04994860664010048\n",
      "Iteration 253, Batch: 1, Loss: 0.043435610830783844\n",
      "Iteration 253, Batch: 2, Loss: 0.05798931047320366\n",
      "Iteration 253, Batch: 3, Loss: 0.025836080312728882\n",
      "Iteration 253, Batch: 4, Loss: 0.03937056288123131\n",
      "Iteration 253, Batch: 5, Loss: 0.04101604223251343\n",
      "Iteration 253, Batch: 6, Loss: 0.061725687235593796\n",
      "Iteration 253, Batch: 7, Loss: 0.058954257518053055\n",
      "Iteration 253, Batch: 8, Loss: 0.02422792837023735\n",
      "Iteration 253, Batch: 9, Loss: 0.05851701647043228\n",
      "Iteration 253, Batch: 10, Loss: 0.06946992129087448\n",
      "Iteration 253, Batch: 11, Loss: 0.03704065829515457\n",
      "Iteration 253, Batch: 12, Loss: 0.04898286610841751\n",
      "Iteration 253, Batch: 13, Loss: 0.04718032851815224\n",
      "Iteration 253, Batch: 14, Loss: 0.05663488805294037\n",
      "Iteration 253, Batch: 15, Loss: 0.06842406094074249\n",
      "Iteration 253, Batch: 16, Loss: 0.040547337383031845\n",
      "Iteration 253, Batch: 17, Loss: 0.05897468328475952\n",
      "Iteration 253, Batch: 18, Loss: 0.04986443370580673\n",
      "Iteration 253, Batch: 19, Loss: 0.042442306876182556\n",
      "Iteration 253, Batch: 20, Loss: 0.03241113945841789\n",
      "Iteration 253, Batch: 21, Loss: 0.052782103419303894\n",
      "Iteration 253, Batch: 22, Loss: 0.047398678958415985\n",
      "Iteration 253, Batch: 23, Loss: 0.05212021246552467\n",
      "Iteration 253, Batch: 24, Loss: 0.05625508725643158\n",
      "Iteration 253, Batch: 25, Loss: 0.03586931526660919\n",
      "Iteration 253, Batch: 26, Loss: 0.07214173674583435\n",
      "Iteration 253, Batch: 27, Loss: 0.041322000324726105\n",
      "Iteration 253, Batch: 28, Loss: 0.04097336530685425\n",
      "Iteration 253, Batch: 29, Loss: 0.054658859968185425\n",
      "Iteration 253, Batch: 30, Loss: 0.053299520164728165\n",
      "Iteration 253, Batch: 31, Loss: 0.04715919494628906\n",
      "Iteration 253, Batch: 32, Loss: 0.059387966990470886\n",
      "Iteration 253, Batch: 33, Loss: 0.05265345796942711\n",
      "Iteration 253, Batch: 34, Loss: 0.05202813819050789\n",
      "Iteration 253, Batch: 35, Loss: 0.027776721864938736\n",
      "Iteration 253, Batch: 36, Loss: 0.05506780371069908\n",
      "Iteration 253, Batch: 37, Loss: 0.064501091837883\n",
      "Iteration 253, Batch: 38, Loss: 0.057258348912000656\n",
      "Iteration 253, Batch: 39, Loss: 0.06322526931762695\n",
      "Iteration 253, Batch: 40, Loss: 0.06833081692457199\n",
      "Iteration 253, Batch: 41, Loss: 0.07163839787244797\n",
      "Iteration 253, Batch: 42, Loss: 0.07379581779241562\n",
      "Iteration 253, Batch: 43, Loss: 0.06611007452011108\n",
      "Iteration 253, Batch: 44, Loss: 0.05285954475402832\n",
      "Iteration 253, Batch: 45, Loss: 0.04657423868775368\n",
      "Iteration 253, Batch: 46, Loss: 0.06697791069746017\n",
      "Iteration 253, Batch: 47, Loss: 0.08243808895349503\n",
      "Iteration 253, Batch: 48, Loss: 0.07054873555898666\n",
      "Iteration 253, Batch: 49, Loss: 0.04504144564270973\n",
      "Iteration 254, Batch: 0, Loss: 0.049024734646081924\n",
      "Iteration 254, Batch: 1, Loss: 0.05333605781197548\n",
      "Iteration 254, Batch: 2, Loss: 0.036712199449539185\n",
      "Iteration 254, Batch: 3, Loss: 0.041515104472637177\n",
      "Iteration 254, Batch: 4, Loss: 0.04520517587661743\n",
      "Iteration 254, Batch: 5, Loss: 0.0396825410425663\n",
      "Iteration 254, Batch: 6, Loss: 0.051313161849975586\n",
      "Iteration 254, Batch: 7, Loss: 0.05844833701848984\n",
      "Iteration 254, Batch: 8, Loss: 0.04919425770640373\n",
      "Iteration 254, Batch: 9, Loss: 0.047723740339279175\n",
      "Iteration 254, Batch: 10, Loss: 0.0391228087246418\n",
      "Iteration 254, Batch: 11, Loss: 0.05348880589008331\n",
      "Iteration 254, Batch: 12, Loss: 0.04049712419509888\n",
      "Iteration 254, Batch: 13, Loss: 0.07007424533367157\n",
      "Iteration 254, Batch: 14, Loss: 0.03256841003894806\n",
      "Iteration 254, Batch: 15, Loss: 0.048217762261629105\n",
      "Iteration 254, Batch: 16, Loss: 0.03831109777092934\n",
      "Iteration 254, Batch: 17, Loss: 0.02946941740810871\n",
      "Iteration 254, Batch: 18, Loss: 0.07054892182350159\n",
      "Iteration 254, Batch: 19, Loss: 0.04664885997772217\n",
      "Iteration 254, Batch: 20, Loss: 0.05230662599205971\n",
      "Iteration 254, Batch: 21, Loss: 0.05236513912677765\n",
      "Iteration 254, Batch: 22, Loss: 0.04882058501243591\n",
      "Iteration 254, Batch: 23, Loss: 0.08239836245775223\n",
      "Iteration 254, Batch: 24, Loss: 0.05261588096618652\n",
      "Iteration 254, Batch: 25, Loss: 0.056284256279468536\n",
      "Iteration 254, Batch: 26, Loss: 0.07443565130233765\n",
      "Iteration 254, Batch: 27, Loss: 0.07170893251895905\n",
      "Iteration 254, Batch: 28, Loss: 0.032213158905506134\n",
      "Iteration 254, Batch: 29, Loss: 0.0617266446352005\n",
      "Iteration 254, Batch: 30, Loss: 0.05394589155912399\n",
      "Iteration 254, Batch: 31, Loss: 0.06040381267666817\n",
      "Iteration 254, Batch: 32, Loss: 0.08885017037391663\n",
      "Iteration 254, Batch: 33, Loss: 0.03634609282016754\n",
      "Iteration 254, Batch: 34, Loss: 0.029733382165431976\n",
      "Iteration 254, Batch: 35, Loss: 0.057742487639188766\n",
      "Iteration 254, Batch: 36, Loss: 0.042247142642736435\n",
      "Iteration 254, Batch: 37, Loss: 0.04155970364809036\n",
      "Iteration 254, Batch: 38, Loss: 0.04956883192062378\n",
      "Iteration 254, Batch: 39, Loss: 0.05040012672543526\n",
      "Iteration 254, Batch: 40, Loss: 0.05168737471103668\n",
      "Iteration 254, Batch: 41, Loss: 0.05545813590288162\n",
      "Iteration 254, Batch: 42, Loss: 0.06653483211994171\n",
      "Iteration 254, Batch: 43, Loss: 0.032063599675893784\n",
      "Iteration 254, Batch: 44, Loss: 0.05919491499662399\n",
      "Iteration 254, Batch: 45, Loss: 0.051653727889060974\n",
      "Iteration 254, Batch: 46, Loss: 0.04333461821079254\n",
      "Iteration 254, Batch: 47, Loss: 0.051195740699768066\n",
      "Iteration 254, Batch: 48, Loss: 0.04924319311976433\n",
      "Iteration 254, Batch: 49, Loss: 0.052981238812208176\n",
      "Iteration 255, Batch: 0, Loss: 0.05471084639430046\n",
      "Iteration 255, Batch: 1, Loss: 0.026523785665631294\n",
      "Iteration 255, Batch: 2, Loss: 0.09313546866178513\n",
      "Iteration 255, Batch: 3, Loss: 0.04279382526874542\n",
      "Iteration 255, Batch: 4, Loss: 0.06107510253787041\n",
      "Iteration 255, Batch: 5, Loss: 0.05533474683761597\n",
      "Iteration 255, Batch: 6, Loss: 0.06806012988090515\n",
      "Iteration 255, Batch: 7, Loss: 0.06165909767150879\n",
      "Iteration 255, Batch: 8, Loss: 0.049344949424266815\n",
      "Iteration 255, Batch: 9, Loss: 0.07173437625169754\n",
      "Iteration 255, Batch: 10, Loss: 0.027821514755487442\n",
      "Iteration 255, Batch: 11, Loss: 0.05200958997011185\n",
      "Iteration 255, Batch: 12, Loss: 0.05606812238693237\n",
      "Iteration 255, Batch: 13, Loss: 0.061819400638341904\n",
      "Iteration 255, Batch: 14, Loss: 0.06839463114738464\n",
      "Iteration 255, Batch: 15, Loss: 0.048995617777109146\n",
      "Iteration 255, Batch: 16, Loss: 0.06353417038917542\n",
      "Iteration 255, Batch: 17, Loss: 0.06010422110557556\n",
      "Iteration 255, Batch: 18, Loss: 0.07768338918685913\n",
      "Iteration 255, Batch: 19, Loss: 0.06422165036201477\n",
      "Iteration 255, Batch: 20, Loss: 0.07989761233329773\n",
      "Iteration 255, Batch: 21, Loss: 0.051008567214012146\n",
      "Iteration 255, Batch: 22, Loss: 0.030152330175042152\n",
      "Iteration 255, Batch: 23, Loss: 0.06422798335552216\n",
      "Iteration 255, Batch: 24, Loss: 0.06382934749126434\n",
      "Iteration 255, Batch: 25, Loss: 0.06263969838619232\n",
      "Iteration 255, Batch: 26, Loss: 0.0666225254535675\n",
      "Iteration 255, Batch: 27, Loss: 0.06284298002719879\n",
      "Iteration 255, Batch: 28, Loss: 0.047631874680519104\n",
      "Iteration 255, Batch: 29, Loss: 0.03197956830263138\n",
      "Iteration 255, Batch: 30, Loss: 0.09143280982971191\n",
      "Iteration 255, Batch: 31, Loss: 0.05507371202111244\n",
      "Iteration 255, Batch: 32, Loss: 0.04972844570875168\n",
      "Iteration 255, Batch: 33, Loss: 0.044244520366191864\n",
      "Iteration 255, Batch: 34, Loss: 0.03165609389543533\n",
      "Iteration 255, Batch: 35, Loss: 0.04306008294224739\n",
      "Iteration 255, Batch: 36, Loss: 0.03522585704922676\n",
      "Iteration 255, Batch: 37, Loss: 0.0708278939127922\n",
      "Iteration 255, Batch: 38, Loss: 0.0864078477025032\n",
      "Iteration 255, Batch: 39, Loss: 0.051427438855171204\n",
      "Iteration 255, Batch: 40, Loss: 0.05483695864677429\n",
      "Iteration 255, Batch: 41, Loss: 0.03307615593075752\n",
      "Iteration 255, Batch: 42, Loss: 0.04065895080566406\n",
      "Iteration 255, Batch: 43, Loss: 0.04702102392911911\n",
      "Iteration 255, Batch: 44, Loss: 0.04435287415981293\n",
      "Iteration 255, Batch: 45, Loss: 0.048016563057899475\n",
      "Iteration 255, Batch: 46, Loss: 0.061137448996305466\n",
      "Iteration 255, Batch: 47, Loss: 0.06756801903247833\n",
      "Iteration 255, Batch: 48, Loss: 0.05373188853263855\n",
      "Iteration 255, Batch: 49, Loss: 0.06016876921057701\n",
      "Iteration 256, Batch: 0, Loss: 0.0607200525701046\n",
      "Iteration 256, Batch: 1, Loss: 0.06182393804192543\n",
      "Iteration 256, Batch: 2, Loss: 0.012561802752315998\n",
      "Iteration 256, Batch: 3, Loss: 0.02469189092516899\n",
      "Iteration 256, Batch: 4, Loss: 0.07055190205574036\n",
      "Iteration 256, Batch: 5, Loss: 0.038478460162878036\n",
      "Iteration 256, Batch: 6, Loss: 0.059685200452804565\n",
      "Iteration 256, Batch: 7, Loss: 0.026343239471316338\n",
      "Iteration 256, Batch: 8, Loss: 0.06917987763881683\n",
      "Iteration 256, Batch: 9, Loss: 0.06592486053705215\n",
      "Iteration 256, Batch: 10, Loss: 0.0658782497048378\n",
      "Iteration 256, Batch: 11, Loss: 0.05131331458687782\n",
      "Iteration 256, Batch: 12, Loss: 0.026012815535068512\n",
      "Iteration 256, Batch: 13, Loss: 0.06690750271081924\n",
      "Iteration 256, Batch: 14, Loss: 0.05821618810296059\n",
      "Iteration 256, Batch: 15, Loss: 0.026597891002893448\n",
      "Iteration 256, Batch: 16, Loss: 0.05494803190231323\n",
      "Iteration 256, Batch: 17, Loss: 0.057952966541051865\n",
      "Iteration 256, Batch: 18, Loss: 0.07457828521728516\n",
      "Iteration 256, Batch: 19, Loss: 0.04703991860151291\n",
      "Iteration 256, Batch: 20, Loss: 0.0511755645275116\n",
      "Iteration 256, Batch: 21, Loss: 0.07053685933351517\n",
      "Iteration 256, Batch: 22, Loss: 0.07162123173475266\n",
      "Iteration 256, Batch: 23, Loss: 0.05209867283701897\n",
      "Iteration 256, Batch: 24, Loss: 0.06641935557126999\n",
      "Iteration 256, Batch: 25, Loss: 0.06087817624211311\n",
      "Iteration 256, Batch: 26, Loss: 0.0635858029127121\n",
      "Iteration 256, Batch: 27, Loss: 0.0454786941409111\n",
      "Iteration 256, Batch: 28, Loss: 0.07961776852607727\n",
      "Iteration 256, Batch: 29, Loss: 0.06970898807048798\n",
      "Iteration 256, Batch: 30, Loss: 0.07106874138116837\n",
      "Iteration 256, Batch: 31, Loss: 0.06147921457886696\n",
      "Iteration 256, Batch: 32, Loss: 0.052431654185056686\n",
      "Iteration 256, Batch: 33, Loss: 0.04888998717069626\n",
      "Iteration 256, Batch: 34, Loss: 0.07910057157278061\n",
      "Iteration 256, Batch: 35, Loss: 0.0552106648683548\n",
      "Iteration 256, Batch: 36, Loss: 0.031588371843099594\n",
      "Iteration 256, Batch: 37, Loss: 0.04431896656751633\n",
      "Iteration 256, Batch: 38, Loss: 0.05251135677099228\n",
      "Iteration 256, Batch: 39, Loss: 0.08370000869035721\n",
      "Iteration 256, Batch: 40, Loss: 0.03719392791390419\n",
      "Iteration 256, Batch: 41, Loss: 0.056955818086862564\n",
      "Iteration 256, Batch: 42, Loss: 0.060361213982105255\n",
      "Iteration 256, Batch: 43, Loss: 0.03847215697169304\n",
      "Iteration 256, Batch: 44, Loss: 0.04581950232386589\n",
      "Iteration 256, Batch: 45, Loss: 0.03948436304926872\n",
      "Iteration 256, Batch: 46, Loss: 0.07144726812839508\n",
      "Iteration 256, Batch: 47, Loss: 0.0753534585237503\n",
      "Iteration 256, Batch: 48, Loss: 0.050679586827754974\n",
      "Iteration 256, Batch: 49, Loss: 0.059522513300180435\n",
      "Iteration 257, Batch: 0, Loss: 0.06877374649047852\n",
      "Iteration 257, Batch: 1, Loss: 0.039950378239154816\n",
      "Iteration 257, Batch: 2, Loss: 0.049351248890161514\n",
      "Iteration 257, Batch: 3, Loss: 0.06335639953613281\n",
      "Iteration 257, Batch: 4, Loss: 0.033711981028318405\n",
      "Iteration 257, Batch: 5, Loss: 0.0707324966788292\n",
      "Iteration 257, Batch: 6, Loss: 0.058877285569906235\n",
      "Iteration 257, Batch: 7, Loss: 0.06452786177396774\n",
      "Iteration 257, Batch: 8, Loss: 0.054215896874666214\n",
      "Iteration 257, Batch: 9, Loss: 0.03795098513364792\n",
      "Iteration 257, Batch: 10, Loss: 0.04877324029803276\n",
      "Iteration 257, Batch: 11, Loss: 0.05904575064778328\n",
      "Iteration 257, Batch: 12, Loss: 0.04452088102698326\n",
      "Iteration 257, Batch: 13, Loss: 0.048920419067144394\n",
      "Iteration 257, Batch: 14, Loss: 0.0737699493765831\n",
      "Iteration 257, Batch: 15, Loss: 0.05911799147725105\n",
      "Iteration 257, Batch: 16, Loss: 0.02319672517478466\n",
      "Iteration 257, Batch: 17, Loss: 0.05112328752875328\n",
      "Iteration 257, Batch: 18, Loss: 0.03303465247154236\n",
      "Iteration 257, Batch: 19, Loss: 0.02609514631330967\n",
      "Iteration 257, Batch: 20, Loss: 0.049567773938179016\n",
      "Iteration 257, Batch: 21, Loss: 0.045749224722385406\n",
      "Iteration 257, Batch: 22, Loss: 0.06576573848724365\n",
      "Iteration 257, Batch: 23, Loss: 0.07374367862939835\n",
      "Iteration 257, Batch: 24, Loss: 0.07145235687494278\n",
      "Iteration 257, Batch: 25, Loss: 0.03926606476306915\n",
      "Iteration 257, Batch: 26, Loss: 0.05836038291454315\n",
      "Iteration 257, Batch: 27, Loss: 0.08521270751953125\n",
      "Iteration 257, Batch: 28, Loss: 0.07142209261655807\n",
      "Iteration 257, Batch: 29, Loss: 0.06020539999008179\n",
      "Iteration 257, Batch: 30, Loss: 0.05128675326704979\n",
      "Iteration 257, Batch: 31, Loss: 0.06690898537635803\n",
      "Iteration 257, Batch: 32, Loss: 0.06569898128509521\n",
      "Iteration 257, Batch: 33, Loss: 0.04564839228987694\n",
      "Iteration 257, Batch: 34, Loss: 0.059788625687360764\n",
      "Iteration 257, Batch: 35, Loss: 0.06016569957137108\n",
      "Iteration 257, Batch: 36, Loss: 0.04541650041937828\n",
      "Iteration 257, Batch: 37, Loss: 0.04010389372706413\n",
      "Iteration 257, Batch: 38, Loss: 0.08419661968946457\n",
      "Iteration 257, Batch: 39, Loss: 0.05668599158525467\n",
      "Iteration 257, Batch: 40, Loss: 0.05671544000506401\n",
      "Iteration 257, Batch: 41, Loss: 0.06670651584863663\n",
      "Iteration 257, Batch: 42, Loss: 0.0694580078125\n",
      "Iteration 257, Batch: 43, Loss: 0.06318075954914093\n",
      "Iteration 257, Batch: 44, Loss: 0.0592627078294754\n",
      "Iteration 257, Batch: 45, Loss: 0.06895430386066437\n",
      "Iteration 257, Batch: 46, Loss: 0.08106855303049088\n",
      "Iteration 257, Batch: 47, Loss: 0.055485691875219345\n",
      "Iteration 257, Batch: 48, Loss: 0.05327548459172249\n",
      "Iteration 257, Batch: 49, Loss: 0.05902976915240288\n",
      "Iteration 258, Batch: 0, Loss: 0.06376270949840546\n",
      "Iteration 258, Batch: 1, Loss: 0.056963395327329636\n",
      "Iteration 258, Batch: 2, Loss: 0.045923274010419846\n",
      "Iteration 258, Batch: 3, Loss: 0.0621761679649353\n",
      "Iteration 258, Batch: 4, Loss: 0.03473937511444092\n",
      "Iteration 258, Batch: 5, Loss: 0.07319590449333191\n",
      "Iteration 258, Batch: 6, Loss: 0.057054899632930756\n",
      "Iteration 258, Batch: 7, Loss: 0.07153189182281494\n",
      "Iteration 258, Batch: 8, Loss: 0.057623326778411865\n",
      "Iteration 258, Batch: 9, Loss: 0.06976956874132156\n",
      "Iteration 258, Batch: 10, Loss: 0.048801761120557785\n",
      "Iteration 258, Batch: 11, Loss: 0.05840582773089409\n",
      "Iteration 258, Batch: 12, Loss: 0.05905025452375412\n",
      "Iteration 258, Batch: 13, Loss: 0.055641286075115204\n",
      "Iteration 258, Batch: 14, Loss: 0.060102660208940506\n",
      "Iteration 258, Batch: 15, Loss: 0.0461658276617527\n",
      "Iteration 258, Batch: 16, Loss: 0.05598592013120651\n",
      "Iteration 258, Batch: 17, Loss: 0.054464321583509445\n",
      "Iteration 258, Batch: 18, Loss: 0.03863869234919548\n",
      "Iteration 258, Batch: 19, Loss: 0.061974577605724335\n",
      "Iteration 258, Batch: 20, Loss: 0.06702535599470139\n",
      "Iteration 258, Batch: 21, Loss: 0.03967830911278725\n",
      "Iteration 258, Batch: 22, Loss: 0.057896580547094345\n",
      "Iteration 258, Batch: 23, Loss: 0.06992869824171066\n",
      "Iteration 258, Batch: 24, Loss: 0.04100244492292404\n",
      "Iteration 258, Batch: 25, Loss: 0.07590800523757935\n",
      "Iteration 258, Batch: 26, Loss: 0.04345274716615677\n",
      "Iteration 258, Batch: 27, Loss: 0.05766695365309715\n",
      "Iteration 258, Batch: 28, Loss: 0.0651223361492157\n",
      "Iteration 258, Batch: 29, Loss: 0.0635790228843689\n",
      "Iteration 258, Batch: 30, Loss: 0.055298082530498505\n",
      "Iteration 258, Batch: 31, Loss: 0.06188911944627762\n",
      "Iteration 258, Batch: 32, Loss: 0.050286490470170975\n",
      "Iteration 258, Batch: 33, Loss: 0.0617426335811615\n",
      "Iteration 258, Batch: 34, Loss: 0.06708815693855286\n",
      "Iteration 258, Batch: 35, Loss: 0.0508837029337883\n",
      "Iteration 258, Batch: 36, Loss: 0.04242372512817383\n",
      "Iteration 258, Batch: 37, Loss: 0.0313626192510128\n",
      "Iteration 258, Batch: 38, Loss: 0.06259150058031082\n",
      "Iteration 258, Batch: 39, Loss: 0.08627397567033768\n",
      "Iteration 258, Batch: 40, Loss: 0.07418956607580185\n",
      "Iteration 258, Batch: 41, Loss: 0.07616234570741653\n",
      "Iteration 258, Batch: 42, Loss: 0.05641720071434975\n",
      "Iteration 258, Batch: 43, Loss: 0.04341960698366165\n",
      "Iteration 258, Batch: 44, Loss: 0.07874535024166107\n",
      "Iteration 258, Batch: 45, Loss: 0.05180257558822632\n",
      "Iteration 258, Batch: 46, Loss: 0.02962772734463215\n",
      "Iteration 258, Batch: 47, Loss: 0.06751828640699387\n",
      "Iteration 258, Batch: 48, Loss: 0.0655951276421547\n",
      "Iteration 258, Batch: 49, Loss: 0.0855955109000206\n",
      "Iteration 259, Batch: 0, Loss: 0.06038697436451912\n",
      "Iteration 259, Batch: 1, Loss: 0.06249874457716942\n",
      "Iteration 259, Batch: 2, Loss: 0.062032755464315414\n",
      "Iteration 259, Batch: 3, Loss: 0.0610605850815773\n",
      "Iteration 259, Batch: 4, Loss: 0.06874459236860275\n",
      "Iteration 259, Batch: 5, Loss: 0.05247974768280983\n",
      "Iteration 259, Batch: 6, Loss: 0.051476266235113144\n",
      "Iteration 259, Batch: 7, Loss: 0.058489665389060974\n",
      "Iteration 259, Batch: 8, Loss: 0.049813512712717056\n",
      "Iteration 259, Batch: 9, Loss: 0.06440972536802292\n",
      "Iteration 259, Batch: 10, Loss: 0.05096115916967392\n",
      "Iteration 259, Batch: 11, Loss: 0.08579950034618378\n",
      "Iteration 259, Batch: 12, Loss: 0.05675680190324783\n",
      "Iteration 259, Batch: 13, Loss: 0.048872366547584534\n",
      "Iteration 259, Batch: 14, Loss: 0.04411209002137184\n",
      "Iteration 259, Batch: 15, Loss: 0.06168031692504883\n",
      "Iteration 259, Batch: 16, Loss: 0.04839838296175003\n",
      "Iteration 259, Batch: 17, Loss: 0.08753282576799393\n",
      "Iteration 259, Batch: 18, Loss: 0.046136125922203064\n",
      "Iteration 259, Batch: 19, Loss: 0.07277662307024002\n",
      "Iteration 259, Batch: 20, Loss: 0.060798756778240204\n",
      "Iteration 259, Batch: 21, Loss: 0.060531653463840485\n",
      "Iteration 259, Batch: 22, Loss: 0.04807143658399582\n",
      "Iteration 259, Batch: 23, Loss: 0.05857855826616287\n",
      "Iteration 259, Batch: 24, Loss: 0.04393498972058296\n",
      "Iteration 259, Batch: 25, Loss: 0.06564908474683762\n",
      "Iteration 259, Batch: 26, Loss: 0.05377423018217087\n",
      "Iteration 259, Batch: 27, Loss: 0.08532951027154922\n",
      "Iteration 259, Batch: 28, Loss: 0.059103917330503464\n",
      "Iteration 259, Batch: 29, Loss: 0.05035191774368286\n",
      "Iteration 259, Batch: 30, Loss: 0.06270933151245117\n",
      "Iteration 259, Batch: 31, Loss: 0.10180485248565674\n",
      "Iteration 259, Batch: 32, Loss: 0.04354502260684967\n",
      "Iteration 259, Batch: 33, Loss: 0.07087098807096481\n",
      "Iteration 259, Batch: 34, Loss: 0.030369175598025322\n",
      "Iteration 259, Batch: 35, Loss: 0.04025482013821602\n",
      "Iteration 259, Batch: 36, Loss: 0.06445034593343735\n",
      "Iteration 259, Batch: 37, Loss: 0.06657475978136063\n",
      "Iteration 259, Batch: 38, Loss: 0.05272524803876877\n",
      "Iteration 259, Batch: 39, Loss: 0.016911379992961884\n",
      "Iteration 259, Batch: 40, Loss: 0.07331659644842148\n",
      "Iteration 259, Batch: 41, Loss: 0.09748051315546036\n",
      "Iteration 259, Batch: 42, Loss: 0.04163377359509468\n",
      "Iteration 259, Batch: 43, Loss: 0.050624337047338486\n",
      "Iteration 259, Batch: 44, Loss: 0.053915947675704956\n",
      "Iteration 259, Batch: 45, Loss: 0.04922015964984894\n",
      "Iteration 259, Batch: 46, Loss: 0.05293901637196541\n",
      "Iteration 259, Batch: 47, Loss: 0.046689119189977646\n",
      "Iteration 259, Batch: 48, Loss: 0.07186831533908844\n",
      "Iteration 259, Batch: 49, Loss: 0.0626361295580864\n",
      "Iteration 260, Batch: 0, Loss: 0.0758260041475296\n",
      "Iteration 260, Batch: 1, Loss: 0.06498061120510101\n",
      "Iteration 260, Batch: 2, Loss: 0.049321699887514114\n",
      "Iteration 260, Batch: 3, Loss: 0.07108967006206512\n",
      "Iteration 260, Batch: 4, Loss: 0.059067659080028534\n",
      "Iteration 260, Batch: 5, Loss: 0.04867365583777428\n",
      "Iteration 260, Batch: 6, Loss: 0.035090371966362\n",
      "Iteration 260, Batch: 7, Loss: 0.055515822023153305\n",
      "Iteration 260, Batch: 8, Loss: 0.041224539279937744\n",
      "Iteration 260, Batch: 9, Loss: 0.05731673911213875\n",
      "Iteration 260, Batch: 10, Loss: 0.026948856189846992\n",
      "Iteration 260, Batch: 11, Loss: 0.029643813148140907\n",
      "Iteration 260, Batch: 12, Loss: 0.0370509959757328\n",
      "Iteration 260, Batch: 13, Loss: 0.09618300944566727\n",
      "Iteration 260, Batch: 14, Loss: 0.05135458707809448\n",
      "Iteration 260, Batch: 15, Loss: 0.03859160467982292\n",
      "Iteration 260, Batch: 16, Loss: 0.04749865457415581\n",
      "Iteration 260, Batch: 17, Loss: 0.05829709768295288\n",
      "Iteration 260, Batch: 18, Loss: 0.05660068243741989\n",
      "Iteration 260, Batch: 19, Loss: 0.05323595926165581\n",
      "Iteration 260, Batch: 20, Loss: 0.06875395029783249\n",
      "Iteration 260, Batch: 21, Loss: 0.05702543258666992\n",
      "Iteration 260, Batch: 22, Loss: 0.05458734557032585\n",
      "Iteration 260, Batch: 23, Loss: 0.04797656834125519\n",
      "Iteration 260, Batch: 24, Loss: 0.04224548861384392\n",
      "Iteration 260, Batch: 25, Loss: 0.04422561079263687\n",
      "Iteration 260, Batch: 26, Loss: 0.030388792976737022\n",
      "Iteration 260, Batch: 27, Loss: 0.05000787973403931\n",
      "Iteration 260, Batch: 28, Loss: 0.05314183980226517\n",
      "Iteration 260, Batch: 29, Loss: 0.05940118804574013\n",
      "Iteration 260, Batch: 30, Loss: 0.08269575983285904\n",
      "Iteration 260, Batch: 31, Loss: 0.06589141488075256\n",
      "Iteration 260, Batch: 32, Loss: 0.051027555018663406\n",
      "Iteration 260, Batch: 33, Loss: 0.09681103378534317\n",
      "Iteration 260, Batch: 34, Loss: 0.06817853450775146\n",
      "Iteration 260, Batch: 35, Loss: 0.04642487317323685\n",
      "Iteration 260, Batch: 36, Loss: 0.0393560454249382\n",
      "Iteration 260, Batch: 37, Loss: 0.049810074269771576\n",
      "Iteration 260, Batch: 38, Loss: 0.0842040404677391\n",
      "Iteration 260, Batch: 39, Loss: 0.05877919867634773\n",
      "Iteration 260, Batch: 40, Loss: 0.03946014866232872\n",
      "Iteration 260, Batch: 41, Loss: 0.03121226280927658\n",
      "Iteration 260, Batch: 42, Loss: 0.08491890132427216\n",
      "Iteration 260, Batch: 43, Loss: 0.04636593535542488\n",
      "Iteration 260, Batch: 44, Loss: 0.04817710816860199\n",
      "Iteration 260, Batch: 45, Loss: 0.06120166555047035\n",
      "Iteration 260, Batch: 46, Loss: 0.06937582790851593\n",
      "Iteration 260, Batch: 47, Loss: 0.04151257500052452\n",
      "Iteration 260, Batch: 48, Loss: 0.07239683717489243\n",
      "Iteration 260, Batch: 49, Loss: 0.07362884283065796\n",
      "Iteration 261, Batch: 0, Loss: 0.04543740302324295\n",
      "Iteration 261, Batch: 1, Loss: 0.024263037368655205\n",
      "Iteration 261, Batch: 2, Loss: 0.0690125897526741\n",
      "Iteration 261, Batch: 3, Loss: 0.08230516314506531\n",
      "Iteration 261, Batch: 4, Loss: 0.034940529614686966\n",
      "Iteration 261, Batch: 5, Loss: 0.0401294119656086\n",
      "Iteration 261, Batch: 6, Loss: 0.06790348142385483\n",
      "Iteration 261, Batch: 7, Loss: 0.06707913428544998\n",
      "Iteration 261, Batch: 8, Loss: 0.056770019233226776\n",
      "Iteration 261, Batch: 9, Loss: 0.05747053027153015\n",
      "Iteration 261, Batch: 10, Loss: 0.04863441362977028\n",
      "Iteration 261, Batch: 11, Loss: 0.07151365280151367\n",
      "Iteration 261, Batch: 12, Loss: 0.05675526335835457\n",
      "Iteration 261, Batch: 13, Loss: 0.05612827092409134\n",
      "Iteration 261, Batch: 14, Loss: 0.0672273263335228\n",
      "Iteration 261, Batch: 15, Loss: 0.05605018511414528\n",
      "Iteration 261, Batch: 16, Loss: 0.03412873297929764\n",
      "Iteration 261, Batch: 17, Loss: 0.048081349581480026\n",
      "Iteration 261, Batch: 18, Loss: 0.03265492990612984\n",
      "Iteration 261, Batch: 19, Loss: 0.03343909978866577\n",
      "Iteration 261, Batch: 20, Loss: 0.030368013307452202\n",
      "Iteration 261, Batch: 21, Loss: 0.05165095254778862\n",
      "Iteration 261, Batch: 22, Loss: 0.05499889329075813\n",
      "Iteration 261, Batch: 23, Loss: 0.04830178990960121\n",
      "Iteration 261, Batch: 24, Loss: 0.06986753642559052\n",
      "Iteration 261, Batch: 25, Loss: 0.0493897907435894\n",
      "Iteration 261, Batch: 26, Loss: 0.05829981341958046\n",
      "Iteration 261, Batch: 27, Loss: 0.02951887995004654\n",
      "Iteration 261, Batch: 28, Loss: 0.061884865164756775\n",
      "Iteration 261, Batch: 29, Loss: 0.04412679746747017\n",
      "Iteration 261, Batch: 30, Loss: 0.04174407944083214\n",
      "Iteration 261, Batch: 31, Loss: 0.05258261412382126\n",
      "Iteration 261, Batch: 32, Loss: 0.047028835862874985\n",
      "Iteration 261, Batch: 33, Loss: 0.04128970950841904\n",
      "Iteration 261, Batch: 34, Loss: 0.04761052131652832\n",
      "Iteration 261, Batch: 35, Loss: 0.053080733865499496\n",
      "Iteration 261, Batch: 36, Loss: 0.10336720198392868\n",
      "Iteration 261, Batch: 37, Loss: 0.06937582790851593\n",
      "Iteration 261, Batch: 38, Loss: 0.06873427331447601\n",
      "Iteration 261, Batch: 39, Loss: 0.06425169855356216\n",
      "Iteration 261, Batch: 40, Loss: 0.04447169974446297\n",
      "Iteration 261, Batch: 41, Loss: 0.05808309465646744\n",
      "Iteration 261, Batch: 42, Loss: 0.05798296630382538\n",
      "Iteration 261, Batch: 43, Loss: 0.045324623584747314\n",
      "Iteration 261, Batch: 44, Loss: 0.05243229866027832\n",
      "Iteration 261, Batch: 45, Loss: 0.07169655710458755\n",
      "Iteration 261, Batch: 46, Loss: 0.04809999465942383\n",
      "Iteration 261, Batch: 47, Loss: 0.0585775263607502\n",
      "Iteration 261, Batch: 48, Loss: 0.04802258312702179\n",
      "Iteration 261, Batch: 49, Loss: 0.07096103578805923\n",
      "Iteration 262, Batch: 0, Loss: 0.0338323675096035\n",
      "Iteration 262, Batch: 1, Loss: 0.03226255625486374\n",
      "Iteration 262, Batch: 2, Loss: 0.053447723388671875\n",
      "Iteration 262, Batch: 3, Loss: 0.028424205258488655\n",
      "Iteration 262, Batch: 4, Loss: 0.048053912818431854\n",
      "Iteration 262, Batch: 5, Loss: 0.06507281213998795\n",
      "Iteration 262, Batch: 6, Loss: 0.040027521550655365\n",
      "Iteration 262, Batch: 7, Loss: 0.04036002233624458\n",
      "Iteration 262, Batch: 8, Loss: 0.07094047218561172\n",
      "Iteration 262, Batch: 9, Loss: 0.09244252741336823\n",
      "Iteration 262, Batch: 10, Loss: 0.07719070464372635\n",
      "Iteration 262, Batch: 11, Loss: 0.04384501278400421\n",
      "Iteration 262, Batch: 12, Loss: 0.08416211605072021\n",
      "Iteration 262, Batch: 13, Loss: 0.0628548339009285\n",
      "Iteration 262, Batch: 14, Loss: 0.06380970776081085\n",
      "Iteration 262, Batch: 15, Loss: 0.0493595115840435\n",
      "Iteration 262, Batch: 16, Loss: 0.05320295691490173\n",
      "Iteration 262, Batch: 17, Loss: 0.07243707776069641\n",
      "Iteration 262, Batch: 18, Loss: 0.035442885011434555\n",
      "Iteration 262, Batch: 19, Loss: 0.04524047300219536\n",
      "Iteration 262, Batch: 20, Loss: 0.03386131301522255\n",
      "Iteration 262, Batch: 21, Loss: 0.05782303586602211\n",
      "Iteration 262, Batch: 22, Loss: 0.058132294565439224\n",
      "Iteration 262, Batch: 23, Loss: 0.06304515898227692\n",
      "Iteration 262, Batch: 24, Loss: 0.05942148715257645\n",
      "Iteration 262, Batch: 25, Loss: 0.06568905711174011\n",
      "Iteration 262, Batch: 26, Loss: 0.045000892132520676\n",
      "Iteration 262, Batch: 27, Loss: 0.06623288989067078\n",
      "Iteration 262, Batch: 28, Loss: 0.05074814334511757\n",
      "Iteration 262, Batch: 29, Loss: 0.04901091009378433\n",
      "Iteration 262, Batch: 30, Loss: 0.049428001046180725\n",
      "Iteration 262, Batch: 31, Loss: 0.04233571141958237\n",
      "Iteration 262, Batch: 32, Loss: 0.06731760501861572\n",
      "Iteration 262, Batch: 33, Loss: 0.09328988939523697\n",
      "Iteration 262, Batch: 34, Loss: 0.07880827784538269\n",
      "Iteration 262, Batch: 35, Loss: 0.04861964285373688\n",
      "Iteration 262, Batch: 36, Loss: 0.06650907546281815\n",
      "Iteration 262, Batch: 37, Loss: 0.07443054765462875\n",
      "Iteration 262, Batch: 38, Loss: 0.06548469513654709\n",
      "Iteration 262, Batch: 39, Loss: 0.05539839714765549\n",
      "Iteration 262, Batch: 40, Loss: 0.0710267648100853\n",
      "Iteration 262, Batch: 41, Loss: 0.0477105975151062\n",
      "Iteration 262, Batch: 42, Loss: 0.05702361464500427\n",
      "Iteration 262, Batch: 43, Loss: 0.06439194828271866\n",
      "Iteration 262, Batch: 44, Loss: 0.058515701442956924\n",
      "Iteration 262, Batch: 45, Loss: 0.06782634556293488\n",
      "Iteration 262, Batch: 46, Loss: 0.05281643569469452\n",
      "Iteration 262, Batch: 47, Loss: 0.06794343888759613\n",
      "Iteration 262, Batch: 48, Loss: 0.07287251204252243\n",
      "Iteration 262, Batch: 49, Loss: 0.07325369864702225\n",
      "Iteration 263, Batch: 0, Loss: 0.0734405368566513\n",
      "Iteration 263, Batch: 1, Loss: 0.07409945130348206\n",
      "Iteration 263, Batch: 2, Loss: 0.07357814908027649\n",
      "Iteration 263, Batch: 3, Loss: 0.07281357795000076\n",
      "Iteration 263, Batch: 4, Loss: 0.06304941326379776\n",
      "Iteration 263, Batch: 5, Loss: 0.06888797879219055\n",
      "Iteration 263, Batch: 6, Loss: 0.04039619863033295\n",
      "Iteration 263, Batch: 7, Loss: 0.037633683532476425\n",
      "Iteration 263, Batch: 8, Loss: 0.053428180515766144\n",
      "Iteration 263, Batch: 9, Loss: 0.056263554841279984\n",
      "Iteration 263, Batch: 10, Loss: 0.04546437785029411\n",
      "Iteration 263, Batch: 11, Loss: 0.04213083162903786\n",
      "Iteration 263, Batch: 12, Loss: 0.051006220281124115\n",
      "Iteration 263, Batch: 13, Loss: 0.045694854110479355\n",
      "Iteration 263, Batch: 14, Loss: 0.054555680602788925\n",
      "Iteration 263, Batch: 15, Loss: 0.05168908089399338\n",
      "Iteration 263, Batch: 16, Loss: 0.0754164308309555\n",
      "Iteration 263, Batch: 17, Loss: 0.06973540782928467\n",
      "Iteration 263, Batch: 18, Loss: 0.051839932799339294\n",
      "Iteration 263, Batch: 19, Loss: 0.03039485774934292\n",
      "Iteration 263, Batch: 20, Loss: 0.04355262592434883\n",
      "Iteration 263, Batch: 21, Loss: 0.0384075865149498\n",
      "Iteration 263, Batch: 22, Loss: 0.06822125613689423\n",
      "Iteration 263, Batch: 23, Loss: 0.06001909077167511\n",
      "Iteration 263, Batch: 24, Loss: 0.036114487797021866\n",
      "Iteration 263, Batch: 25, Loss: 0.0415782630443573\n",
      "Iteration 263, Batch: 26, Loss: 0.0695795863866806\n",
      "Iteration 263, Batch: 27, Loss: 0.038710832595825195\n",
      "Iteration 263, Batch: 28, Loss: 0.026892613619565964\n",
      "Iteration 263, Batch: 29, Loss: 0.04077233374118805\n",
      "Iteration 263, Batch: 30, Loss: 0.06071854755282402\n",
      "Iteration 263, Batch: 31, Loss: 0.045702047646045685\n",
      "Iteration 263, Batch: 32, Loss: 0.027301408350467682\n",
      "Iteration 263, Batch: 33, Loss: 0.04081252962350845\n",
      "Iteration 263, Batch: 34, Loss: 0.06396785378456116\n",
      "Iteration 263, Batch: 35, Loss: 0.034628842025995255\n",
      "Iteration 263, Batch: 36, Loss: 0.055989332497119904\n",
      "Iteration 263, Batch: 37, Loss: 0.06440135836601257\n",
      "Iteration 263, Batch: 38, Loss: 0.08200351893901825\n",
      "Iteration 263, Batch: 39, Loss: 0.06823816895484924\n",
      "Iteration 263, Batch: 40, Loss: 0.050304096192121506\n",
      "Iteration 263, Batch: 41, Loss: 0.054133202880620956\n",
      "Iteration 263, Batch: 42, Loss: 0.029655789956450462\n",
      "Iteration 263, Batch: 43, Loss: 0.05739428848028183\n",
      "Iteration 263, Batch: 44, Loss: 0.056495025753974915\n",
      "Iteration 263, Batch: 45, Loss: 0.07294146716594696\n",
      "Iteration 263, Batch: 46, Loss: 0.07318498194217682\n",
      "Iteration 263, Batch: 47, Loss: 0.042466383427381516\n",
      "Iteration 263, Batch: 48, Loss: 0.05186299607157707\n",
      "Iteration 263, Batch: 49, Loss: 0.05578559637069702\n",
      "Iteration 264, Batch: 0, Loss: 0.05522529035806656\n",
      "Iteration 264, Batch: 1, Loss: 0.03464600443840027\n",
      "Iteration 264, Batch: 2, Loss: 0.04808425158262253\n",
      "Iteration 264, Batch: 3, Loss: 0.0598931685090065\n",
      "Iteration 264, Batch: 4, Loss: 0.060050684958696365\n",
      "Iteration 264, Batch: 5, Loss: 0.07358453422784805\n",
      "Iteration 264, Batch: 6, Loss: 0.0722760260105133\n",
      "Iteration 264, Batch: 7, Loss: 0.06467199325561523\n",
      "Iteration 264, Batch: 8, Loss: 0.04209095984697342\n",
      "Iteration 264, Batch: 9, Loss: 0.044679153710603714\n",
      "Iteration 264, Batch: 10, Loss: 0.04544699192047119\n",
      "Iteration 264, Batch: 11, Loss: 0.03191155940294266\n",
      "Iteration 264, Batch: 12, Loss: 0.06414373219013214\n",
      "Iteration 264, Batch: 13, Loss: 0.054311566054821014\n",
      "Iteration 264, Batch: 14, Loss: 0.05526883900165558\n",
      "Iteration 264, Batch: 15, Loss: 0.04522395506501198\n",
      "Iteration 264, Batch: 16, Loss: 0.05542021617293358\n",
      "Iteration 264, Batch: 17, Loss: 0.0489957220852375\n",
      "Iteration 264, Batch: 18, Loss: 0.06072128936648369\n",
      "Iteration 264, Batch: 19, Loss: 0.049374308437108994\n",
      "Iteration 264, Batch: 20, Loss: 0.046912502497434616\n",
      "Iteration 264, Batch: 21, Loss: 0.03319387510418892\n",
      "Iteration 264, Batch: 22, Loss: 0.04914114996790886\n",
      "Iteration 264, Batch: 23, Loss: 0.049403753131628036\n",
      "Iteration 264, Batch: 24, Loss: 0.06004104018211365\n",
      "Iteration 264, Batch: 25, Loss: 0.05651486665010452\n",
      "Iteration 264, Batch: 26, Loss: 0.08214515447616577\n",
      "Iteration 264, Batch: 27, Loss: 0.03870576247572899\n",
      "Iteration 264, Batch: 28, Loss: 0.04340283200144768\n",
      "Iteration 264, Batch: 29, Loss: 0.06958210468292236\n",
      "Iteration 264, Batch: 30, Loss: 0.04379170387983322\n",
      "Iteration 264, Batch: 31, Loss: 0.07426168024539948\n",
      "Iteration 264, Batch: 32, Loss: 0.07815535366535187\n",
      "Iteration 264, Batch: 33, Loss: 0.03721189871430397\n",
      "Iteration 264, Batch: 34, Loss: 0.04382341355085373\n",
      "Iteration 264, Batch: 35, Loss: 0.04228983819484711\n",
      "Iteration 264, Batch: 36, Loss: 0.0908547192811966\n",
      "Iteration 264, Batch: 37, Loss: 0.05190674215555191\n",
      "Iteration 264, Batch: 38, Loss: 0.04179539903998375\n",
      "Iteration 264, Batch: 39, Loss: 0.0634068176150322\n",
      "Iteration 264, Batch: 40, Loss: 0.06259550154209137\n",
      "Iteration 264, Batch: 41, Loss: 0.06050216406583786\n",
      "Iteration 264, Batch: 42, Loss: 0.05708591267466545\n",
      "Iteration 264, Batch: 43, Loss: 0.040665317326784134\n",
      "Iteration 264, Batch: 44, Loss: 0.05103708803653717\n",
      "Iteration 264, Batch: 45, Loss: 0.03368854895234108\n",
      "Iteration 264, Batch: 46, Loss: 0.06400776654481888\n",
      "Iteration 264, Batch: 47, Loss: 0.07548025995492935\n",
      "Iteration 264, Batch: 48, Loss: 0.08497115969657898\n",
      "Iteration 264, Batch: 49, Loss: 0.05450168624520302\n",
      "Iteration 265, Batch: 0, Loss: 0.0757487490773201\n",
      "Iteration 265, Batch: 1, Loss: 0.049478642642498016\n",
      "Iteration 265, Batch: 2, Loss: 0.0579691007733345\n",
      "Iteration 265, Batch: 3, Loss: 0.033953431993722916\n",
      "Iteration 265, Batch: 4, Loss: 0.052086275070905685\n",
      "Iteration 265, Batch: 5, Loss: 0.06345682591199875\n",
      "Iteration 265, Batch: 6, Loss: 0.0434839241206646\n",
      "Iteration 265, Batch: 7, Loss: 0.043092675507068634\n",
      "Iteration 265, Batch: 8, Loss: 0.07008637487888336\n",
      "Iteration 265, Batch: 9, Loss: 0.05875615030527115\n",
      "Iteration 265, Batch: 10, Loss: 0.052110642194747925\n",
      "Iteration 265, Batch: 11, Loss: 0.04246962070465088\n",
      "Iteration 265, Batch: 12, Loss: 0.04324783757328987\n",
      "Iteration 265, Batch: 13, Loss: 0.05941028520464897\n",
      "Iteration 265, Batch: 14, Loss: 0.05896203964948654\n",
      "Iteration 265, Batch: 15, Loss: 0.05184417590498924\n",
      "Iteration 265, Batch: 16, Loss: 0.06068302318453789\n",
      "Iteration 265, Batch: 17, Loss: 0.06817307323217392\n",
      "Iteration 265, Batch: 18, Loss: 0.043947525322437286\n",
      "Iteration 265, Batch: 19, Loss: 0.06545524299144745\n",
      "Iteration 265, Batch: 20, Loss: 0.05590929836034775\n",
      "Iteration 265, Batch: 21, Loss: 0.050525955855846405\n",
      "Iteration 265, Batch: 22, Loss: 0.04293755069375038\n",
      "Iteration 265, Batch: 23, Loss: 0.06478150933980942\n",
      "Iteration 265, Batch: 24, Loss: 0.05045115202665329\n",
      "Iteration 265, Batch: 25, Loss: 0.05122004821896553\n",
      "Iteration 265, Batch: 26, Loss: 0.06658413261175156\n",
      "Iteration 265, Batch: 27, Loss: 0.05210407078266144\n",
      "Iteration 265, Batch: 28, Loss: 0.04869181662797928\n",
      "Iteration 265, Batch: 29, Loss: 0.04322291165590286\n",
      "Iteration 265, Batch: 30, Loss: 0.0422547422349453\n",
      "Iteration 265, Batch: 31, Loss: 0.04122302308678627\n",
      "Iteration 265, Batch: 32, Loss: 0.02864670194685459\n",
      "Iteration 265, Batch: 33, Loss: 0.027051042765378952\n",
      "Iteration 265, Batch: 34, Loss: 0.04880799353122711\n",
      "Iteration 265, Batch: 35, Loss: 0.04297321289777756\n",
      "Iteration 265, Batch: 36, Loss: 0.05979132652282715\n",
      "Iteration 265, Batch: 37, Loss: 0.03522278741002083\n",
      "Iteration 265, Batch: 38, Loss: 0.053988952189683914\n",
      "Iteration 265, Batch: 39, Loss: 0.07703863084316254\n",
      "Iteration 265, Batch: 40, Loss: 0.03409634903073311\n",
      "Iteration 265, Batch: 41, Loss: 0.07463948428630829\n",
      "Iteration 265, Batch: 42, Loss: 0.0683118924498558\n",
      "Iteration 265, Batch: 43, Loss: 0.05822543054819107\n",
      "Iteration 265, Batch: 44, Loss: 0.0781843438744545\n",
      "Iteration 265, Batch: 45, Loss: 0.06156790629029274\n",
      "Iteration 265, Batch: 46, Loss: 0.04487122595310211\n",
      "Iteration 265, Batch: 47, Loss: 0.038107629865407944\n",
      "Iteration 265, Batch: 48, Loss: 0.05560341849923134\n",
      "Iteration 265, Batch: 49, Loss: 0.06456197798252106\n",
      "Iteration 266, Batch: 0, Loss: 0.043910350650548935\n",
      "Iteration 266, Batch: 1, Loss: 0.05166075751185417\n",
      "Iteration 266, Batch: 2, Loss: 0.0660625696182251\n",
      "Iteration 266, Batch: 3, Loss: 0.05029623955488205\n",
      "Iteration 266, Batch: 4, Loss: 0.05980074778199196\n",
      "Iteration 266, Batch: 5, Loss: 0.08185962587594986\n",
      "Iteration 266, Batch: 6, Loss: 0.05196891352534294\n",
      "Iteration 266, Batch: 7, Loss: 0.05721018835902214\n",
      "Iteration 266, Batch: 8, Loss: 0.031234992668032646\n",
      "Iteration 266, Batch: 9, Loss: 0.08026494830846786\n",
      "Iteration 266, Batch: 10, Loss: 0.054925497621297836\n",
      "Iteration 266, Batch: 11, Loss: 0.03540566563606262\n",
      "Iteration 266, Batch: 12, Loss: 0.05326482281088829\n",
      "Iteration 266, Batch: 13, Loss: 0.05824517458677292\n",
      "Iteration 266, Batch: 14, Loss: 0.04543610289692879\n",
      "Iteration 266, Batch: 15, Loss: 0.0777202695608139\n",
      "Iteration 266, Batch: 16, Loss: 0.0659860372543335\n",
      "Iteration 266, Batch: 17, Loss: 0.05965535342693329\n",
      "Iteration 266, Batch: 18, Loss: 0.0607241615653038\n",
      "Iteration 266, Batch: 19, Loss: 0.026410674676299095\n",
      "Iteration 266, Batch: 20, Loss: 0.06910309940576553\n",
      "Iteration 266, Batch: 21, Loss: 0.04513924568891525\n",
      "Iteration 266, Batch: 22, Loss: 0.04071109741926193\n",
      "Iteration 266, Batch: 23, Loss: 0.05895588919520378\n",
      "Iteration 266, Batch: 24, Loss: 0.040467094630002975\n",
      "Iteration 266, Batch: 25, Loss: 0.053633756935596466\n",
      "Iteration 266, Batch: 26, Loss: 0.07910113036632538\n",
      "Iteration 266, Batch: 27, Loss: 0.03484857827425003\n",
      "Iteration 266, Batch: 28, Loss: 0.05673407390713692\n",
      "Iteration 266, Batch: 29, Loss: 0.054438311606645584\n",
      "Iteration 266, Batch: 30, Loss: 0.04373637214303017\n",
      "Iteration 266, Batch: 31, Loss: 0.04820739105343819\n",
      "Iteration 266, Batch: 32, Loss: 0.05540375038981438\n",
      "Iteration 266, Batch: 33, Loss: 0.062001485377550125\n",
      "Iteration 266, Batch: 34, Loss: 0.04526999220252037\n",
      "Iteration 266, Batch: 35, Loss: 0.0572395846247673\n",
      "Iteration 266, Batch: 36, Loss: 0.0581408254802227\n",
      "Iteration 266, Batch: 37, Loss: 0.03258603811264038\n",
      "Iteration 266, Batch: 38, Loss: 0.0340016633272171\n",
      "Iteration 266, Batch: 39, Loss: 0.0370447039604187\n",
      "Iteration 266, Batch: 40, Loss: 0.05220725014805794\n",
      "Iteration 266, Batch: 41, Loss: 0.056027602404356\n",
      "Iteration 266, Batch: 42, Loss: 0.03352831304073334\n",
      "Iteration 266, Batch: 43, Loss: 0.04082530736923218\n",
      "Iteration 266, Batch: 44, Loss: 0.053050220012664795\n",
      "Iteration 266, Batch: 45, Loss: 0.03868424519896507\n",
      "Iteration 266, Batch: 46, Loss: 0.05742698535323143\n",
      "Iteration 266, Batch: 47, Loss: 0.053155459463596344\n",
      "Iteration 266, Batch: 48, Loss: 0.08216147869825363\n",
      "Iteration 266, Batch: 49, Loss: 0.05117849260568619\n",
      "Iteration 267, Batch: 0, Loss: 0.05893260985612869\n",
      "Iteration 267, Batch: 1, Loss: 0.06192625313997269\n",
      "Iteration 267, Batch: 2, Loss: 0.04515128582715988\n",
      "Iteration 267, Batch: 3, Loss: 0.053976237773895264\n",
      "Iteration 267, Batch: 4, Loss: 0.06830663233995438\n",
      "Iteration 267, Batch: 5, Loss: 0.0582774356007576\n",
      "Iteration 267, Batch: 6, Loss: 0.05305540934205055\n",
      "Iteration 267, Batch: 7, Loss: 0.0736125260591507\n",
      "Iteration 267, Batch: 8, Loss: 0.054721660912036896\n",
      "Iteration 267, Batch: 9, Loss: 0.02956579439342022\n",
      "Iteration 267, Batch: 10, Loss: 0.04965677484869957\n",
      "Iteration 267, Batch: 11, Loss: 0.06000262126326561\n",
      "Iteration 267, Batch: 12, Loss: 0.06793432682752609\n",
      "Iteration 267, Batch: 13, Loss: 0.07516119629144669\n",
      "Iteration 267, Batch: 14, Loss: 0.07036751508712769\n",
      "Iteration 267, Batch: 15, Loss: 0.043542224913835526\n",
      "Iteration 267, Batch: 16, Loss: 0.036483004689216614\n",
      "Iteration 267, Batch: 17, Loss: 0.03580275550484657\n",
      "Iteration 267, Batch: 18, Loss: 0.059195954352617264\n",
      "Iteration 267, Batch: 19, Loss: 0.060936231166124344\n",
      "Iteration 267, Batch: 20, Loss: 0.06385505944490433\n",
      "Iteration 267, Batch: 21, Loss: 0.05855467543005943\n",
      "Iteration 267, Batch: 22, Loss: 0.043802566826343536\n",
      "Iteration 267, Batch: 23, Loss: 0.0506637841463089\n",
      "Iteration 267, Batch: 24, Loss: 0.10102242976427078\n",
      "Iteration 267, Batch: 25, Loss: 0.07443313300609589\n",
      "Iteration 267, Batch: 26, Loss: 0.066846564412117\n",
      "Iteration 267, Batch: 27, Loss: 0.07455562055110931\n",
      "Iteration 267, Batch: 28, Loss: 0.05115986615419388\n",
      "Iteration 267, Batch: 29, Loss: 0.04853292927145958\n",
      "Iteration 267, Batch: 30, Loss: 0.046043626964092255\n",
      "Iteration 267, Batch: 31, Loss: 0.043003879487514496\n",
      "Iteration 267, Batch: 32, Loss: 0.0511854887008667\n",
      "Iteration 267, Batch: 33, Loss: 0.04132300242781639\n",
      "Iteration 267, Batch: 34, Loss: 0.052363332360982895\n",
      "Iteration 267, Batch: 35, Loss: 0.05666901916265488\n",
      "Iteration 267, Batch: 36, Loss: 0.06527187675237656\n",
      "Iteration 267, Batch: 37, Loss: 0.04226020351052284\n",
      "Iteration 267, Batch: 38, Loss: 0.0753370150923729\n",
      "Iteration 267, Batch: 39, Loss: 0.04396499693393707\n",
      "Iteration 267, Batch: 40, Loss: 0.06413689255714417\n",
      "Iteration 267, Batch: 41, Loss: 0.06251314282417297\n",
      "Iteration 267, Batch: 42, Loss: 0.06663808971643448\n",
      "Iteration 267, Batch: 43, Loss: 0.0373837985098362\n",
      "Iteration 267, Batch: 44, Loss: 0.048899997025728226\n",
      "Iteration 267, Batch: 45, Loss: 0.051840901374816895\n",
      "Iteration 267, Batch: 46, Loss: 0.055201996117830276\n",
      "Iteration 267, Batch: 47, Loss: 0.057656142860651016\n",
      "Iteration 267, Batch: 48, Loss: 0.07242792099714279\n",
      "Iteration 267, Batch: 49, Loss: 0.03584335371851921\n",
      "Iteration 268, Batch: 0, Loss: 0.04512767866253853\n",
      "Iteration 268, Batch: 1, Loss: 0.04580366611480713\n",
      "Iteration 268, Batch: 2, Loss: 0.05917651951313019\n",
      "Iteration 268, Batch: 3, Loss: 0.04180111736059189\n",
      "Iteration 268, Batch: 4, Loss: 0.0661117359995842\n",
      "Iteration 268, Batch: 5, Loss: 0.046838030219078064\n",
      "Iteration 268, Batch: 6, Loss: 0.049380648881196976\n",
      "Iteration 268, Batch: 7, Loss: 0.05537419021129608\n",
      "Iteration 268, Batch: 8, Loss: 0.05009457468986511\n",
      "Iteration 268, Batch: 9, Loss: 0.05393754690885544\n",
      "Iteration 268, Batch: 10, Loss: 0.0738765299320221\n",
      "Iteration 268, Batch: 11, Loss: 0.06880747526884079\n",
      "Iteration 268, Batch: 12, Loss: 0.06149687245488167\n",
      "Iteration 268, Batch: 13, Loss: 0.05864780768752098\n",
      "Iteration 268, Batch: 14, Loss: 0.04220596328377724\n",
      "Iteration 268, Batch: 15, Loss: 0.04937383532524109\n",
      "Iteration 268, Batch: 16, Loss: 0.057050835341215134\n",
      "Iteration 268, Batch: 17, Loss: 0.030237771570682526\n",
      "Iteration 268, Batch: 18, Loss: 0.05121784657239914\n",
      "Iteration 268, Batch: 19, Loss: 0.04501927271485329\n",
      "Iteration 268, Batch: 20, Loss: 0.059924859553575516\n",
      "Iteration 268, Batch: 21, Loss: 0.05913498252630234\n",
      "Iteration 268, Batch: 22, Loss: 0.04413170367479324\n",
      "Iteration 268, Batch: 23, Loss: 0.07050752639770508\n",
      "Iteration 268, Batch: 24, Loss: 0.029526976868510246\n",
      "Iteration 268, Batch: 25, Loss: 0.05427476763725281\n",
      "Iteration 268, Batch: 26, Loss: 0.06679186969995499\n",
      "Iteration 268, Batch: 27, Loss: 0.0824253037571907\n",
      "Iteration 268, Batch: 28, Loss: 0.04604166001081467\n",
      "Iteration 268, Batch: 29, Loss: 0.062321737408638\n",
      "Iteration 268, Batch: 30, Loss: 0.08415206521749496\n",
      "Iteration 268, Batch: 31, Loss: 0.046801112592220306\n",
      "Iteration 268, Batch: 32, Loss: 0.05681927129626274\n",
      "Iteration 268, Batch: 33, Loss: 0.06180884316563606\n",
      "Iteration 268, Batch: 34, Loss: 0.05446039140224457\n",
      "Iteration 268, Batch: 35, Loss: 0.05317157506942749\n",
      "Iteration 268, Batch: 36, Loss: 0.05102062225341797\n",
      "Iteration 268, Batch: 37, Loss: 0.0356697216629982\n",
      "Iteration 268, Batch: 38, Loss: 0.049022018909454346\n",
      "Iteration 268, Batch: 39, Loss: 0.04359465092420578\n",
      "Iteration 268, Batch: 40, Loss: 0.04389476776123047\n",
      "Iteration 268, Batch: 41, Loss: 0.06461378931999207\n",
      "Iteration 268, Batch: 42, Loss: 0.04218147322535515\n",
      "Iteration 268, Batch: 43, Loss: 0.058125656098127365\n",
      "Iteration 268, Batch: 44, Loss: 0.04101190343499184\n",
      "Iteration 268, Batch: 45, Loss: 0.04659038409590721\n",
      "Iteration 268, Batch: 46, Loss: 0.07536549121141434\n",
      "Iteration 268, Batch: 47, Loss: 0.04899364337325096\n",
      "Iteration 268, Batch: 48, Loss: 0.04971928521990776\n",
      "Iteration 268, Batch: 49, Loss: 0.05088653415441513\n",
      "Iteration 269, Batch: 0, Loss: 0.0570792481303215\n",
      "Iteration 269, Batch: 1, Loss: 0.06091432273387909\n",
      "Iteration 269, Batch: 2, Loss: 0.06682579219341278\n",
      "Iteration 269, Batch: 3, Loss: 0.04830062761902809\n",
      "Iteration 269, Batch: 4, Loss: 0.06641718745231628\n",
      "Iteration 269, Batch: 5, Loss: 0.05732760205864906\n",
      "Iteration 269, Batch: 6, Loss: 0.05635341629385948\n",
      "Iteration 269, Batch: 7, Loss: 0.04191814363002777\n",
      "Iteration 269, Batch: 8, Loss: 0.061411213129758835\n",
      "Iteration 269, Batch: 9, Loss: 0.032635658979415894\n",
      "Iteration 269, Batch: 10, Loss: 0.03998779132962227\n",
      "Iteration 269, Batch: 11, Loss: 0.045267991721630096\n",
      "Iteration 269, Batch: 12, Loss: 0.04812736436724663\n",
      "Iteration 269, Batch: 13, Loss: 0.05454854667186737\n",
      "Iteration 269, Batch: 14, Loss: 0.05710817128419876\n",
      "Iteration 269, Batch: 15, Loss: 0.059880245476961136\n",
      "Iteration 269, Batch: 16, Loss: 0.07229002565145493\n",
      "Iteration 269, Batch: 17, Loss: 0.08112239837646484\n",
      "Iteration 269, Batch: 18, Loss: 0.05674370005726814\n",
      "Iteration 269, Batch: 19, Loss: 0.05255436897277832\n",
      "Iteration 269, Batch: 20, Loss: 0.04780742526054382\n",
      "Iteration 269, Batch: 21, Loss: 0.042103491723537445\n",
      "Iteration 269, Batch: 22, Loss: 0.053485676646232605\n",
      "Iteration 269, Batch: 23, Loss: 0.05109288543462753\n",
      "Iteration 269, Batch: 24, Loss: 0.06576154381036758\n",
      "Iteration 269, Batch: 25, Loss: 0.041138600558042526\n",
      "Iteration 269, Batch: 26, Loss: 0.026441415771842003\n",
      "Iteration 269, Batch: 27, Loss: 0.036926522850990295\n",
      "Iteration 269, Batch: 28, Loss: 0.06903378665447235\n",
      "Iteration 269, Batch: 29, Loss: 0.036683447659015656\n",
      "Iteration 269, Batch: 30, Loss: 0.050536200404167175\n",
      "Iteration 269, Batch: 31, Loss: 0.049661118537187576\n",
      "Iteration 269, Batch: 32, Loss: 0.05599497631192207\n",
      "Iteration 269, Batch: 33, Loss: 0.052219703793525696\n",
      "Iteration 269, Batch: 34, Loss: 0.091699980199337\n",
      "Iteration 269, Batch: 35, Loss: 0.054678935557603836\n",
      "Iteration 269, Batch: 36, Loss: 0.07028146088123322\n",
      "Iteration 269, Batch: 37, Loss: 0.06001129001379013\n",
      "Iteration 269, Batch: 38, Loss: 0.028692876920104027\n",
      "Iteration 269, Batch: 39, Loss: 0.050461120903491974\n",
      "Iteration 269, Batch: 40, Loss: 0.039254799485206604\n",
      "Iteration 269, Batch: 41, Loss: 0.07571487873792648\n",
      "Iteration 269, Batch: 42, Loss: 0.08878173679113388\n",
      "Iteration 269, Batch: 43, Loss: 0.06219789758324623\n",
      "Iteration 269, Batch: 44, Loss: 0.055129602551460266\n",
      "Iteration 269, Batch: 45, Loss: 0.05816207453608513\n",
      "Iteration 269, Batch: 46, Loss: 0.03145400062203407\n",
      "Iteration 269, Batch: 47, Loss: 0.03176870569586754\n",
      "Iteration 269, Batch: 48, Loss: 0.04774918779730797\n",
      "Iteration 269, Batch: 49, Loss: 0.032509054988622665\n",
      "Iteration 270, Batch: 0, Loss: 0.055893249809741974\n",
      "Iteration 270, Batch: 1, Loss: 0.0470820888876915\n",
      "Iteration 270, Batch: 2, Loss: 0.05625734105706215\n",
      "Iteration 270, Batch: 3, Loss: 0.054236553609371185\n",
      "Iteration 270, Batch: 4, Loss: 0.04424571990966797\n",
      "Iteration 270, Batch: 5, Loss: 0.04544217139482498\n",
      "Iteration 270, Batch: 6, Loss: 0.07151501625776291\n",
      "Iteration 270, Batch: 7, Loss: 0.045352399349212646\n",
      "Iteration 270, Batch: 8, Loss: 0.06072123348712921\n",
      "Iteration 270, Batch: 9, Loss: 0.05574518069624901\n",
      "Iteration 270, Batch: 10, Loss: 0.03584922105073929\n",
      "Iteration 270, Batch: 11, Loss: 0.0372462160885334\n",
      "Iteration 270, Batch: 12, Loss: 0.046775177121162415\n",
      "Iteration 270, Batch: 13, Loss: 0.03431897982954979\n",
      "Iteration 270, Batch: 14, Loss: 0.042367495596408844\n",
      "Iteration 270, Batch: 15, Loss: 0.06606052070856094\n",
      "Iteration 270, Batch: 16, Loss: 0.050971660763025284\n",
      "Iteration 270, Batch: 17, Loss: 0.05368805304169655\n",
      "Iteration 270, Batch: 18, Loss: 0.04432862251996994\n",
      "Iteration 270, Batch: 19, Loss: 0.050907786935567856\n",
      "Iteration 270, Batch: 20, Loss: 0.04891159012913704\n",
      "Iteration 270, Batch: 21, Loss: 0.07122152298688889\n",
      "Iteration 270, Batch: 22, Loss: 0.0804135873913765\n",
      "Iteration 270, Batch: 23, Loss: 0.05470643565058708\n",
      "Iteration 270, Batch: 24, Loss: 0.05464281141757965\n",
      "Iteration 270, Batch: 25, Loss: 0.03971422463655472\n",
      "Iteration 270, Batch: 26, Loss: 0.06599531322717667\n",
      "Iteration 270, Batch: 27, Loss: 0.0654834434390068\n",
      "Iteration 270, Batch: 28, Loss: 0.05829082056879997\n",
      "Iteration 270, Batch: 29, Loss: 0.044476527720689774\n",
      "Iteration 270, Batch: 30, Loss: 0.046929020434617996\n",
      "Iteration 270, Batch: 31, Loss: 0.027866847813129425\n",
      "Iteration 270, Batch: 32, Loss: 0.028399204835295677\n",
      "Iteration 270, Batch: 33, Loss: 0.054847802966833115\n",
      "Iteration 270, Batch: 34, Loss: 0.041531648486852646\n",
      "Iteration 270, Batch: 35, Loss: 0.05548539012670517\n",
      "Iteration 270, Batch: 36, Loss: 0.07655060291290283\n",
      "Iteration 270, Batch: 37, Loss: 0.056672703474760056\n",
      "Iteration 270, Batch: 38, Loss: 0.04290725663304329\n",
      "Iteration 270, Batch: 39, Loss: 0.04527386650443077\n",
      "Iteration 270, Batch: 40, Loss: 0.026314139366149902\n",
      "Iteration 270, Batch: 41, Loss: 0.04034217819571495\n",
      "Iteration 270, Batch: 42, Loss: 0.03718721866607666\n",
      "Iteration 270, Batch: 43, Loss: 0.05387722700834274\n",
      "Iteration 270, Batch: 44, Loss: 0.0684048980474472\n",
      "Iteration 270, Batch: 45, Loss: 0.05378066003322601\n",
      "Iteration 270, Batch: 46, Loss: 0.04779309034347534\n",
      "Iteration 270, Batch: 47, Loss: 0.0606238916516304\n",
      "Iteration 270, Batch: 48, Loss: 0.05266110226511955\n",
      "Iteration 270, Batch: 49, Loss: 0.07061794400215149\n",
      "Iteration 271, Batch: 0, Loss: 0.050316065549850464\n",
      "Iteration 271, Batch: 1, Loss: 0.049932707101106644\n",
      "Iteration 271, Batch: 2, Loss: 0.06872852146625519\n",
      "Iteration 271, Batch: 3, Loss: 0.0427522212266922\n",
      "Iteration 271, Batch: 4, Loss: 0.05219452828168869\n",
      "Iteration 271, Batch: 5, Loss: 0.04310952126979828\n",
      "Iteration 271, Batch: 6, Loss: 0.0318945050239563\n",
      "Iteration 271, Batch: 7, Loss: 0.06745032966136932\n",
      "Iteration 271, Batch: 8, Loss: 0.05047129839658737\n",
      "Iteration 271, Batch: 9, Loss: 0.05286024138331413\n",
      "Iteration 271, Batch: 10, Loss: 0.041047241538763046\n",
      "Iteration 271, Batch: 11, Loss: 0.05255572125315666\n",
      "Iteration 271, Batch: 12, Loss: 0.028123388066887856\n",
      "Iteration 271, Batch: 13, Loss: 0.050359029322862625\n",
      "Iteration 271, Batch: 14, Loss: 0.05245686322450638\n",
      "Iteration 271, Batch: 15, Loss: 0.042541734874248505\n",
      "Iteration 271, Batch: 16, Loss: 0.06609635055065155\n",
      "Iteration 271, Batch: 17, Loss: 0.03375560790300369\n",
      "Iteration 271, Batch: 18, Loss: 0.04104965552687645\n",
      "Iteration 271, Batch: 19, Loss: 0.02807743288576603\n",
      "Iteration 271, Batch: 20, Loss: 0.05254651978611946\n",
      "Iteration 271, Batch: 21, Loss: 0.039930395781993866\n",
      "Iteration 271, Batch: 22, Loss: 0.029525574296712875\n",
      "Iteration 271, Batch: 23, Loss: 0.035387855023145676\n",
      "Iteration 271, Batch: 24, Loss: 0.03266945481300354\n",
      "Iteration 271, Batch: 25, Loss: 0.0704314187169075\n",
      "Iteration 271, Batch: 26, Loss: 0.04066930338740349\n",
      "Iteration 271, Batch: 27, Loss: 0.039103053510189056\n",
      "Iteration 271, Batch: 28, Loss: 0.06531713902950287\n",
      "Iteration 271, Batch: 29, Loss: 0.05824640020728111\n",
      "Iteration 271, Batch: 30, Loss: 0.0690314918756485\n",
      "Iteration 271, Batch: 31, Loss: 0.06608820706605911\n",
      "Iteration 271, Batch: 32, Loss: 0.06777399778366089\n",
      "Iteration 271, Batch: 33, Loss: 0.0588526614010334\n",
      "Iteration 271, Batch: 34, Loss: 0.05160960182547569\n",
      "Iteration 271, Batch: 35, Loss: 0.04535635933279991\n",
      "Iteration 271, Batch: 36, Loss: 0.027230452746152878\n",
      "Iteration 271, Batch: 37, Loss: 0.049293339252471924\n",
      "Iteration 271, Batch: 38, Loss: 0.05301690846681595\n",
      "Iteration 271, Batch: 39, Loss: 0.04854103922843933\n",
      "Iteration 271, Batch: 40, Loss: 0.04560015723109245\n",
      "Iteration 271, Batch: 41, Loss: 0.05618862807750702\n",
      "Iteration 271, Batch: 42, Loss: 0.0615004226565361\n",
      "Iteration 271, Batch: 43, Loss: 0.06331349909305573\n",
      "Iteration 271, Batch: 44, Loss: 0.05611946061253548\n",
      "Iteration 271, Batch: 45, Loss: 0.040103692561388016\n",
      "Iteration 271, Batch: 46, Loss: 0.08351313322782516\n",
      "Iteration 271, Batch: 47, Loss: 0.05563132464885712\n",
      "Iteration 271, Batch: 48, Loss: 0.06316182762384415\n",
      "Iteration 271, Batch: 49, Loss: 0.05568511784076691\n",
      "Iteration 272, Batch: 0, Loss: 0.07381124049425125\n",
      "Iteration 272, Batch: 1, Loss: 0.03747664764523506\n",
      "Iteration 272, Batch: 2, Loss: 0.04094225913286209\n",
      "Iteration 272, Batch: 3, Loss: 0.052810437977313995\n",
      "Iteration 272, Batch: 4, Loss: 0.03767473250627518\n",
      "Iteration 272, Batch: 5, Loss: 0.05768448859453201\n",
      "Iteration 272, Batch: 6, Loss: 0.0760192945599556\n",
      "Iteration 272, Batch: 7, Loss: 0.053132906556129456\n",
      "Iteration 272, Batch: 8, Loss: 0.04554301127791405\n",
      "Iteration 272, Batch: 9, Loss: 0.047860536724328995\n",
      "Iteration 272, Batch: 10, Loss: 0.08997271209955215\n",
      "Iteration 272, Batch: 11, Loss: 0.03356938809156418\n",
      "Iteration 272, Batch: 12, Loss: 0.03760431706905365\n",
      "Iteration 272, Batch: 13, Loss: 0.05079790577292442\n",
      "Iteration 272, Batch: 14, Loss: 0.04925280436873436\n",
      "Iteration 272, Batch: 15, Loss: 0.030603760853409767\n",
      "Iteration 272, Batch: 16, Loss: 0.05817939713597298\n",
      "Iteration 272, Batch: 17, Loss: 0.040796540677547455\n",
      "Iteration 272, Batch: 18, Loss: 0.049671925604343414\n",
      "Iteration 272, Batch: 19, Loss: 0.04495755210518837\n",
      "Iteration 272, Batch: 20, Loss: 0.07397393882274628\n",
      "Iteration 272, Batch: 21, Loss: 0.03895770385861397\n",
      "Iteration 272, Batch: 22, Loss: 0.03376008942723274\n",
      "Iteration 272, Batch: 23, Loss: 0.039063211530447006\n",
      "Iteration 272, Batch: 24, Loss: 0.07428579032421112\n",
      "Iteration 272, Batch: 25, Loss: 0.04492945969104767\n",
      "Iteration 272, Batch: 26, Loss: 0.05512034147977829\n",
      "Iteration 272, Batch: 27, Loss: 0.03986283764243126\n",
      "Iteration 272, Batch: 28, Loss: 0.030752109363675117\n",
      "Iteration 272, Batch: 29, Loss: 0.039783649146556854\n",
      "Iteration 272, Batch: 30, Loss: 0.026731329038739204\n",
      "Iteration 272, Batch: 31, Loss: 0.065506212413311\n",
      "Iteration 272, Batch: 32, Loss: 0.054576072841882706\n",
      "Iteration 272, Batch: 33, Loss: 0.04851510748267174\n",
      "Iteration 272, Batch: 34, Loss: 0.032109182327985764\n",
      "Iteration 272, Batch: 35, Loss: 0.03897419944405556\n",
      "Iteration 272, Batch: 36, Loss: 0.08845742791891098\n",
      "Iteration 272, Batch: 37, Loss: 0.0576094314455986\n",
      "Iteration 272, Batch: 38, Loss: 0.05801043286919594\n",
      "Iteration 272, Batch: 39, Loss: 0.05845263972878456\n",
      "Iteration 272, Batch: 40, Loss: 0.0619637556374073\n",
      "Iteration 272, Batch: 41, Loss: 0.07210203260183334\n",
      "Iteration 272, Batch: 42, Loss: 0.06535334140062332\n",
      "Iteration 272, Batch: 43, Loss: 0.046849969774484634\n",
      "Iteration 272, Batch: 44, Loss: 0.06959236413240433\n",
      "Iteration 272, Batch: 45, Loss: 0.054362859576940536\n",
      "Iteration 272, Batch: 46, Loss: 0.04883680120110512\n",
      "Iteration 272, Batch: 47, Loss: 0.08945224434137344\n",
      "Iteration 272, Batch: 48, Loss: 0.07474030554294586\n",
      "Iteration 272, Batch: 49, Loss: 0.06065991148352623\n",
      "Iteration 273, Batch: 0, Loss: 0.07598322629928589\n",
      "Iteration 273, Batch: 1, Loss: 0.08369893580675125\n",
      "Iteration 273, Batch: 2, Loss: 0.061128657311201096\n",
      "Iteration 273, Batch: 3, Loss: 0.042234666645526886\n",
      "Iteration 273, Batch: 4, Loss: 0.04478013515472412\n",
      "Iteration 273, Batch: 5, Loss: 0.058877814561128616\n",
      "Iteration 273, Batch: 6, Loss: 0.07693155109882355\n",
      "Iteration 273, Batch: 7, Loss: 0.07556173205375671\n",
      "Iteration 273, Batch: 8, Loss: 0.05038372799754143\n",
      "Iteration 273, Batch: 9, Loss: 0.05536944791674614\n",
      "Iteration 273, Batch: 10, Loss: 0.05806310102343559\n",
      "Iteration 273, Batch: 11, Loss: 0.08818197250366211\n",
      "Iteration 273, Batch: 12, Loss: 0.050936009734869\n",
      "Iteration 273, Batch: 13, Loss: 0.03705614432692528\n",
      "Iteration 273, Batch: 14, Loss: 0.05629479140043259\n",
      "Iteration 273, Batch: 15, Loss: 0.03357911854982376\n",
      "Iteration 273, Batch: 16, Loss: 0.0449872724711895\n",
      "Iteration 273, Batch: 17, Loss: 0.04767870157957077\n",
      "Iteration 273, Batch: 18, Loss: 0.04238233342766762\n",
      "Iteration 273, Batch: 19, Loss: 0.09110382199287415\n",
      "Iteration 273, Batch: 20, Loss: 0.024599146097898483\n",
      "Iteration 273, Batch: 21, Loss: 0.040461741387844086\n",
      "Iteration 273, Batch: 22, Loss: 0.058581527322530746\n",
      "Iteration 273, Batch: 23, Loss: 0.039389386773109436\n",
      "Iteration 273, Batch: 24, Loss: 0.04321789741516113\n",
      "Iteration 273, Batch: 25, Loss: 0.06863396614789963\n",
      "Iteration 273, Batch: 26, Loss: 0.057307351380586624\n",
      "Iteration 273, Batch: 27, Loss: 0.024916082620620728\n",
      "Iteration 273, Batch: 28, Loss: 0.04231398552656174\n",
      "Iteration 273, Batch: 29, Loss: 0.054639749228954315\n",
      "Iteration 273, Batch: 30, Loss: 0.03011331334710121\n",
      "Iteration 273, Batch: 31, Loss: 0.04316361993551254\n",
      "Iteration 273, Batch: 32, Loss: 0.06485610455274582\n",
      "Iteration 273, Batch: 33, Loss: 0.07666714489459991\n",
      "Iteration 273, Batch: 34, Loss: 0.05021204054355621\n",
      "Iteration 273, Batch: 35, Loss: 0.04483992978930473\n",
      "Iteration 273, Batch: 36, Loss: 0.051618173718452454\n",
      "Iteration 273, Batch: 37, Loss: 0.05893581360578537\n",
      "Iteration 273, Batch: 38, Loss: 0.04640699923038483\n",
      "Iteration 273, Batch: 39, Loss: 0.04654174670577049\n",
      "Iteration 273, Batch: 40, Loss: 0.04843496158719063\n",
      "Iteration 273, Batch: 41, Loss: 0.02163361757993698\n",
      "Iteration 273, Batch: 42, Loss: 0.035374898463487625\n",
      "Iteration 273, Batch: 43, Loss: 0.05391905829310417\n",
      "Iteration 273, Batch: 44, Loss: 0.03360401466488838\n",
      "Iteration 273, Batch: 45, Loss: 0.04794444516301155\n",
      "Iteration 273, Batch: 46, Loss: 0.0395478680729866\n",
      "Iteration 273, Batch: 47, Loss: 0.039255283772945404\n",
      "Iteration 273, Batch: 48, Loss: 0.045196618884801865\n",
      "Iteration 273, Batch: 49, Loss: 0.038778651505708694\n",
      "Iteration 274, Batch: 0, Loss: 0.06307041645050049\n",
      "Iteration 274, Batch: 1, Loss: 0.05838041007518768\n",
      "Iteration 274, Batch: 2, Loss: 0.036637820303440094\n",
      "Iteration 274, Batch: 3, Loss: 0.035211410373449326\n",
      "Iteration 274, Batch: 4, Loss: 0.035606563091278076\n",
      "Iteration 274, Batch: 5, Loss: 0.05318262428045273\n",
      "Iteration 274, Batch: 6, Loss: 0.04502521827816963\n",
      "Iteration 274, Batch: 7, Loss: 0.04012630879878998\n",
      "Iteration 274, Batch: 8, Loss: 0.07857905328273773\n",
      "Iteration 274, Batch: 9, Loss: 0.07715483754873276\n",
      "Iteration 274, Batch: 10, Loss: 0.04574523866176605\n",
      "Iteration 274, Batch: 11, Loss: 0.05700626224279404\n",
      "Iteration 274, Batch: 12, Loss: 0.04500192403793335\n",
      "Iteration 274, Batch: 13, Loss: 0.055612318217754364\n",
      "Iteration 274, Batch: 14, Loss: 0.02996424213051796\n",
      "Iteration 274, Batch: 15, Loss: 0.03254099190235138\n",
      "Iteration 274, Batch: 16, Loss: 0.03792364150285721\n",
      "Iteration 274, Batch: 17, Loss: 0.054820265620946884\n",
      "Iteration 274, Batch: 18, Loss: 0.030182501301169395\n",
      "Iteration 274, Batch: 19, Loss: 0.038919318467378616\n",
      "Iteration 274, Batch: 20, Loss: 0.038231417536735535\n",
      "Iteration 274, Batch: 21, Loss: 0.03838859125971794\n",
      "Iteration 274, Batch: 22, Loss: 0.0431455597281456\n",
      "Iteration 274, Batch: 23, Loss: 0.06666520237922668\n",
      "Iteration 274, Batch: 24, Loss: 0.05677502974867821\n",
      "Iteration 274, Batch: 25, Loss: 0.01969534158706665\n",
      "Iteration 274, Batch: 26, Loss: 0.0416574664413929\n",
      "Iteration 274, Batch: 27, Loss: 0.041566140949726105\n",
      "Iteration 274, Batch: 28, Loss: 0.04042860120534897\n",
      "Iteration 274, Batch: 29, Loss: 0.05353468284010887\n",
      "Iteration 274, Batch: 30, Loss: 0.05579862371087074\n",
      "Iteration 274, Batch: 31, Loss: 0.06168371066451073\n",
      "Iteration 274, Batch: 32, Loss: 0.04116575047373772\n",
      "Iteration 274, Batch: 33, Loss: 0.0498686209321022\n",
      "Iteration 274, Batch: 34, Loss: 0.02373095043003559\n",
      "Iteration 274, Batch: 35, Loss: 0.04044072702527046\n",
      "Iteration 274, Batch: 36, Loss: 0.03989695757627487\n",
      "Iteration 274, Batch: 37, Loss: 0.04503495618700981\n",
      "Iteration 274, Batch: 38, Loss: 0.07613971829414368\n",
      "Iteration 274, Batch: 39, Loss: 0.053223617374897\n",
      "Iteration 274, Batch: 40, Loss: 0.041832245886325836\n",
      "Iteration 274, Batch: 41, Loss: 0.04978960379958153\n",
      "Iteration 274, Batch: 42, Loss: 0.026972787454724312\n",
      "Iteration 274, Batch: 43, Loss: 0.03004361130297184\n",
      "Iteration 274, Batch: 44, Loss: 0.052608609199523926\n",
      "Iteration 274, Batch: 45, Loss: 0.06152504310011864\n",
      "Iteration 274, Batch: 46, Loss: 0.02866014651954174\n",
      "Iteration 274, Batch: 47, Loss: 0.021987309679389\n",
      "Iteration 274, Batch: 48, Loss: 0.03862643986940384\n",
      "Iteration 274, Batch: 49, Loss: 0.037016406655311584\n",
      "Iteration 275, Batch: 0, Loss: 0.018824929371476173\n",
      "Iteration 275, Batch: 1, Loss: 0.050377000123262405\n",
      "Iteration 275, Batch: 2, Loss: 0.06603662669658661\n",
      "Iteration 275, Batch: 3, Loss: 0.05471991375088692\n",
      "Iteration 275, Batch: 4, Loss: 0.042481474578380585\n",
      "Iteration 275, Batch: 5, Loss: 0.04127544164657593\n",
      "Iteration 275, Batch: 6, Loss: 0.04596986621618271\n",
      "Iteration 275, Batch: 7, Loss: 0.05347345396876335\n",
      "Iteration 275, Batch: 8, Loss: 0.04904757812619209\n",
      "Iteration 275, Batch: 9, Loss: 0.04842201992869377\n",
      "Iteration 275, Batch: 10, Loss: 0.0577264167368412\n",
      "Iteration 275, Batch: 11, Loss: 0.017035940662026405\n",
      "Iteration 275, Batch: 12, Loss: 0.05205926671624184\n",
      "Iteration 275, Batch: 13, Loss: 0.09984994679689407\n",
      "Iteration 275, Batch: 14, Loss: 0.05251450464129448\n",
      "Iteration 275, Batch: 15, Loss: 0.05833561718463898\n",
      "Iteration 275, Batch: 16, Loss: 0.025038065388798714\n",
      "Iteration 275, Batch: 17, Loss: 0.0397920086979866\n",
      "Iteration 275, Batch: 18, Loss: 0.0486394427716732\n",
      "Iteration 275, Batch: 19, Loss: 0.051140088587999344\n",
      "Iteration 275, Batch: 20, Loss: 0.04713569954037666\n",
      "Iteration 275, Batch: 21, Loss: 0.06197873130440712\n",
      "Iteration 275, Batch: 22, Loss: 0.06850066781044006\n",
      "Iteration 275, Batch: 23, Loss: 0.07199347019195557\n",
      "Iteration 275, Batch: 24, Loss: 0.05958152934908867\n",
      "Iteration 275, Batch: 25, Loss: 0.028694642707705498\n",
      "Iteration 275, Batch: 26, Loss: 0.050655297935009\n",
      "Iteration 275, Batch: 27, Loss: 0.03697732463479042\n",
      "Iteration 275, Batch: 28, Loss: 0.05167681723833084\n",
      "Iteration 275, Batch: 29, Loss: 0.06200868636369705\n",
      "Iteration 275, Batch: 30, Loss: 0.05579331889748573\n",
      "Iteration 275, Batch: 31, Loss: 0.037548311054706573\n",
      "Iteration 275, Batch: 32, Loss: 0.07312134653329849\n",
      "Iteration 275, Batch: 33, Loss: 0.028159737586975098\n",
      "Iteration 275, Batch: 34, Loss: 0.05679803714156151\n",
      "Iteration 275, Batch: 35, Loss: 0.05156361311674118\n",
      "Iteration 275, Batch: 36, Loss: 0.023759974166750908\n",
      "Iteration 275, Batch: 37, Loss: 0.05544743314385414\n",
      "Iteration 275, Batch: 38, Loss: 0.046408023685216904\n",
      "Iteration 275, Batch: 39, Loss: 0.03934802487492561\n",
      "Iteration 275, Batch: 40, Loss: 0.05232170596718788\n",
      "Iteration 275, Batch: 41, Loss: 0.045079849660396576\n",
      "Iteration 275, Batch: 42, Loss: 0.060179632157087326\n",
      "Iteration 275, Batch: 43, Loss: 0.07274545729160309\n",
      "Iteration 275, Batch: 44, Loss: 0.059263523668050766\n",
      "Iteration 275, Batch: 45, Loss: 0.05997910350561142\n",
      "Iteration 275, Batch: 46, Loss: 0.025406062602996826\n",
      "Iteration 275, Batch: 47, Loss: 0.05458670109510422\n",
      "Iteration 275, Batch: 48, Loss: 0.07197310030460358\n",
      "Iteration 275, Batch: 49, Loss: 0.04188260808587074\n",
      "Iteration 276, Batch: 0, Loss: 0.04781109839677811\n",
      "Iteration 276, Batch: 1, Loss: 0.042084407061338425\n",
      "Iteration 276, Batch: 2, Loss: 0.05429462343454361\n",
      "Iteration 276, Batch: 3, Loss: 0.04052184894680977\n",
      "Iteration 276, Batch: 4, Loss: 0.06516117602586746\n",
      "Iteration 276, Batch: 5, Loss: 0.05832758918404579\n",
      "Iteration 276, Batch: 6, Loss: 0.060668110847473145\n",
      "Iteration 276, Batch: 7, Loss: 0.03593776747584343\n",
      "Iteration 276, Batch: 8, Loss: 0.05245761573314667\n",
      "Iteration 276, Batch: 9, Loss: 0.05589789152145386\n",
      "Iteration 276, Batch: 10, Loss: 0.06578164547681808\n",
      "Iteration 276, Batch: 11, Loss: 0.055801019072532654\n",
      "Iteration 276, Batch: 12, Loss: 0.0528792180120945\n",
      "Iteration 276, Batch: 13, Loss: 0.055891864001750946\n",
      "Iteration 276, Batch: 14, Loss: 0.050352029502391815\n",
      "Iteration 276, Batch: 15, Loss: 0.059312235563993454\n",
      "Iteration 276, Batch: 16, Loss: 0.04166150838136673\n",
      "Iteration 276, Batch: 17, Loss: 0.057922884821891785\n",
      "Iteration 276, Batch: 18, Loss: 0.06402719020843506\n",
      "Iteration 276, Batch: 19, Loss: 0.08624877035617828\n",
      "Iteration 276, Batch: 20, Loss: 0.05557349696755409\n",
      "Iteration 276, Batch: 21, Loss: 0.03646373376250267\n",
      "Iteration 276, Batch: 22, Loss: 0.07343105971813202\n",
      "Iteration 276, Batch: 23, Loss: 0.06145801395177841\n",
      "Iteration 276, Batch: 24, Loss: 0.06568866223096848\n",
      "Iteration 276, Batch: 25, Loss: 0.06029185652732849\n",
      "Iteration 276, Batch: 26, Loss: 0.06915508210659027\n",
      "Iteration 276, Batch: 27, Loss: 0.05862892046570778\n",
      "Iteration 276, Batch: 28, Loss: 0.0704929381608963\n",
      "Iteration 276, Batch: 29, Loss: 0.05351411551237106\n",
      "Iteration 276, Batch: 30, Loss: 0.05577321723103523\n",
      "Iteration 276, Batch: 31, Loss: 0.08274251967668533\n",
      "Iteration 276, Batch: 32, Loss: 0.08362819254398346\n",
      "Iteration 276, Batch: 33, Loss: 0.05988183990120888\n",
      "Iteration 276, Batch: 34, Loss: 0.0467810332775116\n",
      "Iteration 276, Batch: 35, Loss: 0.03858074173331261\n",
      "Iteration 276, Batch: 36, Loss: 0.06130003184080124\n",
      "Iteration 276, Batch: 37, Loss: 0.023551026359200478\n",
      "Iteration 276, Batch: 38, Loss: 0.07170833647251129\n",
      "Iteration 276, Batch: 39, Loss: 0.03473852574825287\n",
      "Iteration 276, Batch: 40, Loss: 0.05111851915717125\n",
      "Iteration 276, Batch: 41, Loss: 0.04367941617965698\n",
      "Iteration 276, Batch: 42, Loss: 0.04543670639395714\n",
      "Iteration 276, Batch: 43, Loss: 0.037431854754686356\n",
      "Iteration 276, Batch: 44, Loss: 0.08297860622406006\n",
      "Iteration 276, Batch: 45, Loss: 0.0652250424027443\n",
      "Iteration 276, Batch: 46, Loss: 0.06888995319604874\n",
      "Iteration 276, Batch: 47, Loss: 0.056416310369968414\n",
      "Iteration 276, Batch: 48, Loss: 0.06052834540605545\n",
      "Iteration 276, Batch: 49, Loss: 0.04349600151181221\n",
      "Iteration 277, Batch: 0, Loss: 0.046582333743572235\n",
      "Iteration 277, Batch: 1, Loss: 0.049351196736097336\n",
      "Iteration 277, Batch: 2, Loss: 0.08357396721839905\n",
      "Iteration 277, Batch: 3, Loss: 0.03021656721830368\n",
      "Iteration 277, Batch: 4, Loss: 0.05991003289818764\n",
      "Iteration 277, Batch: 5, Loss: 0.050684645771980286\n",
      "Iteration 277, Batch: 6, Loss: 0.05648854002356529\n",
      "Iteration 277, Batch: 7, Loss: 0.03608519956469536\n",
      "Iteration 277, Batch: 8, Loss: 0.05432393029332161\n",
      "Iteration 277, Batch: 9, Loss: 0.057884521782398224\n",
      "Iteration 277, Batch: 10, Loss: 0.10002315044403076\n",
      "Iteration 277, Batch: 11, Loss: 0.04826127737760544\n",
      "Iteration 277, Batch: 12, Loss: 0.05421265587210655\n",
      "Iteration 277, Batch: 13, Loss: 0.05556640401482582\n",
      "Iteration 277, Batch: 14, Loss: 0.05109073966741562\n",
      "Iteration 277, Batch: 15, Loss: 0.04272613301873207\n",
      "Iteration 277, Batch: 16, Loss: 0.027395980432629585\n",
      "Iteration 277, Batch: 17, Loss: 0.03573518991470337\n",
      "Iteration 277, Batch: 18, Loss: 0.06733057647943497\n",
      "Iteration 277, Batch: 19, Loss: 0.015028747729957104\n",
      "Iteration 277, Batch: 20, Loss: 0.06867757439613342\n",
      "Iteration 277, Batch: 21, Loss: 0.05201248824596405\n",
      "Iteration 277, Batch: 22, Loss: 0.07411657273769379\n",
      "Iteration 277, Batch: 23, Loss: 0.06055429205298424\n",
      "Iteration 277, Batch: 24, Loss: 0.04826763644814491\n",
      "Iteration 277, Batch: 25, Loss: 0.048475105315446854\n",
      "Iteration 277, Batch: 26, Loss: 0.04961441084742546\n",
      "Iteration 277, Batch: 27, Loss: 0.050307054072618484\n",
      "Iteration 277, Batch: 28, Loss: 0.05415911227464676\n",
      "Iteration 277, Batch: 29, Loss: 0.0377584770321846\n",
      "Iteration 277, Batch: 30, Loss: 0.053571026772260666\n",
      "Iteration 277, Batch: 31, Loss: 0.04739274084568024\n",
      "Iteration 277, Batch: 32, Loss: 0.033734824508428574\n",
      "Iteration 277, Batch: 33, Loss: 0.03986920043826103\n",
      "Iteration 277, Batch: 34, Loss: 0.08196187764406204\n",
      "Iteration 277, Batch: 35, Loss: 0.06541607528924942\n",
      "Iteration 277, Batch: 36, Loss: 0.0749606266617775\n",
      "Iteration 277, Batch: 37, Loss: 0.04740601032972336\n",
      "Iteration 277, Batch: 38, Loss: 0.026867488399147987\n",
      "Iteration 277, Batch: 39, Loss: 1.8340669870376587\n",
      "Iteration 277, Batch: 40, Loss: 0.04074092209339142\n",
      "Iteration 277, Batch: 41, Loss: 0.0415024571120739\n",
      "Iteration 277, Batch: 42, Loss: 0.06635638326406479\n",
      "Iteration 277, Batch: 43, Loss: 0.053436584770679474\n",
      "Iteration 277, Batch: 44, Loss: 0.0430319681763649\n",
      "Iteration 277, Batch: 45, Loss: 0.0294458270072937\n",
      "Iteration 277, Batch: 46, Loss: 0.08323829621076584\n",
      "Iteration 277, Batch: 47, Loss: 0.06834116578102112\n",
      "Iteration 277, Batch: 48, Loss: 0.04802393913269043\n",
      "Iteration 277, Batch: 49, Loss: 0.027648624032735825\n",
      "Iteration 278, Batch: 0, Loss: 0.049406517297029495\n",
      "Iteration 278, Batch: 1, Loss: 0.0373770073056221\n",
      "Iteration 278, Batch: 2, Loss: 0.04126570001244545\n",
      "Iteration 278, Batch: 3, Loss: 0.05264955013990402\n",
      "Iteration 278, Batch: 4, Loss: 0.048106152564287186\n",
      "Iteration 278, Batch: 5, Loss: 0.054613761603832245\n",
      "Iteration 278, Batch: 6, Loss: 0.037073493003845215\n",
      "Iteration 278, Batch: 7, Loss: 0.050743475556373596\n",
      "Iteration 278, Batch: 8, Loss: 0.06697843968868256\n",
      "Iteration 278, Batch: 9, Loss: 0.04014895483851433\n",
      "Iteration 278, Batch: 10, Loss: 0.043002501130104065\n",
      "Iteration 278, Batch: 11, Loss: 0.042631786316633224\n",
      "Iteration 278, Batch: 12, Loss: 0.06589514017105103\n",
      "Iteration 278, Batch: 13, Loss: 0.04323592409491539\n",
      "Iteration 278, Batch: 14, Loss: 0.04721258953213692\n",
      "Iteration 278, Batch: 15, Loss: 0.07569611072540283\n",
      "Iteration 278, Batch: 16, Loss: 0.06267091631889343\n",
      "Iteration 278, Batch: 17, Loss: 0.0465974286198616\n",
      "Iteration 278, Batch: 18, Loss: 0.0413808636367321\n",
      "Iteration 278, Batch: 19, Loss: 0.041301459074020386\n",
      "Iteration 278, Batch: 20, Loss: 0.029552698135375977\n",
      "Iteration 278, Batch: 21, Loss: 0.03665725886821747\n",
      "Iteration 278, Batch: 22, Loss: 0.052272386848926544\n",
      "Iteration 278, Batch: 23, Loss: 0.020381862297654152\n",
      "Iteration 278, Batch: 24, Loss: 0.04436738044023514\n",
      "Iteration 278, Batch: 25, Loss: 0.03749921917915344\n",
      "Iteration 278, Batch: 26, Loss: 0.041388873010873795\n",
      "Iteration 278, Batch: 27, Loss: 0.049546945840120316\n",
      "Iteration 278, Batch: 28, Loss: 0.03762250021100044\n",
      "Iteration 278, Batch: 29, Loss: 0.024675408378243446\n",
      "Iteration 278, Batch: 30, Loss: 0.06406942009925842\n",
      "Iteration 278, Batch: 31, Loss: 0.05571458861231804\n",
      "Iteration 278, Batch: 32, Loss: 0.05033285915851593\n",
      "Iteration 278, Batch: 33, Loss: 0.03131009638309479\n",
      "Iteration 278, Batch: 34, Loss: 0.034671127796173096\n",
      "Iteration 278, Batch: 35, Loss: 0.057239826768636703\n",
      "Iteration 278, Batch: 36, Loss: 0.046136438846588135\n",
      "Iteration 278, Batch: 37, Loss: 0.053340695798397064\n",
      "Iteration 278, Batch: 38, Loss: 0.030509646981954575\n",
      "Iteration 278, Batch: 39, Loss: 0.0329919196665287\n",
      "Iteration 278, Batch: 40, Loss: 0.041349709033966064\n",
      "Iteration 278, Batch: 41, Loss: 0.029707007110118866\n",
      "Iteration 278, Batch: 42, Loss: 0.05227993056178093\n",
      "Iteration 278, Batch: 43, Loss: 0.049670252948999405\n",
      "Iteration 278, Batch: 44, Loss: 0.04714266583323479\n",
      "Iteration 278, Batch: 45, Loss: 0.04090025648474693\n",
      "Iteration 278, Batch: 46, Loss: 0.03240511566400528\n",
      "Iteration 278, Batch: 47, Loss: 0.04193180799484253\n",
      "Iteration 278, Batch: 48, Loss: 0.05488697811961174\n",
      "Iteration 278, Batch: 49, Loss: 0.04198594018816948\n",
      "Iteration 279, Batch: 0, Loss: 0.06031114608049393\n",
      "Iteration 279, Batch: 1, Loss: 0.03596792742609978\n",
      "Iteration 279, Batch: 2, Loss: 0.042776353657245636\n",
      "Iteration 279, Batch: 3, Loss: 0.04142240062355995\n",
      "Iteration 279, Batch: 4, Loss: 0.06717648357152939\n",
      "Iteration 279, Batch: 5, Loss: 0.04537821561098099\n",
      "Iteration 279, Batch: 6, Loss: 0.037054747343063354\n",
      "Iteration 279, Batch: 7, Loss: 0.04577465355396271\n",
      "Iteration 279, Batch: 8, Loss: 0.0383773110806942\n",
      "Iteration 279, Batch: 9, Loss: 0.04807167127728462\n",
      "Iteration 279, Batch: 10, Loss: 0.043095119297504425\n",
      "Iteration 279, Batch: 11, Loss: 0.05471362918615341\n",
      "Iteration 279, Batch: 12, Loss: 0.046098578721284866\n",
      "Iteration 279, Batch: 13, Loss: 0.051477763801813126\n",
      "Iteration 279, Batch: 14, Loss: 0.04102015867829323\n",
      "Iteration 279, Batch: 15, Loss: 0.047344136983156204\n",
      "Iteration 279, Batch: 16, Loss: 0.04760070517659187\n",
      "Iteration 279, Batch: 17, Loss: 0.06478745490312576\n",
      "Iteration 279, Batch: 18, Loss: 0.05281607434153557\n",
      "Iteration 279, Batch: 19, Loss: 0.03708887845277786\n",
      "Iteration 279, Batch: 20, Loss: 0.07470698654651642\n",
      "Iteration 279, Batch: 21, Loss: 0.0667286291718483\n",
      "Iteration 279, Batch: 22, Loss: 0.06948622316122055\n",
      "Iteration 279, Batch: 23, Loss: 0.06333374977111816\n",
      "Iteration 279, Batch: 24, Loss: 0.0417875275015831\n",
      "Iteration 279, Batch: 25, Loss: 0.03829284757375717\n",
      "Iteration 279, Batch: 26, Loss: 0.07816106826066971\n",
      "Iteration 279, Batch: 27, Loss: 0.03759640082716942\n",
      "Iteration 279, Batch: 28, Loss: 0.04843968152999878\n",
      "Iteration 279, Batch: 29, Loss: 0.04082699865102768\n",
      "Iteration 279, Batch: 30, Loss: 0.04369988664984703\n",
      "Iteration 279, Batch: 31, Loss: 0.058643072843551636\n",
      "Iteration 279, Batch: 32, Loss: 0.04829153046011925\n",
      "Iteration 279, Batch: 33, Loss: 0.08741340786218643\n",
      "Iteration 279, Batch: 34, Loss: 0.053033579140901566\n",
      "Iteration 279, Batch: 35, Loss: 0.06604783236980438\n",
      "Iteration 279, Batch: 36, Loss: 0.06983233243227005\n",
      "Iteration 279, Batch: 37, Loss: 0.05919104069471359\n",
      "Iteration 279, Batch: 38, Loss: 0.048752378672361374\n",
      "Iteration 279, Batch: 39, Loss: 0.07272101938724518\n",
      "Iteration 279, Batch: 40, Loss: 0.06343746185302734\n",
      "Iteration 279, Batch: 41, Loss: 0.04168340563774109\n",
      "Iteration 279, Batch: 42, Loss: 0.056067295372486115\n",
      "Iteration 279, Batch: 43, Loss: 0.03481113910675049\n",
      "Iteration 279, Batch: 44, Loss: 0.0532766729593277\n",
      "Iteration 279, Batch: 45, Loss: 0.024714311584830284\n",
      "Iteration 279, Batch: 46, Loss: 0.040194883942604065\n",
      "Iteration 279, Batch: 47, Loss: 0.05449814349412918\n",
      "Iteration 279, Batch: 48, Loss: 0.029355570673942566\n",
      "Iteration 279, Batch: 49, Loss: 0.0691118985414505\n",
      "Iteration 280, Batch: 0, Loss: 0.06256528943777084\n",
      "Iteration 280, Batch: 1, Loss: 0.04453988000750542\n",
      "Iteration 280, Batch: 2, Loss: 0.04780782759189606\n",
      "Iteration 280, Batch: 3, Loss: 0.06474471092224121\n",
      "Iteration 280, Batch: 4, Loss: 0.020696667954325676\n",
      "Iteration 280, Batch: 5, Loss: 0.02142905816435814\n",
      "Iteration 280, Batch: 6, Loss: 0.0392674058675766\n",
      "Iteration 280, Batch: 7, Loss: 0.02752300724387169\n",
      "Iteration 280, Batch: 8, Loss: 0.05217008292675018\n",
      "Iteration 280, Batch: 9, Loss: 0.05846250429749489\n",
      "Iteration 280, Batch: 10, Loss: 0.02592216618359089\n",
      "Iteration 280, Batch: 11, Loss: 0.04038972780108452\n",
      "Iteration 280, Batch: 12, Loss: 0.04844258353114128\n",
      "Iteration 280, Batch: 13, Loss: 0.05912382900714874\n",
      "Iteration 280, Batch: 14, Loss: 0.036199264228343964\n",
      "Iteration 280, Batch: 15, Loss: 0.054786328226327896\n",
      "Iteration 280, Batch: 16, Loss: 0.0630522221326828\n",
      "Iteration 280, Batch: 17, Loss: 0.04306472837924957\n",
      "Iteration 280, Batch: 18, Loss: 0.08046461641788483\n",
      "Iteration 280, Batch: 19, Loss: 0.03833723068237305\n",
      "Iteration 280, Batch: 20, Loss: 0.04607405886054039\n",
      "Iteration 280, Batch: 21, Loss: 0.045022811740636826\n",
      "Iteration 280, Batch: 22, Loss: 0.06397014856338501\n",
      "Iteration 280, Batch: 23, Loss: 0.044678058475255966\n",
      "Iteration 280, Batch: 24, Loss: 0.02776012383401394\n",
      "Iteration 280, Batch: 25, Loss: 0.049033984541893005\n",
      "Iteration 280, Batch: 26, Loss: 0.05472688376903534\n",
      "Iteration 280, Batch: 27, Loss: 0.038826607167720795\n",
      "Iteration 280, Batch: 28, Loss: 0.05763278901576996\n",
      "Iteration 280, Batch: 29, Loss: 0.03953152894973755\n",
      "Iteration 280, Batch: 30, Loss: 0.07487089931964874\n",
      "Iteration 280, Batch: 31, Loss: 0.04193216189742088\n",
      "Iteration 280, Batch: 32, Loss: 0.04486061632633209\n",
      "Iteration 280, Batch: 33, Loss: 0.044901493936777115\n",
      "Iteration 280, Batch: 34, Loss: 0.0659419596195221\n",
      "Iteration 280, Batch: 35, Loss: 0.04695571959018707\n",
      "Iteration 280, Batch: 36, Loss: 0.02996944822371006\n",
      "Iteration 280, Batch: 37, Loss: 0.031579114496707916\n",
      "Iteration 280, Batch: 38, Loss: 0.06997266411781311\n",
      "Iteration 280, Batch: 39, Loss: 0.03978467732667923\n",
      "Iteration 280, Batch: 40, Loss: 0.04692213609814644\n",
      "Iteration 280, Batch: 41, Loss: 0.06090737506747246\n",
      "Iteration 280, Batch: 42, Loss: 0.05239999666810036\n",
      "Iteration 280, Batch: 43, Loss: 0.04944561421871185\n",
      "Iteration 280, Batch: 44, Loss: 0.04951515048742294\n",
      "Iteration 280, Batch: 45, Loss: 0.03847918286919594\n",
      "Iteration 280, Batch: 46, Loss: 0.051961783319711685\n",
      "Iteration 280, Batch: 47, Loss: 0.02639813907444477\n",
      "Iteration 280, Batch: 48, Loss: 0.04081665724515915\n",
      "Iteration 280, Batch: 49, Loss: 0.04312910884618759\n",
      "Iteration 281, Batch: 0, Loss: 0.03210620954632759\n",
      "Iteration 281, Batch: 1, Loss: 0.06986753642559052\n",
      "Iteration 281, Batch: 2, Loss: 0.06355449557304382\n",
      "Iteration 281, Batch: 3, Loss: 0.06903897225856781\n",
      "Iteration 281, Batch: 4, Loss: 0.0524989552795887\n",
      "Iteration 281, Batch: 5, Loss: 0.06471533328294754\n",
      "Iteration 281, Batch: 6, Loss: 0.05890360847115517\n",
      "Iteration 281, Batch: 7, Loss: 0.029795020818710327\n",
      "Iteration 281, Batch: 8, Loss: 0.05601370707154274\n",
      "Iteration 281, Batch: 9, Loss: 0.05919894203543663\n",
      "Iteration 281, Batch: 10, Loss: 0.07787389308214188\n",
      "Iteration 281, Batch: 11, Loss: 0.08769567310810089\n",
      "Iteration 281, Batch: 12, Loss: 0.058848436921834946\n",
      "Iteration 281, Batch: 13, Loss: 0.033216677606105804\n",
      "Iteration 281, Batch: 14, Loss: 0.06852816045284271\n",
      "Iteration 281, Batch: 15, Loss: 0.057075414806604385\n",
      "Iteration 281, Batch: 16, Loss: 0.060490306466817856\n",
      "Iteration 281, Batch: 17, Loss: 0.07527543604373932\n",
      "Iteration 281, Batch: 18, Loss: 0.049515750259160995\n",
      "Iteration 281, Batch: 19, Loss: 0.03499264270067215\n",
      "Iteration 281, Batch: 20, Loss: 0.040161408483982086\n",
      "Iteration 281, Batch: 21, Loss: 0.06105193868279457\n",
      "Iteration 281, Batch: 22, Loss: 0.05449817702174187\n",
      "Iteration 281, Batch: 23, Loss: 0.04868296533823013\n",
      "Iteration 281, Batch: 24, Loss: 0.06440132856369019\n",
      "Iteration 281, Batch: 25, Loss: 0.051945943385362625\n",
      "Iteration 281, Batch: 26, Loss: 0.07364480197429657\n",
      "Iteration 281, Batch: 27, Loss: 0.05823986977338791\n",
      "Iteration 281, Batch: 28, Loss: 0.027345692738890648\n",
      "Iteration 281, Batch: 29, Loss: 0.03074732981622219\n",
      "Iteration 281, Batch: 30, Loss: 0.047318942844867706\n",
      "Iteration 281, Batch: 31, Loss: 0.053124114871025085\n",
      "Iteration 281, Batch: 32, Loss: 0.0708044245839119\n",
      "Iteration 281, Batch: 33, Loss: 0.05627118796110153\n",
      "Iteration 281, Batch: 34, Loss: 0.03757805749773979\n",
      "Iteration 281, Batch: 35, Loss: 0.040050309151411057\n",
      "Iteration 281, Batch: 36, Loss: 0.048031874001026154\n",
      "Iteration 281, Batch: 37, Loss: 0.05149514228105545\n",
      "Iteration 281, Batch: 38, Loss: 0.046484775841236115\n",
      "Iteration 281, Batch: 39, Loss: 0.0560280866920948\n",
      "Iteration 281, Batch: 40, Loss: 0.08640266209840775\n",
      "Iteration 281, Batch: 41, Loss: 0.019851861521601677\n",
      "Iteration 281, Batch: 42, Loss: 0.05316568911075592\n",
      "Iteration 281, Batch: 43, Loss: 0.07452493906021118\n",
      "Iteration 281, Batch: 44, Loss: 0.07084477692842484\n",
      "Iteration 281, Batch: 45, Loss: 0.06624793261289597\n",
      "Iteration 281, Batch: 46, Loss: 0.04522599279880524\n",
      "Iteration 281, Batch: 47, Loss: 0.06422904133796692\n",
      "Iteration 281, Batch: 48, Loss: 0.06583752483129501\n",
      "Iteration 281, Batch: 49, Loss: 0.02883642353117466\n",
      "Iteration 282, Batch: 0, Loss: 0.05345679819583893\n",
      "Iteration 282, Batch: 1, Loss: 0.05557272210717201\n",
      "Iteration 282, Batch: 2, Loss: 0.042195625603199005\n",
      "Iteration 282, Batch: 3, Loss: 0.04323584958910942\n",
      "Iteration 282, Batch: 4, Loss: 0.04079156741499901\n",
      "Iteration 282, Batch: 5, Loss: 0.05265052244067192\n",
      "Iteration 282, Batch: 6, Loss: 0.059445831924676895\n",
      "Iteration 282, Batch: 7, Loss: 0.06427604705095291\n",
      "Iteration 282, Batch: 8, Loss: 0.0433611162006855\n",
      "Iteration 282, Batch: 9, Loss: 0.04110421985387802\n",
      "Iteration 282, Batch: 10, Loss: 0.052740275859832764\n",
      "Iteration 282, Batch: 11, Loss: 0.050045788288116455\n",
      "Iteration 282, Batch: 12, Loss: 0.04182453081011772\n",
      "Iteration 282, Batch: 13, Loss: 0.0521155446767807\n",
      "Iteration 282, Batch: 14, Loss: 0.05365607514977455\n",
      "Iteration 282, Batch: 15, Loss: 0.060846924781799316\n",
      "Iteration 282, Batch: 16, Loss: 0.060796260833740234\n",
      "Iteration 282, Batch: 17, Loss: 0.06332632154226303\n",
      "Iteration 282, Batch: 18, Loss: 0.04857472702860832\n",
      "Iteration 282, Batch: 19, Loss: 0.09114621579647064\n",
      "Iteration 282, Batch: 20, Loss: 0.1130356565117836\n",
      "Iteration 282, Batch: 21, Loss: 0.07744866609573364\n",
      "Iteration 282, Batch: 22, Loss: 0.0466972179710865\n",
      "Iteration 282, Batch: 23, Loss: 0.036220990121364594\n",
      "Iteration 282, Batch: 24, Loss: 0.057459600269794464\n",
      "Iteration 282, Batch: 25, Loss: 0.05630376562476158\n",
      "Iteration 282, Batch: 26, Loss: 0.07381033897399902\n",
      "Iteration 282, Batch: 27, Loss: 0.07683180272579193\n",
      "Iteration 282, Batch: 28, Loss: 0.09355676174163818\n",
      "Iteration 282, Batch: 29, Loss: 0.05478831008076668\n",
      "Iteration 282, Batch: 30, Loss: 0.06392604857683182\n",
      "Iteration 282, Batch: 31, Loss: 0.06351354718208313\n",
      "Iteration 282, Batch: 32, Loss: 0.04827097803354263\n",
      "Iteration 282, Batch: 33, Loss: 0.04412750527262688\n",
      "Iteration 282, Batch: 34, Loss: 0.07211288064718246\n",
      "Iteration 282, Batch: 35, Loss: 0.03992237523198128\n",
      "Iteration 282, Batch: 36, Loss: 0.06013598293066025\n",
      "Iteration 282, Batch: 37, Loss: 0.057094886898994446\n",
      "Iteration 282, Batch: 38, Loss: 0.059367235749959946\n",
      "Iteration 282, Batch: 39, Loss: 0.057136911898851395\n",
      "Iteration 282, Batch: 40, Loss: 0.03865077719092369\n",
      "Iteration 282, Batch: 41, Loss: 0.028256645426154137\n",
      "Iteration 282, Batch: 42, Loss: 0.041924137622117996\n",
      "Iteration 282, Batch: 43, Loss: 0.0520462803542614\n",
      "Iteration 282, Batch: 44, Loss: 0.05753606557846069\n",
      "Iteration 282, Batch: 45, Loss: 0.07108289748430252\n",
      "Iteration 282, Batch: 46, Loss: 0.08626249432563782\n",
      "Iteration 282, Batch: 47, Loss: 0.051895029842853546\n",
      "Iteration 282, Batch: 48, Loss: 0.06973785907030106\n",
      "Iteration 282, Batch: 49, Loss: 0.05457666143774986\n",
      "Iteration 283, Batch: 0, Loss: 0.0643753632903099\n",
      "Iteration 283, Batch: 1, Loss: 0.058731574565172195\n",
      "Iteration 283, Batch: 2, Loss: 0.06633680313825607\n",
      "Iteration 283, Batch: 3, Loss: 0.0799831822514534\n",
      "Iteration 283, Batch: 4, Loss: 0.06773151457309723\n",
      "Iteration 283, Batch: 5, Loss: 0.0776883140206337\n",
      "Iteration 283, Batch: 6, Loss: 0.05041301250457764\n",
      "Iteration 283, Batch: 7, Loss: 0.06571722030639648\n",
      "Iteration 283, Batch: 8, Loss: 0.041573427617549896\n",
      "Iteration 283, Batch: 9, Loss: 0.049716509878635406\n",
      "Iteration 283, Batch: 10, Loss: 0.041894569993019104\n",
      "Iteration 283, Batch: 11, Loss: 0.0430026613175869\n",
      "Iteration 283, Batch: 12, Loss: 0.056364756077528\n",
      "Iteration 283, Batch: 13, Loss: 0.07644055038690567\n",
      "Iteration 283, Batch: 14, Loss: 0.07111559808254242\n",
      "Iteration 283, Batch: 15, Loss: 0.07399146258831024\n",
      "Iteration 283, Batch: 16, Loss: 0.07709509879350662\n",
      "Iteration 283, Batch: 17, Loss: 0.07576096802949905\n",
      "Iteration 283, Batch: 18, Loss: 0.05282139778137207\n",
      "Iteration 283, Batch: 19, Loss: 0.07683116942644119\n",
      "Iteration 283, Batch: 20, Loss: 0.0533536933362484\n",
      "Iteration 283, Batch: 21, Loss: 0.10530021786689758\n",
      "Iteration 283, Batch: 22, Loss: 0.051697634160518646\n",
      "Iteration 283, Batch: 23, Loss: 0.0550091527402401\n",
      "Iteration 283, Batch: 24, Loss: 0.06206338852643967\n",
      "Iteration 283, Batch: 25, Loss: 0.04740731045603752\n",
      "Iteration 283, Batch: 26, Loss: 0.07081899046897888\n",
      "Iteration 283, Batch: 27, Loss: 0.06817006319761276\n",
      "Iteration 283, Batch: 28, Loss: 0.07857108861207962\n",
      "Iteration 283, Batch: 29, Loss: 0.08020474016666412\n",
      "Iteration 283, Batch: 30, Loss: 0.049317531287670135\n",
      "Iteration 283, Batch: 31, Loss: 0.06067676469683647\n",
      "Iteration 283, Batch: 32, Loss: 0.034493543207645416\n",
      "Iteration 283, Batch: 33, Loss: 0.06464879959821701\n",
      "Iteration 283, Batch: 34, Loss: 0.04986407607793808\n",
      "Iteration 283, Batch: 35, Loss: 0.09003765136003494\n",
      "Iteration 283, Batch: 36, Loss: 0.05162850394845009\n",
      "Iteration 283, Batch: 37, Loss: 0.0494343638420105\n",
      "Iteration 283, Batch: 38, Loss: 0.0789964348077774\n",
      "Iteration 283, Batch: 39, Loss: 0.04968129098415375\n",
      "Iteration 283, Batch: 40, Loss: 0.07639545202255249\n",
      "Iteration 283, Batch: 41, Loss: 0.05979648604989052\n",
      "Iteration 283, Batch: 42, Loss: 0.06470492482185364\n",
      "Iteration 283, Batch: 43, Loss: 0.048547495156526566\n",
      "Iteration 283, Batch: 44, Loss: 0.05999999865889549\n",
      "Iteration 283, Batch: 45, Loss: 0.08448421210050583\n",
      "Iteration 283, Batch: 46, Loss: 0.05530305206775665\n",
      "Iteration 283, Batch: 47, Loss: 0.04624895006418228\n",
      "Iteration 283, Batch: 48, Loss: 0.04733898118138313\n",
      "Iteration 283, Batch: 49, Loss: 0.058774154633283615\n",
      "Iteration 284, Batch: 0, Loss: 0.06663685292005539\n",
      "Iteration 284, Batch: 1, Loss: 0.03667129948735237\n",
      "Iteration 284, Batch: 2, Loss: 0.05677704140543938\n",
      "Iteration 284, Batch: 3, Loss: 0.0540982186794281\n",
      "Iteration 284, Batch: 4, Loss: 0.04268137738108635\n",
      "Iteration 284, Batch: 5, Loss: 0.06661013513803482\n",
      "Iteration 284, Batch: 6, Loss: 0.06597282737493515\n",
      "Iteration 284, Batch: 7, Loss: 0.06704576313495636\n",
      "Iteration 284, Batch: 8, Loss: 0.06671374291181564\n",
      "Iteration 284, Batch: 9, Loss: 0.05609671398997307\n",
      "Iteration 284, Batch: 10, Loss: 0.10292984545230865\n",
      "Iteration 284, Batch: 11, Loss: 0.05960366129875183\n",
      "Iteration 284, Batch: 12, Loss: 0.04295404255390167\n",
      "Iteration 284, Batch: 13, Loss: 0.050199296325445175\n",
      "Iteration 284, Batch: 14, Loss: 0.06601470708847046\n",
      "Iteration 284, Batch: 15, Loss: 0.05402521789073944\n",
      "Iteration 284, Batch: 16, Loss: 0.06455026566982269\n",
      "Iteration 284, Batch: 17, Loss: 0.07674747705459595\n",
      "Iteration 284, Batch: 18, Loss: 0.05556030198931694\n",
      "Iteration 284, Batch: 19, Loss: 0.07084479928016663\n",
      "Iteration 284, Batch: 20, Loss: 0.07125810533761978\n",
      "Iteration 284, Batch: 21, Loss: 0.07638955861330032\n",
      "Iteration 284, Batch: 22, Loss: 0.09386631846427917\n",
      "Iteration 284, Batch: 23, Loss: 0.06449620425701141\n",
      "Iteration 284, Batch: 24, Loss: 0.06206504628062248\n",
      "Iteration 284, Batch: 25, Loss: 0.051509857177734375\n",
      "Iteration 284, Batch: 26, Loss: 0.05369143187999725\n",
      "Iteration 284, Batch: 27, Loss: 0.07095187157392502\n",
      "Iteration 284, Batch: 28, Loss: 0.09880338609218597\n",
      "Iteration 284, Batch: 29, Loss: 0.0760442465543747\n",
      "Iteration 284, Batch: 30, Loss: 0.06615883857011795\n",
      "Iteration 284, Batch: 31, Loss: 0.06869050115346909\n",
      "Iteration 284, Batch: 32, Loss: 0.08222845196723938\n",
      "Iteration 284, Batch: 33, Loss: 0.08081433176994324\n",
      "Iteration 284, Batch: 34, Loss: 0.06488443911075592\n",
      "Iteration 284, Batch: 35, Loss: 0.04586707800626755\n",
      "Iteration 284, Batch: 36, Loss: 0.04064518213272095\n",
      "Iteration 284, Batch: 37, Loss: 0.056944120675325394\n",
      "Iteration 284, Batch: 38, Loss: 0.06379362940788269\n",
      "Iteration 284, Batch: 39, Loss: 0.07117664068937302\n",
      "Iteration 284, Batch: 40, Loss: 0.07836591452360153\n",
      "Iteration 284, Batch: 41, Loss: 0.08211980760097504\n",
      "Iteration 284, Batch: 42, Loss: 0.06967383623123169\n",
      "Iteration 284, Batch: 43, Loss: 0.05936715006828308\n",
      "Iteration 284, Batch: 44, Loss: 0.046991221606731415\n",
      "Iteration 284, Batch: 45, Loss: 0.11006433516740799\n",
      "Iteration 284, Batch: 46, Loss: 0.05423489958047867\n",
      "Iteration 284, Batch: 47, Loss: 0.047960832715034485\n",
      "Iteration 284, Batch: 48, Loss: 0.06840889900922775\n",
      "Iteration 284, Batch: 49, Loss: 0.0826159343123436\n",
      "Iteration 285, Batch: 0, Loss: 0.04599441587924957\n",
      "Iteration 285, Batch: 1, Loss: 0.057558972388505936\n",
      "Iteration 285, Batch: 2, Loss: 0.06745358556509018\n",
      "Iteration 285, Batch: 3, Loss: 0.07612933218479156\n",
      "Iteration 285, Batch: 4, Loss: 0.07224320620298386\n",
      "Iteration 285, Batch: 5, Loss: 0.07195422053337097\n",
      "Iteration 285, Batch: 6, Loss: 0.07086510956287384\n",
      "Iteration 285, Batch: 7, Loss: 0.09273237735033035\n",
      "Iteration 285, Batch: 8, Loss: 0.07711639255285263\n",
      "Iteration 285, Batch: 9, Loss: 0.05206262320280075\n",
      "Iteration 285, Batch: 10, Loss: 0.07940378785133362\n",
      "Iteration 285, Batch: 11, Loss: 0.05208175629377365\n",
      "Iteration 285, Batch: 12, Loss: 0.07424627989530563\n",
      "Iteration 285, Batch: 13, Loss: 0.06610923260450363\n",
      "Iteration 285, Batch: 14, Loss: 0.10063914954662323\n",
      "Iteration 285, Batch: 15, Loss: 0.10117067396640778\n",
      "Iteration 285, Batch: 16, Loss: 0.09305436909198761\n",
      "Iteration 285, Batch: 17, Loss: 0.07640492171049118\n",
      "Iteration 285, Batch: 18, Loss: 0.05016757920384407\n",
      "Iteration 285, Batch: 19, Loss: 0.08303813636302948\n",
      "Iteration 285, Batch: 20, Loss: 0.07100360840559006\n",
      "Iteration 285, Batch: 21, Loss: 0.06824976950883865\n",
      "Iteration 285, Batch: 22, Loss: 0.0613115020096302\n",
      "Iteration 285, Batch: 23, Loss: 0.08124790340662003\n",
      "Iteration 285, Batch: 24, Loss: 0.06621536612510681\n",
      "Iteration 285, Batch: 25, Loss: 0.05815204977989197\n",
      "Iteration 285, Batch: 26, Loss: 0.06552950292825699\n",
      "Iteration 285, Batch: 27, Loss: 0.06871010363101959\n",
      "Iteration 285, Batch: 28, Loss: 0.03971104696393013\n",
      "Iteration 285, Batch: 29, Loss: 0.06688696891069412\n",
      "Iteration 285, Batch: 30, Loss: 0.03951241075992584\n",
      "Iteration 285, Batch: 31, Loss: 0.100430428981781\n",
      "Iteration 285, Batch: 32, Loss: 0.11300899088382721\n",
      "Iteration 285, Batch: 33, Loss: 0.05659542605280876\n",
      "Iteration 285, Batch: 34, Loss: 0.04700171947479248\n",
      "Iteration 285, Batch: 35, Loss: 0.056016042828559875\n",
      "Iteration 285, Batch: 36, Loss: 0.06413134187459946\n",
      "Iteration 285, Batch: 37, Loss: 0.0886719673871994\n",
      "Iteration 285, Batch: 38, Loss: 0.053824108093976974\n",
      "Iteration 285, Batch: 39, Loss: 0.06848935782909393\n",
      "Iteration 285, Batch: 40, Loss: 0.0591849684715271\n",
      "Iteration 285, Batch: 41, Loss: 0.05449031665921211\n",
      "Iteration 285, Batch: 42, Loss: 0.058934297412633896\n",
      "Iteration 285, Batch: 43, Loss: 0.0589667446911335\n",
      "Iteration 285, Batch: 44, Loss: 0.06293091922998428\n",
      "Iteration 285, Batch: 45, Loss: 0.11618048697710037\n",
      "Iteration 285, Batch: 46, Loss: 0.0820498839020729\n",
      "Iteration 285, Batch: 47, Loss: 0.10496839135885239\n",
      "Iteration 285, Batch: 48, Loss: 0.04849500581622124\n",
      "Iteration 285, Batch: 49, Loss: 0.07109087705612183\n",
      "Iteration 286, Batch: 0, Loss: 0.057065315544605255\n",
      "Iteration 286, Batch: 1, Loss: 0.061198800802230835\n",
      "Iteration 286, Batch: 2, Loss: 0.07199213653802872\n",
      "Iteration 286, Batch: 3, Loss: 0.0509914755821228\n",
      "Iteration 286, Batch: 4, Loss: 0.07338470220565796\n",
      "Iteration 286, Batch: 5, Loss: 0.06146717071533203\n",
      "Iteration 286, Batch: 6, Loss: 0.06083282455801964\n",
      "Iteration 286, Batch: 7, Loss: 0.06495078653097153\n",
      "Iteration 286, Batch: 8, Loss: 0.058690451085567474\n",
      "Iteration 286, Batch: 9, Loss: 0.05564432591199875\n",
      "Iteration 286, Batch: 10, Loss: 0.04770806059241295\n",
      "Iteration 286, Batch: 11, Loss: 0.048456210643053055\n",
      "Iteration 286, Batch: 12, Loss: 0.08802114427089691\n",
      "Iteration 286, Batch: 13, Loss: 0.08205956965684891\n",
      "Iteration 286, Batch: 14, Loss: 0.08125422149896622\n",
      "Iteration 286, Batch: 15, Loss: 0.0503656230866909\n",
      "Iteration 286, Batch: 16, Loss: 0.04648588225245476\n",
      "Iteration 286, Batch: 17, Loss: 0.04270288348197937\n",
      "Iteration 286, Batch: 18, Loss: 0.0574871189892292\n",
      "Iteration 286, Batch: 19, Loss: 0.061494242399930954\n",
      "Iteration 286, Batch: 20, Loss: 0.07369766384363174\n",
      "Iteration 286, Batch: 21, Loss: 0.06441831588745117\n",
      "Iteration 286, Batch: 22, Loss: 0.0764654129743576\n",
      "Iteration 286, Batch: 23, Loss: 0.06857679039239883\n",
      "Iteration 286, Batch: 24, Loss: 0.06691033393144608\n",
      "Iteration 286, Batch: 25, Loss: 0.053789347410202026\n",
      "Iteration 286, Batch: 26, Loss: 0.06399930268526077\n",
      "Iteration 286, Batch: 27, Loss: 0.07112329453229904\n",
      "Iteration 286, Batch: 28, Loss: 0.056222762912511826\n",
      "Iteration 286, Batch: 29, Loss: 0.04212052375078201\n",
      "Iteration 286, Batch: 30, Loss: 0.05622994899749756\n",
      "Iteration 286, Batch: 31, Loss: 0.1103799045085907\n",
      "Iteration 286, Batch: 32, Loss: 0.06233707815408707\n",
      "Iteration 286, Batch: 33, Loss: 0.048927586525678635\n",
      "Iteration 286, Batch: 34, Loss: 0.05978992208838463\n",
      "Iteration 286, Batch: 35, Loss: 0.06452546268701553\n",
      "Iteration 286, Batch: 36, Loss: 0.08541631698608398\n",
      "Iteration 286, Batch: 37, Loss: 0.05209152027964592\n",
      "Iteration 286, Batch: 38, Loss: 0.07851175218820572\n",
      "Iteration 286, Batch: 39, Loss: 0.038285546004772186\n",
      "Iteration 286, Batch: 40, Loss: 0.07198907434940338\n",
      "Iteration 286, Batch: 41, Loss: 0.06388048827648163\n",
      "Iteration 286, Batch: 42, Loss: 0.05953069031238556\n",
      "Iteration 286, Batch: 43, Loss: 0.07891558110713959\n",
      "Iteration 286, Batch: 44, Loss: 0.06832270324230194\n",
      "Iteration 286, Batch: 45, Loss: 0.051749370992183685\n",
      "Iteration 286, Batch: 46, Loss: 0.07920513302087784\n",
      "Iteration 286, Batch: 47, Loss: 0.0647144764661789\n",
      "Iteration 286, Batch: 48, Loss: 0.061738021671772\n",
      "Iteration 286, Batch: 49, Loss: 0.03681914880871773\n",
      "Iteration 287, Batch: 0, Loss: 0.04678491875529289\n",
      "Iteration 287, Batch: 1, Loss: 0.043678153306245804\n",
      "Iteration 287, Batch: 2, Loss: 0.08668241649866104\n",
      "Iteration 287, Batch: 3, Loss: 0.07284927368164062\n",
      "Iteration 287, Batch: 4, Loss: 0.06631232053041458\n",
      "Iteration 287, Batch: 5, Loss: 0.06927888840436935\n",
      "Iteration 287, Batch: 6, Loss: 0.05751756578683853\n",
      "Iteration 287, Batch: 7, Loss: 0.07714606076478958\n",
      "Iteration 287, Batch: 8, Loss: 0.05879867449402809\n",
      "Iteration 287, Batch: 9, Loss: 0.06805741041898727\n",
      "Iteration 287, Batch: 10, Loss: 0.05455000698566437\n",
      "Iteration 287, Batch: 11, Loss: 0.061477165669202805\n",
      "Iteration 287, Batch: 12, Loss: 0.07335824519395828\n",
      "Iteration 287, Batch: 13, Loss: 0.05734676122665405\n",
      "Iteration 287, Batch: 14, Loss: 0.05731470137834549\n",
      "Iteration 287, Batch: 15, Loss: 0.032749250531196594\n",
      "Iteration 287, Batch: 16, Loss: 0.05322789028286934\n",
      "Iteration 287, Batch: 17, Loss: 0.06221647188067436\n",
      "Iteration 287, Batch: 18, Loss: 0.061654310673475266\n",
      "Iteration 287, Batch: 19, Loss: 0.053872931748628616\n",
      "Iteration 287, Batch: 20, Loss: 0.06570487469434738\n",
      "Iteration 287, Batch: 21, Loss: 0.09056256711483002\n",
      "Iteration 287, Batch: 22, Loss: 0.07168405503034592\n",
      "Iteration 287, Batch: 23, Loss: 0.09399513155221939\n",
      "Iteration 287, Batch: 24, Loss: 0.05142461508512497\n",
      "Iteration 287, Batch: 25, Loss: 0.11105633527040482\n",
      "Iteration 287, Batch: 26, Loss: 0.06251762062311172\n",
      "Iteration 287, Batch: 27, Loss: 0.062462516129016876\n",
      "Iteration 287, Batch: 28, Loss: 0.05584771931171417\n",
      "Iteration 287, Batch: 29, Loss: 0.09397441893815994\n",
      "Iteration 287, Batch: 30, Loss: 0.08058714866638184\n",
      "Iteration 287, Batch: 31, Loss: 0.0658838078379631\n",
      "Iteration 287, Batch: 32, Loss: 0.0757499486207962\n",
      "Iteration 287, Batch: 33, Loss: 0.0482235811650753\n",
      "Iteration 287, Batch: 34, Loss: 0.06937292218208313\n",
      "Iteration 287, Batch: 35, Loss: 0.059922389686107635\n",
      "Iteration 287, Batch: 36, Loss: 0.05608312413096428\n",
      "Iteration 287, Batch: 37, Loss: 0.04473521560430527\n",
      "Iteration 287, Batch: 38, Loss: 0.04346749186515808\n",
      "Iteration 287, Batch: 39, Loss: 0.06262873113155365\n",
      "Iteration 287, Batch: 40, Loss: 0.05452263727784157\n",
      "Iteration 287, Batch: 41, Loss: 0.06655897945165634\n",
      "Iteration 287, Batch: 42, Loss: 0.11391906440258026\n",
      "Iteration 287, Batch: 43, Loss: 0.08199277520179749\n",
      "Iteration 287, Batch: 44, Loss: 0.07358463853597641\n",
      "Iteration 287, Batch: 45, Loss: 0.060038011521101\n",
      "Iteration 287, Batch: 46, Loss: 0.09468730539083481\n",
      "Iteration 287, Batch: 47, Loss: 0.06978689879179001\n",
      "Iteration 287, Batch: 48, Loss: 0.06223159283399582\n",
      "Iteration 287, Batch: 49, Loss: 0.08503730595111847\n",
      "Iteration 288, Batch: 0, Loss: 0.09743984043598175\n",
      "Iteration 288, Batch: 1, Loss: 0.10729508101940155\n",
      "Iteration 288, Batch: 2, Loss: 0.08937638998031616\n",
      "Iteration 288, Batch: 3, Loss: 0.08401267975568771\n",
      "Iteration 288, Batch: 4, Loss: 0.07571692764759064\n",
      "Iteration 288, Batch: 5, Loss: 0.10307139903306961\n",
      "Iteration 288, Batch: 6, Loss: 0.04544888436794281\n",
      "Iteration 288, Batch: 7, Loss: 0.04363906756043434\n",
      "Iteration 288, Batch: 8, Loss: 0.08318065106868744\n",
      "Iteration 288, Batch: 9, Loss: 0.07328281551599503\n",
      "Iteration 288, Batch: 10, Loss: 0.04079822450876236\n",
      "Iteration 288, Batch: 11, Loss: 0.07332039624452591\n",
      "Iteration 288, Batch: 12, Loss: 0.09866132587194443\n",
      "Iteration 288, Batch: 13, Loss: 0.056706350296735764\n",
      "Iteration 288, Batch: 14, Loss: 0.04207933694124222\n",
      "Iteration 288, Batch: 15, Loss: 0.08135783672332764\n",
      "Iteration 288, Batch: 16, Loss: 0.051351096481084824\n",
      "Iteration 288, Batch: 17, Loss: 0.04372232034802437\n",
      "Iteration 288, Batch: 18, Loss: 0.08779430389404297\n",
      "Iteration 288, Batch: 19, Loss: 0.09489039331674576\n",
      "Iteration 288, Batch: 20, Loss: 0.03311373293399811\n",
      "Iteration 288, Batch: 21, Loss: 0.07735996693372726\n",
      "Iteration 288, Batch: 22, Loss: 0.06530855596065521\n",
      "Iteration 288, Batch: 23, Loss: 0.061102867126464844\n",
      "Iteration 288, Batch: 24, Loss: 0.06975633651018143\n",
      "Iteration 288, Batch: 25, Loss: 0.07557428628206253\n",
      "Iteration 288, Batch: 26, Loss: 0.06690429896116257\n",
      "Iteration 288, Batch: 27, Loss: 0.06722057610750198\n",
      "Iteration 288, Batch: 28, Loss: 0.0758746787905693\n",
      "Iteration 288, Batch: 29, Loss: 0.06336695700883865\n",
      "Iteration 288, Batch: 30, Loss: 0.07028810679912567\n",
      "Iteration 288, Batch: 31, Loss: 0.07790316641330719\n",
      "Iteration 288, Batch: 32, Loss: 0.07314164936542511\n",
      "Iteration 288, Batch: 33, Loss: 0.06263624131679535\n",
      "Iteration 288, Batch: 34, Loss: 0.07554683089256287\n",
      "Iteration 288, Batch: 35, Loss: 0.09560234099626541\n",
      "Iteration 288, Batch: 36, Loss: 0.06296472251415253\n",
      "Iteration 288, Batch: 37, Loss: 0.06408967077732086\n",
      "Iteration 288, Batch: 38, Loss: 0.0791550800204277\n",
      "Iteration 288, Batch: 39, Loss: 0.05945953354239464\n",
      "Iteration 288, Batch: 40, Loss: 0.051369789987802505\n",
      "Iteration 288, Batch: 41, Loss: 0.05707782506942749\n",
      "Iteration 288, Batch: 42, Loss: 0.07998891919851303\n",
      "Iteration 288, Batch: 43, Loss: 0.054985225200653076\n",
      "Iteration 288, Batch: 44, Loss: 0.031219355762004852\n",
      "Iteration 288, Batch: 45, Loss: 0.059309348464012146\n",
      "Iteration 288, Batch: 46, Loss: 0.07260257005691528\n",
      "Iteration 288, Batch: 47, Loss: 0.07795747369527817\n",
      "Iteration 288, Batch: 48, Loss: 0.03503452613949776\n",
      "Iteration 288, Batch: 49, Loss: 0.04377302899956703\n",
      "Iteration 289, Batch: 0, Loss: 0.04437505453824997\n",
      "Iteration 289, Batch: 1, Loss: 0.0737215206027031\n",
      "Iteration 289, Batch: 2, Loss: 0.07242909073829651\n",
      "Iteration 289, Batch: 3, Loss: 0.06680576503276825\n",
      "Iteration 289, Batch: 4, Loss: 0.06915213912725449\n",
      "Iteration 289, Batch: 5, Loss: 0.07446394115686417\n",
      "Iteration 289, Batch: 6, Loss: 0.057568762451410294\n",
      "Iteration 289, Batch: 7, Loss: 0.043627724051475525\n",
      "Iteration 289, Batch: 8, Loss: 0.0656069740653038\n",
      "Iteration 289, Batch: 9, Loss: 0.053439345210790634\n",
      "Iteration 289, Batch: 10, Loss: 0.05252087488770485\n",
      "Iteration 289, Batch: 11, Loss: 0.05334664136171341\n",
      "Iteration 289, Batch: 12, Loss: 0.059173159301280975\n",
      "Iteration 289, Batch: 13, Loss: 0.074200339615345\n",
      "Iteration 289, Batch: 14, Loss: 0.07602861523628235\n",
      "Iteration 289, Batch: 15, Loss: 0.06386081874370575\n",
      "Iteration 289, Batch: 16, Loss: 0.09814800322055817\n",
      "Iteration 289, Batch: 17, Loss: 0.06866077333688736\n",
      "Iteration 289, Batch: 18, Loss: 0.10159602016210556\n",
      "Iteration 289, Batch: 19, Loss: 0.08249494433403015\n",
      "Iteration 289, Batch: 20, Loss: 0.07179522514343262\n",
      "Iteration 289, Batch: 21, Loss: 0.07575474679470062\n",
      "Iteration 289, Batch: 22, Loss: 0.045494113117456436\n",
      "Iteration 289, Batch: 23, Loss: 0.06969223916530609\n",
      "Iteration 289, Batch: 24, Loss: 0.05950005352497101\n",
      "Iteration 289, Batch: 25, Loss: 0.07518134266138077\n",
      "Iteration 289, Batch: 26, Loss: 0.08890920132398605\n",
      "Iteration 289, Batch: 27, Loss: 0.06832680851221085\n",
      "Iteration 289, Batch: 28, Loss: 0.07884297519922256\n",
      "Iteration 289, Batch: 29, Loss: 0.09016252309083939\n",
      "Iteration 289, Batch: 30, Loss: 0.06080763041973114\n",
      "Iteration 289, Batch: 31, Loss: 0.05317123606801033\n",
      "Iteration 289, Batch: 32, Loss: 0.06587856262922287\n",
      "Iteration 289, Batch: 33, Loss: 0.06798400729894638\n",
      "Iteration 289, Batch: 34, Loss: 0.0785655602812767\n",
      "Iteration 289, Batch: 35, Loss: 0.05451984331011772\n",
      "Iteration 289, Batch: 36, Loss: 0.07039931416511536\n",
      "Iteration 289, Batch: 37, Loss: 0.055892132222652435\n",
      "Iteration 289, Batch: 38, Loss: 0.033290982246398926\n",
      "Iteration 289, Batch: 39, Loss: 0.058953847736120224\n",
      "Iteration 289, Batch: 40, Loss: 0.043294843286275864\n",
      "Iteration 289, Batch: 41, Loss: 0.07192885875701904\n",
      "Iteration 289, Batch: 42, Loss: 0.06710516661405563\n",
      "Iteration 289, Batch: 43, Loss: 0.046508412808179855\n",
      "Iteration 289, Batch: 44, Loss: 0.0666033998131752\n",
      "Iteration 289, Batch: 45, Loss: 0.04160236194729805\n",
      "Iteration 289, Batch: 46, Loss: 0.037599459290504456\n",
      "Iteration 289, Batch: 47, Loss: 0.081911601126194\n",
      "Iteration 289, Batch: 48, Loss: 0.06039241701364517\n",
      "Iteration 289, Batch: 49, Loss: 0.07853294163942337\n",
      "Iteration 290, Batch: 0, Loss: 0.061942413449287415\n",
      "Iteration 290, Batch: 1, Loss: 0.07682542502880096\n",
      "Iteration 290, Batch: 2, Loss: 0.056177038699388504\n",
      "Iteration 290, Batch: 3, Loss: 0.0664467141032219\n",
      "Iteration 290, Batch: 4, Loss: 0.0412786602973938\n",
      "Iteration 290, Batch: 5, Loss: 0.06637667119503021\n",
      "Iteration 290, Batch: 6, Loss: 0.0595005601644516\n",
      "Iteration 290, Batch: 7, Loss: 0.05932445451617241\n",
      "Iteration 290, Batch: 8, Loss: 0.07238417118787766\n",
      "Iteration 290, Batch: 9, Loss: 0.0919046401977539\n",
      "Iteration 290, Batch: 10, Loss: 0.04587872326374054\n",
      "Iteration 290, Batch: 11, Loss: 0.04928603023290634\n",
      "Iteration 290, Batch: 12, Loss: 0.06326080858707428\n",
      "Iteration 290, Batch: 13, Loss: 0.06590243428945541\n",
      "Iteration 290, Batch: 14, Loss: 0.07627380639314651\n",
      "Iteration 290, Batch: 15, Loss: 0.060274265706539154\n",
      "Iteration 290, Batch: 16, Loss: 0.06628277152776718\n",
      "Iteration 290, Batch: 17, Loss: 0.054346997290849686\n",
      "Iteration 290, Batch: 18, Loss: 0.05640647932887077\n",
      "Iteration 290, Batch: 19, Loss: 0.06960918009281158\n",
      "Iteration 290, Batch: 20, Loss: 0.05834105238318443\n",
      "Iteration 290, Batch: 21, Loss: 0.04862300679087639\n",
      "Iteration 290, Batch: 22, Loss: 0.0493156723678112\n",
      "Iteration 290, Batch: 23, Loss: 0.08394691348075867\n",
      "Iteration 290, Batch: 24, Loss: 0.06400475651025772\n",
      "Iteration 290, Batch: 25, Loss: 0.0623050294816494\n",
      "Iteration 290, Batch: 26, Loss: 0.04858555644750595\n",
      "Iteration 290, Batch: 27, Loss: 0.09531477093696594\n",
      "Iteration 290, Batch: 28, Loss: 0.08217617869377136\n",
      "Iteration 290, Batch: 29, Loss: 0.08486214280128479\n",
      "Iteration 290, Batch: 30, Loss: 0.07428281009197235\n",
      "Iteration 290, Batch: 31, Loss: 0.07331105321645737\n",
      "Iteration 290, Batch: 32, Loss: 0.04299107566475868\n",
      "Iteration 290, Batch: 33, Loss: 0.05863631144165993\n",
      "Iteration 290, Batch: 34, Loss: 0.07419730722904205\n",
      "Iteration 290, Batch: 35, Loss: 0.07954338192939758\n",
      "Iteration 290, Batch: 36, Loss: 0.054211266338825226\n",
      "Iteration 290, Batch: 37, Loss: 0.07173192501068115\n",
      "Iteration 290, Batch: 38, Loss: 0.07041710615158081\n",
      "Iteration 290, Batch: 39, Loss: 0.07581795752048492\n",
      "Iteration 290, Batch: 40, Loss: 0.038316432386636734\n",
      "Iteration 290, Batch: 41, Loss: 0.07324400544166565\n",
      "Iteration 290, Batch: 42, Loss: 0.08996331691741943\n",
      "Iteration 290, Batch: 43, Loss: 0.09119844436645508\n",
      "Iteration 290, Batch: 44, Loss: 0.038082998245954514\n",
      "Iteration 290, Batch: 45, Loss: 0.05172615870833397\n",
      "Iteration 290, Batch: 46, Loss: 0.04160989820957184\n",
      "Iteration 290, Batch: 47, Loss: 0.07054685801267624\n",
      "Iteration 290, Batch: 48, Loss: 0.06491685658693314\n",
      "Iteration 290, Batch: 49, Loss: 0.05535802245140076\n",
      "Iteration 291, Batch: 0, Loss: 0.032887499779462814\n",
      "Iteration 291, Batch: 1, Loss: 0.04544474557042122\n",
      "Iteration 291, Batch: 2, Loss: 0.06102666258811951\n",
      "Iteration 291, Batch: 3, Loss: 0.0585840679705143\n",
      "Iteration 291, Batch: 4, Loss: 0.07317966222763062\n",
      "Iteration 291, Batch: 5, Loss: 0.06824736297130585\n",
      "Iteration 291, Batch: 6, Loss: 0.058056123554706573\n",
      "Iteration 291, Batch: 7, Loss: 0.06580701470375061\n",
      "Iteration 291, Batch: 8, Loss: 0.049117621034383774\n",
      "Iteration 291, Batch: 9, Loss: 0.07946140319108963\n",
      "Iteration 291, Batch: 10, Loss: 0.06086765602231026\n",
      "Iteration 291, Batch: 11, Loss: 0.10141520947217941\n",
      "Iteration 291, Batch: 12, Loss: 0.07151325047016144\n",
      "Iteration 291, Batch: 13, Loss: 0.05392594262957573\n",
      "Iteration 291, Batch: 14, Loss: 0.06771557033061981\n",
      "Iteration 291, Batch: 15, Loss: 0.09545698016881943\n",
      "Iteration 291, Batch: 16, Loss: 0.0592518113553524\n",
      "Iteration 291, Batch: 17, Loss: 0.06221318244934082\n",
      "Iteration 291, Batch: 18, Loss: 0.08025499433279037\n",
      "Iteration 291, Batch: 19, Loss: 0.05631094425916672\n",
      "Iteration 291, Batch: 20, Loss: 0.07569518685340881\n",
      "Iteration 291, Batch: 21, Loss: 0.06461768597364426\n",
      "Iteration 291, Batch: 22, Loss: 0.06095654517412186\n",
      "Iteration 291, Batch: 23, Loss: 0.055091794580221176\n",
      "Iteration 291, Batch: 24, Loss: 0.054029081016778946\n",
      "Iteration 291, Batch: 25, Loss: 0.04971405491232872\n",
      "Iteration 291, Batch: 26, Loss: 0.03689885139465332\n",
      "Iteration 291, Batch: 27, Loss: 0.06577261537313461\n",
      "Iteration 291, Batch: 28, Loss: 0.06337056308984756\n",
      "Iteration 291, Batch: 29, Loss: 0.0791233628988266\n",
      "Iteration 291, Batch: 30, Loss: 0.08116695284843445\n",
      "Iteration 291, Batch: 31, Loss: 0.07063751667737961\n",
      "Iteration 291, Batch: 32, Loss: 0.05851009115576744\n",
      "Iteration 291, Batch: 33, Loss: 0.08535102009773254\n",
      "Iteration 291, Batch: 34, Loss: 0.04919111728668213\n",
      "Iteration 291, Batch: 35, Loss: 0.04368629306554794\n",
      "Iteration 291, Batch: 36, Loss: 0.06559479236602783\n",
      "Iteration 291, Batch: 37, Loss: 0.06144513934850693\n",
      "Iteration 291, Batch: 38, Loss: 0.066614530980587\n",
      "Iteration 291, Batch: 39, Loss: 0.06732993572950363\n",
      "Iteration 291, Batch: 40, Loss: 0.08988676220178604\n",
      "Iteration 291, Batch: 41, Loss: 0.0964042991399765\n",
      "Iteration 291, Batch: 42, Loss: 0.07191788405179977\n",
      "Iteration 291, Batch: 43, Loss: 0.038835566490888596\n",
      "Iteration 291, Batch: 44, Loss: 0.05750229209661484\n",
      "Iteration 291, Batch: 45, Loss: 0.04303552582859993\n",
      "Iteration 291, Batch: 46, Loss: 0.08261231333017349\n",
      "Iteration 291, Batch: 47, Loss: 0.07173439860343933\n",
      "Iteration 291, Batch: 48, Loss: 0.06643470376729965\n",
      "Iteration 291, Batch: 49, Loss: 0.06655063480138779\n",
      "Iteration 292, Batch: 0, Loss: 0.07351464033126831\n",
      "Iteration 292, Batch: 1, Loss: 0.05467085540294647\n",
      "Iteration 292, Batch: 2, Loss: 0.061845388263463974\n",
      "Iteration 292, Batch: 3, Loss: 0.07031887024641037\n",
      "Iteration 292, Batch: 4, Loss: 0.06214816868305206\n",
      "Iteration 292, Batch: 5, Loss: 0.06650126725435257\n",
      "Iteration 292, Batch: 6, Loss: 0.07738998532295227\n",
      "Iteration 292, Batch: 7, Loss: 0.06321459263563156\n",
      "Iteration 292, Batch: 8, Loss: 0.08004207164049149\n",
      "Iteration 292, Batch: 9, Loss: 0.04619830846786499\n",
      "Iteration 292, Batch: 10, Loss: 0.07166893035173416\n",
      "Iteration 292, Batch: 11, Loss: 0.06893523782491684\n",
      "Iteration 292, Batch: 12, Loss: 0.06099892035126686\n",
      "Iteration 292, Batch: 13, Loss: 0.0643174946308136\n",
      "Iteration 292, Batch: 14, Loss: 0.08213531225919724\n",
      "Iteration 292, Batch: 15, Loss: 0.10631565004587173\n",
      "Iteration 292, Batch: 16, Loss: 0.07113584131002426\n",
      "Iteration 292, Batch: 17, Loss: 0.06170692667365074\n",
      "Iteration 292, Batch: 18, Loss: 0.06473074853420258\n",
      "Iteration 292, Batch: 19, Loss: 0.05667833983898163\n",
      "Iteration 292, Batch: 20, Loss: 0.09282334893941879\n",
      "Iteration 292, Batch: 21, Loss: 0.06410824507474899\n",
      "Iteration 292, Batch: 22, Loss: 0.07445379346609116\n",
      "Iteration 292, Batch: 23, Loss: 0.0549258328974247\n",
      "Iteration 292, Batch: 24, Loss: 0.05359799414873123\n",
      "Iteration 292, Batch: 25, Loss: 0.07562322169542313\n",
      "Iteration 292, Batch: 26, Loss: 0.05456458032131195\n",
      "Iteration 292, Batch: 27, Loss: 0.03461576998233795\n",
      "Iteration 292, Batch: 28, Loss: 0.04187380522489548\n",
      "Iteration 292, Batch: 29, Loss: 0.044862013310194016\n",
      "Iteration 292, Batch: 30, Loss: 0.055498361587524414\n",
      "Iteration 292, Batch: 31, Loss: 0.05674830451607704\n",
      "Iteration 292, Batch: 32, Loss: 0.04967648908495903\n",
      "Iteration 292, Batch: 33, Loss: 0.04232794791460037\n",
      "Iteration 292, Batch: 34, Loss: 0.05435337871313095\n",
      "Iteration 292, Batch: 35, Loss: 0.0641508474946022\n",
      "Iteration 292, Batch: 36, Loss: 0.04987986013293266\n",
      "Iteration 292, Batch: 37, Loss: 0.054320599883794785\n",
      "Iteration 292, Batch: 38, Loss: 0.057041022926568985\n",
      "Iteration 292, Batch: 39, Loss: 0.053594231605529785\n",
      "Iteration 292, Batch: 40, Loss: 0.07402629405260086\n",
      "Iteration 292, Batch: 41, Loss: 0.07941992580890656\n",
      "Iteration 292, Batch: 42, Loss: 0.06622075289487839\n",
      "Iteration 292, Batch: 43, Loss: 0.053453318774700165\n",
      "Iteration 292, Batch: 44, Loss: 0.04562675952911377\n",
      "Iteration 292, Batch: 45, Loss: 0.07922033965587616\n",
      "Iteration 292, Batch: 46, Loss: 0.09859675914049149\n",
      "Iteration 292, Batch: 47, Loss: 0.061788272112607956\n",
      "Iteration 292, Batch: 48, Loss: 0.07455474883317947\n",
      "Iteration 292, Batch: 49, Loss: 0.06772004812955856\n",
      "Iteration 293, Batch: 0, Loss: 0.04537105932831764\n",
      "Iteration 293, Batch: 1, Loss: 0.08323660492897034\n",
      "Iteration 293, Batch: 2, Loss: 0.04692431911826134\n",
      "Iteration 293, Batch: 3, Loss: 0.07871243357658386\n",
      "Iteration 293, Batch: 4, Loss: 0.08484583348035812\n",
      "Iteration 293, Batch: 5, Loss: 0.0233528520911932\n",
      "Iteration 293, Batch: 6, Loss: 0.06313955038785934\n",
      "Iteration 293, Batch: 7, Loss: 0.05789736658334732\n",
      "Iteration 293, Batch: 8, Loss: 0.07753896713256836\n",
      "Iteration 293, Batch: 9, Loss: 0.060429200530052185\n",
      "Iteration 293, Batch: 10, Loss: 0.06237414851784706\n",
      "Iteration 293, Batch: 11, Loss: 0.040133822709321976\n",
      "Iteration 293, Batch: 12, Loss: 0.07253726571798325\n",
      "Iteration 293, Batch: 13, Loss: 0.06961548328399658\n",
      "Iteration 293, Batch: 14, Loss: 0.06881584227085114\n",
      "Iteration 293, Batch: 15, Loss: 0.07693597674369812\n",
      "Iteration 293, Batch: 16, Loss: 0.0663393959403038\n",
      "Iteration 293, Batch: 17, Loss: 0.05697357654571533\n",
      "Iteration 293, Batch: 18, Loss: 0.02918386459350586\n",
      "Iteration 293, Batch: 19, Loss: 0.058728888630867004\n",
      "Iteration 293, Batch: 20, Loss: 0.07932339608669281\n",
      "Iteration 293, Batch: 21, Loss: 0.06548826396465302\n",
      "Iteration 293, Batch: 22, Loss: 0.03981062024831772\n",
      "Iteration 293, Batch: 23, Loss: 0.0583195686340332\n",
      "Iteration 293, Batch: 24, Loss: 0.06342415511608124\n",
      "Iteration 293, Batch: 25, Loss: 0.043211109936237335\n",
      "Iteration 293, Batch: 26, Loss: 0.05409184470772743\n",
      "Iteration 293, Batch: 27, Loss: 0.036296892911195755\n",
      "Iteration 293, Batch: 28, Loss: 0.05942704901099205\n",
      "Iteration 293, Batch: 29, Loss: 0.04779697209596634\n",
      "Iteration 293, Batch: 30, Loss: 0.0649944469332695\n",
      "Iteration 293, Batch: 31, Loss: 0.03878675773739815\n",
      "Iteration 293, Batch: 32, Loss: 0.06572279334068298\n",
      "Iteration 293, Batch: 33, Loss: 0.06922189146280289\n",
      "Iteration 293, Batch: 34, Loss: 0.07868558168411255\n",
      "Iteration 293, Batch: 35, Loss: 0.0547073632478714\n",
      "Iteration 293, Batch: 36, Loss: 0.060649268329143524\n",
      "Iteration 293, Batch: 37, Loss: 0.05412526801228523\n",
      "Iteration 293, Batch: 38, Loss: 0.07293084263801575\n",
      "Iteration 293, Batch: 39, Loss: 0.05374003201723099\n",
      "Iteration 293, Batch: 40, Loss: 0.06319203972816467\n",
      "Iteration 293, Batch: 41, Loss: 0.05253768712282181\n",
      "Iteration 293, Batch: 42, Loss: 0.05268855020403862\n",
      "Iteration 293, Batch: 43, Loss: 0.039484210312366486\n",
      "Iteration 293, Batch: 44, Loss: 0.09957105666399002\n",
      "Iteration 293, Batch: 45, Loss: 0.059900760650634766\n",
      "Iteration 293, Batch: 46, Loss: 0.0660257339477539\n",
      "Iteration 293, Batch: 47, Loss: 0.05260168015956879\n",
      "Iteration 293, Batch: 48, Loss: 0.06129887327551842\n",
      "Iteration 293, Batch: 49, Loss: 0.06977609544992447\n",
      "Iteration 294, Batch: 0, Loss: 0.08858766406774521\n",
      "Iteration 294, Batch: 1, Loss: 0.061658646911382675\n",
      "Iteration 294, Batch: 2, Loss: 0.04060244932770729\n",
      "Iteration 294, Batch: 3, Loss: 0.044064853340387344\n",
      "Iteration 294, Batch: 4, Loss: 0.038814954459667206\n",
      "Iteration 294, Batch: 5, Loss: 0.09065699577331543\n",
      "Iteration 294, Batch: 6, Loss: 0.056220877915620804\n",
      "Iteration 294, Batch: 7, Loss: 0.05999007076025009\n",
      "Iteration 294, Batch: 8, Loss: 0.043002013117074966\n",
      "Iteration 294, Batch: 9, Loss: 0.034968141466379166\n",
      "Iteration 294, Batch: 10, Loss: 0.05885319039225578\n",
      "Iteration 294, Batch: 11, Loss: 0.07862407714128494\n",
      "Iteration 294, Batch: 12, Loss: 0.05616401880979538\n",
      "Iteration 294, Batch: 13, Loss: 0.06307505816221237\n",
      "Iteration 294, Batch: 14, Loss: 0.04560908302664757\n",
      "Iteration 294, Batch: 15, Loss: 0.052524492144584656\n",
      "Iteration 294, Batch: 16, Loss: 0.07686823606491089\n",
      "Iteration 294, Batch: 17, Loss: 0.05428817495703697\n",
      "Iteration 294, Batch: 18, Loss: 0.0614856593310833\n",
      "Iteration 294, Batch: 19, Loss: 0.037801988422870636\n",
      "Iteration 294, Batch: 20, Loss: 0.04756515473127365\n",
      "Iteration 294, Batch: 21, Loss: 0.04279254749417305\n",
      "Iteration 294, Batch: 22, Loss: 0.03508761525154114\n",
      "Iteration 294, Batch: 23, Loss: 0.06284737586975098\n",
      "Iteration 294, Batch: 24, Loss: 0.035859983414411545\n",
      "Iteration 294, Batch: 25, Loss: 0.03830670192837715\n",
      "Iteration 294, Batch: 26, Loss: 0.044288888573646545\n",
      "Iteration 294, Batch: 27, Loss: 0.044424254447221756\n",
      "Iteration 294, Batch: 28, Loss: 0.07157190144062042\n",
      "Iteration 294, Batch: 29, Loss: 0.04041426628828049\n",
      "Iteration 294, Batch: 30, Loss: 0.05404191464185715\n",
      "Iteration 294, Batch: 31, Loss: 0.025485750287771225\n",
      "Iteration 294, Batch: 32, Loss: 0.038906507194042206\n",
      "Iteration 294, Batch: 33, Loss: 0.05037578567862511\n",
      "Iteration 294, Batch: 34, Loss: 0.04545645788311958\n",
      "Iteration 294, Batch: 35, Loss: 0.035681698471307755\n",
      "Iteration 294, Batch: 36, Loss: 0.0464954748749733\n",
      "Iteration 294, Batch: 37, Loss: 0.04980534315109253\n",
      "Iteration 294, Batch: 38, Loss: 0.046289458870887756\n",
      "Iteration 294, Batch: 39, Loss: 0.05025487765669823\n",
      "Iteration 294, Batch: 40, Loss: 0.04978080093860626\n",
      "Iteration 294, Batch: 41, Loss: 0.029980765655636787\n",
      "Iteration 294, Batch: 42, Loss: 0.06744401901960373\n",
      "Iteration 294, Batch: 43, Loss: 0.03531891852617264\n",
      "Iteration 294, Batch: 44, Loss: 0.0655762106180191\n",
      "Iteration 294, Batch: 45, Loss: 0.04303937405347824\n",
      "Iteration 294, Batch: 46, Loss: 0.06206681206822395\n",
      "Iteration 294, Batch: 47, Loss: 0.045903682708740234\n",
      "Iteration 294, Batch: 48, Loss: 0.05410521849989891\n",
      "Iteration 294, Batch: 49, Loss: 0.05350084975361824\n",
      "Iteration 295, Batch: 0, Loss: 0.06595244258642197\n",
      "Iteration 295, Batch: 1, Loss: 0.05570842698216438\n",
      "Iteration 295, Batch: 2, Loss: 0.04755575582385063\n",
      "Iteration 295, Batch: 3, Loss: 0.044103916734457016\n",
      "Iteration 295, Batch: 4, Loss: 0.05172038450837135\n",
      "Iteration 295, Batch: 5, Loss: 0.059136271476745605\n",
      "Iteration 295, Batch: 6, Loss: 0.054948125034570694\n",
      "Iteration 295, Batch: 7, Loss: 0.06596600264310837\n",
      "Iteration 295, Batch: 8, Loss: 0.042594339698553085\n",
      "Iteration 295, Batch: 9, Loss: 0.05043422430753708\n",
      "Iteration 295, Batch: 10, Loss: 0.0757904052734375\n",
      "Iteration 295, Batch: 11, Loss: 0.05581967160105705\n",
      "Iteration 295, Batch: 12, Loss: 0.036974161863327026\n",
      "Iteration 295, Batch: 13, Loss: 0.046848125755786896\n",
      "Iteration 295, Batch: 14, Loss: 0.04047878086566925\n",
      "Iteration 295, Batch: 15, Loss: 0.0440041609108448\n",
      "Iteration 295, Batch: 16, Loss: 0.043489377945661545\n",
      "Iteration 295, Batch: 17, Loss: 0.03536998853087425\n",
      "Iteration 295, Batch: 18, Loss: 0.04544378072023392\n",
      "Iteration 295, Batch: 19, Loss: 0.07142070680856705\n",
      "Iteration 295, Batch: 20, Loss: 0.04687807336449623\n",
      "Iteration 295, Batch: 21, Loss: 0.06547944992780685\n",
      "Iteration 295, Batch: 22, Loss: 0.0762542337179184\n",
      "Iteration 295, Batch: 23, Loss: 0.05863200128078461\n",
      "Iteration 295, Batch: 24, Loss: 0.06263906508684158\n",
      "Iteration 295, Batch: 25, Loss: 0.09976162761449814\n",
      "Iteration 295, Batch: 26, Loss: 0.06484965980052948\n",
      "Iteration 295, Batch: 27, Loss: 0.06761310249567032\n",
      "Iteration 295, Batch: 28, Loss: 0.03618589788675308\n",
      "Iteration 295, Batch: 29, Loss: 0.058999769389629364\n",
      "Iteration 295, Batch: 30, Loss: 0.05473624914884567\n",
      "Iteration 295, Batch: 31, Loss: 0.05103845149278641\n",
      "Iteration 295, Batch: 32, Loss: 0.054446201771497726\n",
      "Iteration 295, Batch: 33, Loss: 0.03968660533428192\n",
      "Iteration 295, Batch: 34, Loss: 0.06967408210039139\n",
      "Iteration 295, Batch: 35, Loss: 0.06528358161449432\n",
      "Iteration 295, Batch: 36, Loss: 0.09778105467557907\n",
      "Iteration 295, Batch: 37, Loss: 0.05028408020734787\n",
      "Iteration 295, Batch: 38, Loss: 0.06017572432756424\n",
      "Iteration 295, Batch: 39, Loss: 0.0692281424999237\n",
      "Iteration 295, Batch: 40, Loss: 0.040469877421855927\n",
      "Iteration 295, Batch: 41, Loss: 0.048115409910678864\n",
      "Iteration 295, Batch: 42, Loss: 0.06232834607362747\n",
      "Iteration 295, Batch: 43, Loss: 0.037986524403095245\n",
      "Iteration 295, Batch: 44, Loss: 0.050191864371299744\n",
      "Iteration 295, Batch: 45, Loss: 0.05014850199222565\n",
      "Iteration 295, Batch: 46, Loss: 0.02425941452383995\n",
      "Iteration 295, Batch: 47, Loss: 0.07308029383420944\n",
      "Iteration 295, Batch: 48, Loss: 0.06454005837440491\n",
      "Iteration 295, Batch: 49, Loss: 0.038357049226760864\n",
      "Iteration 296, Batch: 0, Loss: 0.048766057938337326\n",
      "Iteration 296, Batch: 1, Loss: 0.06523299217224121\n",
      "Iteration 296, Batch: 2, Loss: 0.034575410187244415\n",
      "Iteration 296, Batch: 3, Loss: 0.03381101414561272\n",
      "Iteration 296, Batch: 4, Loss: 0.056836217641830444\n",
      "Iteration 296, Batch: 5, Loss: 0.04914756491780281\n",
      "Iteration 296, Batch: 6, Loss: 0.06420160830020905\n",
      "Iteration 296, Batch: 7, Loss: 0.04791497066617012\n",
      "Iteration 296, Batch: 8, Loss: 0.05452485755085945\n",
      "Iteration 296, Batch: 9, Loss: 0.04204285517334938\n",
      "Iteration 296, Batch: 10, Loss: 0.06097232550382614\n",
      "Iteration 296, Batch: 11, Loss: 0.05486362427473068\n",
      "Iteration 296, Batch: 12, Loss: 0.05171402916312218\n",
      "Iteration 296, Batch: 13, Loss: 0.05959023907780647\n",
      "Iteration 296, Batch: 14, Loss: 0.058450162410736084\n",
      "Iteration 296, Batch: 15, Loss: 0.046817597001791\n",
      "Iteration 296, Batch: 16, Loss: 0.03888356685638428\n",
      "Iteration 296, Batch: 17, Loss: 0.03487880155444145\n",
      "Iteration 296, Batch: 18, Loss: 0.04458004981279373\n",
      "Iteration 296, Batch: 19, Loss: 0.05275687947869301\n",
      "Iteration 296, Batch: 20, Loss: 0.05644043907523155\n",
      "Iteration 296, Batch: 21, Loss: 0.041022565215826035\n",
      "Iteration 296, Batch: 22, Loss: 0.047154560685157776\n",
      "Iteration 296, Batch: 23, Loss: 0.047989919781684875\n",
      "Iteration 296, Batch: 24, Loss: 0.06541750580072403\n",
      "Iteration 296, Batch: 25, Loss: 0.04372445493936539\n",
      "Iteration 296, Batch: 26, Loss: 0.05797687917947769\n",
      "Iteration 296, Batch: 27, Loss: 0.04450080171227455\n",
      "Iteration 296, Batch: 28, Loss: 0.07111253589391708\n",
      "Iteration 296, Batch: 29, Loss: 0.051741838455200195\n",
      "Iteration 296, Batch: 30, Loss: 0.04582630842924118\n",
      "Iteration 296, Batch: 31, Loss: 0.06605973839759827\n",
      "Iteration 296, Batch: 32, Loss: 0.07739865034818649\n",
      "Iteration 296, Batch: 33, Loss: 0.05057374760508537\n",
      "Iteration 296, Batch: 34, Loss: 0.04690495878458023\n",
      "Iteration 296, Batch: 35, Loss: 0.05147264897823334\n",
      "Iteration 296, Batch: 36, Loss: 0.04421057179570198\n",
      "Iteration 296, Batch: 37, Loss: 0.04207247123122215\n",
      "Iteration 296, Batch: 38, Loss: 0.065635547041893\n",
      "Iteration 296, Batch: 39, Loss: 0.058368146419525146\n",
      "Iteration 296, Batch: 40, Loss: 0.05978046730160713\n",
      "Iteration 296, Batch: 41, Loss: 0.03232743963599205\n",
      "Iteration 296, Batch: 42, Loss: 0.03929317370057106\n",
      "Iteration 296, Batch: 43, Loss: 0.0583813413977623\n",
      "Iteration 296, Batch: 44, Loss: 0.06487980484962463\n",
      "Iteration 296, Batch: 45, Loss: 0.06755457073450089\n",
      "Iteration 296, Batch: 46, Loss: 0.07317646592855453\n",
      "Iteration 296, Batch: 47, Loss: 0.05957137048244476\n",
      "Iteration 296, Batch: 48, Loss: 0.06157610937952995\n",
      "Iteration 296, Batch: 49, Loss: 0.05313350632786751\n",
      "Iteration 297, Batch: 0, Loss: 0.06607673317193985\n",
      "Iteration 297, Batch: 1, Loss: 0.06282937526702881\n",
      "Iteration 297, Batch: 2, Loss: 0.06892363727092743\n",
      "Iteration 297, Batch: 3, Loss: 0.06989292800426483\n",
      "Iteration 297, Batch: 4, Loss: 0.03870754316449165\n",
      "Iteration 297, Batch: 5, Loss: 0.05611969903111458\n",
      "Iteration 297, Batch: 6, Loss: 0.05546034872531891\n",
      "Iteration 297, Batch: 7, Loss: 0.040689047425985336\n",
      "Iteration 297, Batch: 8, Loss: 0.04276768118143082\n",
      "Iteration 297, Batch: 9, Loss: 0.06168272718787193\n",
      "Iteration 297, Batch: 10, Loss: 0.06211113929748535\n",
      "Iteration 297, Batch: 11, Loss: 0.05532890185713768\n",
      "Iteration 297, Batch: 12, Loss: 0.0731770321726799\n",
      "Iteration 297, Batch: 13, Loss: 0.053234364837408066\n",
      "Iteration 297, Batch: 14, Loss: 0.050806283950805664\n",
      "Iteration 297, Batch: 15, Loss: 0.05425545573234558\n",
      "Iteration 297, Batch: 16, Loss: 0.05839712545275688\n",
      "Iteration 297, Batch: 17, Loss: 0.04618049040436745\n",
      "Iteration 297, Batch: 18, Loss: 0.03266967087984085\n",
      "Iteration 297, Batch: 19, Loss: 0.015119345858693123\n",
      "Iteration 297, Batch: 20, Loss: 0.04496479034423828\n",
      "Iteration 297, Batch: 21, Loss: 0.06351891160011292\n",
      "Iteration 297, Batch: 22, Loss: 0.07743988186120987\n",
      "Iteration 297, Batch: 23, Loss: 0.06777554005384445\n",
      "Iteration 297, Batch: 24, Loss: 0.052350301295518875\n",
      "Iteration 297, Batch: 25, Loss: 0.053312528878450394\n",
      "Iteration 297, Batch: 26, Loss: 0.040771760046482086\n",
      "Iteration 297, Batch: 27, Loss: 0.0323004350066185\n",
      "Iteration 297, Batch: 28, Loss: 0.04737774282693863\n",
      "Iteration 297, Batch: 29, Loss: 0.04337989538908005\n",
      "Iteration 297, Batch: 30, Loss: 0.024047818034887314\n",
      "Iteration 297, Batch: 31, Loss: 0.0624738410115242\n",
      "Iteration 297, Batch: 32, Loss: 0.0557880662381649\n",
      "Iteration 297, Batch: 33, Loss: 0.06168011575937271\n",
      "Iteration 297, Batch: 34, Loss: 0.051663681864738464\n",
      "Iteration 297, Batch: 35, Loss: 0.061893071979284286\n",
      "Iteration 297, Batch: 36, Loss: 0.03664148226380348\n",
      "Iteration 297, Batch: 37, Loss: 0.05509915202856064\n",
      "Iteration 297, Batch: 38, Loss: 0.04432675242424011\n",
      "Iteration 297, Batch: 39, Loss: 0.04060785099864006\n",
      "Iteration 297, Batch: 40, Loss: 0.0705571100115776\n",
      "Iteration 297, Batch: 41, Loss: 0.026679852977395058\n",
      "Iteration 297, Batch: 42, Loss: 0.100811667740345\n",
      "Iteration 297, Batch: 43, Loss: 0.07306885719299316\n",
      "Iteration 297, Batch: 44, Loss: 0.05038494989275932\n",
      "Iteration 297, Batch: 45, Loss: 0.08369871973991394\n",
      "Iteration 297, Batch: 46, Loss: 0.04898238182067871\n",
      "Iteration 297, Batch: 47, Loss: 0.09015379101037979\n",
      "Iteration 297, Batch: 48, Loss: 0.059136681258678436\n",
      "Iteration 297, Batch: 49, Loss: 0.047644615173339844\n",
      "Iteration 298, Batch: 0, Loss: 0.048728570342063904\n",
      "Iteration 298, Batch: 1, Loss: 0.05781269818544388\n",
      "Iteration 298, Batch: 2, Loss: 0.07436347007751465\n",
      "Iteration 298, Batch: 3, Loss: 0.06269638985395432\n",
      "Iteration 298, Batch: 4, Loss: 0.053204283118247986\n",
      "Iteration 298, Batch: 5, Loss: 0.04318409785628319\n",
      "Iteration 298, Batch: 6, Loss: 0.04200504347681999\n",
      "Iteration 298, Batch: 7, Loss: 0.023985881358385086\n",
      "Iteration 298, Batch: 8, Loss: 0.06618163734674454\n",
      "Iteration 298, Batch: 9, Loss: 0.028072426095604897\n",
      "Iteration 298, Batch: 10, Loss: 0.0462704561650753\n",
      "Iteration 298, Batch: 11, Loss: 0.06422588974237442\n",
      "Iteration 298, Batch: 12, Loss: 0.04758048057556152\n",
      "Iteration 298, Batch: 13, Loss: 0.05682450532913208\n",
      "Iteration 298, Batch: 14, Loss: 0.03924693167209625\n",
      "Iteration 298, Batch: 15, Loss: 0.04255625605583191\n",
      "Iteration 298, Batch: 16, Loss: 0.02547728456556797\n",
      "Iteration 298, Batch: 17, Loss: 0.04817749932408333\n",
      "Iteration 298, Batch: 18, Loss: 0.054692819714546204\n",
      "Iteration 298, Batch: 19, Loss: 0.06886565685272217\n",
      "Iteration 298, Batch: 20, Loss: 0.04803619161248207\n",
      "Iteration 298, Batch: 21, Loss: 0.050744082778692245\n",
      "Iteration 298, Batch: 22, Loss: 0.03936384618282318\n",
      "Iteration 298, Batch: 23, Loss: 0.041596416383981705\n",
      "Iteration 298, Batch: 24, Loss: 0.04307723790407181\n",
      "Iteration 298, Batch: 25, Loss: 0.05289846658706665\n",
      "Iteration 298, Batch: 26, Loss: 0.05940374359488487\n",
      "Iteration 298, Batch: 27, Loss: 0.048318441957235336\n",
      "Iteration 298, Batch: 28, Loss: 0.06731480360031128\n",
      "Iteration 298, Batch: 29, Loss: 0.08238629251718521\n",
      "Iteration 298, Batch: 30, Loss: 0.07297178357839584\n",
      "Iteration 298, Batch: 31, Loss: 0.0506502129137516\n",
      "Iteration 298, Batch: 32, Loss: 0.06342224776744843\n",
      "Iteration 298, Batch: 33, Loss: 0.0403250977396965\n",
      "Iteration 298, Batch: 34, Loss: 0.039237964898347855\n",
      "Iteration 298, Batch: 35, Loss: 0.02458379976451397\n",
      "Iteration 298, Batch: 36, Loss: 0.06386981904506683\n",
      "Iteration 298, Batch: 37, Loss: 0.04977444186806679\n",
      "Iteration 298, Batch: 38, Loss: 0.08785596489906311\n",
      "Iteration 298, Batch: 39, Loss: 0.04507637768983841\n",
      "Iteration 298, Batch: 40, Loss: 0.025573330000042915\n",
      "Iteration 298, Batch: 41, Loss: 0.04240872338414192\n",
      "Iteration 298, Batch: 42, Loss: 0.05034544691443443\n",
      "Iteration 298, Batch: 43, Loss: 0.05604284256696701\n",
      "Iteration 298, Batch: 44, Loss: 0.06742589175701141\n",
      "Iteration 298, Batch: 45, Loss: 0.06454300135374069\n",
      "Iteration 298, Batch: 46, Loss: 0.0534689761698246\n",
      "Iteration 298, Batch: 47, Loss: 0.07568534463644028\n",
      "Iteration 298, Batch: 48, Loss: 0.05807473883032799\n",
      "Iteration 298, Batch: 49, Loss: 0.07786884158849716\n",
      "Iteration 299, Batch: 0, Loss: 0.07233258336782455\n",
      "Iteration 299, Batch: 1, Loss: 0.0478055402636528\n",
      "Iteration 299, Batch: 2, Loss: 0.04580209031701088\n",
      "Iteration 299, Batch: 3, Loss: 0.06558184325695038\n",
      "Iteration 299, Batch: 4, Loss: 0.07342375069856644\n",
      "Iteration 299, Batch: 5, Loss: 0.04325849562883377\n",
      "Iteration 299, Batch: 6, Loss: 0.06685861200094223\n",
      "Iteration 299, Batch: 7, Loss: 0.056656911969184875\n",
      "Iteration 299, Batch: 8, Loss: 0.04680592566728592\n",
      "Iteration 299, Batch: 9, Loss: 0.037620436400175095\n",
      "Iteration 299, Batch: 10, Loss: 0.05555170029401779\n",
      "Iteration 299, Batch: 11, Loss: 0.05263756215572357\n",
      "Iteration 299, Batch: 12, Loss: 0.03958392143249512\n",
      "Iteration 299, Batch: 13, Loss: 0.0343741849064827\n",
      "Iteration 299, Batch: 14, Loss: 0.07364889234304428\n",
      "Iteration 299, Batch: 15, Loss: 0.05174432694911957\n",
      "Iteration 299, Batch: 16, Loss: 0.0464622862637043\n",
      "Iteration 299, Batch: 17, Loss: 0.04128371924161911\n",
      "Iteration 299, Batch: 18, Loss: 0.06954304873943329\n",
      "Iteration 299, Batch: 19, Loss: 0.04965165629982948\n",
      "Iteration 299, Batch: 20, Loss: 0.07431679219007492\n",
      "Iteration 299, Batch: 21, Loss: 0.0551101379096508\n",
      "Iteration 299, Batch: 22, Loss: 0.02010476589202881\n",
      "Iteration 299, Batch: 23, Loss: 0.07176756858825684\n",
      "Iteration 299, Batch: 24, Loss: 0.08929992467164993\n",
      "Iteration 299, Batch: 25, Loss: 0.05931645259261131\n",
      "Iteration 299, Batch: 26, Loss: 0.07478371262550354\n",
      "Iteration 299, Batch: 27, Loss: 0.0359320193529129\n",
      "Iteration 299, Batch: 28, Loss: 0.06656350940465927\n",
      "Iteration 299, Batch: 29, Loss: 0.053999513387680054\n",
      "Iteration 299, Batch: 30, Loss: 0.05974443629384041\n",
      "Iteration 299, Batch: 31, Loss: 0.06093562766909599\n",
      "Iteration 299, Batch: 32, Loss: 0.06482185423374176\n",
      "Iteration 299, Batch: 33, Loss: 0.04478185251355171\n",
      "Iteration 299, Batch: 34, Loss: 0.03592637926340103\n",
      "Iteration 299, Batch: 35, Loss: 0.061510875821113586\n",
      "Iteration 299, Batch: 36, Loss: 0.054632872343063354\n",
      "Iteration 299, Batch: 37, Loss: 0.027372751384973526\n",
      "Iteration 299, Batch: 38, Loss: 0.056887272745370865\n",
      "Iteration 299, Batch: 39, Loss: 0.04336614906787872\n",
      "Iteration 299, Batch: 40, Loss: 0.07721413671970367\n",
      "Iteration 299, Batch: 41, Loss: 0.08049632608890533\n",
      "Iteration 299, Batch: 42, Loss: 0.07814241945743561\n",
      "Iteration 299, Batch: 43, Loss: 0.05001801252365112\n",
      "Iteration 299, Batch: 44, Loss: 0.07292986661195755\n",
      "Iteration 299, Batch: 45, Loss: 0.06502790004014969\n",
      "Iteration 299, Batch: 46, Loss: 0.055984631180763245\n",
      "Iteration 299, Batch: 47, Loss: 0.04741711542010307\n",
      "Iteration 299, Batch: 48, Loss: 0.05114249885082245\n",
      "Iteration 299, Batch: 49, Loss: 0.059018027037382126\n"
     ]
    }
   ],
   "source": [
    "# First fine-tuning round for n = 350. Do not touch for n = 150\n",
    "# Specify parameters\n",
    "\"\"\"\n",
    "N = 10000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Reload model\n",
    "model = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10, alpha=0.5)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"model_weights_intermediate_1.pth\"))\n",
    "\n",
    "# Create dataset for training\n",
    "train_data = sample_data(N, n, k)\n",
    "train_dataset = SubmatrixDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "# Specify parameters\n",
    "num_epochs = 300\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "# When fine-tune, have to reset min_loss to take into account the new points\n",
    "min_loss = float(\"inf\")\n",
    "model_opt = deepcopy(model)\n",
    "\"\"\"\n",
    "for it in range(228, num_epochs):\n",
    "    counter = 0\n",
    "    loss_total = 0\n",
    "    for batch in train_dataloader:\n",
    "        # Get loss from model\n",
    "        loss = mc_loss_batch_simul(model, batch, time_array, n, k, num_steps=None, time_threshold=325, p_bad=0.05)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "        # Backpropagate\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Iteration {}, Batch: {}, Loss: {}\".format(it, counter, loss.item()))\n",
    "\n",
    "        # Update counter\n",
    "        counter += 1\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    # Update best model\n",
    "    with torch.no_grad():\n",
    "        if loss_total < min_loss:\n",
    "            min_loss = loss_total\n",
    "            model_opt = deepcopy(model)\n",
    "\n",
    "# Save optimal model to save time for generation\n",
    "torch.save(model_opt.state_dict(), \"model_weights_intermediate_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d40dc00b-99a7-4cc9-8b9f-a2bbb9e9a763",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First fine-tuning round for n = 350. Do not touch for n = 150\n",
    "# Specify parameters\n",
    "\"\"\"\n",
    "N = 10000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Reload model\n",
    "model = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10, alpha=0.5)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"model_weights_intermediate_2.pth\"))\n",
    "\n",
    "# Create dataset for training\n",
    "train_data = sample_data(N, n, k)\n",
    "train_dataset = SubmatrixDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "# Specify parameters\n",
    "num_epochs = 300\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=7e-4)\n",
    "\n",
    "# When fine-tune, have to reset min_loss to take into account the new points\n",
    "min_loss = float(\"inf\")\n",
    "model_opt = deepcopy(model)\n",
    "\n",
    "for it in range(225, num_epochs):\n",
    "    counter = 0\n",
    "    loss_total = 0\n",
    "    for batch in train_dataloader:\n",
    "        # Get loss from model\n",
    "        loss = mc_loss_batch_simul(model, batch, time_array, n, k, num_steps=None, time_threshold=205, p_bad=0.05)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.9)\n",
    "    \n",
    "        # Backpropagate\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Iteration {}, Batch: {}, Loss: {}\".format(it, counter, loss.item()))\n",
    "\n",
    "        # Update counter\n",
    "        counter += 1\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    # Update best model\n",
    "    with torch.no_grad():\n",
    "        if loss_total < min_loss:\n",
    "            min_loss = loss_total\n",
    "            model_opt = deepcopy(model)\n",
    "\"\"\"\n",
    "# Save optimal model to save time for generation\n",
    "torch.save(model_opt.state_dict(), \"model_weights_opt_350.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f54b759e-fffd-4536-85b4-83d826c1446f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 225, Loss: 0.005832150112837553\n",
      "Batch: 225, Loss: 0.00164308724924922\n",
      "Batch: 225, Loss: 0.003085605800151825\n",
      "Batch: 225, Loss: 0.005660827737301588\n",
      "Batch: 225, Loss: 0.0013992857420817018\n",
      "Batch: 225, Loss: 0.006659707520157099\n",
      "Batch: 225, Loss: 0.0014007798163220286\n",
      "Batch: 225, Loss: 0.0023490944877266884\n",
      "Batch: 225, Loss: 0.001954854466021061\n",
      "Batch: 225, Loss: 0.00428406335413456\n",
      "Batch: 225, Loss: 0.0050771888345479965\n",
      "Batch: 225, Loss: 0.0036090058274567127\n",
      "Batch: 225, Loss: 0.0021852045319974422\n",
      "Batch: 225, Loss: 0.002741427393630147\n",
      "Batch: 225, Loss: 0.0055734217166900635\n",
      "Batch: 225, Loss: 0.00492010498419404\n",
      "Batch: 225, Loss: 0.0009869051864370704\n",
      "Batch: 225, Loss: 0.0004674903175327927\n",
      "Batch: 225, Loss: 0.002226416952908039\n",
      "Batch: 225, Loss: 0.0036675489973276854\n",
      "Batch: 225, Loss: 0.0015513957478106022\n",
      "Batch: 225, Loss: 0.0033184916246682405\n",
      "Batch: 225, Loss: 0.00352348736487329\n",
      "Batch: 225, Loss: 0.004214061889797449\n",
      "Batch: 225, Loss: 0.0005769624258391559\n",
      "Batch: 225, Loss: 0.0037492706906050444\n",
      "Batch: 225, Loss: 0.004674662835896015\n",
      "Batch: 225, Loss: 0.0015718715731054544\n",
      "Batch: 225, Loss: 0.002293354133144021\n",
      "Batch: 225, Loss: 0.00686635123565793\n",
      "Mean loss: 0.003268802685003417\n"
     ]
    }
   ],
   "source": [
    "# Specify parameters\n",
    "N = 15000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "test_data = sample_data(N, n, k)\n",
    "test_dataset = SubmatrixDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=500, shuffle=True)\n",
    "\n",
    "# Test with same time\n",
    "t = 600\n",
    "\n",
    "# Evaluate test loss\n",
    "with torch.no_grad():\n",
    "    loss_total = 0\n",
    "    counter = 0\n",
    "    for batch in test_dataloader:\n",
    "        # Get loss from model\n",
    "        loss = mc_loss_batch_fixed(model_opt, batch, t, n, k, num_steps=None)\n",
    "        \n",
    "        print(\"Batch: {}, Loss: {}\".format(it, loss.item()))\n",
    "\n",
    "        # Update counter\n",
    "        counter += 1\n",
    "        loss_total += loss.item() * batch.shape[0]\n",
    "\n",
    "    loss_mean = loss_total / len(test_dataloader.dataset)\n",
    "    print(\"Mean loss: {}\".format(loss_mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7f6a83-62a0-4fc8-bf2a-afa60af4ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save optimal model to save time for generation\n",
    "# Comment next line out if model is already trained.\n",
    "# torch.save(model_opt.state_dict(), \"model_weights_opt_350.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0529270-d542-4065-9041-b9d65cb514bc",
   "metadata": {},
   "source": [
    "# Work on the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c46e598e-99d5-44e0-85d0-efb737ce9f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload model\n",
    "n = 350\n",
    "k = 20\n",
    "model_350 = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10, alpha=0.5)\n",
    "model_350.to(device)\n",
    "model_350.load_state_dict(torch.load(\"model_weights_opt_350.pth\", weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3248ba75-5b68-4dc0-a7e7-70fc00db0d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5755], device='cuda:0')\n",
      "tensor([0.3160], device='cuda:0')\n",
      "tensor([0.5959], device='cuda:0')\n",
      "tensor([0.3944], device='cuda:0')\n",
      "tensor([0.1881], device='cuda:0')\n",
      "tensor([0.4601], device='cuda:0')\n",
      "tensor([0.2460], device='cuda:0')\n",
      "tensor([0.2720], device='cuda:0')\n",
      "tensor([0.1727], device='cuda:0')\n",
      "tensor([0.2169], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for l in range(model_350.num_layers):\n",
    "        print(torch.sigmoid(model_350.logit_alpha[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a19c3a0-100a-4478-b552-37c9e665c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write loss function\n",
    "def mc_loss_batch_pred(model, batch, t, n, k, num_steps=4):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    - model: diffusion denoiser\n",
    "    - dataloader: data to train model of size [data_size, n, n]\n",
    "    - t: time point to compute loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize loss\n",
    "    loss = 0\n",
    "    \n",
    "    # Batch is of size [batch_size, n, n]\n",
    "\n",
    "    # T1: Use fixed time\n",
    "    time_array = torch.ones((batch.shape[0], 1, 1), device=batch.device) * t\n",
    "    \n",
    "    # T2: Generate an array of random times, one for each observation in the batch\n",
    "    #time_array = torch.distributions.Exponential(0.001).sample((batch.shape[0], 1, 1)).to(device)\n",
    "\n",
    "    # Create noisy input to the denoiser\n",
    "    batch_outer = batch[:, :, None] * batch[:, None, :]\n",
    "    y_t = batch_outer + torch.randn(batch_outer.shape, device=device) * (time_array ** (-1/2))\n",
    "    y_t = (1/2) * (y_t + y_t.transpose(-1, -2))\n",
    "\n",
    "    # Get random node embeddings and normalize them\n",
    "    x = torch.randn((batch.shape[0], n), device=device)\n",
    "    x = x / x.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    # Match dimensions\n",
    "    x_expand = x.unsqueeze(-1)\n",
    "    y_t_expand = y_t.unsqueeze(-1)\n",
    "    \n",
    "    # Get output from the denoiser\n",
    "    if num_steps is not None:\n",
    "        out = model.predict(x_expand, y_t_expand, time_array, num_steps=num_steps)\n",
    "\n",
    "    else:\n",
    "        out = model.predict(x_expand, y_t_expand, time_array)\n",
    "\n",
    "    # Get loss by rank-1 matrices\n",
    "    loss = torch.sum((out - batch_outer) ** 2)\n",
    "\n",
    "    # Return memory\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    return loss / batch.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8f6a4a1-c7c1-467c-b007-323e94764e4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 20.0 finished!\n",
      "tensor(1.4952, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 40.0 finished!\n",
      "tensor(1.4920, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 60.0 finished!\n",
      "tensor(1.4524, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 80.0 finished!\n",
      "tensor(1.2443, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 100.0 finished!\n",
      "tensor(0.8771, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 120.0 finished!\n",
      "tensor(0.6309, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 140.0 finished!\n",
      "tensor(0.5358, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 160.0 finished!\n",
      "tensor(0.5140, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 180.0 finished!\n",
      "tensor(0.5068, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 200.0 finished!\n",
      "tensor(0.4975, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 220.0 finished!\n",
      "tensor(0.4809, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 240.0 finished!\n",
      "tensor(0.4527, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 260.0 finished!\n",
      "tensor(0.4132, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 280.0 finished!\n",
      "tensor(0.3667, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 300.0 finished!\n",
      "tensor(0.3177, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 320.0 finished!\n",
      "tensor(0.2586, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 340.0 finished!\n",
      "tensor(0.2069, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 360.0 finished!\n",
      "tensor(0.1649, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 380.0 finished!\n",
      "tensor(0.1208, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 400.0 finished!\n",
      "tensor(0.0911, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 420.0 finished!\n",
      "tensor(0.0665, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 440.0 finished!\n",
      "tensor(0.0509, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 460.0 finished!\n",
      "tensor(0.0353, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 480.0 finished!\n",
      "tensor(0.0249, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 500.0 finished!\n",
      "tensor(0.0170, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 520.0 finished!\n",
      "tensor(0.0117, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 540.0 finished!\n",
      "tensor(0.0083, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 560.0 finished!\n",
      "tensor(0.0057, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 580.0 finished!\n",
      "tensor(0.0036, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 600.0 finished!\n",
      "tensor(0.0026, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 620.0 finished!\n",
      "tensor(0.0017, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 640.0 finished!\n",
      "tensor(0.0011, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 660.0 finished!\n",
      "tensor(0.0008, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 680.0 finished!\n",
      "tensor(0.0005, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 700.0 finished!\n",
      "tensor(0.0002, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe374f3fbc0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make test dataset\n",
    "N = 15000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "test_data = sample_data(N, n, k)\n",
    "test_dataset = SubmatrixDataset(test_data)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=250, shuffle=True)\n",
    "\n",
    "# Draw a loss curve\n",
    "loss_opt = []\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 700, 35, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "\n",
    "        # Evaluate the optimal polynomial time estimator\n",
    "        loss_1 = evaluate_estimator(optimal_polynomial_estimator, test_dataloader, t)\n",
    "        loss_opt.append(min(loss_1.item(), 1/2 - 1 / (4 * n)))\n",
    "\n",
    "        print(\"Time {} finished!\".format(t))\n",
    "        print(loss_1)\n",
    "\n",
    "# Plot\n",
    "plt.plot(time_array.detach().cpu().numpy(), loss_opt, label=\"optimal poly alg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4797cb85-9cc8-46e5-a89c-6900fbb2144d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 720.0 finished!\n",
      "tensor(0.0002, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 740.0 finished!\n",
      "tensor(0.0001, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 760.0 finished!\n",
      "tensor(9.1000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 780.0 finished!\n",
      "tensor(2.6000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 800.0 finished!\n",
      "tensor(1.3000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 820.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 840.0 finished!\n",
      "tensor(1.3000e-05, device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 860.0 finished!\n",
      "tensor(0., device='cuda:0')\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n"
     ]
    }
   ],
   "source": [
    "# Make an array of time points\n",
    "time_array = torch.linspace(720, 1400, 35, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "\n",
    "        # Evaluate the optimal polynomial time estimator\n",
    "        loss_1 = evaluate_estimator(optimal_polynomial_estimator, test_dataloader, t)\n",
    "        loss_opt.append(min(loss_1.item(), 1/2 - 1 / (4 * n)))\n",
    "\n",
    "        print(\"Time {} finished!\".format(t))\n",
    "        print(loss_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daf8213d-57b3-4257-9028-8419dbfcb9a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n"
     ]
    }
   ],
   "source": [
    "# Draw a loss curve\n",
    "loss_ls = []\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 700, 35, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "    \n",
    "        # Evaluate model\n",
    "        for batch in test_dataloader:\n",
    "            loss_b = mc_loss_batch_pred(model_350, batch, t, n, k, num_steps=None)\n",
    "            loss += loss_b.item() * batch.shape[0]\n",
    "\n",
    "            # Print\n",
    "            print(\"Batch {} finished!\".format(counter))\n",
    "            counter += 1\n",
    "\n",
    "        # Store loss\n",
    "        loss_ls.append(loss / len(test_dataloader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8cadfbbe-bc5f-451b-8c8b-4632d57c40fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 720.0 finished!\n",
      "0.002054754902079973\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 740.0 finished!\n",
      "0.0017414745917146016\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 760.0 finished!\n",
      "0.0012073749049401764\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 780.0 finished!\n",
      "0.0016077183235514289\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 800.0 finished!\n",
      "0.001462925924230755\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 820.0 finished!\n",
      "0.0010356414310081163\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 840.0 finished!\n",
      "0.001095458748022793\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 860.0 finished!\n",
      "0.0011852949332630184\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 880.0 finished!\n",
      "0.0009440497091418365\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 900.0 finished!\n",
      "0.0012547692299752574\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 920.0 finished!\n",
      "0.000926916550573272\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 940.0 finished!\n",
      "0.000951449535083763\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 960.0 finished!\n",
      "0.0008740217996091815\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 980.0 finished!\n",
      "0.0009227863150348033\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1000.0 finished!\n",
      "0.0009003825063094458\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1020.0 finished!\n",
      "0.001205633891064887\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1040.0 finished!\n",
      "0.00092340047400891\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1060.0 finished!\n",
      "0.0012830155136422641\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1080.0 finished!\n",
      "0.0014568947299873495\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1100.0 finished!\n",
      "0.0010503061216998806\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1120.0 finished!\n",
      "0.0014849324729463357\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1140.0 finished!\n",
      "0.0010580752365664619\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1160.0 finished!\n",
      "0.0017025320916824664\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1180.0 finished!\n",
      "0.0013826916211958937\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1200.0 finished!\n",
      "0.0011256809170542208\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1220.0 finished!\n",
      "0.0012737823820013242\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1240.0 finished!\n",
      "0.0013306051251371779\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1260.0 finished!\n",
      "0.0012033247686000928\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1280.0 finished!\n",
      "0.0014264657806052129\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1300.0 finished!\n",
      "0.0014905700398230692\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1320.0 finished!\n",
      "0.0010078677178777676\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1340.0 finished!\n",
      "0.0018095188030808156\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1360.0 finished!\n",
      "0.0014271497641554257\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1380.0 finished!\n",
      "0.0013534953136210485\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1400.0 finished!\n",
      "0.0013393467776040779\n"
     ]
    }
   ],
   "source": [
    "# Make an array of time points\n",
    "time_array = torch.linspace(720, 1400, 35, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        loss = 0\n",
    "        counter = 0\n",
    "        \n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "    \n",
    "        # Evaluate model\n",
    "        for batch in test_dataloader:\n",
    "            loss_b = mc_loss_batch_pred(model_350, batch, t, n, k, num_steps=None)\n",
    "            loss += loss_b.item() * batch.shape[0]\n",
    "\n",
    "            # Print\n",
    "            print(\"Batch {} finished!\".format(counter))\n",
    "            counter += 1\n",
    "\n",
    "        # Print time\n",
    "        print(\"Time {} finished!\".format(t))\n",
    "\n",
    "        # Store loss\n",
    "        loss_t = loss / len(test_dataloader.dataset)\n",
    "        print(loss_t)\n",
    "        loss_ls.append(loss_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c3abf23-57ef-453d-95b6-ad597293e352",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loss_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b55b7333-de0f-4d2f-a8d5-acacae60c430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")                # smaller font sizes, tight layout\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "mpl.rcParams[\"text.usetex\"] = False\n",
    "import scienceplots\n",
    "plt.style.use([\"science\", \"ieee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "39873974-cd12-449b-9b6d-1b84515cf17b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 20.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 40.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 60.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 80.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 100.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 120.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 140.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 160.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 180.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 200.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 220.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 240.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 260.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 280.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 300.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 320.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 340.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 360.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 380.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 400.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 420.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 440.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 460.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 480.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 500.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 520.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 540.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 560.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 580.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 600.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 620.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 640.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 660.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 680.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 700.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 720.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 740.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 760.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 780.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 800.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 820.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 840.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 860.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 880.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 900.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 920.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 940.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 960.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 980.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1000.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1020.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1040.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1060.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1080.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1100.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1120.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1140.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1160.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1180.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1200.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1220.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1240.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1260.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1280.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1300.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1320.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1340.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1360.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1380.0 finished!\n",
      "Batch 0 finished!\n",
      "Batch 1 finished!\n",
      "Batch 2 finished!\n",
      "Batch 3 finished!\n",
      "Batch 4 finished!\n",
      "Batch 5 finished!\n",
      "Batch 6 finished!\n",
      "Batch 7 finished!\n",
      "Batch 8 finished!\n",
      "Batch 9 finished!\n",
      "Batch 10 finished!\n",
      "Batch 11 finished!\n",
      "Batch 12 finished!\n",
      "Batch 13 finished!\n",
      "Batch 14 finished!\n",
      "Batch 15 finished!\n",
      "Batch 16 finished!\n",
      "Batch 17 finished!\n",
      "Batch 18 finished!\n",
      "Batch 19 finished!\n",
      "Batch 20 finished!\n",
      "Batch 21 finished!\n",
      "Batch 22 finished!\n",
      "Batch 23 finished!\n",
      "Batch 24 finished!\n",
      "Batch 25 finished!\n",
      "Batch 26 finished!\n",
      "Batch 27 finished!\n",
      "Batch 28 finished!\n",
      "Batch 29 finished!\n",
      "Batch 30 finished!\n",
      "Batch 31 finished!\n",
      "Batch 32 finished!\n",
      "Batch 33 finished!\n",
      "Batch 34 finished!\n",
      "Batch 35 finished!\n",
      "Batch 36 finished!\n",
      "Batch 37 finished!\n",
      "Batch 38 finished!\n",
      "Batch 39 finished!\n",
      "Batch 40 finished!\n",
      "Batch 41 finished!\n",
      "Batch 42 finished!\n",
      "Batch 43 finished!\n",
      "Batch 44 finished!\n",
      "Batch 45 finished!\n",
      "Batch 46 finished!\n",
      "Batch 47 finished!\n",
      "Batch 48 finished!\n",
      "Batch 49 finished!\n",
      "Batch 50 finished!\n",
      "Batch 51 finished!\n",
      "Batch 52 finished!\n",
      "Batch 53 finished!\n",
      "Batch 54 finished!\n",
      "Batch 55 finished!\n",
      "Batch 56 finished!\n",
      "Batch 57 finished!\n",
      "Batch 58 finished!\n",
      "Batch 59 finished!\n",
      "Time 1400.0 finished!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe37d61dc10>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code for loss of power iteration\n",
    "# Draw a loss curve\n",
    "loss_power = []\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for it in range(len(time_array)):\n",
    "        # Set time\n",
    "        t = time_array[it].item()\n",
    "\n",
    "        # Evaluate the optimal polynomial time estimator\n",
    "        loss_1 = evaluate_estimator(power_iteration_estimator, test_dataloader, t, num_iters=25)\n",
    "        loss_power.append(min(loss_1.item(), 1/2 - 1 / (4 * n)))\n",
    "\n",
    "        print(\"Time {} finished!\".format(t))\n",
    "\n",
    "# Plot\n",
    "plt.plot(time_array.detach().cpu().numpy(), loss_power, label=\"optimal poly alg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5092ddf0-f4f3-451b-974d-211f86379be0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcdefaults()    # re-load all rcParams from the built-in defaults\n",
    "\n",
    "mpl.rcParams[\"text.usetex\"]      = False\n",
    "mpl.rcParams[\"mathtext.fontset\"] = \"stix\"    # or \"dejavuserif\"\n",
    "mpl.rcParams[\"font.family\"]      = \"serif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "9a5bc977-3144-4d4a-ab10-5a94a21d126f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting\n",
    "time_array = torch.linspace(20, 1400, 70, device=device)\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, np.minimum(np.array(loss_ls), 1/2 - 1 / (4 * n)), label=\"Graph NN Denoiser\")\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, loss_opt, label=\"Spectral Algorithm (Alg. 1)\")\n",
    "plt.plot(time_array.detach().cpu().numpy() / n, loss_power, label=\"Power Iteration (25 steps) + Projection\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"time / n\")\n",
    "plt.ylabel(\"MSE loss\")\n",
    "plt.axvline(x=0.5, color='k', linestyle='--', linewidth=1)\n",
    "ymin, ymax = plt.ylim()\n",
    "y_mid = 0.5 * (ymin + ymax)\n",
    "\n",
    "# add vertical text\n",
    "plt.text(\n",
    "    0.44, y_mid,              # (x, y) in data coords\n",
    "    'phase transition cutoff',            # your label\n",
    "    rotation='vertical',    # run text vertically\n",
    "    va='center',            # center vertically on y_mid\n",
    "    ha='right',             # right‐align so it sits just left of the line\n",
    "    backgroundcolor='white' # optional: makes it legible if lines cross\n",
    ")\n",
    "plt.savefig(\"n=350 plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3fdd90-85b6-41c5-bc03-34f65b367282",
   "metadata": {},
   "source": [
    "**Generation code for n=350 case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "084f4655-690e-4588-9526-cc1b48e61301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(model, num_samples, n, time_step, terminal_time, device=device):\n",
    "    # Create current values\n",
    "    y = torch.zeros(num_samples, n, n, device=device)\n",
    "    diff_sample = torch.zeros(num_samples, n, n, device=device)\n",
    "    t = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while t <= terminal_time:\n",
    "            if t == 0:\n",
    "               t += time_step\n",
    "    \n",
    "            else:\n",
    "                # Update y\n",
    "                y = y + time_step * diff_sample + torch.randn_like(y, device=device) * (time_step ** (1/2))\n",
    "    \n",
    "                # Symmetrize y as input\n",
    "                y_symm = (1/2) * (y + y.transpose(-1, -2))\n",
    "                y_symm_expand = y_symm.unsqueeze(-1)\n",
    "    \n",
    "                # Update diffusion sample\n",
    "                # Generate random node embedding\n",
    "                x_init = torch.randn((num_samples, n), device=device)\n",
    "                x_init = x_init / x_init.norm(dim=1, keepdim=True)\n",
    "                x_init_expand = x_init.unsqueeze(-1)\n",
    "    \n",
    "                # Create time array\n",
    "                time_array = torch.ones((num_samples, 1, 1), device=device) * t\n",
    "    \n",
    "                # Evaluate the diffusion sample\n",
    "                diff_sample = model.predict(x_init_expand, y_symm_expand / t, time_array)\n",
    "    \n",
    "                # Increment t\n",
    "                t += time_step\n",
    "\n",
    "            # Print out t\n",
    "            print(\"Time {} updated\".format(t))\n",
    "\n",
    "    return diff_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10795610-dad6-4ffd-8946-8f50659c43ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TestMPNN_3(\n",
       "  (msg_mlps): ModuleList(\n",
       "    (0-9): 10 x Sequential(\n",
       "      (0): Linear(in_features=3, out_features=32, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (upd_mlps): ModuleList(\n",
       "    (0-9): 10 x Sequential(\n",
       "      (0): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (1): LeakyReLU(negative_slope=0.01)\n",
       "      (2): Linear(in_features=16, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (sparsifier): TopKStraightThrough()\n",
       "  (logit_alpha): ParameterList(\n",
       "      (0): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "      (1): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "      (2): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "      (3): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "      (4): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "      (5): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "      (6): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "      (7): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "      (8): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "      (9): Parameter containing: [torch.float32 of size 1 (cuda:0)]\n",
       "  )\n",
       "  (tiny_decoder): Sequential(\n",
       "    (0): Linear(in_features=2, out_features=4, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=4, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test = TestMPNN_3(k, hidden_dim_1=32, hidden_dim_2=16, hidden_dim_3=4, num_layers=10)\n",
    "model_test.to(device)\n",
    "model_test.load_state_dict(torch.load(\"test_2_opt_350.pth\", weights_only=False))\n",
    "model_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f235bedb-4a6e-4f83-b293-7bd9a321a96c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.5 updated\n",
      "Time 1.0 updated\n",
      "Time 1.5 updated\n",
      "Time 2.0 updated\n",
      "Time 2.5 updated\n",
      "Time 3.0 updated\n",
      "Time 3.5 updated\n",
      "Time 4.0 updated\n",
      "Time 4.5 updated\n",
      "Time 5.0 updated\n",
      "Time 5.5 updated\n",
      "Time 6.0 updated\n",
      "Time 6.5 updated\n",
      "Time 7.0 updated\n",
      "Time 7.5 updated\n",
      "Time 8.0 updated\n",
      "Time 8.5 updated\n",
      "Time 9.0 updated\n",
      "Time 9.5 updated\n",
      "Time 10.0 updated\n",
      "Time 10.5 updated\n",
      "Time 11.0 updated\n",
      "Time 11.5 updated\n",
      "Time 12.0 updated\n",
      "Time 12.5 updated\n",
      "Time 13.0 updated\n",
      "Time 13.5 updated\n",
      "Time 14.0 updated\n",
      "Time 14.5 updated\n",
      "Time 15.0 updated\n",
      "Time 15.5 updated\n",
      "Time 16.0 updated\n",
      "Time 16.5 updated\n",
      "Time 17.0 updated\n",
      "Time 17.5 updated\n",
      "Time 18.0 updated\n",
      "Time 18.5 updated\n",
      "Time 19.0 updated\n",
      "Time 19.5 updated\n",
      "Time 20.0 updated\n",
      "Time 20.5 updated\n",
      "Time 21.0 updated\n",
      "Time 21.5 updated\n",
      "Time 22.0 updated\n",
      "Time 22.5 updated\n",
      "Time 23.0 updated\n",
      "Time 23.5 updated\n",
      "Time 24.0 updated\n",
      "Time 24.5 updated\n",
      "Time 25.0 updated\n",
      "Time 25.5 updated\n",
      "Time 26.0 updated\n",
      "Time 26.5 updated\n",
      "Time 27.0 updated\n",
      "Time 27.5 updated\n",
      "Time 28.0 updated\n",
      "Time 28.5 updated\n",
      "Time 29.0 updated\n",
      "Time 29.5 updated\n",
      "Time 30.0 updated\n",
      "Time 30.5 updated\n",
      "Time 31.0 updated\n",
      "Time 31.5 updated\n",
      "Time 32.0 updated\n",
      "Time 32.5 updated\n",
      "Time 33.0 updated\n",
      "Time 33.5 updated\n",
      "Time 34.0 updated\n",
      "Time 34.5 updated\n",
      "Time 35.0 updated\n",
      "Time 35.5 updated\n",
      "Time 36.0 updated\n",
      "Time 36.5 updated\n",
      "Time 37.0 updated\n",
      "Time 37.5 updated\n",
      "Time 38.0 updated\n",
      "Time 38.5 updated\n",
      "Time 39.0 updated\n",
      "Time 39.5 updated\n",
      "Time 40.0 updated\n",
      "Time 40.5 updated\n",
      "Time 41.0 updated\n",
      "Time 41.5 updated\n",
      "Time 42.0 updated\n",
      "Time 42.5 updated\n",
      "Time 43.0 updated\n",
      "Time 43.5 updated\n",
      "Time 44.0 updated\n",
      "Time 44.5 updated\n",
      "Time 45.0 updated\n",
      "Time 45.5 updated\n",
      "Time 46.0 updated\n",
      "Time 46.5 updated\n",
      "Time 47.0 updated\n",
      "Time 47.5 updated\n",
      "Time 48.0 updated\n",
      "Time 48.5 updated\n",
      "Time 49.0 updated\n",
      "Time 49.5 updated\n",
      "Time 50.0 updated\n",
      "Time 50.5 updated\n",
      "Time 51.0 updated\n",
      "Time 51.5 updated\n",
      "Time 52.0 updated\n",
      "Time 52.5 updated\n",
      "Time 53.0 updated\n",
      "Time 53.5 updated\n",
      "Time 54.0 updated\n",
      "Time 54.5 updated\n",
      "Time 55.0 updated\n",
      "Time 55.5 updated\n",
      "Time 56.0 updated\n",
      "Time 56.5 updated\n",
      "Time 57.0 updated\n",
      "Time 57.5 updated\n",
      "Time 58.0 updated\n",
      "Time 58.5 updated\n",
      "Time 59.0 updated\n",
      "Time 59.5 updated\n",
      "Time 60.0 updated\n",
      "Time 60.5 updated\n",
      "Time 61.0 updated\n",
      "Time 61.5 updated\n",
      "Time 62.0 updated\n",
      "Time 62.5 updated\n",
      "Time 63.0 updated\n",
      "Time 63.5 updated\n",
      "Time 64.0 updated\n",
      "Time 64.5 updated\n",
      "Time 65.0 updated\n",
      "Time 65.5 updated\n",
      "Time 66.0 updated\n",
      "Time 66.5 updated\n",
      "Time 67.0 updated\n",
      "Time 67.5 updated\n",
      "Time 68.0 updated\n",
      "Time 68.5 updated\n",
      "Time 69.0 updated\n",
      "Time 69.5 updated\n",
      "Time 70.0 updated\n",
      "Time 70.5 updated\n",
      "Time 71.0 updated\n",
      "Time 71.5 updated\n",
      "Time 72.0 updated\n",
      "Time 72.5 updated\n",
      "Time 73.0 updated\n",
      "Time 73.5 updated\n",
      "Time 74.0 updated\n",
      "Time 74.5 updated\n",
      "Time 75.0 updated\n",
      "Time 75.5 updated\n",
      "Time 76.0 updated\n",
      "Time 76.5 updated\n",
      "Time 77.0 updated\n",
      "Time 77.5 updated\n",
      "Time 78.0 updated\n",
      "Time 78.5 updated\n",
      "Time 79.0 updated\n",
      "Time 79.5 updated\n",
      "Time 80.0 updated\n",
      "Time 80.5 updated\n",
      "Time 81.0 updated\n",
      "Time 81.5 updated\n",
      "Time 82.0 updated\n",
      "Time 82.5 updated\n",
      "Time 83.0 updated\n",
      "Time 83.5 updated\n",
      "Time 84.0 updated\n",
      "Time 84.5 updated\n",
      "Time 85.0 updated\n",
      "Time 85.5 updated\n",
      "Time 86.0 updated\n",
      "Time 86.5 updated\n",
      "Time 87.0 updated\n",
      "Time 87.5 updated\n",
      "Time 88.0 updated\n",
      "Time 88.5 updated\n",
      "Time 89.0 updated\n",
      "Time 89.5 updated\n",
      "Time 90.0 updated\n",
      "Time 90.5 updated\n",
      "Time 91.0 updated\n",
      "Time 91.5 updated\n",
      "Time 92.0 updated\n",
      "Time 92.5 updated\n",
      "Time 93.0 updated\n",
      "Time 93.5 updated\n",
      "Time 94.0 updated\n",
      "Time 94.5 updated\n",
      "Time 95.0 updated\n",
      "Time 95.5 updated\n",
      "Time 96.0 updated\n",
      "Time 96.5 updated\n",
      "Time 97.0 updated\n",
      "Time 97.5 updated\n",
      "Time 98.0 updated\n",
      "Time 98.5 updated\n",
      "Time 99.0 updated\n",
      "Time 99.5 updated\n",
      "Time 100.0 updated\n",
      "Time 100.5 updated\n",
      "Time 101.0 updated\n",
      "Time 101.5 updated\n",
      "Time 102.0 updated\n",
      "Time 102.5 updated\n",
      "Time 103.0 updated\n",
      "Time 103.5 updated\n",
      "Time 104.0 updated\n",
      "Time 104.5 updated\n",
      "Time 105.0 updated\n",
      "Time 105.5 updated\n",
      "Time 106.0 updated\n",
      "Time 106.5 updated\n",
      "Time 107.0 updated\n",
      "Time 107.5 updated\n",
      "Time 108.0 updated\n",
      "Time 108.5 updated\n",
      "Time 109.0 updated\n",
      "Time 109.5 updated\n",
      "Time 110.0 updated\n",
      "Time 110.5 updated\n",
      "Time 111.0 updated\n",
      "Time 111.5 updated\n",
      "Time 112.0 updated\n",
      "Time 112.5 updated\n",
      "Time 113.0 updated\n",
      "Time 113.5 updated\n",
      "Time 114.0 updated\n",
      "Time 114.5 updated\n",
      "Time 115.0 updated\n",
      "Time 115.5 updated\n",
      "Time 116.0 updated\n",
      "Time 116.5 updated\n",
      "Time 117.0 updated\n",
      "Time 117.5 updated\n",
      "Time 118.0 updated\n",
      "Time 118.5 updated\n",
      "Time 119.0 updated\n",
      "Time 119.5 updated\n",
      "Time 120.0 updated\n",
      "Time 120.5 updated\n",
      "Time 121.0 updated\n",
      "Time 121.5 updated\n",
      "Time 122.0 updated\n",
      "Time 122.5 updated\n",
      "Time 123.0 updated\n",
      "Time 123.5 updated\n",
      "Time 124.0 updated\n",
      "Time 124.5 updated\n",
      "Time 125.0 updated\n",
      "Time 125.5 updated\n",
      "Time 126.0 updated\n",
      "Time 126.5 updated\n",
      "Time 127.0 updated\n",
      "Time 127.5 updated\n",
      "Time 128.0 updated\n",
      "Time 128.5 updated\n",
      "Time 129.0 updated\n",
      "Time 129.5 updated\n",
      "Time 130.0 updated\n",
      "Time 130.5 updated\n",
      "Time 131.0 updated\n",
      "Time 131.5 updated\n",
      "Time 132.0 updated\n",
      "Time 132.5 updated\n",
      "Time 133.0 updated\n",
      "Time 133.5 updated\n",
      "Time 134.0 updated\n",
      "Time 134.5 updated\n",
      "Time 135.0 updated\n",
      "Time 135.5 updated\n",
      "Time 136.0 updated\n",
      "Time 136.5 updated\n",
      "Time 137.0 updated\n",
      "Time 137.5 updated\n",
      "Time 138.0 updated\n",
      "Time 138.5 updated\n",
      "Time 139.0 updated\n",
      "Time 139.5 updated\n",
      "Time 140.0 updated\n",
      "Time 140.5 updated\n",
      "Time 141.0 updated\n",
      "Time 141.5 updated\n",
      "Time 142.0 updated\n",
      "Time 142.5 updated\n",
      "Time 143.0 updated\n",
      "Time 143.5 updated\n",
      "Time 144.0 updated\n",
      "Time 144.5 updated\n",
      "Time 145.0 updated\n",
      "Time 145.5 updated\n",
      "Time 146.0 updated\n",
      "Time 146.5 updated\n",
      "Time 147.0 updated\n",
      "Time 147.5 updated\n",
      "Time 148.0 updated\n",
      "Time 148.5 updated\n",
      "Time 149.0 updated\n",
      "Time 149.5 updated\n",
      "Time 150.0 updated\n",
      "Time 150.5 updated\n",
      "Time 151.0 updated\n",
      "Time 151.5 updated\n",
      "Time 152.0 updated\n",
      "Time 152.5 updated\n",
      "Time 153.0 updated\n",
      "Time 153.5 updated\n",
      "Time 154.0 updated\n",
      "Time 154.5 updated\n",
      "Time 155.0 updated\n",
      "Time 155.5 updated\n",
      "Time 156.0 updated\n",
      "Time 156.5 updated\n",
      "Time 157.0 updated\n",
      "Time 157.5 updated\n",
      "Time 158.0 updated\n",
      "Time 158.5 updated\n",
      "Time 159.0 updated\n",
      "Time 159.5 updated\n",
      "Time 160.0 updated\n",
      "Time 160.5 updated\n",
      "Time 161.0 updated\n",
      "Time 161.5 updated\n",
      "Time 162.0 updated\n",
      "Time 162.5 updated\n",
      "Time 163.0 updated\n",
      "Time 163.5 updated\n",
      "Time 164.0 updated\n",
      "Time 164.5 updated\n",
      "Time 165.0 updated\n",
      "Time 165.5 updated\n",
      "Time 166.0 updated\n",
      "Time 166.5 updated\n",
      "Time 167.0 updated\n",
      "Time 167.5 updated\n",
      "Time 168.0 updated\n",
      "Time 168.5 updated\n",
      "Time 169.0 updated\n",
      "Time 169.5 updated\n",
      "Time 170.0 updated\n",
      "Time 170.5 updated\n",
      "Time 171.0 updated\n",
      "Time 171.5 updated\n",
      "Time 172.0 updated\n",
      "Time 172.5 updated\n",
      "Time 173.0 updated\n",
      "Time 173.5 updated\n",
      "Time 174.0 updated\n",
      "Time 174.5 updated\n",
      "Time 175.0 updated\n",
      "Time 175.5 updated\n",
      "Time 176.0 updated\n",
      "Time 176.5 updated\n",
      "Time 177.0 updated\n",
      "Time 177.5 updated\n",
      "Time 178.0 updated\n",
      "Time 178.5 updated\n",
      "Time 179.0 updated\n",
      "Time 179.5 updated\n",
      "Time 180.0 updated\n",
      "Time 180.5 updated\n",
      "Time 181.0 updated\n",
      "Time 181.5 updated\n",
      "Time 182.0 updated\n",
      "Time 182.5 updated\n",
      "Time 183.0 updated\n",
      "Time 183.5 updated\n",
      "Time 184.0 updated\n",
      "Time 184.5 updated\n",
      "Time 185.0 updated\n",
      "Time 185.5 updated\n",
      "Time 186.0 updated\n",
      "Time 186.5 updated\n",
      "Time 187.0 updated\n",
      "Time 187.5 updated\n",
      "Time 188.0 updated\n",
      "Time 188.5 updated\n",
      "Time 189.0 updated\n",
      "Time 189.5 updated\n",
      "Time 190.0 updated\n",
      "Time 190.5 updated\n",
      "Time 191.0 updated\n",
      "Time 191.5 updated\n",
      "Time 192.0 updated\n",
      "Time 192.5 updated\n",
      "Time 193.0 updated\n",
      "Time 193.5 updated\n",
      "Time 194.0 updated\n",
      "Time 194.5 updated\n",
      "Time 195.0 updated\n",
      "Time 195.5 updated\n",
      "Time 196.0 updated\n",
      "Time 196.5 updated\n",
      "Time 197.0 updated\n",
      "Time 197.5 updated\n",
      "Time 198.0 updated\n",
      "Time 198.5 updated\n",
      "Time 199.0 updated\n",
      "Time 199.5 updated\n",
      "Time 200.0 updated\n",
      "Time 200.5 updated\n",
      "Time 201.0 updated\n",
      "Time 201.5 updated\n",
      "Time 202.0 updated\n",
      "Time 202.5 updated\n",
      "Time 203.0 updated\n",
      "Time 203.5 updated\n",
      "Time 204.0 updated\n",
      "Time 204.5 updated\n",
      "Time 205.0 updated\n",
      "Time 205.5 updated\n",
      "Time 206.0 updated\n",
      "Time 206.5 updated\n",
      "Time 207.0 updated\n",
      "Time 207.5 updated\n",
      "Time 208.0 updated\n",
      "Time 208.5 updated\n",
      "Time 209.0 updated\n",
      "Time 209.5 updated\n",
      "Time 210.0 updated\n",
      "Time 210.5 updated\n",
      "Time 211.0 updated\n",
      "Time 211.5 updated\n",
      "Time 212.0 updated\n",
      "Time 212.5 updated\n",
      "Time 213.0 updated\n",
      "Time 213.5 updated\n",
      "Time 214.0 updated\n",
      "Time 214.5 updated\n",
      "Time 215.0 updated\n",
      "Time 215.5 updated\n",
      "Time 216.0 updated\n",
      "Time 216.5 updated\n",
      "Time 217.0 updated\n",
      "Time 217.5 updated\n",
      "Time 218.0 updated\n",
      "Time 218.5 updated\n",
      "Time 219.0 updated\n",
      "Time 219.5 updated\n",
      "Time 220.0 updated\n",
      "Time 220.5 updated\n",
      "Time 221.0 updated\n",
      "Time 221.5 updated\n",
      "Time 222.0 updated\n",
      "Time 222.5 updated\n",
      "Time 223.0 updated\n",
      "Time 223.5 updated\n",
      "Time 224.0 updated\n",
      "Time 224.5 updated\n",
      "Time 225.0 updated\n",
      "Time 225.5 updated\n",
      "Time 226.0 updated\n",
      "Time 226.5 updated\n",
      "Time 227.0 updated\n",
      "Time 227.5 updated\n",
      "Time 228.0 updated\n",
      "Time 228.5 updated\n",
      "Time 229.0 updated\n",
      "Time 229.5 updated\n",
      "Time 230.0 updated\n",
      "Time 230.5 updated\n",
      "Time 231.0 updated\n",
      "Time 231.5 updated\n",
      "Time 232.0 updated\n",
      "Time 232.5 updated\n",
      "Time 233.0 updated\n",
      "Time 233.5 updated\n",
      "Time 234.0 updated\n",
      "Time 234.5 updated\n",
      "Time 235.0 updated\n",
      "Time 235.5 updated\n",
      "Time 236.0 updated\n",
      "Time 236.5 updated\n",
      "Time 237.0 updated\n",
      "Time 237.5 updated\n",
      "Time 238.0 updated\n",
      "Time 238.5 updated\n",
      "Time 239.0 updated\n",
      "Time 239.5 updated\n",
      "Time 240.0 updated\n",
      "Time 240.5 updated\n",
      "Time 241.0 updated\n",
      "Time 241.5 updated\n",
      "Time 242.0 updated\n",
      "Time 242.5 updated\n",
      "Time 243.0 updated\n",
      "Time 243.5 updated\n",
      "Time 244.0 updated\n",
      "Time 244.5 updated\n",
      "Time 245.0 updated\n",
      "Time 245.5 updated\n",
      "Time 246.0 updated\n",
      "Time 246.5 updated\n",
      "Time 247.0 updated\n",
      "Time 247.5 updated\n",
      "Time 248.0 updated\n",
      "Time 248.5 updated\n",
      "Time 249.0 updated\n",
      "Time 249.5 updated\n",
      "Time 250.0 updated\n",
      "Time 250.5 updated\n",
      "Time 251.0 updated\n",
      "Time 251.5 updated\n",
      "Time 252.0 updated\n",
      "Time 252.5 updated\n",
      "Time 253.0 updated\n",
      "Time 253.5 updated\n",
      "Time 254.0 updated\n",
      "Time 254.5 updated\n",
      "Time 255.0 updated\n",
      "Time 255.5 updated\n",
      "Time 256.0 updated\n",
      "Time 256.5 updated\n",
      "Time 257.0 updated\n",
      "Time 257.5 updated\n",
      "Time 258.0 updated\n",
      "Time 258.5 updated\n",
      "Time 259.0 updated\n",
      "Time 259.5 updated\n",
      "Time 260.0 updated\n",
      "Time 260.5 updated\n",
      "Time 261.0 updated\n",
      "Time 261.5 updated\n",
      "Time 262.0 updated\n",
      "Time 262.5 updated\n",
      "Time 263.0 updated\n",
      "Time 263.5 updated\n",
      "Time 264.0 updated\n",
      "Time 264.5 updated\n",
      "Time 265.0 updated\n",
      "Time 265.5 updated\n",
      "Time 266.0 updated\n",
      "Time 266.5 updated\n",
      "Time 267.0 updated\n",
      "Time 267.5 updated\n",
      "Time 268.0 updated\n",
      "Time 268.5 updated\n",
      "Time 269.0 updated\n",
      "Time 269.5 updated\n",
      "Time 270.0 updated\n",
      "Time 270.5 updated\n",
      "Time 271.0 updated\n",
      "Time 271.5 updated\n",
      "Time 272.0 updated\n",
      "Time 272.5 updated\n",
      "Time 273.0 updated\n",
      "Time 273.5 updated\n",
      "Time 274.0 updated\n",
      "Time 274.5 updated\n",
      "Time 275.0 updated\n",
      "Time 275.5 updated\n",
      "Time 276.0 updated\n",
      "Time 276.5 updated\n",
      "Time 277.0 updated\n",
      "Time 277.5 updated\n",
      "Time 278.0 updated\n",
      "Time 278.5 updated\n",
      "Time 279.0 updated\n",
      "Time 279.5 updated\n",
      "Time 280.0 updated\n",
      "Time 280.5 updated\n",
      "Time 281.0 updated\n",
      "Time 281.5 updated\n",
      "Time 282.0 updated\n",
      "Time 282.5 updated\n",
      "Time 283.0 updated\n",
      "Time 283.5 updated\n",
      "Time 284.0 updated\n",
      "Time 284.5 updated\n",
      "Time 285.0 updated\n",
      "Time 285.5 updated\n",
      "Time 286.0 updated\n",
      "Time 286.5 updated\n",
      "Time 287.0 updated\n",
      "Time 287.5 updated\n",
      "Time 288.0 updated\n",
      "Time 288.5 updated\n",
      "Time 289.0 updated\n",
      "Time 289.5 updated\n",
      "Time 290.0 updated\n",
      "Time 290.5 updated\n",
      "Time 291.0 updated\n",
      "Time 291.5 updated\n",
      "Time 292.0 updated\n",
      "Time 292.5 updated\n",
      "Time 293.0 updated\n",
      "Time 293.5 updated\n",
      "Time 294.0 updated\n",
      "Time 294.5 updated\n",
      "Time 295.0 updated\n",
      "Time 295.5 updated\n",
      "Time 296.0 updated\n",
      "Time 296.5 updated\n",
      "Time 297.0 updated\n",
      "Time 297.5 updated\n",
      "Time 298.0 updated\n",
      "Time 298.5 updated\n",
      "Time 299.0 updated\n",
      "Time 299.5 updated\n",
      "Time 300.0 updated\n",
      "Time 300.5 updated\n",
      "Time 301.0 updated\n",
      "Time 301.5 updated\n",
      "Time 302.0 updated\n",
      "Time 302.5 updated\n",
      "Time 303.0 updated\n",
      "Time 303.5 updated\n",
      "Time 304.0 updated\n",
      "Time 304.5 updated\n",
      "Time 305.0 updated\n",
      "Time 305.5 updated\n",
      "Time 306.0 updated\n",
      "Time 306.5 updated\n",
      "Time 307.0 updated\n",
      "Time 307.5 updated\n",
      "Time 308.0 updated\n",
      "Time 308.5 updated\n",
      "Time 309.0 updated\n",
      "Time 309.5 updated\n",
      "Time 310.0 updated\n",
      "Time 310.5 updated\n",
      "Time 311.0 updated\n",
      "Time 311.5 updated\n",
      "Time 312.0 updated\n",
      "Time 312.5 updated\n",
      "Time 313.0 updated\n",
      "Time 313.5 updated\n",
      "Time 314.0 updated\n",
      "Time 314.5 updated\n",
      "Time 315.0 updated\n",
      "Time 315.5 updated\n",
      "Time 316.0 updated\n",
      "Time 316.5 updated\n",
      "Time 317.0 updated\n",
      "Time 317.5 updated\n",
      "Time 318.0 updated\n",
      "Time 318.5 updated\n",
      "Time 319.0 updated\n",
      "Time 319.5 updated\n",
      "Time 320.0 updated\n",
      "Time 320.5 updated\n",
      "Time 321.0 updated\n",
      "Time 321.5 updated\n",
      "Time 322.0 updated\n",
      "Time 322.5 updated\n",
      "Time 323.0 updated\n",
      "Time 323.5 updated\n",
      "Time 324.0 updated\n",
      "Time 324.5 updated\n",
      "Time 325.0 updated\n",
      "Time 325.5 updated\n",
      "Time 326.0 updated\n",
      "Time 326.5 updated\n",
      "Time 327.0 updated\n",
      "Time 327.5 updated\n",
      "Time 328.0 updated\n",
      "Time 328.5 updated\n",
      "Time 329.0 updated\n",
      "Time 329.5 updated\n",
      "Time 330.0 updated\n",
      "Time 330.5 updated\n",
      "Time 331.0 updated\n",
      "Time 331.5 updated\n",
      "Time 332.0 updated\n",
      "Time 332.5 updated\n",
      "Time 333.0 updated\n",
      "Time 333.5 updated\n",
      "Time 334.0 updated\n",
      "Time 334.5 updated\n",
      "Time 335.0 updated\n",
      "Time 335.5 updated\n",
      "Time 336.0 updated\n",
      "Time 336.5 updated\n",
      "Time 337.0 updated\n",
      "Time 337.5 updated\n",
      "Time 338.0 updated\n",
      "Time 338.5 updated\n",
      "Time 339.0 updated\n",
      "Time 339.5 updated\n",
      "Time 340.0 updated\n",
      "Time 340.5 updated\n",
      "Time 341.0 updated\n",
      "Time 341.5 updated\n",
      "Time 342.0 updated\n",
      "Time 342.5 updated\n",
      "Time 343.0 updated\n",
      "Time 343.5 updated\n",
      "Time 344.0 updated\n",
      "Time 344.5 updated\n",
      "Time 345.0 updated\n",
      "Time 345.5 updated\n",
      "Time 346.0 updated\n",
      "Time 346.5 updated\n",
      "Time 347.0 updated\n",
      "Time 347.5 updated\n",
      "Time 348.0 updated\n",
      "Time 348.5 updated\n",
      "Time 349.0 updated\n",
      "Time 349.5 updated\n",
      "Time 350.0 updated\n",
      "Time 350.5 updated\n",
      "Time 351.0 updated\n",
      "Time 351.5 updated\n",
      "Time 352.0 updated\n",
      "Time 352.5 updated\n",
      "Time 353.0 updated\n",
      "Time 353.5 updated\n",
      "Time 354.0 updated\n",
      "Time 354.5 updated\n",
      "Time 355.0 updated\n",
      "Time 355.5 updated\n",
      "Time 356.0 updated\n",
      "Time 356.5 updated\n",
      "Time 357.0 updated\n",
      "Time 357.5 updated\n",
      "Time 358.0 updated\n",
      "Time 358.5 updated\n",
      "Time 359.0 updated\n",
      "Time 359.5 updated\n",
      "Time 360.0 updated\n",
      "Time 360.5 updated\n",
      "Time 361.0 updated\n",
      "Time 361.5 updated\n",
      "Time 362.0 updated\n",
      "Time 362.5 updated\n",
      "Time 363.0 updated\n",
      "Time 363.5 updated\n",
      "Time 364.0 updated\n",
      "Time 364.5 updated\n",
      "Time 365.0 updated\n",
      "Time 365.5 updated\n",
      "Time 366.0 updated\n",
      "Time 366.5 updated\n",
      "Time 367.0 updated\n",
      "Time 367.5 updated\n",
      "Time 368.0 updated\n",
      "Time 368.5 updated\n",
      "Time 369.0 updated\n",
      "Time 369.5 updated\n",
      "Time 370.0 updated\n",
      "Time 370.5 updated\n",
      "Time 371.0 updated\n",
      "Time 371.5 updated\n",
      "Time 372.0 updated\n",
      "Time 372.5 updated\n",
      "Time 373.0 updated\n",
      "Time 373.5 updated\n",
      "Time 374.0 updated\n",
      "Time 374.5 updated\n",
      "Time 375.0 updated\n",
      "Time 375.5 updated\n",
      "Time 376.0 updated\n",
      "Time 376.5 updated\n",
      "Time 377.0 updated\n",
      "Time 377.5 updated\n",
      "Time 378.0 updated\n",
      "Time 378.5 updated\n",
      "Time 379.0 updated\n",
      "Time 379.5 updated\n",
      "Time 380.0 updated\n",
      "Time 380.5 updated\n",
      "Time 381.0 updated\n",
      "Time 381.5 updated\n",
      "Time 382.0 updated\n",
      "Time 382.5 updated\n",
      "Time 383.0 updated\n",
      "Time 383.5 updated\n",
      "Time 384.0 updated\n",
      "Time 384.5 updated\n",
      "Time 385.0 updated\n",
      "Time 385.5 updated\n",
      "Time 386.0 updated\n",
      "Time 386.5 updated\n",
      "Time 387.0 updated\n",
      "Time 387.5 updated\n",
      "Time 388.0 updated\n",
      "Time 388.5 updated\n",
      "Time 389.0 updated\n",
      "Time 389.5 updated\n",
      "Time 390.0 updated\n",
      "Time 390.5 updated\n",
      "Time 391.0 updated\n",
      "Time 391.5 updated\n",
      "Time 392.0 updated\n",
      "Time 392.5 updated\n",
      "Time 393.0 updated\n",
      "Time 393.5 updated\n",
      "Time 394.0 updated\n",
      "Time 394.5 updated\n",
      "Time 395.0 updated\n",
      "Time 395.5 updated\n",
      "Time 396.0 updated\n",
      "Time 396.5 updated\n",
      "Time 397.0 updated\n",
      "Time 397.5 updated\n",
      "Time 398.0 updated\n",
      "Time 398.5 updated\n",
      "Time 399.0 updated\n",
      "Time 399.5 updated\n",
      "Time 400.0 updated\n",
      "Time 400.5 updated\n",
      "Time 401.0 updated\n",
      "Time 401.5 updated\n",
      "Time 402.0 updated\n",
      "Time 402.5 updated\n",
      "Time 403.0 updated\n",
      "Time 403.5 updated\n",
      "Time 404.0 updated\n",
      "Time 404.5 updated\n",
      "Time 405.0 updated\n",
      "Time 405.5 updated\n",
      "Time 406.0 updated\n",
      "Time 406.5 updated\n",
      "Time 407.0 updated\n",
      "Time 407.5 updated\n",
      "Time 408.0 updated\n",
      "Time 408.5 updated\n",
      "Time 409.0 updated\n",
      "Time 409.5 updated\n",
      "Time 410.0 updated\n",
      "Time 410.5 updated\n",
      "Time 411.0 updated\n",
      "Time 411.5 updated\n",
      "Time 412.0 updated\n",
      "Time 412.5 updated\n",
      "Time 413.0 updated\n",
      "Time 413.5 updated\n",
      "Time 414.0 updated\n",
      "Time 414.5 updated\n",
      "Time 415.0 updated\n",
      "Time 415.5 updated\n",
      "Time 416.0 updated\n",
      "Time 416.5 updated\n",
      "Time 417.0 updated\n",
      "Time 417.5 updated\n",
      "Time 418.0 updated\n",
      "Time 418.5 updated\n",
      "Time 419.0 updated\n",
      "Time 419.5 updated\n",
      "Time 420.0 updated\n",
      "Time 420.5 updated\n",
      "Time 421.0 updated\n",
      "Time 421.5 updated\n",
      "Time 422.0 updated\n",
      "Time 422.5 updated\n",
      "Time 423.0 updated\n",
      "Time 423.5 updated\n",
      "Time 424.0 updated\n",
      "Time 424.5 updated\n",
      "Time 425.0 updated\n",
      "Time 425.5 updated\n",
      "Time 426.0 updated\n",
      "Time 426.5 updated\n",
      "Time 427.0 updated\n",
      "Time 427.5 updated\n",
      "Time 428.0 updated\n",
      "Time 428.5 updated\n",
      "Time 429.0 updated\n",
      "Time 429.5 updated\n",
      "Time 430.0 updated\n",
      "Time 430.5 updated\n",
      "Time 431.0 updated\n",
      "Time 431.5 updated\n",
      "Time 432.0 updated\n",
      "Time 432.5 updated\n",
      "Time 433.0 updated\n",
      "Time 433.5 updated\n",
      "Time 434.0 updated\n",
      "Time 434.5 updated\n",
      "Time 435.0 updated\n",
      "Time 435.5 updated\n",
      "Time 436.0 updated\n",
      "Time 436.5 updated\n",
      "Time 437.0 updated\n",
      "Time 437.5 updated\n",
      "Time 438.0 updated\n",
      "Time 438.5 updated\n",
      "Time 439.0 updated\n",
      "Time 439.5 updated\n",
      "Time 440.0 updated\n",
      "Time 440.5 updated\n",
      "Time 441.0 updated\n",
      "Time 441.5 updated\n",
      "Time 442.0 updated\n",
      "Time 442.5 updated\n",
      "Time 443.0 updated\n",
      "Time 443.5 updated\n",
      "Time 444.0 updated\n",
      "Time 444.5 updated\n",
      "Time 445.0 updated\n",
      "Time 445.5 updated\n",
      "Time 446.0 updated\n",
      "Time 446.5 updated\n",
      "Time 447.0 updated\n",
      "Time 447.5 updated\n",
      "Time 448.0 updated\n",
      "Time 448.5 updated\n",
      "Time 449.0 updated\n",
      "Time 449.5 updated\n",
      "Time 450.0 updated\n",
      "Time 450.5 updated\n",
      "Time 451.0 updated\n",
      "Time 451.5 updated\n",
      "Time 452.0 updated\n",
      "Time 452.5 updated\n",
      "Time 453.0 updated\n",
      "Time 453.5 updated\n",
      "Time 454.0 updated\n",
      "Time 454.5 updated\n",
      "Time 455.0 updated\n",
      "Time 455.5 updated\n",
      "Time 456.0 updated\n",
      "Time 456.5 updated\n",
      "Time 457.0 updated\n",
      "Time 457.5 updated\n",
      "Time 458.0 updated\n",
      "Time 458.5 updated\n",
      "Time 459.0 updated\n",
      "Time 459.5 updated\n",
      "Time 460.0 updated\n",
      "Time 460.5 updated\n",
      "Time 461.0 updated\n",
      "Time 461.5 updated\n",
      "Time 462.0 updated\n",
      "Time 462.5 updated\n",
      "Time 463.0 updated\n",
      "Time 463.5 updated\n",
      "Time 464.0 updated\n",
      "Time 464.5 updated\n",
      "Time 465.0 updated\n",
      "Time 465.5 updated\n",
      "Time 466.0 updated\n",
      "Time 466.5 updated\n",
      "Time 467.0 updated\n",
      "Time 467.5 updated\n",
      "Time 468.0 updated\n",
      "Time 468.5 updated\n",
      "Time 469.0 updated\n",
      "Time 469.5 updated\n",
      "Time 470.0 updated\n",
      "Time 470.5 updated\n",
      "Time 471.0 updated\n",
      "Time 471.5 updated\n",
      "Time 472.0 updated\n",
      "Time 472.5 updated\n",
      "Time 473.0 updated\n",
      "Time 473.5 updated\n",
      "Time 474.0 updated\n",
      "Time 474.5 updated\n",
      "Time 475.0 updated\n",
      "Time 475.5 updated\n",
      "Time 476.0 updated\n",
      "Time 476.5 updated\n",
      "Time 477.0 updated\n",
      "Time 477.5 updated\n",
      "Time 478.0 updated\n",
      "Time 478.5 updated\n",
      "Time 479.0 updated\n",
      "Time 479.5 updated\n",
      "Time 480.0 updated\n",
      "Time 480.5 updated\n",
      "Time 481.0 updated\n",
      "Time 481.5 updated\n",
      "Time 482.0 updated\n",
      "Time 482.5 updated\n",
      "Time 483.0 updated\n",
      "Time 483.5 updated\n",
      "Time 484.0 updated\n",
      "Time 484.5 updated\n",
      "Time 485.0 updated\n",
      "Time 485.5 updated\n",
      "Time 486.0 updated\n",
      "Time 486.5 updated\n",
      "Time 487.0 updated\n",
      "Time 487.5 updated\n",
      "Time 488.0 updated\n",
      "Time 488.5 updated\n",
      "Time 489.0 updated\n",
      "Time 489.5 updated\n",
      "Time 490.0 updated\n",
      "Time 490.5 updated\n",
      "Time 491.0 updated\n",
      "Time 491.5 updated\n",
      "Time 492.0 updated\n",
      "Time 492.5 updated\n",
      "Time 493.0 updated\n",
      "Time 493.5 updated\n",
      "Time 494.0 updated\n",
      "Time 494.5 updated\n",
      "Time 495.0 updated\n",
      "Time 495.5 updated\n",
      "Time 496.0 updated\n",
      "Time 496.5 updated\n",
      "Time 497.0 updated\n",
      "Time 497.5 updated\n",
      "Time 498.0 updated\n",
      "Time 498.5 updated\n",
      "Time 499.0 updated\n",
      "Time 499.5 updated\n",
      "Time 500.0 updated\n",
      "Time 500.5 updated\n",
      "Time 501.0 updated\n",
      "Time 501.5 updated\n",
      "Time 502.0 updated\n",
      "Time 502.5 updated\n",
      "Time 503.0 updated\n",
      "Time 503.5 updated\n",
      "Time 504.0 updated\n",
      "Time 504.5 updated\n",
      "Time 505.0 updated\n",
      "Time 505.5 updated\n",
      "Time 506.0 updated\n",
      "Time 506.5 updated\n",
      "Time 507.0 updated\n",
      "Time 507.5 updated\n",
      "Time 508.0 updated\n",
      "Time 508.5 updated\n",
      "Time 509.0 updated\n",
      "Time 509.5 updated\n",
      "Time 510.0 updated\n",
      "Time 510.5 updated\n",
      "Time 511.0 updated\n",
      "Time 511.5 updated\n",
      "Time 512.0 updated\n",
      "Time 512.5 updated\n",
      "Time 513.0 updated\n",
      "Time 513.5 updated\n",
      "Time 514.0 updated\n",
      "Time 514.5 updated\n",
      "Time 515.0 updated\n",
      "Time 515.5 updated\n",
      "Time 516.0 updated\n",
      "Time 516.5 updated\n",
      "Time 517.0 updated\n",
      "Time 517.5 updated\n",
      "Time 518.0 updated\n",
      "Time 518.5 updated\n",
      "Time 519.0 updated\n",
      "Time 519.5 updated\n",
      "Time 520.0 updated\n",
      "Time 520.5 updated\n",
      "Time 521.0 updated\n",
      "Time 521.5 updated\n",
      "Time 522.0 updated\n",
      "Time 522.5 updated\n",
      "Time 523.0 updated\n",
      "Time 523.5 updated\n",
      "Time 524.0 updated\n",
      "Time 524.5 updated\n",
      "Time 525.0 updated\n",
      "Time 525.5 updated\n",
      "Time 526.0 updated\n",
      "Time 526.5 updated\n",
      "Time 527.0 updated\n",
      "Time 527.5 updated\n",
      "Time 528.0 updated\n",
      "Time 528.5 updated\n",
      "Time 529.0 updated\n",
      "Time 529.5 updated\n",
      "Time 530.0 updated\n",
      "Time 530.5 updated\n",
      "Time 531.0 updated\n",
      "Time 531.5 updated\n",
      "Time 532.0 updated\n",
      "Time 532.5 updated\n",
      "Time 533.0 updated\n",
      "Time 533.5 updated\n",
      "Time 534.0 updated\n",
      "Time 534.5 updated\n",
      "Time 535.0 updated\n",
      "Time 535.5 updated\n",
      "Time 536.0 updated\n",
      "Time 536.5 updated\n",
      "Time 537.0 updated\n",
      "Time 537.5 updated\n",
      "Time 538.0 updated\n",
      "Time 538.5 updated\n",
      "Time 539.0 updated\n",
      "Time 539.5 updated\n",
      "Time 540.0 updated\n",
      "Time 540.5 updated\n",
      "Time 541.0 updated\n",
      "Time 541.5 updated\n",
      "Time 542.0 updated\n",
      "Time 542.5 updated\n",
      "Time 543.0 updated\n",
      "Time 543.5 updated\n",
      "Time 544.0 updated\n",
      "Time 544.5 updated\n",
      "Time 545.0 updated\n",
      "Time 545.5 updated\n",
      "Time 546.0 updated\n",
      "Time 546.5 updated\n",
      "Time 547.0 updated\n",
      "Time 547.5 updated\n",
      "Time 548.0 updated\n",
      "Time 548.5 updated\n",
      "Time 549.0 updated\n",
      "Time 549.5 updated\n",
      "Time 550.0 updated\n",
      "Time 550.5 updated\n",
      "Time 551.0 updated\n",
      "Time 551.5 updated\n",
      "Time 552.0 updated\n",
      "Time 552.5 updated\n",
      "Time 553.0 updated\n",
      "Time 553.5 updated\n",
      "Time 554.0 updated\n",
      "Time 554.5 updated\n",
      "Time 555.0 updated\n",
      "Time 555.5 updated\n",
      "Time 556.0 updated\n",
      "Time 556.5 updated\n",
      "Time 557.0 updated\n",
      "Time 557.5 updated\n",
      "Time 558.0 updated\n",
      "Time 558.5 updated\n",
      "Time 559.0 updated\n",
      "Time 559.5 updated\n",
      "Time 560.0 updated\n",
      "Time 560.5 updated\n",
      "Time 561.0 updated\n",
      "Time 561.5 updated\n",
      "Time 562.0 updated\n",
      "Time 562.5 updated\n",
      "Time 563.0 updated\n",
      "Time 563.5 updated\n",
      "Time 564.0 updated\n",
      "Time 564.5 updated\n",
      "Time 565.0 updated\n",
      "Time 565.5 updated\n",
      "Time 566.0 updated\n",
      "Time 566.5 updated\n",
      "Time 567.0 updated\n",
      "Time 567.5 updated\n",
      "Time 568.0 updated\n",
      "Time 568.5 updated\n",
      "Time 569.0 updated\n",
      "Time 569.5 updated\n",
      "Time 570.0 updated\n",
      "Time 570.5 updated\n",
      "Time 571.0 updated\n",
      "Time 571.5 updated\n",
      "Time 572.0 updated\n",
      "Time 572.5 updated\n",
      "Time 573.0 updated\n",
      "Time 573.5 updated\n",
      "Time 574.0 updated\n",
      "Time 574.5 updated\n",
      "Time 575.0 updated\n",
      "Time 575.5 updated\n",
      "Time 576.0 updated\n",
      "Time 576.5 updated\n",
      "Time 577.0 updated\n",
      "Time 577.5 updated\n",
      "Time 578.0 updated\n",
      "Time 578.5 updated\n",
      "Time 579.0 updated\n",
      "Time 579.5 updated\n",
      "Time 580.0 updated\n",
      "Time 580.5 updated\n",
      "Time 581.0 updated\n",
      "Time 581.5 updated\n",
      "Time 582.0 updated\n",
      "Time 582.5 updated\n",
      "Time 583.0 updated\n",
      "Time 583.5 updated\n",
      "Time 584.0 updated\n",
      "Time 584.5 updated\n",
      "Time 585.0 updated\n",
      "Time 585.5 updated\n",
      "Time 586.0 updated\n",
      "Time 586.5 updated\n",
      "Time 587.0 updated\n",
      "Time 587.5 updated\n",
      "Time 588.0 updated\n",
      "Time 588.5 updated\n",
      "Time 589.0 updated\n",
      "Time 589.5 updated\n",
      "Time 590.0 updated\n",
      "Time 590.5 updated\n",
      "Time 591.0 updated\n",
      "Time 591.5 updated\n",
      "Time 592.0 updated\n",
      "Time 592.5 updated\n",
      "Time 593.0 updated\n",
      "Time 593.5 updated\n",
      "Time 594.0 updated\n",
      "Time 594.5 updated\n",
      "Time 595.0 updated\n",
      "Time 595.5 updated\n",
      "Time 596.0 updated\n",
      "Time 596.5 updated\n",
      "Time 597.0 updated\n",
      "Time 597.5 updated\n",
      "Time 598.0 updated\n",
      "Time 598.5 updated\n",
      "Time 599.0 updated\n",
      "Time 599.5 updated\n",
      "Time 600.0 updated\n",
      "Time 600.5 updated\n",
      "Time 601.0 updated\n",
      "Time 601.5 updated\n",
      "Time 602.0 updated\n",
      "Time 602.5 updated\n",
      "Time 603.0 updated\n",
      "Time 603.5 updated\n",
      "Time 604.0 updated\n",
      "Time 604.5 updated\n",
      "Time 605.0 updated\n",
      "Time 605.5 updated\n",
      "Time 606.0 updated\n",
      "Time 606.5 updated\n",
      "Time 607.0 updated\n",
      "Time 607.5 updated\n",
      "Time 608.0 updated\n",
      "Time 608.5 updated\n",
      "Time 609.0 updated\n",
      "Time 609.5 updated\n",
      "Time 610.0 updated\n",
      "Time 610.5 updated\n",
      "Time 611.0 updated\n",
      "Time 611.5 updated\n",
      "Time 612.0 updated\n",
      "Time 612.5 updated\n",
      "Time 613.0 updated\n",
      "Time 613.5 updated\n",
      "Time 614.0 updated\n",
      "Time 614.5 updated\n",
      "Time 615.0 updated\n",
      "Time 615.5 updated\n",
      "Time 616.0 updated\n",
      "Time 616.5 updated\n",
      "Time 617.0 updated\n",
      "Time 617.5 updated\n",
      "Time 618.0 updated\n",
      "Time 618.5 updated\n",
      "Time 619.0 updated\n",
      "Time 619.5 updated\n",
      "Time 620.0 updated\n",
      "Time 620.5 updated\n",
      "Time 621.0 updated\n",
      "Time 621.5 updated\n",
      "Time 622.0 updated\n",
      "Time 622.5 updated\n",
      "Time 623.0 updated\n",
      "Time 623.5 updated\n",
      "Time 624.0 updated\n",
      "Time 624.5 updated\n",
      "Time 625.0 updated\n",
      "Time 625.5 updated\n",
      "Time 626.0 updated\n",
      "Time 626.5 updated\n",
      "Time 627.0 updated\n",
      "Time 627.5 updated\n",
      "Time 628.0 updated\n",
      "Time 628.5 updated\n",
      "Time 629.0 updated\n",
      "Time 629.5 updated\n",
      "Time 630.0 updated\n",
      "Time 630.5 updated\n",
      "Time 631.0 updated\n",
      "Time 631.5 updated\n",
      "Time 632.0 updated\n",
      "Time 632.5 updated\n",
      "Time 633.0 updated\n",
      "Time 633.5 updated\n",
      "Time 634.0 updated\n",
      "Time 634.5 updated\n",
      "Time 635.0 updated\n",
      "Time 635.5 updated\n",
      "Time 636.0 updated\n",
      "Time 636.5 updated\n",
      "Time 637.0 updated\n",
      "Time 637.5 updated\n",
      "Time 638.0 updated\n",
      "Time 638.5 updated\n",
      "Time 639.0 updated\n",
      "Time 639.5 updated\n",
      "Time 640.0 updated\n",
      "Time 640.5 updated\n",
      "Time 641.0 updated\n",
      "Time 641.5 updated\n",
      "Time 642.0 updated\n",
      "Time 642.5 updated\n",
      "Time 643.0 updated\n",
      "Time 643.5 updated\n",
      "Time 644.0 updated\n",
      "Time 644.5 updated\n",
      "Time 645.0 updated\n",
      "Time 645.5 updated\n",
      "Time 646.0 updated\n",
      "Time 646.5 updated\n",
      "Time 647.0 updated\n",
      "Time 647.5 updated\n",
      "Time 648.0 updated\n",
      "Time 648.5 updated\n",
      "Time 649.0 updated\n",
      "Time 649.5 updated\n",
      "Time 650.0 updated\n",
      "Time 650.5 updated\n",
      "Time 651.0 updated\n",
      "Time 651.5 updated\n",
      "Time 652.0 updated\n",
      "Time 652.5 updated\n",
      "Time 653.0 updated\n",
      "Time 653.5 updated\n",
      "Time 654.0 updated\n",
      "Time 654.5 updated\n",
      "Time 655.0 updated\n",
      "Time 655.5 updated\n",
      "Time 656.0 updated\n",
      "Time 656.5 updated\n",
      "Time 657.0 updated\n",
      "Time 657.5 updated\n",
      "Time 658.0 updated\n",
      "Time 658.5 updated\n",
      "Time 659.0 updated\n",
      "Time 659.5 updated\n",
      "Time 660.0 updated\n",
      "Time 660.5 updated\n",
      "Time 661.0 updated\n",
      "Time 661.5 updated\n",
      "Time 662.0 updated\n",
      "Time 662.5 updated\n",
      "Time 663.0 updated\n",
      "Time 663.5 updated\n",
      "Time 664.0 updated\n",
      "Time 664.5 updated\n",
      "Time 665.0 updated\n",
      "Time 665.5 updated\n",
      "Time 666.0 updated\n",
      "Time 666.5 updated\n",
      "Time 667.0 updated\n",
      "Time 667.5 updated\n",
      "Time 668.0 updated\n",
      "Time 668.5 updated\n",
      "Time 669.0 updated\n",
      "Time 669.5 updated\n",
      "Time 670.0 updated\n",
      "Time 670.5 updated\n",
      "Time 671.0 updated\n",
      "Time 671.5 updated\n",
      "Time 672.0 updated\n",
      "Time 672.5 updated\n",
      "Time 673.0 updated\n",
      "Time 673.5 updated\n",
      "Time 674.0 updated\n",
      "Time 674.5 updated\n",
      "Time 675.0 updated\n",
      "Time 675.5 updated\n",
      "Time 676.0 updated\n",
      "Time 676.5 updated\n",
      "Time 677.0 updated\n",
      "Time 677.5 updated\n",
      "Time 678.0 updated\n",
      "Time 678.5 updated\n",
      "Time 679.0 updated\n",
      "Time 679.5 updated\n",
      "Time 680.0 updated\n",
      "Time 680.5 updated\n",
      "Time 681.0 updated\n",
      "Time 681.5 updated\n",
      "Time 682.0 updated\n",
      "Time 682.5 updated\n",
      "Time 683.0 updated\n",
      "Time 683.5 updated\n",
      "Time 684.0 updated\n",
      "Time 684.5 updated\n",
      "Time 685.0 updated\n",
      "Time 685.5 updated\n",
      "Time 686.0 updated\n",
      "Time 686.5 updated\n",
      "Time 687.0 updated\n",
      "Time 687.5 updated\n",
      "Time 688.0 updated\n",
      "Time 688.5 updated\n",
      "Time 689.0 updated\n",
      "Time 689.5 updated\n",
      "Time 690.0 updated\n",
      "Time 690.5 updated\n",
      "Time 691.0 updated\n",
      "Time 691.5 updated\n",
      "Time 692.0 updated\n",
      "Time 692.5 updated\n",
      "Time 693.0 updated\n",
      "Time 693.5 updated\n",
      "Time 694.0 updated\n",
      "Time 694.5 updated\n",
      "Time 695.0 updated\n",
      "Time 695.5 updated\n",
      "Time 696.0 updated\n",
      "Time 696.5 updated\n",
      "Time 697.0 updated\n",
      "Time 697.5 updated\n",
      "Time 698.0 updated\n",
      "Time 698.5 updated\n",
      "Time 699.0 updated\n",
      "Time 699.5 updated\n",
      "Time 700.0 updated\n",
      "Time 700.5 updated\n",
      "Time 701.0 updated\n",
      "Time 701.5 updated\n",
      "Time 702.0 updated\n",
      "Time 702.5 updated\n",
      "Time 703.0 updated\n",
      "Time 703.5 updated\n",
      "Time 704.0 updated\n",
      "Time 704.5 updated\n",
      "Time 705.0 updated\n",
      "Time 705.5 updated\n",
      "Time 706.0 updated\n",
      "Time 706.5 updated\n",
      "Time 707.0 updated\n",
      "Time 707.5 updated\n",
      "Time 708.0 updated\n",
      "Time 708.5 updated\n",
      "Time 709.0 updated\n",
      "Time 709.5 updated\n",
      "Time 710.0 updated\n",
      "Time 710.5 updated\n",
      "Time 711.0 updated\n",
      "Time 711.5 updated\n",
      "Time 712.0 updated\n",
      "Time 712.5 updated\n",
      "Time 713.0 updated\n",
      "Time 713.5 updated\n",
      "Time 714.0 updated\n",
      "Time 714.5 updated\n",
      "Time 715.0 updated\n",
      "Time 715.5 updated\n",
      "Time 716.0 updated\n",
      "Time 716.5 updated\n",
      "Time 717.0 updated\n",
      "Time 717.5 updated\n",
      "Time 718.0 updated\n",
      "Time 718.5 updated\n",
      "Time 719.0 updated\n",
      "Time 719.5 updated\n",
      "Time 720.0 updated\n",
      "Time 720.5 updated\n",
      "Time 721.0 updated\n",
      "Time 721.5 updated\n",
      "Time 722.0 updated\n",
      "Time 722.5 updated\n",
      "Time 723.0 updated\n",
      "Time 723.5 updated\n",
      "Time 724.0 updated\n",
      "Time 724.5 updated\n",
      "Time 725.0 updated\n",
      "Time 725.5 updated\n",
      "Time 726.0 updated\n",
      "Time 726.5 updated\n",
      "Time 727.0 updated\n",
      "Time 727.5 updated\n",
      "Time 728.0 updated\n",
      "Time 728.5 updated\n",
      "Time 729.0 updated\n",
      "Time 729.5 updated\n",
      "Time 730.0 updated\n",
      "Time 730.5 updated\n",
      "Time 731.0 updated\n",
      "Time 731.5 updated\n",
      "Time 732.0 updated\n",
      "Time 732.5 updated\n",
      "Time 733.0 updated\n",
      "Time 733.5 updated\n",
      "Time 734.0 updated\n",
      "Time 734.5 updated\n",
      "Time 735.0 updated\n",
      "Time 735.5 updated\n",
      "Time 736.0 updated\n",
      "Time 736.5 updated\n",
      "Time 737.0 updated\n",
      "Time 737.5 updated\n",
      "Time 738.0 updated\n",
      "Time 738.5 updated\n",
      "Time 739.0 updated\n",
      "Time 739.5 updated\n",
      "Time 740.0 updated\n",
      "Time 740.5 updated\n",
      "Time 741.0 updated\n",
      "Time 741.5 updated\n",
      "Time 742.0 updated\n",
      "Time 742.5 updated\n",
      "Time 743.0 updated\n",
      "Time 743.5 updated\n",
      "Time 744.0 updated\n",
      "Time 744.5 updated\n",
      "Time 745.0 updated\n",
      "Time 745.5 updated\n",
      "Time 746.0 updated\n",
      "Time 746.5 updated\n",
      "Time 747.0 updated\n",
      "Time 747.5 updated\n",
      "Time 748.0 updated\n",
      "Time 748.5 updated\n",
      "Time 749.0 updated\n",
      "Time 749.5 updated\n",
      "Time 750.0 updated\n",
      "Time 750.5 updated\n",
      "Time 751.0 updated\n",
      "Time 751.5 updated\n",
      "Time 752.0 updated\n",
      "Time 752.5 updated\n",
      "Time 753.0 updated\n",
      "Time 753.5 updated\n",
      "Time 754.0 updated\n",
      "Time 754.5 updated\n",
      "Time 755.0 updated\n",
      "Time 755.5 updated\n",
      "Time 756.0 updated\n",
      "Time 756.5 updated\n",
      "Time 757.0 updated\n",
      "Time 757.5 updated\n",
      "Time 758.0 updated\n",
      "Time 758.5 updated\n",
      "Time 759.0 updated\n",
      "Time 759.5 updated\n",
      "Time 760.0 updated\n",
      "Time 760.5 updated\n",
      "Time 761.0 updated\n",
      "Time 761.5 updated\n",
      "Time 762.0 updated\n",
      "Time 762.5 updated\n",
      "Time 763.0 updated\n",
      "Time 763.5 updated\n",
      "Time 764.0 updated\n",
      "Time 764.5 updated\n",
      "Time 765.0 updated\n",
      "Time 765.5 updated\n",
      "Time 766.0 updated\n",
      "Time 766.5 updated\n",
      "Time 767.0 updated\n",
      "Time 767.5 updated\n",
      "Time 768.0 updated\n",
      "Time 768.5 updated\n",
      "Time 769.0 updated\n",
      "Time 769.5 updated\n",
      "Time 770.0 updated\n",
      "Time 770.5 updated\n",
      "Time 771.0 updated\n",
      "Time 771.5 updated\n",
      "Time 772.0 updated\n",
      "Time 772.5 updated\n",
      "Time 773.0 updated\n",
      "Time 773.5 updated\n",
      "Time 774.0 updated\n",
      "Time 774.5 updated\n",
      "Time 775.0 updated\n",
      "Time 775.5 updated\n",
      "Time 776.0 updated\n",
      "Time 776.5 updated\n",
      "Time 777.0 updated\n",
      "Time 777.5 updated\n",
      "Time 778.0 updated\n",
      "Time 778.5 updated\n",
      "Time 779.0 updated\n",
      "Time 779.5 updated\n",
      "Time 780.0 updated\n",
      "Time 780.5 updated\n",
      "Time 781.0 updated\n",
      "Time 781.5 updated\n",
      "Time 782.0 updated\n",
      "Time 782.5 updated\n",
      "Time 783.0 updated\n",
      "Time 783.5 updated\n",
      "Time 784.0 updated\n",
      "Time 784.5 updated\n",
      "Time 785.0 updated\n",
      "Time 785.5 updated\n",
      "Time 786.0 updated\n",
      "Time 786.5 updated\n",
      "Time 787.0 updated\n",
      "Time 787.5 updated\n",
      "Time 788.0 updated\n",
      "Time 788.5 updated\n",
      "Time 789.0 updated\n",
      "Time 789.5 updated\n",
      "Time 790.0 updated\n",
      "Time 790.5 updated\n",
      "Time 791.0 updated\n",
      "Time 791.5 updated\n",
      "Time 792.0 updated\n",
      "Time 792.5 updated\n",
      "Time 793.0 updated\n",
      "Time 793.5 updated\n",
      "Time 794.0 updated\n",
      "Time 794.5 updated\n",
      "Time 795.0 updated\n",
      "Time 795.5 updated\n",
      "Time 796.0 updated\n",
      "Time 796.5 updated\n",
      "Time 797.0 updated\n",
      "Time 797.5 updated\n",
      "Time 798.0 updated\n",
      "Time 798.5 updated\n",
      "Time 799.0 updated\n",
      "Time 799.5 updated\n",
      "Time 800.0 updated\n",
      "Time 800.5 updated\n",
      "Time 801.0 updated\n",
      "Time 801.5 updated\n",
      "Time 802.0 updated\n",
      "Time 802.5 updated\n",
      "Time 803.0 updated\n",
      "Time 803.5 updated\n",
      "Time 804.0 updated\n",
      "Time 804.5 updated\n",
      "Time 805.0 updated\n",
      "Time 805.5 updated\n",
      "Time 806.0 updated\n",
      "Time 806.5 updated\n",
      "Time 807.0 updated\n",
      "Time 807.5 updated\n",
      "Time 808.0 updated\n",
      "Time 808.5 updated\n",
      "Time 809.0 updated\n",
      "Time 809.5 updated\n",
      "Time 810.0 updated\n",
      "Time 810.5 updated\n",
      "Time 811.0 updated\n",
      "Time 811.5 updated\n",
      "Time 812.0 updated\n",
      "Time 812.5 updated\n",
      "Time 813.0 updated\n",
      "Time 813.5 updated\n",
      "Time 814.0 updated\n",
      "Time 814.5 updated\n",
      "Time 815.0 updated\n",
      "Time 815.5 updated\n",
      "Time 816.0 updated\n",
      "Time 816.5 updated\n",
      "Time 817.0 updated\n",
      "Time 817.5 updated\n",
      "Time 818.0 updated\n",
      "Time 818.5 updated\n",
      "Time 819.0 updated\n",
      "Time 819.5 updated\n",
      "Time 820.0 updated\n",
      "Time 820.5 updated\n",
      "Time 821.0 updated\n",
      "Time 821.5 updated\n",
      "Time 822.0 updated\n",
      "Time 822.5 updated\n",
      "Time 823.0 updated\n",
      "Time 823.5 updated\n",
      "Time 824.0 updated\n",
      "Time 824.5 updated\n",
      "Time 825.0 updated\n",
      "Time 825.5 updated\n",
      "Time 826.0 updated\n",
      "Time 826.5 updated\n",
      "Time 827.0 updated\n",
      "Time 827.5 updated\n",
      "Time 828.0 updated\n",
      "Time 828.5 updated\n",
      "Time 829.0 updated\n",
      "Time 829.5 updated\n",
      "Time 830.0 updated\n",
      "Time 830.5 updated\n",
      "Time 831.0 updated\n",
      "Time 831.5 updated\n",
      "Time 832.0 updated\n",
      "Time 832.5 updated\n",
      "Time 833.0 updated\n",
      "Time 833.5 updated\n",
      "Time 834.0 updated\n",
      "Time 834.5 updated\n",
      "Time 835.0 updated\n",
      "Time 835.5 updated\n",
      "Time 836.0 updated\n",
      "Time 836.5 updated\n",
      "Time 837.0 updated\n",
      "Time 837.5 updated\n",
      "Time 838.0 updated\n",
      "Time 838.5 updated\n",
      "Time 839.0 updated\n",
      "Time 839.5 updated\n",
      "Time 840.0 updated\n",
      "Time 840.5 updated\n",
      "Time 841.0 updated\n",
      "Time 841.5 updated\n",
      "Time 842.0 updated\n",
      "Time 842.5 updated\n",
      "Time 843.0 updated\n",
      "Time 843.5 updated\n",
      "Time 844.0 updated\n",
      "Time 844.5 updated\n",
      "Time 845.0 updated\n",
      "Time 845.5 updated\n",
      "Time 846.0 updated\n",
      "Time 846.5 updated\n",
      "Time 847.0 updated\n",
      "Time 847.5 updated\n",
      "Time 848.0 updated\n",
      "Time 848.5 updated\n",
      "Time 849.0 updated\n",
      "Time 849.5 updated\n",
      "Time 850.0 updated\n",
      "Time 850.5 updated\n",
      "Time 851.0 updated\n",
      "Time 851.5 updated\n",
      "Time 852.0 updated\n",
      "Time 852.5 updated\n",
      "Time 853.0 updated\n",
      "Time 853.5 updated\n",
      "Time 854.0 updated\n",
      "Time 854.5 updated\n",
      "Time 855.0 updated\n",
      "Time 855.5 updated\n",
      "Time 856.0 updated\n",
      "Time 856.5 updated\n",
      "Time 857.0 updated\n",
      "Time 857.5 updated\n",
      "Time 858.0 updated\n",
      "Time 858.5 updated\n",
      "Time 859.0 updated\n",
      "Time 859.5 updated\n",
      "Time 860.0 updated\n",
      "Time 860.5 updated\n",
      "Time 861.0 updated\n",
      "Time 861.5 updated\n",
      "Time 862.0 updated\n",
      "Time 862.5 updated\n",
      "Time 863.0 updated\n",
      "Time 863.5 updated\n",
      "Time 864.0 updated\n",
      "Time 864.5 updated\n",
      "Time 865.0 updated\n",
      "Time 865.5 updated\n",
      "Time 866.0 updated\n",
      "Time 866.5 updated\n",
      "Time 867.0 updated\n",
      "Time 867.5 updated\n",
      "Time 868.0 updated\n",
      "Time 868.5 updated\n",
      "Time 869.0 updated\n",
      "Time 869.5 updated\n",
      "Time 870.0 updated\n",
      "Time 870.5 updated\n",
      "Time 871.0 updated\n",
      "Time 871.5 updated\n",
      "Time 872.0 updated\n",
      "Time 872.5 updated\n",
      "Time 873.0 updated\n",
      "Time 873.5 updated\n",
      "Time 874.0 updated\n",
      "Time 874.5 updated\n",
      "Time 875.0 updated\n",
      "Time 875.5 updated\n",
      "Time 876.0 updated\n",
      "Time 876.5 updated\n",
      "Time 877.0 updated\n",
      "Time 877.5 updated\n",
      "Time 878.0 updated\n",
      "Time 878.5 updated\n",
      "Time 879.0 updated\n",
      "Time 879.5 updated\n",
      "Time 880.0 updated\n",
      "Time 880.5 updated\n",
      "Time 881.0 updated\n",
      "Time 881.5 updated\n",
      "Time 882.0 updated\n",
      "Time 882.5 updated\n",
      "Time 883.0 updated\n",
      "Time 883.5 updated\n",
      "Time 884.0 updated\n",
      "Time 884.5 updated\n",
      "Time 885.0 updated\n",
      "Time 885.5 updated\n",
      "Time 886.0 updated\n",
      "Time 886.5 updated\n",
      "Time 887.0 updated\n",
      "Time 887.5 updated\n",
      "Time 888.0 updated\n",
      "Time 888.5 updated\n",
      "Time 889.0 updated\n",
      "Time 889.5 updated\n",
      "Time 890.0 updated\n",
      "Time 890.5 updated\n",
      "Time 891.0 updated\n",
      "Time 891.5 updated\n",
      "Time 892.0 updated\n",
      "Time 892.5 updated\n",
      "Time 893.0 updated\n",
      "Time 893.5 updated\n",
      "Time 894.0 updated\n",
      "Time 894.5 updated\n",
      "Time 895.0 updated\n",
      "Time 895.5 updated\n",
      "Time 896.0 updated\n",
      "Time 896.5 updated\n",
      "Time 897.0 updated\n",
      "Time 897.5 updated\n",
      "Time 898.0 updated\n",
      "Time 898.5 updated\n",
      "Time 899.0 updated\n",
      "Time 899.5 updated\n",
      "Time 900.0 updated\n",
      "Time 900.5 updated\n",
      "Time 901.0 updated\n",
      "Time 901.5 updated\n",
      "Time 902.0 updated\n",
      "Time 902.5 updated\n",
      "Time 903.0 updated\n",
      "Time 903.5 updated\n",
      "Time 904.0 updated\n",
      "Time 904.5 updated\n",
      "Time 905.0 updated\n",
      "Time 905.5 updated\n",
      "Time 906.0 updated\n",
      "Time 906.5 updated\n",
      "Time 907.0 updated\n",
      "Time 907.5 updated\n",
      "Time 908.0 updated\n",
      "Time 908.5 updated\n",
      "Time 909.0 updated\n",
      "Time 909.5 updated\n",
      "Time 910.0 updated\n",
      "Time 910.5 updated\n",
      "Time 911.0 updated\n",
      "Time 911.5 updated\n",
      "Time 912.0 updated\n",
      "Time 912.5 updated\n",
      "Time 913.0 updated\n",
      "Time 913.5 updated\n",
      "Time 914.0 updated\n",
      "Time 914.5 updated\n",
      "Time 915.0 updated\n",
      "Time 915.5 updated\n",
      "Time 916.0 updated\n",
      "Time 916.5 updated\n",
      "Time 917.0 updated\n",
      "Time 917.5 updated\n",
      "Time 918.0 updated\n",
      "Time 918.5 updated\n",
      "Time 919.0 updated\n",
      "Time 919.5 updated\n",
      "Time 920.0 updated\n",
      "Time 920.5 updated\n",
      "Time 921.0 updated\n",
      "Time 921.5 updated\n",
      "Time 922.0 updated\n",
      "Time 922.5 updated\n",
      "Time 923.0 updated\n",
      "Time 923.5 updated\n",
      "Time 924.0 updated\n",
      "Time 924.5 updated\n",
      "Time 925.0 updated\n",
      "Time 925.5 updated\n",
      "Time 926.0 updated\n",
      "Time 926.5 updated\n",
      "Time 927.0 updated\n",
      "Time 927.5 updated\n",
      "Time 928.0 updated\n",
      "Time 928.5 updated\n",
      "Time 929.0 updated\n",
      "Time 929.5 updated\n",
      "Time 930.0 updated\n",
      "Time 930.5 updated\n",
      "Time 931.0 updated\n",
      "Time 931.5 updated\n",
      "Time 932.0 updated\n",
      "Time 932.5 updated\n",
      "Time 933.0 updated\n",
      "Time 933.5 updated\n",
      "Time 934.0 updated\n",
      "Time 934.5 updated\n",
      "Time 935.0 updated\n",
      "Time 935.5 updated\n",
      "Time 936.0 updated\n",
      "Time 936.5 updated\n",
      "Time 937.0 updated\n",
      "Time 937.5 updated\n",
      "Time 938.0 updated\n",
      "Time 938.5 updated\n",
      "Time 939.0 updated\n",
      "Time 939.5 updated\n",
      "Time 940.0 updated\n",
      "Time 940.5 updated\n",
      "Time 941.0 updated\n",
      "Time 941.5 updated\n",
      "Time 942.0 updated\n",
      "Time 942.5 updated\n",
      "Time 943.0 updated\n",
      "Time 943.5 updated\n",
      "Time 944.0 updated\n",
      "Time 944.5 updated\n",
      "Time 945.0 updated\n",
      "Time 945.5 updated\n",
      "Time 946.0 updated\n",
      "Time 946.5 updated\n",
      "Time 947.0 updated\n",
      "Time 947.5 updated\n",
      "Time 948.0 updated\n",
      "Time 948.5 updated\n",
      "Time 949.0 updated\n",
      "Time 949.5 updated\n",
      "Time 950.0 updated\n",
      "Time 950.5 updated\n",
      "Time 951.0 updated\n",
      "Time 951.5 updated\n",
      "Time 952.0 updated\n",
      "Time 952.5 updated\n",
      "Time 953.0 updated\n",
      "Time 953.5 updated\n",
      "Time 954.0 updated\n",
      "Time 954.5 updated\n",
      "Time 955.0 updated\n",
      "Time 955.5 updated\n",
      "Time 956.0 updated\n",
      "Time 956.5 updated\n",
      "Time 957.0 updated\n",
      "Time 957.5 updated\n",
      "Time 958.0 updated\n",
      "Time 958.5 updated\n",
      "Time 959.0 updated\n",
      "Time 959.5 updated\n",
      "Time 960.0 updated\n",
      "Time 960.5 updated\n",
      "Time 961.0 updated\n",
      "Time 961.5 updated\n",
      "Time 962.0 updated\n",
      "Time 962.5 updated\n",
      "Time 963.0 updated\n",
      "Time 963.5 updated\n",
      "Time 964.0 updated\n",
      "Time 964.5 updated\n",
      "Time 965.0 updated\n",
      "Time 965.5 updated\n",
      "Time 966.0 updated\n",
      "Time 966.5 updated\n",
      "Time 967.0 updated\n",
      "Time 967.5 updated\n",
      "Time 968.0 updated\n",
      "Time 968.5 updated\n",
      "Time 969.0 updated\n",
      "Time 969.5 updated\n",
      "Time 970.0 updated\n",
      "Time 970.5 updated\n",
      "Time 971.0 updated\n",
      "Time 971.5 updated\n",
      "Time 972.0 updated\n",
      "Time 972.5 updated\n",
      "Time 973.0 updated\n",
      "Time 973.5 updated\n",
      "Time 974.0 updated\n",
      "Time 974.5 updated\n",
      "Time 975.0 updated\n",
      "Time 975.5 updated\n",
      "Time 976.0 updated\n",
      "Time 976.5 updated\n",
      "Time 977.0 updated\n",
      "Time 977.5 updated\n",
      "Time 978.0 updated\n",
      "Time 978.5 updated\n",
      "Time 979.0 updated\n",
      "Time 979.5 updated\n",
      "Time 980.0 updated\n",
      "Time 980.5 updated\n",
      "Time 981.0 updated\n",
      "Time 981.5 updated\n",
      "Time 982.0 updated\n",
      "Time 982.5 updated\n",
      "Time 983.0 updated\n",
      "Time 983.5 updated\n",
      "Time 984.0 updated\n",
      "Time 984.5 updated\n",
      "Time 985.0 updated\n",
      "Time 985.5 updated\n",
      "Time 986.0 updated\n",
      "Time 986.5 updated\n",
      "Time 987.0 updated\n",
      "Time 987.5 updated\n",
      "Time 988.0 updated\n",
      "Time 988.5 updated\n",
      "Time 989.0 updated\n",
      "Time 989.5 updated\n",
      "Time 990.0 updated\n",
      "Time 990.5 updated\n",
      "Time 991.0 updated\n",
      "Time 991.5 updated\n",
      "Time 992.0 updated\n",
      "Time 992.5 updated\n",
      "Time 993.0 updated\n",
      "Time 993.5 updated\n",
      "Time 994.0 updated\n",
      "Time 994.5 updated\n",
      "Time 995.0 updated\n",
      "Time 995.5 updated\n",
      "Time 996.0 updated\n",
      "Time 996.5 updated\n",
      "Time 997.0 updated\n",
      "Time 997.5 updated\n",
      "Time 998.0 updated\n",
      "Time 998.5 updated\n",
      "Time 999.0 updated\n",
      "Time 999.5 updated\n",
      "Time 1000.0 updated\n",
      "Time 1000.5 updated\n",
      "Time 1001.0 updated\n",
      "Time 1001.5 updated\n",
      "Time 1002.0 updated\n",
      "Time 1002.5 updated\n",
      "Time 1003.0 updated\n",
      "Time 1003.5 updated\n",
      "Time 1004.0 updated\n",
      "Time 1004.5 updated\n",
      "Time 1005.0 updated\n",
      "Time 1005.5 updated\n",
      "Time 1006.0 updated\n",
      "Time 1006.5 updated\n",
      "Time 1007.0 updated\n",
      "Time 1007.5 updated\n",
      "Time 1008.0 updated\n",
      "Time 1008.5 updated\n",
      "Time 1009.0 updated\n",
      "Time 1009.5 updated\n",
      "Time 1010.0 updated\n",
      "Time 1010.5 updated\n",
      "Time 1011.0 updated\n",
      "Time 1011.5 updated\n",
      "Time 1012.0 updated\n",
      "Time 1012.5 updated\n",
      "Time 1013.0 updated\n",
      "Time 1013.5 updated\n",
      "Time 1014.0 updated\n",
      "Time 1014.5 updated\n",
      "Time 1015.0 updated\n",
      "Time 1015.5 updated\n",
      "Time 1016.0 updated\n",
      "Time 1016.5 updated\n",
      "Time 1017.0 updated\n",
      "Time 1017.5 updated\n",
      "Time 1018.0 updated\n",
      "Time 1018.5 updated\n",
      "Time 1019.0 updated\n",
      "Time 1019.5 updated\n",
      "Time 1020.0 updated\n",
      "Time 1020.5 updated\n",
      "Time 1021.0 updated\n",
      "Time 1021.5 updated\n",
      "Time 1022.0 updated\n",
      "Time 1022.5 updated\n",
      "Time 1023.0 updated\n",
      "Time 1023.5 updated\n",
      "Time 1024.0 updated\n",
      "Time 1024.5 updated\n",
      "Time 1025.0 updated\n",
      "Time 1025.5 updated\n",
      "Time 1026.0 updated\n",
      "Time 1026.5 updated\n",
      "Time 1027.0 updated\n",
      "Time 1027.5 updated\n",
      "Time 1028.0 updated\n",
      "Time 1028.5 updated\n",
      "Time 1029.0 updated\n",
      "Time 1029.5 updated\n",
      "Time 1030.0 updated\n",
      "Time 1030.5 updated\n",
      "Time 1031.0 updated\n",
      "Time 1031.5 updated\n",
      "Time 1032.0 updated\n",
      "Time 1032.5 updated\n",
      "Time 1033.0 updated\n",
      "Time 1033.5 updated\n",
      "Time 1034.0 updated\n",
      "Time 1034.5 updated\n",
      "Time 1035.0 updated\n",
      "Time 1035.5 updated\n",
      "Time 1036.0 updated\n",
      "Time 1036.5 updated\n",
      "Time 1037.0 updated\n",
      "Time 1037.5 updated\n",
      "Time 1038.0 updated\n",
      "Time 1038.5 updated\n",
      "Time 1039.0 updated\n",
      "Time 1039.5 updated\n",
      "Time 1040.0 updated\n",
      "Time 1040.5 updated\n",
      "Time 1041.0 updated\n",
      "Time 1041.5 updated\n",
      "Time 1042.0 updated\n",
      "Time 1042.5 updated\n",
      "Time 1043.0 updated\n",
      "Time 1043.5 updated\n",
      "Time 1044.0 updated\n",
      "Time 1044.5 updated\n",
      "Time 1045.0 updated\n",
      "Time 1045.5 updated\n",
      "Time 1046.0 updated\n",
      "Time 1046.5 updated\n",
      "Time 1047.0 updated\n",
      "Time 1047.5 updated\n",
      "Time 1048.0 updated\n",
      "Time 1048.5 updated\n",
      "Time 1049.0 updated\n",
      "Time 1049.5 updated\n",
      "Time 1050.0 updated\n",
      "Time 1050.5 updated\n",
      "Time 1051.0 updated\n",
      "Time 1051.5 updated\n",
      "Time 1052.0 updated\n",
      "Time 1052.5 updated\n",
      "Time 1053.0 updated\n",
      "Time 1053.5 updated\n",
      "Time 1054.0 updated\n",
      "Time 1054.5 updated\n",
      "Time 1055.0 updated\n",
      "Time 1055.5 updated\n",
      "Time 1056.0 updated\n",
      "Time 1056.5 updated\n",
      "Time 1057.0 updated\n",
      "Time 1057.5 updated\n",
      "Time 1058.0 updated\n",
      "Time 1058.5 updated\n",
      "Time 1059.0 updated\n",
      "Time 1059.5 updated\n",
      "Time 1060.0 updated\n",
      "Time 1060.5 updated\n",
      "Time 1061.0 updated\n",
      "Time 1061.5 updated\n",
      "Time 1062.0 updated\n",
      "Time 1062.5 updated\n",
      "Time 1063.0 updated\n",
      "Time 1063.5 updated\n",
      "Time 1064.0 updated\n",
      "Time 1064.5 updated\n",
      "Time 1065.0 updated\n",
      "Time 1065.5 updated\n",
      "Time 1066.0 updated\n",
      "Time 1066.5 updated\n",
      "Time 1067.0 updated\n",
      "Time 1067.5 updated\n",
      "Time 1068.0 updated\n",
      "Time 1068.5 updated\n",
      "Time 1069.0 updated\n",
      "Time 1069.5 updated\n",
      "Time 1070.0 updated\n",
      "Time 1070.5 updated\n",
      "Time 1071.0 updated\n",
      "Time 1071.5 updated\n",
      "Time 1072.0 updated\n",
      "Time 1072.5 updated\n",
      "Time 1073.0 updated\n",
      "Time 1073.5 updated\n",
      "Time 1074.0 updated\n",
      "Time 1074.5 updated\n",
      "Time 1075.0 updated\n",
      "Time 1075.5 updated\n",
      "Time 1076.0 updated\n",
      "Time 1076.5 updated\n",
      "Time 1077.0 updated\n",
      "Time 1077.5 updated\n",
      "Time 1078.0 updated\n",
      "Time 1078.5 updated\n",
      "Time 1079.0 updated\n",
      "Time 1079.5 updated\n",
      "Time 1080.0 updated\n",
      "Time 1080.5 updated\n",
      "Time 1081.0 updated\n",
      "Time 1081.5 updated\n",
      "Time 1082.0 updated\n",
      "Time 1082.5 updated\n",
      "Time 1083.0 updated\n",
      "Time 1083.5 updated\n",
      "Time 1084.0 updated\n",
      "Time 1084.5 updated\n",
      "Time 1085.0 updated\n",
      "Time 1085.5 updated\n",
      "Time 1086.0 updated\n",
      "Time 1086.5 updated\n",
      "Time 1087.0 updated\n",
      "Time 1087.5 updated\n",
      "Time 1088.0 updated\n",
      "Time 1088.5 updated\n",
      "Time 1089.0 updated\n",
      "Time 1089.5 updated\n",
      "Time 1090.0 updated\n",
      "Time 1090.5 updated\n",
      "Time 1091.0 updated\n",
      "Time 1091.5 updated\n",
      "Time 1092.0 updated\n",
      "Time 1092.5 updated\n",
      "Time 1093.0 updated\n",
      "Time 1093.5 updated\n",
      "Time 1094.0 updated\n",
      "Time 1094.5 updated\n",
      "Time 1095.0 updated\n",
      "Time 1095.5 updated\n",
      "Time 1096.0 updated\n",
      "Time 1096.5 updated\n",
      "Time 1097.0 updated\n",
      "Time 1097.5 updated\n",
      "Time 1098.0 updated\n",
      "Time 1098.5 updated\n",
      "Time 1099.0 updated\n",
      "Time 1099.5 updated\n",
      "Time 1100.0 updated\n",
      "Time 1100.5 updated\n",
      "Time 1101.0 updated\n",
      "Time 1101.5 updated\n",
      "Time 1102.0 updated\n",
      "Time 1102.5 updated\n",
      "Time 1103.0 updated\n",
      "Time 1103.5 updated\n",
      "Time 1104.0 updated\n",
      "Time 1104.5 updated\n",
      "Time 1105.0 updated\n",
      "Time 1105.5 updated\n",
      "Time 1106.0 updated\n",
      "Time 1106.5 updated\n",
      "Time 1107.0 updated\n",
      "Time 1107.5 updated\n",
      "Time 1108.0 updated\n",
      "Time 1108.5 updated\n",
      "Time 1109.0 updated\n",
      "Time 1109.5 updated\n",
      "Time 1110.0 updated\n",
      "Time 1110.5 updated\n",
      "Time 1111.0 updated\n",
      "Time 1111.5 updated\n",
      "Time 1112.0 updated\n",
      "Time 1112.5 updated\n",
      "Time 1113.0 updated\n",
      "Time 1113.5 updated\n",
      "Time 1114.0 updated\n",
      "Time 1114.5 updated\n",
      "Time 1115.0 updated\n",
      "Time 1115.5 updated\n",
      "Time 1116.0 updated\n",
      "Time 1116.5 updated\n",
      "Time 1117.0 updated\n",
      "Time 1117.5 updated\n",
      "Time 1118.0 updated\n",
      "Time 1118.5 updated\n",
      "Time 1119.0 updated\n",
      "Time 1119.5 updated\n",
      "Time 1120.0 updated\n",
      "Time 1120.5 updated\n",
      "Time 1121.0 updated\n",
      "Time 1121.5 updated\n",
      "Time 1122.0 updated\n",
      "Time 1122.5 updated\n",
      "Time 1123.0 updated\n",
      "Time 1123.5 updated\n",
      "Time 1124.0 updated\n",
      "Time 1124.5 updated\n",
      "Time 1125.0 updated\n",
      "Time 1125.5 updated\n",
      "Time 1126.0 updated\n",
      "Time 1126.5 updated\n",
      "Time 1127.0 updated\n",
      "Time 1127.5 updated\n",
      "Time 1128.0 updated\n",
      "Time 1128.5 updated\n",
      "Time 1129.0 updated\n",
      "Time 1129.5 updated\n",
      "Time 1130.0 updated\n",
      "Time 1130.5 updated\n",
      "Time 1131.0 updated\n",
      "Time 1131.5 updated\n",
      "Time 1132.0 updated\n",
      "Time 1132.5 updated\n",
      "Time 1133.0 updated\n",
      "Time 1133.5 updated\n",
      "Time 1134.0 updated\n",
      "Time 1134.5 updated\n",
      "Time 1135.0 updated\n",
      "Time 1135.5 updated\n",
      "Time 1136.0 updated\n",
      "Time 1136.5 updated\n",
      "Time 1137.0 updated\n",
      "Time 1137.5 updated\n",
      "Time 1138.0 updated\n",
      "Time 1138.5 updated\n",
      "Time 1139.0 updated\n",
      "Time 1139.5 updated\n",
      "Time 1140.0 updated\n",
      "Time 1140.5 updated\n",
      "Time 1141.0 updated\n",
      "Time 1141.5 updated\n",
      "Time 1142.0 updated\n",
      "Time 1142.5 updated\n",
      "Time 1143.0 updated\n",
      "Time 1143.5 updated\n",
      "Time 1144.0 updated\n",
      "Time 1144.5 updated\n",
      "Time 1145.0 updated\n",
      "Time 1145.5 updated\n",
      "Time 1146.0 updated\n",
      "Time 1146.5 updated\n",
      "Time 1147.0 updated\n",
      "Time 1147.5 updated\n",
      "Time 1148.0 updated\n",
      "Time 1148.5 updated\n",
      "Time 1149.0 updated\n",
      "Time 1149.5 updated\n",
      "Time 1150.0 updated\n",
      "Time 1150.5 updated\n",
      "Time 1151.0 updated\n",
      "Time 1151.5 updated\n",
      "Time 1152.0 updated\n",
      "Time 1152.5 updated\n",
      "Time 1153.0 updated\n",
      "Time 1153.5 updated\n",
      "Time 1154.0 updated\n",
      "Time 1154.5 updated\n",
      "Time 1155.0 updated\n",
      "Time 1155.5 updated\n",
      "Time 1156.0 updated\n",
      "Time 1156.5 updated\n",
      "Time 1157.0 updated\n",
      "Time 1157.5 updated\n",
      "Time 1158.0 updated\n",
      "Time 1158.5 updated\n",
      "Time 1159.0 updated\n",
      "Time 1159.5 updated\n",
      "Time 1160.0 updated\n",
      "Time 1160.5 updated\n",
      "Time 1161.0 updated\n",
      "Time 1161.5 updated\n",
      "Time 1162.0 updated\n",
      "Time 1162.5 updated\n",
      "Time 1163.0 updated\n",
      "Time 1163.5 updated\n",
      "Time 1164.0 updated\n",
      "Time 1164.5 updated\n",
      "Time 1165.0 updated\n",
      "Time 1165.5 updated\n",
      "Time 1166.0 updated\n",
      "Time 1166.5 updated\n",
      "Time 1167.0 updated\n",
      "Time 1167.5 updated\n",
      "Time 1168.0 updated\n",
      "Time 1168.5 updated\n",
      "Time 1169.0 updated\n",
      "Time 1169.5 updated\n",
      "Time 1170.0 updated\n",
      "Time 1170.5 updated\n",
      "Time 1171.0 updated\n",
      "Time 1171.5 updated\n",
      "Time 1172.0 updated\n",
      "Time 1172.5 updated\n",
      "Time 1173.0 updated\n",
      "Time 1173.5 updated\n",
      "Time 1174.0 updated\n",
      "Time 1174.5 updated\n",
      "Time 1175.0 updated\n",
      "Time 1175.5 updated\n",
      "Time 1176.0 updated\n",
      "Time 1176.5 updated\n",
      "Time 1177.0 updated\n",
      "Time 1177.5 updated\n",
      "Time 1178.0 updated\n",
      "Time 1178.5 updated\n",
      "Time 1179.0 updated\n",
      "Time 1179.5 updated\n",
      "Time 1180.0 updated\n",
      "Time 1180.5 updated\n",
      "Time 1181.0 updated\n",
      "Time 1181.5 updated\n",
      "Time 1182.0 updated\n",
      "Time 1182.5 updated\n",
      "Time 1183.0 updated\n",
      "Time 1183.5 updated\n",
      "Time 1184.0 updated\n",
      "Time 1184.5 updated\n",
      "Time 1185.0 updated\n",
      "Time 1185.5 updated\n",
      "Time 1186.0 updated\n",
      "Time 1186.5 updated\n",
      "Time 1187.0 updated\n",
      "Time 1187.5 updated\n",
      "Time 1188.0 updated\n",
      "Time 1188.5 updated\n",
      "Time 1189.0 updated\n",
      "Time 1189.5 updated\n",
      "Time 1190.0 updated\n",
      "Time 1190.5 updated\n",
      "Time 1191.0 updated\n",
      "Time 1191.5 updated\n",
      "Time 1192.0 updated\n",
      "Time 1192.5 updated\n",
      "Time 1193.0 updated\n",
      "Time 1193.5 updated\n",
      "Time 1194.0 updated\n",
      "Time 1194.5 updated\n",
      "Time 1195.0 updated\n",
      "Time 1195.5 updated\n",
      "Time 1196.0 updated\n",
      "Time 1196.5 updated\n",
      "Time 1197.0 updated\n",
      "Time 1197.5 updated\n",
      "Time 1198.0 updated\n",
      "Time 1198.5 updated\n",
      "Time 1199.0 updated\n",
      "Time 1199.5 updated\n",
      "Time 1200.0 updated\n",
      "Time 1200.5 updated\n",
      "Time 1201.0 updated\n",
      "Time 1201.5 updated\n",
      "Time 1202.0 updated\n",
      "Time 1202.5 updated\n",
      "Time 1203.0 updated\n",
      "Time 1203.5 updated\n",
      "Time 1204.0 updated\n",
      "Time 1204.5 updated\n",
      "Time 1205.0 updated\n",
      "Time 1205.5 updated\n",
      "Time 1206.0 updated\n",
      "Time 1206.5 updated\n",
      "Time 1207.0 updated\n",
      "Time 1207.5 updated\n",
      "Time 1208.0 updated\n",
      "Time 1208.5 updated\n",
      "Time 1209.0 updated\n",
      "Time 1209.5 updated\n",
      "Time 1210.0 updated\n",
      "Time 1210.5 updated\n",
      "Time 1211.0 updated\n",
      "Time 1211.5 updated\n",
      "Time 1212.0 updated\n",
      "Time 1212.5 updated\n",
      "Time 1213.0 updated\n",
      "Time 1213.5 updated\n",
      "Time 1214.0 updated\n",
      "Time 1214.5 updated\n",
      "Time 1215.0 updated\n",
      "Time 1215.5 updated\n",
      "Time 1216.0 updated\n",
      "Time 1216.5 updated\n",
      "Time 1217.0 updated\n",
      "Time 1217.5 updated\n",
      "Time 1218.0 updated\n",
      "Time 1218.5 updated\n",
      "Time 1219.0 updated\n",
      "Time 1219.5 updated\n",
      "Time 1220.0 updated\n",
      "Time 1220.5 updated\n",
      "Time 1221.0 updated\n",
      "Time 1221.5 updated\n",
      "Time 1222.0 updated\n",
      "Time 1222.5 updated\n",
      "Time 1223.0 updated\n",
      "Time 1223.5 updated\n",
      "Time 1224.0 updated\n",
      "Time 1224.5 updated\n",
      "Time 1225.0 updated\n",
      "Time 1225.5 updated\n",
      "Time 1226.0 updated\n",
      "Time 1226.5 updated\n",
      "Time 1227.0 updated\n",
      "Time 1227.5 updated\n",
      "Time 1228.0 updated\n",
      "Time 1228.5 updated\n",
      "Time 1229.0 updated\n",
      "Time 1229.5 updated\n",
      "Time 1230.0 updated\n",
      "Time 1230.5 updated\n",
      "Time 1231.0 updated\n",
      "Time 1231.5 updated\n",
      "Time 1232.0 updated\n",
      "Time 1232.5 updated\n",
      "Time 1233.0 updated\n",
      "Time 1233.5 updated\n",
      "Time 1234.0 updated\n",
      "Time 1234.5 updated\n",
      "Time 1235.0 updated\n",
      "Time 1235.5 updated\n",
      "Time 1236.0 updated\n",
      "Time 1236.5 updated\n",
      "Time 1237.0 updated\n",
      "Time 1237.5 updated\n",
      "Time 1238.0 updated\n",
      "Time 1238.5 updated\n",
      "Time 1239.0 updated\n",
      "Time 1239.5 updated\n",
      "Time 1240.0 updated\n",
      "Time 1240.5 updated\n",
      "Time 1241.0 updated\n",
      "Time 1241.5 updated\n",
      "Time 1242.0 updated\n",
      "Time 1242.5 updated\n",
      "Time 1243.0 updated\n",
      "Time 1243.5 updated\n",
      "Time 1244.0 updated\n",
      "Time 1244.5 updated\n",
      "Time 1245.0 updated\n",
      "Time 1245.5 updated\n",
      "Time 1246.0 updated\n",
      "Time 1246.5 updated\n",
      "Time 1247.0 updated\n",
      "Time 1247.5 updated\n",
      "Time 1248.0 updated\n",
      "Time 1248.5 updated\n",
      "Time 1249.0 updated\n",
      "Time 1249.5 updated\n",
      "Time 1250.0 updated\n",
      "Time 1250.5 updated\n",
      "Time 1251.0 updated\n",
      "Time 1251.5 updated\n",
      "Time 1252.0 updated\n",
      "Time 1252.5 updated\n",
      "Time 1253.0 updated\n",
      "Time 1253.5 updated\n",
      "Time 1254.0 updated\n",
      "Time 1254.5 updated\n",
      "Time 1255.0 updated\n",
      "Time 1255.5 updated\n",
      "Time 1256.0 updated\n",
      "Time 1256.5 updated\n",
      "Time 1257.0 updated\n",
      "Time 1257.5 updated\n",
      "Time 1258.0 updated\n",
      "Time 1258.5 updated\n",
      "Time 1259.0 updated\n",
      "Time 1259.5 updated\n",
      "Time 1260.0 updated\n",
      "Time 1260.5 updated\n",
      "Time 1261.0 updated\n",
      "Time 1261.5 updated\n",
      "Time 1262.0 updated\n",
      "Time 1262.5 updated\n",
      "Time 1263.0 updated\n",
      "Time 1263.5 updated\n",
      "Time 1264.0 updated\n",
      "Time 1264.5 updated\n",
      "Time 1265.0 updated\n",
      "Time 1265.5 updated\n",
      "Time 1266.0 updated\n",
      "Time 1266.5 updated\n",
      "Time 1267.0 updated\n",
      "Time 1267.5 updated\n",
      "Time 1268.0 updated\n",
      "Time 1268.5 updated\n",
      "Time 1269.0 updated\n",
      "Time 1269.5 updated\n",
      "Time 1270.0 updated\n",
      "Time 1270.5 updated\n",
      "Time 1271.0 updated\n",
      "Time 1271.5 updated\n",
      "Time 1272.0 updated\n",
      "Time 1272.5 updated\n",
      "Time 1273.0 updated\n",
      "Time 1273.5 updated\n",
      "Time 1274.0 updated\n",
      "Time 1274.5 updated\n",
      "Time 1275.0 updated\n",
      "Time 1275.5 updated\n",
      "Time 1276.0 updated\n",
      "Time 1276.5 updated\n",
      "Time 1277.0 updated\n",
      "Time 1277.5 updated\n",
      "Time 1278.0 updated\n",
      "Time 1278.5 updated\n",
      "Time 1279.0 updated\n",
      "Time 1279.5 updated\n",
      "Time 1280.0 updated\n",
      "Time 1280.5 updated\n",
      "Time 1281.0 updated\n",
      "Time 1281.5 updated\n",
      "Time 1282.0 updated\n",
      "Time 1282.5 updated\n",
      "Time 1283.0 updated\n",
      "Time 1283.5 updated\n",
      "Time 1284.0 updated\n",
      "Time 1284.5 updated\n",
      "Time 1285.0 updated\n",
      "Time 1285.5 updated\n",
      "Time 1286.0 updated\n",
      "Time 1286.5 updated\n",
      "Time 1287.0 updated\n",
      "Time 1287.5 updated\n",
      "Time 1288.0 updated\n",
      "Time 1288.5 updated\n",
      "Time 1289.0 updated\n",
      "Time 1289.5 updated\n",
      "Time 1290.0 updated\n",
      "Time 1290.5 updated\n",
      "Time 1291.0 updated\n",
      "Time 1291.5 updated\n",
      "Time 1292.0 updated\n",
      "Time 1292.5 updated\n",
      "Time 1293.0 updated\n",
      "Time 1293.5 updated\n",
      "Time 1294.0 updated\n",
      "Time 1294.5 updated\n",
      "Time 1295.0 updated\n",
      "Time 1295.5 updated\n",
      "Time 1296.0 updated\n",
      "Time 1296.5 updated\n",
      "Time 1297.0 updated\n",
      "Time 1297.5 updated\n",
      "Time 1298.0 updated\n",
      "Time 1298.5 updated\n",
      "Time 1299.0 updated\n",
      "Time 1299.5 updated\n",
      "Time 1300.0 updated\n",
      "Time 1300.5 updated\n",
      "Time 1301.0 updated\n",
      "Time 1301.5 updated\n",
      "Time 1302.0 updated\n",
      "Time 1302.5 updated\n",
      "Time 1303.0 updated\n",
      "Time 1303.5 updated\n",
      "Time 1304.0 updated\n",
      "Time 1304.5 updated\n",
      "Time 1305.0 updated\n",
      "Time 1305.5 updated\n",
      "Time 1306.0 updated\n",
      "Time 1306.5 updated\n",
      "Time 1307.0 updated\n",
      "Time 1307.5 updated\n",
      "Time 1308.0 updated\n",
      "Time 1308.5 updated\n",
      "Time 1309.0 updated\n",
      "Time 1309.5 updated\n",
      "Time 1310.0 updated\n",
      "Time 1310.5 updated\n",
      "Time 1311.0 updated\n",
      "Time 1311.5 updated\n",
      "Time 1312.0 updated\n",
      "Time 1312.5 updated\n",
      "Time 1313.0 updated\n",
      "Time 1313.5 updated\n",
      "Time 1314.0 updated\n",
      "Time 1314.5 updated\n",
      "Time 1315.0 updated\n",
      "Time 1315.5 updated\n",
      "Time 1316.0 updated\n",
      "Time 1316.5 updated\n",
      "Time 1317.0 updated\n",
      "Time 1317.5 updated\n",
      "Time 1318.0 updated\n",
      "Time 1318.5 updated\n",
      "Time 1319.0 updated\n",
      "Time 1319.5 updated\n",
      "Time 1320.0 updated\n",
      "Time 1320.5 updated\n",
      "Time 1321.0 updated\n",
      "Time 1321.5 updated\n",
      "Time 1322.0 updated\n",
      "Time 1322.5 updated\n",
      "Time 1323.0 updated\n",
      "Time 1323.5 updated\n",
      "Time 1324.0 updated\n",
      "Time 1324.5 updated\n",
      "Time 1325.0 updated\n",
      "Time 1325.5 updated\n",
      "Time 1326.0 updated\n",
      "Time 1326.5 updated\n",
      "Time 1327.0 updated\n",
      "Time 1327.5 updated\n",
      "Time 1328.0 updated\n",
      "Time 1328.5 updated\n",
      "Time 1329.0 updated\n",
      "Time 1329.5 updated\n",
      "Time 1330.0 updated\n",
      "Time 1330.5 updated\n",
      "Time 1331.0 updated\n",
      "Time 1331.5 updated\n",
      "Time 1332.0 updated\n",
      "Time 1332.5 updated\n",
      "Time 1333.0 updated\n",
      "Time 1333.5 updated\n",
      "Time 1334.0 updated\n",
      "Time 1334.5 updated\n",
      "Time 1335.0 updated\n",
      "Time 1335.5 updated\n",
      "Time 1336.0 updated\n",
      "Time 1336.5 updated\n",
      "Time 1337.0 updated\n",
      "Time 1337.5 updated\n",
      "Time 1338.0 updated\n",
      "Time 1338.5 updated\n",
      "Time 1339.0 updated\n",
      "Time 1339.5 updated\n",
      "Time 1340.0 updated\n",
      "Time 1340.5 updated\n",
      "Time 1341.0 updated\n",
      "Time 1341.5 updated\n",
      "Time 1342.0 updated\n",
      "Time 1342.5 updated\n",
      "Time 1343.0 updated\n",
      "Time 1343.5 updated\n",
      "Time 1344.0 updated\n",
      "Time 1344.5 updated\n",
      "Time 1345.0 updated\n",
      "Time 1345.5 updated\n",
      "Time 1346.0 updated\n",
      "Time 1346.5 updated\n",
      "Time 1347.0 updated\n",
      "Time 1347.5 updated\n",
      "Time 1348.0 updated\n",
      "Time 1348.5 updated\n",
      "Time 1349.0 updated\n",
      "Time 1349.5 updated\n",
      "Time 1350.0 updated\n",
      "Time 1350.5 updated\n",
      "Time 1351.0 updated\n",
      "Time 1351.5 updated\n",
      "Time 1352.0 updated\n",
      "Time 1352.5 updated\n",
      "Time 1353.0 updated\n",
      "Time 1353.5 updated\n",
      "Time 1354.0 updated\n",
      "Time 1354.5 updated\n",
      "Time 1355.0 updated\n",
      "Time 1355.5 updated\n",
      "Time 1356.0 updated\n",
      "Time 1356.5 updated\n",
      "Time 1357.0 updated\n",
      "Time 1357.5 updated\n",
      "Time 1358.0 updated\n",
      "Time 1358.5 updated\n",
      "Time 1359.0 updated\n",
      "Time 1359.5 updated\n",
      "Time 1360.0 updated\n",
      "Time 1360.5 updated\n",
      "Time 1361.0 updated\n",
      "Time 1361.5 updated\n",
      "Time 1362.0 updated\n",
      "Time 1362.5 updated\n",
      "Time 1363.0 updated\n",
      "Time 1363.5 updated\n",
      "Time 1364.0 updated\n",
      "Time 1364.5 updated\n",
      "Time 1365.0 updated\n",
      "Time 1365.5 updated\n",
      "Time 1366.0 updated\n",
      "Time 1366.5 updated\n",
      "Time 1367.0 updated\n",
      "Time 1367.5 updated\n",
      "Time 1368.0 updated\n",
      "Time 1368.5 updated\n",
      "Time 1369.0 updated\n",
      "Time 1369.5 updated\n",
      "Time 1370.0 updated\n",
      "Time 1370.5 updated\n",
      "Time 1371.0 updated\n",
      "Time 1371.5 updated\n",
      "Time 1372.0 updated\n",
      "Time 1372.5 updated\n",
      "Time 1373.0 updated\n",
      "Time 1373.5 updated\n",
      "Time 1374.0 updated\n",
      "Time 1374.5 updated\n",
      "Time 1375.0 updated\n",
      "Time 1375.5 updated\n",
      "Time 1376.0 updated\n",
      "Time 1376.5 updated\n",
      "Time 1377.0 updated\n",
      "Time 1377.5 updated\n",
      "Time 1378.0 updated\n",
      "Time 1378.5 updated\n",
      "Time 1379.0 updated\n",
      "Time 1379.5 updated\n",
      "Time 1380.0 updated\n",
      "Time 1380.5 updated\n",
      "Time 1381.0 updated\n",
      "Time 1381.5 updated\n",
      "Time 1382.0 updated\n",
      "Time 1382.5 updated\n",
      "Time 1383.0 updated\n",
      "Time 1383.5 updated\n",
      "Time 1384.0 updated\n",
      "Time 1384.5 updated\n",
      "Time 1385.0 updated\n",
      "Time 1385.5 updated\n",
      "Time 1386.0 updated\n",
      "Time 1386.5 updated\n",
      "Time 1387.0 updated\n",
      "Time 1387.5 updated\n",
      "Time 1388.0 updated\n",
      "Time 1388.5 updated\n",
      "Time 1389.0 updated\n",
      "Time 1389.5 updated\n",
      "Time 1390.0 updated\n",
      "Time 1390.5 updated\n",
      "Time 1391.0 updated\n",
      "Time 1391.5 updated\n",
      "Time 1392.0 updated\n",
      "Time 1392.5 updated\n",
      "Time 1393.0 updated\n",
      "Time 1393.5 updated\n",
      "Time 1394.0 updated\n",
      "Time 1394.5 updated\n",
      "Time 1395.0 updated\n",
      "Time 1395.5 updated\n",
      "Time 1396.0 updated\n",
      "Time 1396.5 updated\n",
      "Time 1397.0 updated\n",
      "Time 1397.5 updated\n",
      "Time 1398.0 updated\n",
      "Time 1398.5 updated\n",
      "Time 1399.0 updated\n",
      "Time 1399.5 updated\n",
      "Time 1400.0 updated\n",
      "Time 1400.5 updated\n"
     ]
    }
   ],
   "source": [
    "diff_sample_1 = generation(model_test, 250, n, 0.5, 4 * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "db65336c-fc15-4124-ac47-3ebb31d56a91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.5 updated\n",
      "Time 1.0 updated\n",
      "Time 1.5 updated\n",
      "Time 2.0 updated\n",
      "Time 2.5 updated\n",
      "Time 3.0 updated\n",
      "Time 3.5 updated\n",
      "Time 4.0 updated\n",
      "Time 4.5 updated\n",
      "Time 5.0 updated\n",
      "Time 5.5 updated\n",
      "Time 6.0 updated\n",
      "Time 6.5 updated\n",
      "Time 7.0 updated\n",
      "Time 7.5 updated\n",
      "Time 8.0 updated\n",
      "Time 8.5 updated\n",
      "Time 9.0 updated\n",
      "Time 9.5 updated\n",
      "Time 10.0 updated\n",
      "Time 10.5 updated\n",
      "Time 11.0 updated\n",
      "Time 11.5 updated\n",
      "Time 12.0 updated\n",
      "Time 12.5 updated\n",
      "Time 13.0 updated\n",
      "Time 13.5 updated\n",
      "Time 14.0 updated\n",
      "Time 14.5 updated\n",
      "Time 15.0 updated\n",
      "Time 15.5 updated\n",
      "Time 16.0 updated\n",
      "Time 16.5 updated\n",
      "Time 17.0 updated\n",
      "Time 17.5 updated\n",
      "Time 18.0 updated\n",
      "Time 18.5 updated\n",
      "Time 19.0 updated\n",
      "Time 19.5 updated\n",
      "Time 20.0 updated\n",
      "Time 20.5 updated\n",
      "Time 21.0 updated\n",
      "Time 21.5 updated\n",
      "Time 22.0 updated\n",
      "Time 22.5 updated\n",
      "Time 23.0 updated\n",
      "Time 23.5 updated\n",
      "Time 24.0 updated\n",
      "Time 24.5 updated\n",
      "Time 25.0 updated\n",
      "Time 25.5 updated\n",
      "Time 26.0 updated\n",
      "Time 26.5 updated\n",
      "Time 27.0 updated\n",
      "Time 27.5 updated\n",
      "Time 28.0 updated\n",
      "Time 28.5 updated\n",
      "Time 29.0 updated\n",
      "Time 29.5 updated\n",
      "Time 30.0 updated\n",
      "Time 30.5 updated\n",
      "Time 31.0 updated\n",
      "Time 31.5 updated\n",
      "Time 32.0 updated\n",
      "Time 32.5 updated\n",
      "Time 33.0 updated\n",
      "Time 33.5 updated\n",
      "Time 34.0 updated\n",
      "Time 34.5 updated\n",
      "Time 35.0 updated\n",
      "Time 35.5 updated\n",
      "Time 36.0 updated\n",
      "Time 36.5 updated\n",
      "Time 37.0 updated\n",
      "Time 37.5 updated\n",
      "Time 38.0 updated\n",
      "Time 38.5 updated\n",
      "Time 39.0 updated\n",
      "Time 39.5 updated\n",
      "Time 40.0 updated\n",
      "Time 40.5 updated\n",
      "Time 41.0 updated\n",
      "Time 41.5 updated\n",
      "Time 42.0 updated\n",
      "Time 42.5 updated\n",
      "Time 43.0 updated\n",
      "Time 43.5 updated\n",
      "Time 44.0 updated\n",
      "Time 44.5 updated\n",
      "Time 45.0 updated\n",
      "Time 45.5 updated\n",
      "Time 46.0 updated\n",
      "Time 46.5 updated\n",
      "Time 47.0 updated\n",
      "Time 47.5 updated\n",
      "Time 48.0 updated\n",
      "Time 48.5 updated\n",
      "Time 49.0 updated\n",
      "Time 49.5 updated\n",
      "Time 50.0 updated\n",
      "Time 50.5 updated\n",
      "Time 51.0 updated\n",
      "Time 51.5 updated\n",
      "Time 52.0 updated\n",
      "Time 52.5 updated\n",
      "Time 53.0 updated\n",
      "Time 53.5 updated\n",
      "Time 54.0 updated\n",
      "Time 54.5 updated\n",
      "Time 55.0 updated\n",
      "Time 55.5 updated\n",
      "Time 56.0 updated\n",
      "Time 56.5 updated\n",
      "Time 57.0 updated\n",
      "Time 57.5 updated\n",
      "Time 58.0 updated\n",
      "Time 58.5 updated\n",
      "Time 59.0 updated\n",
      "Time 59.5 updated\n",
      "Time 60.0 updated\n",
      "Time 60.5 updated\n",
      "Time 61.0 updated\n",
      "Time 61.5 updated\n",
      "Time 62.0 updated\n",
      "Time 62.5 updated\n",
      "Time 63.0 updated\n",
      "Time 63.5 updated\n",
      "Time 64.0 updated\n",
      "Time 64.5 updated\n",
      "Time 65.0 updated\n",
      "Time 65.5 updated\n",
      "Time 66.0 updated\n",
      "Time 66.5 updated\n",
      "Time 67.0 updated\n",
      "Time 67.5 updated\n",
      "Time 68.0 updated\n",
      "Time 68.5 updated\n",
      "Time 69.0 updated\n",
      "Time 69.5 updated\n",
      "Time 70.0 updated\n",
      "Time 70.5 updated\n",
      "Time 71.0 updated\n",
      "Time 71.5 updated\n",
      "Time 72.0 updated\n",
      "Time 72.5 updated\n",
      "Time 73.0 updated\n",
      "Time 73.5 updated\n",
      "Time 74.0 updated\n",
      "Time 74.5 updated\n",
      "Time 75.0 updated\n",
      "Time 75.5 updated\n",
      "Time 76.0 updated\n",
      "Time 76.5 updated\n",
      "Time 77.0 updated\n",
      "Time 77.5 updated\n",
      "Time 78.0 updated\n",
      "Time 78.5 updated\n",
      "Time 79.0 updated\n",
      "Time 79.5 updated\n",
      "Time 80.0 updated\n",
      "Time 80.5 updated\n",
      "Time 81.0 updated\n",
      "Time 81.5 updated\n",
      "Time 82.0 updated\n",
      "Time 82.5 updated\n",
      "Time 83.0 updated\n",
      "Time 83.5 updated\n",
      "Time 84.0 updated\n",
      "Time 84.5 updated\n",
      "Time 85.0 updated\n",
      "Time 85.5 updated\n",
      "Time 86.0 updated\n",
      "Time 86.5 updated\n",
      "Time 87.0 updated\n",
      "Time 87.5 updated\n",
      "Time 88.0 updated\n",
      "Time 88.5 updated\n",
      "Time 89.0 updated\n",
      "Time 89.5 updated\n",
      "Time 90.0 updated\n",
      "Time 90.5 updated\n",
      "Time 91.0 updated\n",
      "Time 91.5 updated\n",
      "Time 92.0 updated\n",
      "Time 92.5 updated\n",
      "Time 93.0 updated\n",
      "Time 93.5 updated\n",
      "Time 94.0 updated\n",
      "Time 94.5 updated\n",
      "Time 95.0 updated\n",
      "Time 95.5 updated\n",
      "Time 96.0 updated\n",
      "Time 96.5 updated\n",
      "Time 97.0 updated\n",
      "Time 97.5 updated\n",
      "Time 98.0 updated\n",
      "Time 98.5 updated\n",
      "Time 99.0 updated\n",
      "Time 99.5 updated\n",
      "Time 100.0 updated\n",
      "Time 100.5 updated\n",
      "Time 101.0 updated\n",
      "Time 101.5 updated\n",
      "Time 102.0 updated\n",
      "Time 102.5 updated\n",
      "Time 103.0 updated\n",
      "Time 103.5 updated\n",
      "Time 104.0 updated\n",
      "Time 104.5 updated\n",
      "Time 105.0 updated\n",
      "Time 105.5 updated\n",
      "Time 106.0 updated\n",
      "Time 106.5 updated\n",
      "Time 107.0 updated\n",
      "Time 107.5 updated\n",
      "Time 108.0 updated\n",
      "Time 108.5 updated\n",
      "Time 109.0 updated\n",
      "Time 109.5 updated\n",
      "Time 110.0 updated\n",
      "Time 110.5 updated\n",
      "Time 111.0 updated\n",
      "Time 111.5 updated\n",
      "Time 112.0 updated\n",
      "Time 112.5 updated\n",
      "Time 113.0 updated\n",
      "Time 113.5 updated\n",
      "Time 114.0 updated\n",
      "Time 114.5 updated\n",
      "Time 115.0 updated\n",
      "Time 115.5 updated\n",
      "Time 116.0 updated\n",
      "Time 116.5 updated\n",
      "Time 117.0 updated\n",
      "Time 117.5 updated\n",
      "Time 118.0 updated\n",
      "Time 118.5 updated\n",
      "Time 119.0 updated\n",
      "Time 119.5 updated\n",
      "Time 120.0 updated\n",
      "Time 120.5 updated\n",
      "Time 121.0 updated\n",
      "Time 121.5 updated\n",
      "Time 122.0 updated\n",
      "Time 122.5 updated\n",
      "Time 123.0 updated\n",
      "Time 123.5 updated\n",
      "Time 124.0 updated\n",
      "Time 124.5 updated\n",
      "Time 125.0 updated\n",
      "Time 125.5 updated\n",
      "Time 126.0 updated\n",
      "Time 126.5 updated\n",
      "Time 127.0 updated\n",
      "Time 127.5 updated\n",
      "Time 128.0 updated\n",
      "Time 128.5 updated\n",
      "Time 129.0 updated\n",
      "Time 129.5 updated\n",
      "Time 130.0 updated\n",
      "Time 130.5 updated\n",
      "Time 131.0 updated\n",
      "Time 131.5 updated\n",
      "Time 132.0 updated\n",
      "Time 132.5 updated\n",
      "Time 133.0 updated\n",
      "Time 133.5 updated\n",
      "Time 134.0 updated\n",
      "Time 134.5 updated\n",
      "Time 135.0 updated\n",
      "Time 135.5 updated\n",
      "Time 136.0 updated\n",
      "Time 136.5 updated\n",
      "Time 137.0 updated\n",
      "Time 137.5 updated\n",
      "Time 138.0 updated\n",
      "Time 138.5 updated\n",
      "Time 139.0 updated\n",
      "Time 139.5 updated\n",
      "Time 140.0 updated\n",
      "Time 140.5 updated\n",
      "Time 141.0 updated\n",
      "Time 141.5 updated\n",
      "Time 142.0 updated\n",
      "Time 142.5 updated\n",
      "Time 143.0 updated\n",
      "Time 143.5 updated\n",
      "Time 144.0 updated\n",
      "Time 144.5 updated\n",
      "Time 145.0 updated\n",
      "Time 145.5 updated\n",
      "Time 146.0 updated\n",
      "Time 146.5 updated\n",
      "Time 147.0 updated\n",
      "Time 147.5 updated\n",
      "Time 148.0 updated\n",
      "Time 148.5 updated\n",
      "Time 149.0 updated\n",
      "Time 149.5 updated\n",
      "Time 150.0 updated\n",
      "Time 150.5 updated\n",
      "Time 151.0 updated\n",
      "Time 151.5 updated\n",
      "Time 152.0 updated\n",
      "Time 152.5 updated\n",
      "Time 153.0 updated\n",
      "Time 153.5 updated\n",
      "Time 154.0 updated\n",
      "Time 154.5 updated\n",
      "Time 155.0 updated\n",
      "Time 155.5 updated\n",
      "Time 156.0 updated\n",
      "Time 156.5 updated\n",
      "Time 157.0 updated\n",
      "Time 157.5 updated\n",
      "Time 158.0 updated\n",
      "Time 158.5 updated\n",
      "Time 159.0 updated\n",
      "Time 159.5 updated\n",
      "Time 160.0 updated\n",
      "Time 160.5 updated\n",
      "Time 161.0 updated\n",
      "Time 161.5 updated\n",
      "Time 162.0 updated\n",
      "Time 162.5 updated\n",
      "Time 163.0 updated\n",
      "Time 163.5 updated\n",
      "Time 164.0 updated\n",
      "Time 164.5 updated\n",
      "Time 165.0 updated\n",
      "Time 165.5 updated\n",
      "Time 166.0 updated\n",
      "Time 166.5 updated\n",
      "Time 167.0 updated\n",
      "Time 167.5 updated\n",
      "Time 168.0 updated\n",
      "Time 168.5 updated\n",
      "Time 169.0 updated\n",
      "Time 169.5 updated\n",
      "Time 170.0 updated\n",
      "Time 170.5 updated\n",
      "Time 171.0 updated\n",
      "Time 171.5 updated\n",
      "Time 172.0 updated\n",
      "Time 172.5 updated\n",
      "Time 173.0 updated\n",
      "Time 173.5 updated\n",
      "Time 174.0 updated\n",
      "Time 174.5 updated\n",
      "Time 175.0 updated\n",
      "Time 175.5 updated\n",
      "Time 176.0 updated\n",
      "Time 176.5 updated\n",
      "Time 177.0 updated\n",
      "Time 177.5 updated\n",
      "Time 178.0 updated\n",
      "Time 178.5 updated\n",
      "Time 179.0 updated\n",
      "Time 179.5 updated\n",
      "Time 180.0 updated\n",
      "Time 180.5 updated\n",
      "Time 181.0 updated\n",
      "Time 181.5 updated\n",
      "Time 182.0 updated\n",
      "Time 182.5 updated\n",
      "Time 183.0 updated\n",
      "Time 183.5 updated\n",
      "Time 184.0 updated\n",
      "Time 184.5 updated\n",
      "Time 185.0 updated\n",
      "Time 185.5 updated\n",
      "Time 186.0 updated\n",
      "Time 186.5 updated\n",
      "Time 187.0 updated\n",
      "Time 187.5 updated\n",
      "Time 188.0 updated\n",
      "Time 188.5 updated\n",
      "Time 189.0 updated\n",
      "Time 189.5 updated\n",
      "Time 190.0 updated\n",
      "Time 190.5 updated\n",
      "Time 191.0 updated\n",
      "Time 191.5 updated\n",
      "Time 192.0 updated\n",
      "Time 192.5 updated\n",
      "Time 193.0 updated\n",
      "Time 193.5 updated\n",
      "Time 194.0 updated\n",
      "Time 194.5 updated\n",
      "Time 195.0 updated\n",
      "Time 195.5 updated\n",
      "Time 196.0 updated\n",
      "Time 196.5 updated\n",
      "Time 197.0 updated\n",
      "Time 197.5 updated\n",
      "Time 198.0 updated\n",
      "Time 198.5 updated\n",
      "Time 199.0 updated\n",
      "Time 199.5 updated\n",
      "Time 200.0 updated\n",
      "Time 200.5 updated\n",
      "Time 201.0 updated\n",
      "Time 201.5 updated\n",
      "Time 202.0 updated\n",
      "Time 202.5 updated\n",
      "Time 203.0 updated\n",
      "Time 203.5 updated\n",
      "Time 204.0 updated\n",
      "Time 204.5 updated\n",
      "Time 205.0 updated\n",
      "Time 205.5 updated\n",
      "Time 206.0 updated\n",
      "Time 206.5 updated\n",
      "Time 207.0 updated\n",
      "Time 207.5 updated\n",
      "Time 208.0 updated\n",
      "Time 208.5 updated\n",
      "Time 209.0 updated\n",
      "Time 209.5 updated\n",
      "Time 210.0 updated\n",
      "Time 210.5 updated\n",
      "Time 211.0 updated\n",
      "Time 211.5 updated\n",
      "Time 212.0 updated\n",
      "Time 212.5 updated\n",
      "Time 213.0 updated\n",
      "Time 213.5 updated\n",
      "Time 214.0 updated\n",
      "Time 214.5 updated\n",
      "Time 215.0 updated\n",
      "Time 215.5 updated\n",
      "Time 216.0 updated\n",
      "Time 216.5 updated\n",
      "Time 217.0 updated\n",
      "Time 217.5 updated\n",
      "Time 218.0 updated\n",
      "Time 218.5 updated\n",
      "Time 219.0 updated\n",
      "Time 219.5 updated\n",
      "Time 220.0 updated\n",
      "Time 220.5 updated\n",
      "Time 221.0 updated\n",
      "Time 221.5 updated\n",
      "Time 222.0 updated\n",
      "Time 222.5 updated\n",
      "Time 223.0 updated\n",
      "Time 223.5 updated\n",
      "Time 224.0 updated\n",
      "Time 224.5 updated\n",
      "Time 225.0 updated\n",
      "Time 225.5 updated\n",
      "Time 226.0 updated\n",
      "Time 226.5 updated\n",
      "Time 227.0 updated\n",
      "Time 227.5 updated\n",
      "Time 228.0 updated\n",
      "Time 228.5 updated\n",
      "Time 229.0 updated\n",
      "Time 229.5 updated\n",
      "Time 230.0 updated\n",
      "Time 230.5 updated\n",
      "Time 231.0 updated\n",
      "Time 231.5 updated\n",
      "Time 232.0 updated\n",
      "Time 232.5 updated\n",
      "Time 233.0 updated\n",
      "Time 233.5 updated\n",
      "Time 234.0 updated\n",
      "Time 234.5 updated\n",
      "Time 235.0 updated\n",
      "Time 235.5 updated\n",
      "Time 236.0 updated\n",
      "Time 236.5 updated\n",
      "Time 237.0 updated\n",
      "Time 237.5 updated\n",
      "Time 238.0 updated\n",
      "Time 238.5 updated\n",
      "Time 239.0 updated\n",
      "Time 239.5 updated\n",
      "Time 240.0 updated\n",
      "Time 240.5 updated\n",
      "Time 241.0 updated\n",
      "Time 241.5 updated\n",
      "Time 242.0 updated\n",
      "Time 242.5 updated\n",
      "Time 243.0 updated\n",
      "Time 243.5 updated\n",
      "Time 244.0 updated\n",
      "Time 244.5 updated\n",
      "Time 245.0 updated\n",
      "Time 245.5 updated\n",
      "Time 246.0 updated\n",
      "Time 246.5 updated\n",
      "Time 247.0 updated\n",
      "Time 247.5 updated\n",
      "Time 248.0 updated\n",
      "Time 248.5 updated\n",
      "Time 249.0 updated\n",
      "Time 249.5 updated\n",
      "Time 250.0 updated\n",
      "Time 250.5 updated\n",
      "Time 251.0 updated\n",
      "Time 251.5 updated\n",
      "Time 252.0 updated\n",
      "Time 252.5 updated\n",
      "Time 253.0 updated\n",
      "Time 253.5 updated\n",
      "Time 254.0 updated\n",
      "Time 254.5 updated\n",
      "Time 255.0 updated\n",
      "Time 255.5 updated\n",
      "Time 256.0 updated\n",
      "Time 256.5 updated\n",
      "Time 257.0 updated\n",
      "Time 257.5 updated\n",
      "Time 258.0 updated\n",
      "Time 258.5 updated\n",
      "Time 259.0 updated\n",
      "Time 259.5 updated\n",
      "Time 260.0 updated\n",
      "Time 260.5 updated\n",
      "Time 261.0 updated\n",
      "Time 261.5 updated\n",
      "Time 262.0 updated\n",
      "Time 262.5 updated\n",
      "Time 263.0 updated\n",
      "Time 263.5 updated\n",
      "Time 264.0 updated\n",
      "Time 264.5 updated\n",
      "Time 265.0 updated\n",
      "Time 265.5 updated\n",
      "Time 266.0 updated\n",
      "Time 266.5 updated\n",
      "Time 267.0 updated\n",
      "Time 267.5 updated\n",
      "Time 268.0 updated\n",
      "Time 268.5 updated\n",
      "Time 269.0 updated\n",
      "Time 269.5 updated\n",
      "Time 270.0 updated\n",
      "Time 270.5 updated\n",
      "Time 271.0 updated\n",
      "Time 271.5 updated\n",
      "Time 272.0 updated\n",
      "Time 272.5 updated\n",
      "Time 273.0 updated\n",
      "Time 273.5 updated\n",
      "Time 274.0 updated\n",
      "Time 274.5 updated\n",
      "Time 275.0 updated\n",
      "Time 275.5 updated\n",
      "Time 276.0 updated\n",
      "Time 276.5 updated\n",
      "Time 277.0 updated\n",
      "Time 277.5 updated\n",
      "Time 278.0 updated\n",
      "Time 278.5 updated\n",
      "Time 279.0 updated\n",
      "Time 279.5 updated\n",
      "Time 280.0 updated\n",
      "Time 280.5 updated\n",
      "Time 281.0 updated\n",
      "Time 281.5 updated\n",
      "Time 282.0 updated\n",
      "Time 282.5 updated\n",
      "Time 283.0 updated\n",
      "Time 283.5 updated\n",
      "Time 284.0 updated\n",
      "Time 284.5 updated\n",
      "Time 285.0 updated\n",
      "Time 285.5 updated\n",
      "Time 286.0 updated\n",
      "Time 286.5 updated\n",
      "Time 287.0 updated\n",
      "Time 287.5 updated\n",
      "Time 288.0 updated\n",
      "Time 288.5 updated\n",
      "Time 289.0 updated\n",
      "Time 289.5 updated\n",
      "Time 290.0 updated\n",
      "Time 290.5 updated\n",
      "Time 291.0 updated\n",
      "Time 291.5 updated\n",
      "Time 292.0 updated\n",
      "Time 292.5 updated\n",
      "Time 293.0 updated\n",
      "Time 293.5 updated\n",
      "Time 294.0 updated\n",
      "Time 294.5 updated\n",
      "Time 295.0 updated\n",
      "Time 295.5 updated\n",
      "Time 296.0 updated\n",
      "Time 296.5 updated\n",
      "Time 297.0 updated\n",
      "Time 297.5 updated\n",
      "Time 298.0 updated\n",
      "Time 298.5 updated\n",
      "Time 299.0 updated\n",
      "Time 299.5 updated\n",
      "Time 300.0 updated\n",
      "Time 300.5 updated\n",
      "Time 301.0 updated\n",
      "Time 301.5 updated\n",
      "Time 302.0 updated\n",
      "Time 302.5 updated\n",
      "Time 303.0 updated\n",
      "Time 303.5 updated\n",
      "Time 304.0 updated\n",
      "Time 304.5 updated\n",
      "Time 305.0 updated\n",
      "Time 305.5 updated\n",
      "Time 306.0 updated\n",
      "Time 306.5 updated\n",
      "Time 307.0 updated\n",
      "Time 307.5 updated\n",
      "Time 308.0 updated\n",
      "Time 308.5 updated\n",
      "Time 309.0 updated\n",
      "Time 309.5 updated\n",
      "Time 310.0 updated\n",
      "Time 310.5 updated\n",
      "Time 311.0 updated\n",
      "Time 311.5 updated\n",
      "Time 312.0 updated\n",
      "Time 312.5 updated\n",
      "Time 313.0 updated\n",
      "Time 313.5 updated\n",
      "Time 314.0 updated\n",
      "Time 314.5 updated\n",
      "Time 315.0 updated\n",
      "Time 315.5 updated\n",
      "Time 316.0 updated\n",
      "Time 316.5 updated\n",
      "Time 317.0 updated\n",
      "Time 317.5 updated\n",
      "Time 318.0 updated\n",
      "Time 318.5 updated\n",
      "Time 319.0 updated\n",
      "Time 319.5 updated\n",
      "Time 320.0 updated\n",
      "Time 320.5 updated\n",
      "Time 321.0 updated\n",
      "Time 321.5 updated\n",
      "Time 322.0 updated\n",
      "Time 322.5 updated\n",
      "Time 323.0 updated\n",
      "Time 323.5 updated\n",
      "Time 324.0 updated\n",
      "Time 324.5 updated\n",
      "Time 325.0 updated\n",
      "Time 325.5 updated\n",
      "Time 326.0 updated\n",
      "Time 326.5 updated\n",
      "Time 327.0 updated\n",
      "Time 327.5 updated\n",
      "Time 328.0 updated\n",
      "Time 328.5 updated\n",
      "Time 329.0 updated\n",
      "Time 329.5 updated\n",
      "Time 330.0 updated\n",
      "Time 330.5 updated\n",
      "Time 331.0 updated\n",
      "Time 331.5 updated\n",
      "Time 332.0 updated\n",
      "Time 332.5 updated\n",
      "Time 333.0 updated\n",
      "Time 333.5 updated\n",
      "Time 334.0 updated\n",
      "Time 334.5 updated\n",
      "Time 335.0 updated\n",
      "Time 335.5 updated\n",
      "Time 336.0 updated\n",
      "Time 336.5 updated\n",
      "Time 337.0 updated\n",
      "Time 337.5 updated\n",
      "Time 338.0 updated\n",
      "Time 338.5 updated\n",
      "Time 339.0 updated\n",
      "Time 339.5 updated\n",
      "Time 340.0 updated\n",
      "Time 340.5 updated\n",
      "Time 341.0 updated\n",
      "Time 341.5 updated\n",
      "Time 342.0 updated\n",
      "Time 342.5 updated\n",
      "Time 343.0 updated\n",
      "Time 343.5 updated\n",
      "Time 344.0 updated\n",
      "Time 344.5 updated\n",
      "Time 345.0 updated\n",
      "Time 345.5 updated\n",
      "Time 346.0 updated\n",
      "Time 346.5 updated\n",
      "Time 347.0 updated\n",
      "Time 347.5 updated\n",
      "Time 348.0 updated\n",
      "Time 348.5 updated\n",
      "Time 349.0 updated\n",
      "Time 349.5 updated\n",
      "Time 350.0 updated\n",
      "Time 350.5 updated\n",
      "Time 351.0 updated\n",
      "Time 351.5 updated\n",
      "Time 352.0 updated\n",
      "Time 352.5 updated\n",
      "Time 353.0 updated\n",
      "Time 353.5 updated\n",
      "Time 354.0 updated\n",
      "Time 354.5 updated\n",
      "Time 355.0 updated\n",
      "Time 355.5 updated\n",
      "Time 356.0 updated\n",
      "Time 356.5 updated\n",
      "Time 357.0 updated\n",
      "Time 357.5 updated\n",
      "Time 358.0 updated\n",
      "Time 358.5 updated\n",
      "Time 359.0 updated\n",
      "Time 359.5 updated\n",
      "Time 360.0 updated\n",
      "Time 360.5 updated\n",
      "Time 361.0 updated\n",
      "Time 361.5 updated\n",
      "Time 362.0 updated\n",
      "Time 362.5 updated\n",
      "Time 363.0 updated\n",
      "Time 363.5 updated\n",
      "Time 364.0 updated\n",
      "Time 364.5 updated\n",
      "Time 365.0 updated\n",
      "Time 365.5 updated\n",
      "Time 366.0 updated\n",
      "Time 366.5 updated\n",
      "Time 367.0 updated\n",
      "Time 367.5 updated\n",
      "Time 368.0 updated\n",
      "Time 368.5 updated\n",
      "Time 369.0 updated\n",
      "Time 369.5 updated\n",
      "Time 370.0 updated\n",
      "Time 370.5 updated\n",
      "Time 371.0 updated\n",
      "Time 371.5 updated\n",
      "Time 372.0 updated\n",
      "Time 372.5 updated\n",
      "Time 373.0 updated\n",
      "Time 373.5 updated\n",
      "Time 374.0 updated\n",
      "Time 374.5 updated\n",
      "Time 375.0 updated\n",
      "Time 375.5 updated\n",
      "Time 376.0 updated\n",
      "Time 376.5 updated\n",
      "Time 377.0 updated\n",
      "Time 377.5 updated\n",
      "Time 378.0 updated\n",
      "Time 378.5 updated\n",
      "Time 379.0 updated\n",
      "Time 379.5 updated\n",
      "Time 380.0 updated\n",
      "Time 380.5 updated\n",
      "Time 381.0 updated\n",
      "Time 381.5 updated\n",
      "Time 382.0 updated\n",
      "Time 382.5 updated\n",
      "Time 383.0 updated\n",
      "Time 383.5 updated\n",
      "Time 384.0 updated\n",
      "Time 384.5 updated\n",
      "Time 385.0 updated\n",
      "Time 385.5 updated\n",
      "Time 386.0 updated\n",
      "Time 386.5 updated\n",
      "Time 387.0 updated\n",
      "Time 387.5 updated\n",
      "Time 388.0 updated\n",
      "Time 388.5 updated\n",
      "Time 389.0 updated\n",
      "Time 389.5 updated\n",
      "Time 390.0 updated\n",
      "Time 390.5 updated\n",
      "Time 391.0 updated\n",
      "Time 391.5 updated\n",
      "Time 392.0 updated\n",
      "Time 392.5 updated\n",
      "Time 393.0 updated\n",
      "Time 393.5 updated\n",
      "Time 394.0 updated\n",
      "Time 394.5 updated\n",
      "Time 395.0 updated\n",
      "Time 395.5 updated\n",
      "Time 396.0 updated\n",
      "Time 396.5 updated\n",
      "Time 397.0 updated\n",
      "Time 397.5 updated\n",
      "Time 398.0 updated\n",
      "Time 398.5 updated\n",
      "Time 399.0 updated\n",
      "Time 399.5 updated\n",
      "Time 400.0 updated\n",
      "Time 400.5 updated\n",
      "Time 401.0 updated\n",
      "Time 401.5 updated\n",
      "Time 402.0 updated\n",
      "Time 402.5 updated\n",
      "Time 403.0 updated\n",
      "Time 403.5 updated\n",
      "Time 404.0 updated\n",
      "Time 404.5 updated\n",
      "Time 405.0 updated\n",
      "Time 405.5 updated\n",
      "Time 406.0 updated\n",
      "Time 406.5 updated\n",
      "Time 407.0 updated\n",
      "Time 407.5 updated\n",
      "Time 408.0 updated\n",
      "Time 408.5 updated\n",
      "Time 409.0 updated\n",
      "Time 409.5 updated\n",
      "Time 410.0 updated\n",
      "Time 410.5 updated\n",
      "Time 411.0 updated\n",
      "Time 411.5 updated\n",
      "Time 412.0 updated\n",
      "Time 412.5 updated\n",
      "Time 413.0 updated\n",
      "Time 413.5 updated\n",
      "Time 414.0 updated\n",
      "Time 414.5 updated\n",
      "Time 415.0 updated\n",
      "Time 415.5 updated\n",
      "Time 416.0 updated\n",
      "Time 416.5 updated\n",
      "Time 417.0 updated\n",
      "Time 417.5 updated\n",
      "Time 418.0 updated\n",
      "Time 418.5 updated\n",
      "Time 419.0 updated\n",
      "Time 419.5 updated\n",
      "Time 420.0 updated\n",
      "Time 420.5 updated\n",
      "Time 421.0 updated\n",
      "Time 421.5 updated\n",
      "Time 422.0 updated\n",
      "Time 422.5 updated\n",
      "Time 423.0 updated\n",
      "Time 423.5 updated\n",
      "Time 424.0 updated\n",
      "Time 424.5 updated\n",
      "Time 425.0 updated\n",
      "Time 425.5 updated\n",
      "Time 426.0 updated\n",
      "Time 426.5 updated\n",
      "Time 427.0 updated\n",
      "Time 427.5 updated\n",
      "Time 428.0 updated\n",
      "Time 428.5 updated\n",
      "Time 429.0 updated\n",
      "Time 429.5 updated\n",
      "Time 430.0 updated\n",
      "Time 430.5 updated\n",
      "Time 431.0 updated\n",
      "Time 431.5 updated\n",
      "Time 432.0 updated\n",
      "Time 432.5 updated\n",
      "Time 433.0 updated\n",
      "Time 433.5 updated\n",
      "Time 434.0 updated\n",
      "Time 434.5 updated\n",
      "Time 435.0 updated\n",
      "Time 435.5 updated\n",
      "Time 436.0 updated\n",
      "Time 436.5 updated\n",
      "Time 437.0 updated\n",
      "Time 437.5 updated\n",
      "Time 438.0 updated\n",
      "Time 438.5 updated\n",
      "Time 439.0 updated\n",
      "Time 439.5 updated\n",
      "Time 440.0 updated\n",
      "Time 440.5 updated\n",
      "Time 441.0 updated\n",
      "Time 441.5 updated\n",
      "Time 442.0 updated\n",
      "Time 442.5 updated\n",
      "Time 443.0 updated\n",
      "Time 443.5 updated\n",
      "Time 444.0 updated\n",
      "Time 444.5 updated\n",
      "Time 445.0 updated\n",
      "Time 445.5 updated\n",
      "Time 446.0 updated\n",
      "Time 446.5 updated\n",
      "Time 447.0 updated\n",
      "Time 447.5 updated\n",
      "Time 448.0 updated\n",
      "Time 448.5 updated\n",
      "Time 449.0 updated\n",
      "Time 449.5 updated\n",
      "Time 450.0 updated\n",
      "Time 450.5 updated\n",
      "Time 451.0 updated\n",
      "Time 451.5 updated\n",
      "Time 452.0 updated\n",
      "Time 452.5 updated\n",
      "Time 453.0 updated\n",
      "Time 453.5 updated\n",
      "Time 454.0 updated\n",
      "Time 454.5 updated\n",
      "Time 455.0 updated\n",
      "Time 455.5 updated\n",
      "Time 456.0 updated\n",
      "Time 456.5 updated\n",
      "Time 457.0 updated\n",
      "Time 457.5 updated\n",
      "Time 458.0 updated\n",
      "Time 458.5 updated\n",
      "Time 459.0 updated\n",
      "Time 459.5 updated\n",
      "Time 460.0 updated\n",
      "Time 460.5 updated\n",
      "Time 461.0 updated\n",
      "Time 461.5 updated\n",
      "Time 462.0 updated\n",
      "Time 462.5 updated\n",
      "Time 463.0 updated\n",
      "Time 463.5 updated\n",
      "Time 464.0 updated\n",
      "Time 464.5 updated\n",
      "Time 465.0 updated\n",
      "Time 465.5 updated\n",
      "Time 466.0 updated\n",
      "Time 466.5 updated\n",
      "Time 467.0 updated\n",
      "Time 467.5 updated\n",
      "Time 468.0 updated\n",
      "Time 468.5 updated\n",
      "Time 469.0 updated\n",
      "Time 469.5 updated\n",
      "Time 470.0 updated\n",
      "Time 470.5 updated\n",
      "Time 471.0 updated\n",
      "Time 471.5 updated\n",
      "Time 472.0 updated\n",
      "Time 472.5 updated\n",
      "Time 473.0 updated\n",
      "Time 473.5 updated\n",
      "Time 474.0 updated\n",
      "Time 474.5 updated\n",
      "Time 475.0 updated\n",
      "Time 475.5 updated\n",
      "Time 476.0 updated\n",
      "Time 476.5 updated\n",
      "Time 477.0 updated\n",
      "Time 477.5 updated\n",
      "Time 478.0 updated\n",
      "Time 478.5 updated\n",
      "Time 479.0 updated\n",
      "Time 479.5 updated\n",
      "Time 480.0 updated\n",
      "Time 480.5 updated\n",
      "Time 481.0 updated\n",
      "Time 481.5 updated\n",
      "Time 482.0 updated\n",
      "Time 482.5 updated\n",
      "Time 483.0 updated\n",
      "Time 483.5 updated\n",
      "Time 484.0 updated\n",
      "Time 484.5 updated\n",
      "Time 485.0 updated\n",
      "Time 485.5 updated\n",
      "Time 486.0 updated\n",
      "Time 486.5 updated\n",
      "Time 487.0 updated\n",
      "Time 487.5 updated\n",
      "Time 488.0 updated\n",
      "Time 488.5 updated\n",
      "Time 489.0 updated\n",
      "Time 489.5 updated\n",
      "Time 490.0 updated\n",
      "Time 490.5 updated\n",
      "Time 491.0 updated\n",
      "Time 491.5 updated\n",
      "Time 492.0 updated\n",
      "Time 492.5 updated\n",
      "Time 493.0 updated\n",
      "Time 493.5 updated\n",
      "Time 494.0 updated\n",
      "Time 494.5 updated\n",
      "Time 495.0 updated\n",
      "Time 495.5 updated\n",
      "Time 496.0 updated\n",
      "Time 496.5 updated\n",
      "Time 497.0 updated\n",
      "Time 497.5 updated\n",
      "Time 498.0 updated\n",
      "Time 498.5 updated\n",
      "Time 499.0 updated\n",
      "Time 499.5 updated\n",
      "Time 500.0 updated\n",
      "Time 500.5 updated\n",
      "Time 501.0 updated\n",
      "Time 501.5 updated\n",
      "Time 502.0 updated\n",
      "Time 502.5 updated\n",
      "Time 503.0 updated\n",
      "Time 503.5 updated\n",
      "Time 504.0 updated\n",
      "Time 504.5 updated\n",
      "Time 505.0 updated\n",
      "Time 505.5 updated\n",
      "Time 506.0 updated\n",
      "Time 506.5 updated\n",
      "Time 507.0 updated\n",
      "Time 507.5 updated\n",
      "Time 508.0 updated\n",
      "Time 508.5 updated\n",
      "Time 509.0 updated\n",
      "Time 509.5 updated\n",
      "Time 510.0 updated\n",
      "Time 510.5 updated\n",
      "Time 511.0 updated\n",
      "Time 511.5 updated\n",
      "Time 512.0 updated\n",
      "Time 512.5 updated\n",
      "Time 513.0 updated\n",
      "Time 513.5 updated\n",
      "Time 514.0 updated\n",
      "Time 514.5 updated\n",
      "Time 515.0 updated\n",
      "Time 515.5 updated\n",
      "Time 516.0 updated\n",
      "Time 516.5 updated\n",
      "Time 517.0 updated\n",
      "Time 517.5 updated\n",
      "Time 518.0 updated\n",
      "Time 518.5 updated\n",
      "Time 519.0 updated\n",
      "Time 519.5 updated\n",
      "Time 520.0 updated\n",
      "Time 520.5 updated\n",
      "Time 521.0 updated\n",
      "Time 521.5 updated\n",
      "Time 522.0 updated\n",
      "Time 522.5 updated\n",
      "Time 523.0 updated\n",
      "Time 523.5 updated\n",
      "Time 524.0 updated\n",
      "Time 524.5 updated\n",
      "Time 525.0 updated\n",
      "Time 525.5 updated\n",
      "Time 526.0 updated\n",
      "Time 526.5 updated\n",
      "Time 527.0 updated\n",
      "Time 527.5 updated\n",
      "Time 528.0 updated\n",
      "Time 528.5 updated\n",
      "Time 529.0 updated\n",
      "Time 529.5 updated\n",
      "Time 530.0 updated\n",
      "Time 530.5 updated\n",
      "Time 531.0 updated\n",
      "Time 531.5 updated\n",
      "Time 532.0 updated\n",
      "Time 532.5 updated\n",
      "Time 533.0 updated\n",
      "Time 533.5 updated\n",
      "Time 534.0 updated\n",
      "Time 534.5 updated\n",
      "Time 535.0 updated\n",
      "Time 535.5 updated\n",
      "Time 536.0 updated\n",
      "Time 536.5 updated\n",
      "Time 537.0 updated\n",
      "Time 537.5 updated\n",
      "Time 538.0 updated\n",
      "Time 538.5 updated\n",
      "Time 539.0 updated\n",
      "Time 539.5 updated\n",
      "Time 540.0 updated\n",
      "Time 540.5 updated\n",
      "Time 541.0 updated\n",
      "Time 541.5 updated\n",
      "Time 542.0 updated\n",
      "Time 542.5 updated\n",
      "Time 543.0 updated\n",
      "Time 543.5 updated\n",
      "Time 544.0 updated\n",
      "Time 544.5 updated\n",
      "Time 545.0 updated\n",
      "Time 545.5 updated\n",
      "Time 546.0 updated\n",
      "Time 546.5 updated\n",
      "Time 547.0 updated\n",
      "Time 547.5 updated\n",
      "Time 548.0 updated\n",
      "Time 548.5 updated\n",
      "Time 549.0 updated\n",
      "Time 549.5 updated\n",
      "Time 550.0 updated\n",
      "Time 550.5 updated\n",
      "Time 551.0 updated\n",
      "Time 551.5 updated\n",
      "Time 552.0 updated\n",
      "Time 552.5 updated\n",
      "Time 553.0 updated\n",
      "Time 553.5 updated\n",
      "Time 554.0 updated\n",
      "Time 554.5 updated\n",
      "Time 555.0 updated\n",
      "Time 555.5 updated\n",
      "Time 556.0 updated\n",
      "Time 556.5 updated\n",
      "Time 557.0 updated\n",
      "Time 557.5 updated\n",
      "Time 558.0 updated\n",
      "Time 558.5 updated\n",
      "Time 559.0 updated\n",
      "Time 559.5 updated\n",
      "Time 560.0 updated\n",
      "Time 560.5 updated\n",
      "Time 561.0 updated\n",
      "Time 561.5 updated\n",
      "Time 562.0 updated\n",
      "Time 562.5 updated\n",
      "Time 563.0 updated\n",
      "Time 563.5 updated\n",
      "Time 564.0 updated\n",
      "Time 564.5 updated\n",
      "Time 565.0 updated\n",
      "Time 565.5 updated\n",
      "Time 566.0 updated\n",
      "Time 566.5 updated\n",
      "Time 567.0 updated\n",
      "Time 567.5 updated\n",
      "Time 568.0 updated\n",
      "Time 568.5 updated\n",
      "Time 569.0 updated\n",
      "Time 569.5 updated\n",
      "Time 570.0 updated\n",
      "Time 570.5 updated\n",
      "Time 571.0 updated\n",
      "Time 571.5 updated\n",
      "Time 572.0 updated\n",
      "Time 572.5 updated\n",
      "Time 573.0 updated\n",
      "Time 573.5 updated\n",
      "Time 574.0 updated\n",
      "Time 574.5 updated\n",
      "Time 575.0 updated\n",
      "Time 575.5 updated\n",
      "Time 576.0 updated\n",
      "Time 576.5 updated\n",
      "Time 577.0 updated\n",
      "Time 577.5 updated\n",
      "Time 578.0 updated\n",
      "Time 578.5 updated\n",
      "Time 579.0 updated\n",
      "Time 579.5 updated\n",
      "Time 580.0 updated\n",
      "Time 580.5 updated\n",
      "Time 581.0 updated\n",
      "Time 581.5 updated\n",
      "Time 582.0 updated\n",
      "Time 582.5 updated\n",
      "Time 583.0 updated\n",
      "Time 583.5 updated\n",
      "Time 584.0 updated\n",
      "Time 584.5 updated\n",
      "Time 585.0 updated\n",
      "Time 585.5 updated\n",
      "Time 586.0 updated\n",
      "Time 586.5 updated\n",
      "Time 587.0 updated\n",
      "Time 587.5 updated\n",
      "Time 588.0 updated\n",
      "Time 588.5 updated\n",
      "Time 589.0 updated\n",
      "Time 589.5 updated\n",
      "Time 590.0 updated\n",
      "Time 590.5 updated\n",
      "Time 591.0 updated\n",
      "Time 591.5 updated\n",
      "Time 592.0 updated\n",
      "Time 592.5 updated\n",
      "Time 593.0 updated\n",
      "Time 593.5 updated\n",
      "Time 594.0 updated\n",
      "Time 594.5 updated\n",
      "Time 595.0 updated\n",
      "Time 595.5 updated\n",
      "Time 596.0 updated\n",
      "Time 596.5 updated\n",
      "Time 597.0 updated\n",
      "Time 597.5 updated\n",
      "Time 598.0 updated\n",
      "Time 598.5 updated\n",
      "Time 599.0 updated\n",
      "Time 599.5 updated\n",
      "Time 600.0 updated\n",
      "Time 600.5 updated\n",
      "Time 601.0 updated\n",
      "Time 601.5 updated\n",
      "Time 602.0 updated\n",
      "Time 602.5 updated\n",
      "Time 603.0 updated\n",
      "Time 603.5 updated\n",
      "Time 604.0 updated\n",
      "Time 604.5 updated\n",
      "Time 605.0 updated\n",
      "Time 605.5 updated\n",
      "Time 606.0 updated\n",
      "Time 606.5 updated\n",
      "Time 607.0 updated\n",
      "Time 607.5 updated\n",
      "Time 608.0 updated\n",
      "Time 608.5 updated\n",
      "Time 609.0 updated\n",
      "Time 609.5 updated\n",
      "Time 610.0 updated\n",
      "Time 610.5 updated\n",
      "Time 611.0 updated\n",
      "Time 611.5 updated\n",
      "Time 612.0 updated\n",
      "Time 612.5 updated\n",
      "Time 613.0 updated\n",
      "Time 613.5 updated\n",
      "Time 614.0 updated\n",
      "Time 614.5 updated\n",
      "Time 615.0 updated\n",
      "Time 615.5 updated\n",
      "Time 616.0 updated\n",
      "Time 616.5 updated\n",
      "Time 617.0 updated\n",
      "Time 617.5 updated\n",
      "Time 618.0 updated\n",
      "Time 618.5 updated\n",
      "Time 619.0 updated\n",
      "Time 619.5 updated\n",
      "Time 620.0 updated\n",
      "Time 620.5 updated\n",
      "Time 621.0 updated\n",
      "Time 621.5 updated\n",
      "Time 622.0 updated\n",
      "Time 622.5 updated\n",
      "Time 623.0 updated\n",
      "Time 623.5 updated\n",
      "Time 624.0 updated\n",
      "Time 624.5 updated\n",
      "Time 625.0 updated\n",
      "Time 625.5 updated\n",
      "Time 626.0 updated\n",
      "Time 626.5 updated\n",
      "Time 627.0 updated\n",
      "Time 627.5 updated\n",
      "Time 628.0 updated\n",
      "Time 628.5 updated\n",
      "Time 629.0 updated\n",
      "Time 629.5 updated\n",
      "Time 630.0 updated\n",
      "Time 630.5 updated\n",
      "Time 631.0 updated\n",
      "Time 631.5 updated\n",
      "Time 632.0 updated\n",
      "Time 632.5 updated\n",
      "Time 633.0 updated\n",
      "Time 633.5 updated\n",
      "Time 634.0 updated\n",
      "Time 634.5 updated\n",
      "Time 635.0 updated\n",
      "Time 635.5 updated\n",
      "Time 636.0 updated\n",
      "Time 636.5 updated\n",
      "Time 637.0 updated\n",
      "Time 637.5 updated\n",
      "Time 638.0 updated\n",
      "Time 638.5 updated\n",
      "Time 639.0 updated\n",
      "Time 639.5 updated\n",
      "Time 640.0 updated\n",
      "Time 640.5 updated\n",
      "Time 641.0 updated\n",
      "Time 641.5 updated\n",
      "Time 642.0 updated\n",
      "Time 642.5 updated\n",
      "Time 643.0 updated\n",
      "Time 643.5 updated\n",
      "Time 644.0 updated\n",
      "Time 644.5 updated\n",
      "Time 645.0 updated\n",
      "Time 645.5 updated\n",
      "Time 646.0 updated\n",
      "Time 646.5 updated\n",
      "Time 647.0 updated\n",
      "Time 647.5 updated\n",
      "Time 648.0 updated\n",
      "Time 648.5 updated\n",
      "Time 649.0 updated\n",
      "Time 649.5 updated\n",
      "Time 650.0 updated\n",
      "Time 650.5 updated\n",
      "Time 651.0 updated\n",
      "Time 651.5 updated\n",
      "Time 652.0 updated\n",
      "Time 652.5 updated\n",
      "Time 653.0 updated\n",
      "Time 653.5 updated\n",
      "Time 654.0 updated\n",
      "Time 654.5 updated\n",
      "Time 655.0 updated\n",
      "Time 655.5 updated\n",
      "Time 656.0 updated\n",
      "Time 656.5 updated\n",
      "Time 657.0 updated\n",
      "Time 657.5 updated\n",
      "Time 658.0 updated\n",
      "Time 658.5 updated\n",
      "Time 659.0 updated\n",
      "Time 659.5 updated\n",
      "Time 660.0 updated\n",
      "Time 660.5 updated\n",
      "Time 661.0 updated\n",
      "Time 661.5 updated\n",
      "Time 662.0 updated\n",
      "Time 662.5 updated\n",
      "Time 663.0 updated\n",
      "Time 663.5 updated\n",
      "Time 664.0 updated\n",
      "Time 664.5 updated\n",
      "Time 665.0 updated\n",
      "Time 665.5 updated\n",
      "Time 666.0 updated\n",
      "Time 666.5 updated\n",
      "Time 667.0 updated\n",
      "Time 667.5 updated\n",
      "Time 668.0 updated\n",
      "Time 668.5 updated\n",
      "Time 669.0 updated\n",
      "Time 669.5 updated\n",
      "Time 670.0 updated\n",
      "Time 670.5 updated\n",
      "Time 671.0 updated\n",
      "Time 671.5 updated\n",
      "Time 672.0 updated\n",
      "Time 672.5 updated\n",
      "Time 673.0 updated\n",
      "Time 673.5 updated\n",
      "Time 674.0 updated\n",
      "Time 674.5 updated\n",
      "Time 675.0 updated\n",
      "Time 675.5 updated\n",
      "Time 676.0 updated\n",
      "Time 676.5 updated\n",
      "Time 677.0 updated\n",
      "Time 677.5 updated\n",
      "Time 678.0 updated\n",
      "Time 678.5 updated\n",
      "Time 679.0 updated\n",
      "Time 679.5 updated\n",
      "Time 680.0 updated\n",
      "Time 680.5 updated\n",
      "Time 681.0 updated\n",
      "Time 681.5 updated\n",
      "Time 682.0 updated\n",
      "Time 682.5 updated\n",
      "Time 683.0 updated\n",
      "Time 683.5 updated\n",
      "Time 684.0 updated\n",
      "Time 684.5 updated\n",
      "Time 685.0 updated\n",
      "Time 685.5 updated\n",
      "Time 686.0 updated\n",
      "Time 686.5 updated\n",
      "Time 687.0 updated\n",
      "Time 687.5 updated\n",
      "Time 688.0 updated\n",
      "Time 688.5 updated\n",
      "Time 689.0 updated\n",
      "Time 689.5 updated\n",
      "Time 690.0 updated\n",
      "Time 690.5 updated\n",
      "Time 691.0 updated\n",
      "Time 691.5 updated\n",
      "Time 692.0 updated\n",
      "Time 692.5 updated\n",
      "Time 693.0 updated\n",
      "Time 693.5 updated\n",
      "Time 694.0 updated\n",
      "Time 694.5 updated\n",
      "Time 695.0 updated\n",
      "Time 695.5 updated\n",
      "Time 696.0 updated\n",
      "Time 696.5 updated\n",
      "Time 697.0 updated\n",
      "Time 697.5 updated\n",
      "Time 698.0 updated\n",
      "Time 698.5 updated\n",
      "Time 699.0 updated\n",
      "Time 699.5 updated\n",
      "Time 700.0 updated\n",
      "Time 700.5 updated\n",
      "Time 701.0 updated\n",
      "Time 701.5 updated\n",
      "Time 702.0 updated\n",
      "Time 702.5 updated\n",
      "Time 703.0 updated\n",
      "Time 703.5 updated\n",
      "Time 704.0 updated\n",
      "Time 704.5 updated\n",
      "Time 705.0 updated\n",
      "Time 705.5 updated\n",
      "Time 706.0 updated\n",
      "Time 706.5 updated\n",
      "Time 707.0 updated\n",
      "Time 707.5 updated\n",
      "Time 708.0 updated\n",
      "Time 708.5 updated\n",
      "Time 709.0 updated\n",
      "Time 709.5 updated\n",
      "Time 710.0 updated\n",
      "Time 710.5 updated\n",
      "Time 711.0 updated\n",
      "Time 711.5 updated\n",
      "Time 712.0 updated\n",
      "Time 712.5 updated\n",
      "Time 713.0 updated\n",
      "Time 713.5 updated\n",
      "Time 714.0 updated\n",
      "Time 714.5 updated\n",
      "Time 715.0 updated\n",
      "Time 715.5 updated\n",
      "Time 716.0 updated\n",
      "Time 716.5 updated\n",
      "Time 717.0 updated\n",
      "Time 717.5 updated\n",
      "Time 718.0 updated\n",
      "Time 718.5 updated\n",
      "Time 719.0 updated\n",
      "Time 719.5 updated\n",
      "Time 720.0 updated\n",
      "Time 720.5 updated\n",
      "Time 721.0 updated\n",
      "Time 721.5 updated\n",
      "Time 722.0 updated\n",
      "Time 722.5 updated\n",
      "Time 723.0 updated\n",
      "Time 723.5 updated\n",
      "Time 724.0 updated\n",
      "Time 724.5 updated\n",
      "Time 725.0 updated\n",
      "Time 725.5 updated\n",
      "Time 726.0 updated\n",
      "Time 726.5 updated\n",
      "Time 727.0 updated\n",
      "Time 727.5 updated\n",
      "Time 728.0 updated\n",
      "Time 728.5 updated\n",
      "Time 729.0 updated\n",
      "Time 729.5 updated\n",
      "Time 730.0 updated\n",
      "Time 730.5 updated\n",
      "Time 731.0 updated\n",
      "Time 731.5 updated\n",
      "Time 732.0 updated\n",
      "Time 732.5 updated\n",
      "Time 733.0 updated\n",
      "Time 733.5 updated\n",
      "Time 734.0 updated\n",
      "Time 734.5 updated\n",
      "Time 735.0 updated\n",
      "Time 735.5 updated\n",
      "Time 736.0 updated\n",
      "Time 736.5 updated\n",
      "Time 737.0 updated\n",
      "Time 737.5 updated\n",
      "Time 738.0 updated\n",
      "Time 738.5 updated\n",
      "Time 739.0 updated\n",
      "Time 739.5 updated\n",
      "Time 740.0 updated\n",
      "Time 740.5 updated\n",
      "Time 741.0 updated\n",
      "Time 741.5 updated\n",
      "Time 742.0 updated\n",
      "Time 742.5 updated\n",
      "Time 743.0 updated\n",
      "Time 743.5 updated\n",
      "Time 744.0 updated\n",
      "Time 744.5 updated\n",
      "Time 745.0 updated\n",
      "Time 745.5 updated\n",
      "Time 746.0 updated\n",
      "Time 746.5 updated\n",
      "Time 747.0 updated\n",
      "Time 747.5 updated\n",
      "Time 748.0 updated\n",
      "Time 748.5 updated\n",
      "Time 749.0 updated\n",
      "Time 749.5 updated\n",
      "Time 750.0 updated\n",
      "Time 750.5 updated\n",
      "Time 751.0 updated\n",
      "Time 751.5 updated\n",
      "Time 752.0 updated\n",
      "Time 752.5 updated\n",
      "Time 753.0 updated\n",
      "Time 753.5 updated\n",
      "Time 754.0 updated\n",
      "Time 754.5 updated\n",
      "Time 755.0 updated\n",
      "Time 755.5 updated\n",
      "Time 756.0 updated\n",
      "Time 756.5 updated\n",
      "Time 757.0 updated\n",
      "Time 757.5 updated\n",
      "Time 758.0 updated\n",
      "Time 758.5 updated\n",
      "Time 759.0 updated\n",
      "Time 759.5 updated\n",
      "Time 760.0 updated\n",
      "Time 760.5 updated\n",
      "Time 761.0 updated\n",
      "Time 761.5 updated\n",
      "Time 762.0 updated\n",
      "Time 762.5 updated\n",
      "Time 763.0 updated\n",
      "Time 763.5 updated\n",
      "Time 764.0 updated\n",
      "Time 764.5 updated\n",
      "Time 765.0 updated\n",
      "Time 765.5 updated\n",
      "Time 766.0 updated\n",
      "Time 766.5 updated\n",
      "Time 767.0 updated\n",
      "Time 767.5 updated\n",
      "Time 768.0 updated\n",
      "Time 768.5 updated\n",
      "Time 769.0 updated\n",
      "Time 769.5 updated\n",
      "Time 770.0 updated\n",
      "Time 770.5 updated\n",
      "Time 771.0 updated\n",
      "Time 771.5 updated\n",
      "Time 772.0 updated\n",
      "Time 772.5 updated\n",
      "Time 773.0 updated\n",
      "Time 773.5 updated\n",
      "Time 774.0 updated\n",
      "Time 774.5 updated\n",
      "Time 775.0 updated\n",
      "Time 775.5 updated\n",
      "Time 776.0 updated\n",
      "Time 776.5 updated\n",
      "Time 777.0 updated\n",
      "Time 777.5 updated\n",
      "Time 778.0 updated\n",
      "Time 778.5 updated\n",
      "Time 779.0 updated\n",
      "Time 779.5 updated\n",
      "Time 780.0 updated\n",
      "Time 780.5 updated\n",
      "Time 781.0 updated\n",
      "Time 781.5 updated\n",
      "Time 782.0 updated\n",
      "Time 782.5 updated\n",
      "Time 783.0 updated\n",
      "Time 783.5 updated\n",
      "Time 784.0 updated\n",
      "Time 784.5 updated\n",
      "Time 785.0 updated\n",
      "Time 785.5 updated\n",
      "Time 786.0 updated\n",
      "Time 786.5 updated\n",
      "Time 787.0 updated\n",
      "Time 787.5 updated\n",
      "Time 788.0 updated\n",
      "Time 788.5 updated\n",
      "Time 789.0 updated\n",
      "Time 789.5 updated\n",
      "Time 790.0 updated\n",
      "Time 790.5 updated\n",
      "Time 791.0 updated\n",
      "Time 791.5 updated\n",
      "Time 792.0 updated\n",
      "Time 792.5 updated\n",
      "Time 793.0 updated\n",
      "Time 793.5 updated\n",
      "Time 794.0 updated\n",
      "Time 794.5 updated\n",
      "Time 795.0 updated\n",
      "Time 795.5 updated\n",
      "Time 796.0 updated\n",
      "Time 796.5 updated\n",
      "Time 797.0 updated\n",
      "Time 797.5 updated\n",
      "Time 798.0 updated\n",
      "Time 798.5 updated\n",
      "Time 799.0 updated\n",
      "Time 799.5 updated\n",
      "Time 800.0 updated\n",
      "Time 800.5 updated\n",
      "Time 801.0 updated\n",
      "Time 801.5 updated\n",
      "Time 802.0 updated\n",
      "Time 802.5 updated\n",
      "Time 803.0 updated\n",
      "Time 803.5 updated\n",
      "Time 804.0 updated\n",
      "Time 804.5 updated\n",
      "Time 805.0 updated\n",
      "Time 805.5 updated\n",
      "Time 806.0 updated\n",
      "Time 806.5 updated\n",
      "Time 807.0 updated\n",
      "Time 807.5 updated\n",
      "Time 808.0 updated\n",
      "Time 808.5 updated\n",
      "Time 809.0 updated\n",
      "Time 809.5 updated\n",
      "Time 810.0 updated\n",
      "Time 810.5 updated\n",
      "Time 811.0 updated\n",
      "Time 811.5 updated\n",
      "Time 812.0 updated\n",
      "Time 812.5 updated\n",
      "Time 813.0 updated\n",
      "Time 813.5 updated\n",
      "Time 814.0 updated\n",
      "Time 814.5 updated\n",
      "Time 815.0 updated\n",
      "Time 815.5 updated\n",
      "Time 816.0 updated\n",
      "Time 816.5 updated\n",
      "Time 817.0 updated\n",
      "Time 817.5 updated\n",
      "Time 818.0 updated\n",
      "Time 818.5 updated\n",
      "Time 819.0 updated\n",
      "Time 819.5 updated\n",
      "Time 820.0 updated\n",
      "Time 820.5 updated\n",
      "Time 821.0 updated\n",
      "Time 821.5 updated\n",
      "Time 822.0 updated\n",
      "Time 822.5 updated\n",
      "Time 823.0 updated\n",
      "Time 823.5 updated\n",
      "Time 824.0 updated\n",
      "Time 824.5 updated\n",
      "Time 825.0 updated\n",
      "Time 825.5 updated\n",
      "Time 826.0 updated\n",
      "Time 826.5 updated\n",
      "Time 827.0 updated\n",
      "Time 827.5 updated\n",
      "Time 828.0 updated\n",
      "Time 828.5 updated\n",
      "Time 829.0 updated\n",
      "Time 829.5 updated\n",
      "Time 830.0 updated\n",
      "Time 830.5 updated\n",
      "Time 831.0 updated\n",
      "Time 831.5 updated\n",
      "Time 832.0 updated\n",
      "Time 832.5 updated\n",
      "Time 833.0 updated\n",
      "Time 833.5 updated\n",
      "Time 834.0 updated\n",
      "Time 834.5 updated\n",
      "Time 835.0 updated\n",
      "Time 835.5 updated\n",
      "Time 836.0 updated\n",
      "Time 836.5 updated\n",
      "Time 837.0 updated\n",
      "Time 837.5 updated\n",
      "Time 838.0 updated\n",
      "Time 838.5 updated\n",
      "Time 839.0 updated\n",
      "Time 839.5 updated\n",
      "Time 840.0 updated\n",
      "Time 840.5 updated\n",
      "Time 841.0 updated\n",
      "Time 841.5 updated\n",
      "Time 842.0 updated\n",
      "Time 842.5 updated\n",
      "Time 843.0 updated\n",
      "Time 843.5 updated\n",
      "Time 844.0 updated\n",
      "Time 844.5 updated\n",
      "Time 845.0 updated\n",
      "Time 845.5 updated\n",
      "Time 846.0 updated\n",
      "Time 846.5 updated\n",
      "Time 847.0 updated\n",
      "Time 847.5 updated\n",
      "Time 848.0 updated\n",
      "Time 848.5 updated\n",
      "Time 849.0 updated\n",
      "Time 849.5 updated\n",
      "Time 850.0 updated\n",
      "Time 850.5 updated\n",
      "Time 851.0 updated\n",
      "Time 851.5 updated\n",
      "Time 852.0 updated\n",
      "Time 852.5 updated\n",
      "Time 853.0 updated\n",
      "Time 853.5 updated\n",
      "Time 854.0 updated\n",
      "Time 854.5 updated\n",
      "Time 855.0 updated\n",
      "Time 855.5 updated\n",
      "Time 856.0 updated\n",
      "Time 856.5 updated\n",
      "Time 857.0 updated\n",
      "Time 857.5 updated\n",
      "Time 858.0 updated\n",
      "Time 858.5 updated\n",
      "Time 859.0 updated\n",
      "Time 859.5 updated\n",
      "Time 860.0 updated\n",
      "Time 860.5 updated\n",
      "Time 861.0 updated\n",
      "Time 861.5 updated\n",
      "Time 862.0 updated\n",
      "Time 862.5 updated\n",
      "Time 863.0 updated\n",
      "Time 863.5 updated\n",
      "Time 864.0 updated\n",
      "Time 864.5 updated\n",
      "Time 865.0 updated\n",
      "Time 865.5 updated\n",
      "Time 866.0 updated\n",
      "Time 866.5 updated\n",
      "Time 867.0 updated\n",
      "Time 867.5 updated\n",
      "Time 868.0 updated\n",
      "Time 868.5 updated\n",
      "Time 869.0 updated\n",
      "Time 869.5 updated\n",
      "Time 870.0 updated\n",
      "Time 870.5 updated\n",
      "Time 871.0 updated\n",
      "Time 871.5 updated\n",
      "Time 872.0 updated\n",
      "Time 872.5 updated\n",
      "Time 873.0 updated\n",
      "Time 873.5 updated\n",
      "Time 874.0 updated\n",
      "Time 874.5 updated\n",
      "Time 875.0 updated\n",
      "Time 875.5 updated\n",
      "Time 876.0 updated\n",
      "Time 876.5 updated\n",
      "Time 877.0 updated\n",
      "Time 877.5 updated\n",
      "Time 878.0 updated\n",
      "Time 878.5 updated\n",
      "Time 879.0 updated\n",
      "Time 879.5 updated\n",
      "Time 880.0 updated\n",
      "Time 880.5 updated\n",
      "Time 881.0 updated\n",
      "Time 881.5 updated\n",
      "Time 882.0 updated\n",
      "Time 882.5 updated\n",
      "Time 883.0 updated\n",
      "Time 883.5 updated\n",
      "Time 884.0 updated\n",
      "Time 884.5 updated\n",
      "Time 885.0 updated\n",
      "Time 885.5 updated\n",
      "Time 886.0 updated\n",
      "Time 886.5 updated\n",
      "Time 887.0 updated\n",
      "Time 887.5 updated\n",
      "Time 888.0 updated\n",
      "Time 888.5 updated\n",
      "Time 889.0 updated\n",
      "Time 889.5 updated\n",
      "Time 890.0 updated\n",
      "Time 890.5 updated\n",
      "Time 891.0 updated\n",
      "Time 891.5 updated\n",
      "Time 892.0 updated\n",
      "Time 892.5 updated\n",
      "Time 893.0 updated\n",
      "Time 893.5 updated\n",
      "Time 894.0 updated\n",
      "Time 894.5 updated\n",
      "Time 895.0 updated\n",
      "Time 895.5 updated\n",
      "Time 896.0 updated\n",
      "Time 896.5 updated\n",
      "Time 897.0 updated\n",
      "Time 897.5 updated\n",
      "Time 898.0 updated\n",
      "Time 898.5 updated\n",
      "Time 899.0 updated\n",
      "Time 899.5 updated\n",
      "Time 900.0 updated\n",
      "Time 900.5 updated\n",
      "Time 901.0 updated\n",
      "Time 901.5 updated\n",
      "Time 902.0 updated\n",
      "Time 902.5 updated\n",
      "Time 903.0 updated\n",
      "Time 903.5 updated\n",
      "Time 904.0 updated\n",
      "Time 904.5 updated\n",
      "Time 905.0 updated\n",
      "Time 905.5 updated\n",
      "Time 906.0 updated\n",
      "Time 906.5 updated\n",
      "Time 907.0 updated\n",
      "Time 907.5 updated\n",
      "Time 908.0 updated\n",
      "Time 908.5 updated\n",
      "Time 909.0 updated\n",
      "Time 909.5 updated\n",
      "Time 910.0 updated\n",
      "Time 910.5 updated\n",
      "Time 911.0 updated\n",
      "Time 911.5 updated\n",
      "Time 912.0 updated\n",
      "Time 912.5 updated\n",
      "Time 913.0 updated\n",
      "Time 913.5 updated\n",
      "Time 914.0 updated\n",
      "Time 914.5 updated\n",
      "Time 915.0 updated\n",
      "Time 915.5 updated\n",
      "Time 916.0 updated\n",
      "Time 916.5 updated\n",
      "Time 917.0 updated\n",
      "Time 917.5 updated\n",
      "Time 918.0 updated\n",
      "Time 918.5 updated\n",
      "Time 919.0 updated\n",
      "Time 919.5 updated\n",
      "Time 920.0 updated\n",
      "Time 920.5 updated\n",
      "Time 921.0 updated\n",
      "Time 921.5 updated\n",
      "Time 922.0 updated\n",
      "Time 922.5 updated\n",
      "Time 923.0 updated\n",
      "Time 923.5 updated\n",
      "Time 924.0 updated\n",
      "Time 924.5 updated\n",
      "Time 925.0 updated\n",
      "Time 925.5 updated\n",
      "Time 926.0 updated\n",
      "Time 926.5 updated\n",
      "Time 927.0 updated\n",
      "Time 927.5 updated\n",
      "Time 928.0 updated\n",
      "Time 928.5 updated\n",
      "Time 929.0 updated\n",
      "Time 929.5 updated\n",
      "Time 930.0 updated\n",
      "Time 930.5 updated\n",
      "Time 931.0 updated\n",
      "Time 931.5 updated\n",
      "Time 932.0 updated\n",
      "Time 932.5 updated\n",
      "Time 933.0 updated\n",
      "Time 933.5 updated\n",
      "Time 934.0 updated\n",
      "Time 934.5 updated\n",
      "Time 935.0 updated\n",
      "Time 935.5 updated\n",
      "Time 936.0 updated\n",
      "Time 936.5 updated\n",
      "Time 937.0 updated\n",
      "Time 937.5 updated\n",
      "Time 938.0 updated\n",
      "Time 938.5 updated\n",
      "Time 939.0 updated\n",
      "Time 939.5 updated\n",
      "Time 940.0 updated\n",
      "Time 940.5 updated\n",
      "Time 941.0 updated\n",
      "Time 941.5 updated\n",
      "Time 942.0 updated\n",
      "Time 942.5 updated\n",
      "Time 943.0 updated\n",
      "Time 943.5 updated\n",
      "Time 944.0 updated\n",
      "Time 944.5 updated\n",
      "Time 945.0 updated\n",
      "Time 945.5 updated\n",
      "Time 946.0 updated\n",
      "Time 946.5 updated\n",
      "Time 947.0 updated\n",
      "Time 947.5 updated\n",
      "Time 948.0 updated\n",
      "Time 948.5 updated\n",
      "Time 949.0 updated\n",
      "Time 949.5 updated\n",
      "Time 950.0 updated\n",
      "Time 950.5 updated\n",
      "Time 951.0 updated\n",
      "Time 951.5 updated\n",
      "Time 952.0 updated\n",
      "Time 952.5 updated\n",
      "Time 953.0 updated\n",
      "Time 953.5 updated\n",
      "Time 954.0 updated\n",
      "Time 954.5 updated\n",
      "Time 955.0 updated\n",
      "Time 955.5 updated\n",
      "Time 956.0 updated\n",
      "Time 956.5 updated\n",
      "Time 957.0 updated\n",
      "Time 957.5 updated\n",
      "Time 958.0 updated\n",
      "Time 958.5 updated\n",
      "Time 959.0 updated\n",
      "Time 959.5 updated\n",
      "Time 960.0 updated\n",
      "Time 960.5 updated\n",
      "Time 961.0 updated\n",
      "Time 961.5 updated\n",
      "Time 962.0 updated\n",
      "Time 962.5 updated\n",
      "Time 963.0 updated\n",
      "Time 963.5 updated\n",
      "Time 964.0 updated\n",
      "Time 964.5 updated\n",
      "Time 965.0 updated\n",
      "Time 965.5 updated\n",
      "Time 966.0 updated\n",
      "Time 966.5 updated\n",
      "Time 967.0 updated\n",
      "Time 967.5 updated\n",
      "Time 968.0 updated\n",
      "Time 968.5 updated\n",
      "Time 969.0 updated\n",
      "Time 969.5 updated\n",
      "Time 970.0 updated\n",
      "Time 970.5 updated\n",
      "Time 971.0 updated\n",
      "Time 971.5 updated\n",
      "Time 972.0 updated\n",
      "Time 972.5 updated\n",
      "Time 973.0 updated\n",
      "Time 973.5 updated\n",
      "Time 974.0 updated\n",
      "Time 974.5 updated\n",
      "Time 975.0 updated\n",
      "Time 975.5 updated\n",
      "Time 976.0 updated\n",
      "Time 976.5 updated\n",
      "Time 977.0 updated\n",
      "Time 977.5 updated\n",
      "Time 978.0 updated\n",
      "Time 978.5 updated\n",
      "Time 979.0 updated\n",
      "Time 979.5 updated\n",
      "Time 980.0 updated\n",
      "Time 980.5 updated\n",
      "Time 981.0 updated\n",
      "Time 981.5 updated\n",
      "Time 982.0 updated\n",
      "Time 982.5 updated\n",
      "Time 983.0 updated\n",
      "Time 983.5 updated\n",
      "Time 984.0 updated\n",
      "Time 984.5 updated\n",
      "Time 985.0 updated\n",
      "Time 985.5 updated\n",
      "Time 986.0 updated\n",
      "Time 986.5 updated\n",
      "Time 987.0 updated\n",
      "Time 987.5 updated\n",
      "Time 988.0 updated\n",
      "Time 988.5 updated\n",
      "Time 989.0 updated\n",
      "Time 989.5 updated\n",
      "Time 990.0 updated\n",
      "Time 990.5 updated\n",
      "Time 991.0 updated\n",
      "Time 991.5 updated\n",
      "Time 992.0 updated\n",
      "Time 992.5 updated\n",
      "Time 993.0 updated\n",
      "Time 993.5 updated\n",
      "Time 994.0 updated\n",
      "Time 994.5 updated\n",
      "Time 995.0 updated\n",
      "Time 995.5 updated\n",
      "Time 996.0 updated\n",
      "Time 996.5 updated\n",
      "Time 997.0 updated\n",
      "Time 997.5 updated\n",
      "Time 998.0 updated\n",
      "Time 998.5 updated\n",
      "Time 999.0 updated\n",
      "Time 999.5 updated\n",
      "Time 1000.0 updated\n",
      "Time 1000.5 updated\n",
      "Time 1001.0 updated\n",
      "Time 1001.5 updated\n",
      "Time 1002.0 updated\n",
      "Time 1002.5 updated\n",
      "Time 1003.0 updated\n",
      "Time 1003.5 updated\n",
      "Time 1004.0 updated\n",
      "Time 1004.5 updated\n",
      "Time 1005.0 updated\n",
      "Time 1005.5 updated\n",
      "Time 1006.0 updated\n",
      "Time 1006.5 updated\n",
      "Time 1007.0 updated\n",
      "Time 1007.5 updated\n",
      "Time 1008.0 updated\n",
      "Time 1008.5 updated\n",
      "Time 1009.0 updated\n",
      "Time 1009.5 updated\n",
      "Time 1010.0 updated\n",
      "Time 1010.5 updated\n",
      "Time 1011.0 updated\n",
      "Time 1011.5 updated\n",
      "Time 1012.0 updated\n",
      "Time 1012.5 updated\n",
      "Time 1013.0 updated\n",
      "Time 1013.5 updated\n",
      "Time 1014.0 updated\n",
      "Time 1014.5 updated\n",
      "Time 1015.0 updated\n",
      "Time 1015.5 updated\n",
      "Time 1016.0 updated\n",
      "Time 1016.5 updated\n",
      "Time 1017.0 updated\n",
      "Time 1017.5 updated\n",
      "Time 1018.0 updated\n",
      "Time 1018.5 updated\n",
      "Time 1019.0 updated\n",
      "Time 1019.5 updated\n",
      "Time 1020.0 updated\n",
      "Time 1020.5 updated\n",
      "Time 1021.0 updated\n",
      "Time 1021.5 updated\n",
      "Time 1022.0 updated\n",
      "Time 1022.5 updated\n",
      "Time 1023.0 updated\n",
      "Time 1023.5 updated\n",
      "Time 1024.0 updated\n",
      "Time 1024.5 updated\n",
      "Time 1025.0 updated\n",
      "Time 1025.5 updated\n",
      "Time 1026.0 updated\n",
      "Time 1026.5 updated\n",
      "Time 1027.0 updated\n",
      "Time 1027.5 updated\n",
      "Time 1028.0 updated\n",
      "Time 1028.5 updated\n",
      "Time 1029.0 updated\n",
      "Time 1029.5 updated\n",
      "Time 1030.0 updated\n",
      "Time 1030.5 updated\n",
      "Time 1031.0 updated\n",
      "Time 1031.5 updated\n",
      "Time 1032.0 updated\n",
      "Time 1032.5 updated\n",
      "Time 1033.0 updated\n",
      "Time 1033.5 updated\n",
      "Time 1034.0 updated\n",
      "Time 1034.5 updated\n",
      "Time 1035.0 updated\n",
      "Time 1035.5 updated\n",
      "Time 1036.0 updated\n",
      "Time 1036.5 updated\n",
      "Time 1037.0 updated\n",
      "Time 1037.5 updated\n",
      "Time 1038.0 updated\n",
      "Time 1038.5 updated\n",
      "Time 1039.0 updated\n",
      "Time 1039.5 updated\n",
      "Time 1040.0 updated\n",
      "Time 1040.5 updated\n",
      "Time 1041.0 updated\n",
      "Time 1041.5 updated\n",
      "Time 1042.0 updated\n",
      "Time 1042.5 updated\n",
      "Time 1043.0 updated\n",
      "Time 1043.5 updated\n",
      "Time 1044.0 updated\n",
      "Time 1044.5 updated\n",
      "Time 1045.0 updated\n",
      "Time 1045.5 updated\n",
      "Time 1046.0 updated\n",
      "Time 1046.5 updated\n",
      "Time 1047.0 updated\n",
      "Time 1047.5 updated\n",
      "Time 1048.0 updated\n",
      "Time 1048.5 updated\n",
      "Time 1049.0 updated\n",
      "Time 1049.5 updated\n",
      "Time 1050.0 updated\n",
      "Time 1050.5 updated\n",
      "Time 1051.0 updated\n",
      "Time 1051.5 updated\n",
      "Time 1052.0 updated\n",
      "Time 1052.5 updated\n",
      "Time 1053.0 updated\n",
      "Time 1053.5 updated\n",
      "Time 1054.0 updated\n",
      "Time 1054.5 updated\n",
      "Time 1055.0 updated\n",
      "Time 1055.5 updated\n",
      "Time 1056.0 updated\n",
      "Time 1056.5 updated\n",
      "Time 1057.0 updated\n",
      "Time 1057.5 updated\n",
      "Time 1058.0 updated\n",
      "Time 1058.5 updated\n",
      "Time 1059.0 updated\n",
      "Time 1059.5 updated\n",
      "Time 1060.0 updated\n",
      "Time 1060.5 updated\n",
      "Time 1061.0 updated\n",
      "Time 1061.5 updated\n",
      "Time 1062.0 updated\n",
      "Time 1062.5 updated\n",
      "Time 1063.0 updated\n",
      "Time 1063.5 updated\n",
      "Time 1064.0 updated\n",
      "Time 1064.5 updated\n",
      "Time 1065.0 updated\n",
      "Time 1065.5 updated\n",
      "Time 1066.0 updated\n",
      "Time 1066.5 updated\n",
      "Time 1067.0 updated\n",
      "Time 1067.5 updated\n",
      "Time 1068.0 updated\n",
      "Time 1068.5 updated\n",
      "Time 1069.0 updated\n",
      "Time 1069.5 updated\n",
      "Time 1070.0 updated\n",
      "Time 1070.5 updated\n",
      "Time 1071.0 updated\n",
      "Time 1071.5 updated\n",
      "Time 1072.0 updated\n",
      "Time 1072.5 updated\n",
      "Time 1073.0 updated\n",
      "Time 1073.5 updated\n",
      "Time 1074.0 updated\n",
      "Time 1074.5 updated\n",
      "Time 1075.0 updated\n",
      "Time 1075.5 updated\n",
      "Time 1076.0 updated\n",
      "Time 1076.5 updated\n",
      "Time 1077.0 updated\n",
      "Time 1077.5 updated\n",
      "Time 1078.0 updated\n",
      "Time 1078.5 updated\n",
      "Time 1079.0 updated\n",
      "Time 1079.5 updated\n",
      "Time 1080.0 updated\n",
      "Time 1080.5 updated\n",
      "Time 1081.0 updated\n",
      "Time 1081.5 updated\n",
      "Time 1082.0 updated\n",
      "Time 1082.5 updated\n",
      "Time 1083.0 updated\n",
      "Time 1083.5 updated\n",
      "Time 1084.0 updated\n",
      "Time 1084.5 updated\n",
      "Time 1085.0 updated\n",
      "Time 1085.5 updated\n",
      "Time 1086.0 updated\n",
      "Time 1086.5 updated\n",
      "Time 1087.0 updated\n",
      "Time 1087.5 updated\n",
      "Time 1088.0 updated\n",
      "Time 1088.5 updated\n",
      "Time 1089.0 updated\n",
      "Time 1089.5 updated\n",
      "Time 1090.0 updated\n",
      "Time 1090.5 updated\n",
      "Time 1091.0 updated\n",
      "Time 1091.5 updated\n",
      "Time 1092.0 updated\n",
      "Time 1092.5 updated\n",
      "Time 1093.0 updated\n",
      "Time 1093.5 updated\n",
      "Time 1094.0 updated\n",
      "Time 1094.5 updated\n",
      "Time 1095.0 updated\n",
      "Time 1095.5 updated\n",
      "Time 1096.0 updated\n",
      "Time 1096.5 updated\n",
      "Time 1097.0 updated\n",
      "Time 1097.5 updated\n",
      "Time 1098.0 updated\n",
      "Time 1098.5 updated\n",
      "Time 1099.0 updated\n",
      "Time 1099.5 updated\n",
      "Time 1100.0 updated\n",
      "Time 1100.5 updated\n",
      "Time 1101.0 updated\n",
      "Time 1101.5 updated\n",
      "Time 1102.0 updated\n",
      "Time 1102.5 updated\n",
      "Time 1103.0 updated\n",
      "Time 1103.5 updated\n",
      "Time 1104.0 updated\n",
      "Time 1104.5 updated\n",
      "Time 1105.0 updated\n",
      "Time 1105.5 updated\n",
      "Time 1106.0 updated\n",
      "Time 1106.5 updated\n",
      "Time 1107.0 updated\n",
      "Time 1107.5 updated\n",
      "Time 1108.0 updated\n",
      "Time 1108.5 updated\n",
      "Time 1109.0 updated\n",
      "Time 1109.5 updated\n",
      "Time 1110.0 updated\n",
      "Time 1110.5 updated\n",
      "Time 1111.0 updated\n",
      "Time 1111.5 updated\n",
      "Time 1112.0 updated\n",
      "Time 1112.5 updated\n",
      "Time 1113.0 updated\n",
      "Time 1113.5 updated\n",
      "Time 1114.0 updated\n",
      "Time 1114.5 updated\n",
      "Time 1115.0 updated\n",
      "Time 1115.5 updated\n",
      "Time 1116.0 updated\n",
      "Time 1116.5 updated\n",
      "Time 1117.0 updated\n",
      "Time 1117.5 updated\n",
      "Time 1118.0 updated\n",
      "Time 1118.5 updated\n",
      "Time 1119.0 updated\n",
      "Time 1119.5 updated\n",
      "Time 1120.0 updated\n",
      "Time 1120.5 updated\n",
      "Time 1121.0 updated\n",
      "Time 1121.5 updated\n",
      "Time 1122.0 updated\n",
      "Time 1122.5 updated\n",
      "Time 1123.0 updated\n",
      "Time 1123.5 updated\n",
      "Time 1124.0 updated\n",
      "Time 1124.5 updated\n",
      "Time 1125.0 updated\n",
      "Time 1125.5 updated\n",
      "Time 1126.0 updated\n",
      "Time 1126.5 updated\n",
      "Time 1127.0 updated\n",
      "Time 1127.5 updated\n",
      "Time 1128.0 updated\n",
      "Time 1128.5 updated\n",
      "Time 1129.0 updated\n",
      "Time 1129.5 updated\n",
      "Time 1130.0 updated\n",
      "Time 1130.5 updated\n",
      "Time 1131.0 updated\n",
      "Time 1131.5 updated\n",
      "Time 1132.0 updated\n",
      "Time 1132.5 updated\n",
      "Time 1133.0 updated\n",
      "Time 1133.5 updated\n",
      "Time 1134.0 updated\n",
      "Time 1134.5 updated\n",
      "Time 1135.0 updated\n",
      "Time 1135.5 updated\n",
      "Time 1136.0 updated\n",
      "Time 1136.5 updated\n",
      "Time 1137.0 updated\n",
      "Time 1137.5 updated\n",
      "Time 1138.0 updated\n",
      "Time 1138.5 updated\n",
      "Time 1139.0 updated\n",
      "Time 1139.5 updated\n",
      "Time 1140.0 updated\n",
      "Time 1140.5 updated\n",
      "Time 1141.0 updated\n",
      "Time 1141.5 updated\n",
      "Time 1142.0 updated\n",
      "Time 1142.5 updated\n",
      "Time 1143.0 updated\n",
      "Time 1143.5 updated\n",
      "Time 1144.0 updated\n",
      "Time 1144.5 updated\n",
      "Time 1145.0 updated\n",
      "Time 1145.5 updated\n",
      "Time 1146.0 updated\n",
      "Time 1146.5 updated\n",
      "Time 1147.0 updated\n",
      "Time 1147.5 updated\n",
      "Time 1148.0 updated\n",
      "Time 1148.5 updated\n",
      "Time 1149.0 updated\n",
      "Time 1149.5 updated\n",
      "Time 1150.0 updated\n",
      "Time 1150.5 updated\n",
      "Time 1151.0 updated\n",
      "Time 1151.5 updated\n",
      "Time 1152.0 updated\n",
      "Time 1152.5 updated\n",
      "Time 1153.0 updated\n",
      "Time 1153.5 updated\n",
      "Time 1154.0 updated\n",
      "Time 1154.5 updated\n",
      "Time 1155.0 updated\n",
      "Time 1155.5 updated\n",
      "Time 1156.0 updated\n",
      "Time 1156.5 updated\n",
      "Time 1157.0 updated\n",
      "Time 1157.5 updated\n",
      "Time 1158.0 updated\n",
      "Time 1158.5 updated\n",
      "Time 1159.0 updated\n",
      "Time 1159.5 updated\n",
      "Time 1160.0 updated\n",
      "Time 1160.5 updated\n",
      "Time 1161.0 updated\n",
      "Time 1161.5 updated\n",
      "Time 1162.0 updated\n",
      "Time 1162.5 updated\n",
      "Time 1163.0 updated\n",
      "Time 1163.5 updated\n",
      "Time 1164.0 updated\n",
      "Time 1164.5 updated\n",
      "Time 1165.0 updated\n",
      "Time 1165.5 updated\n",
      "Time 1166.0 updated\n",
      "Time 1166.5 updated\n",
      "Time 1167.0 updated\n",
      "Time 1167.5 updated\n",
      "Time 1168.0 updated\n",
      "Time 1168.5 updated\n",
      "Time 1169.0 updated\n",
      "Time 1169.5 updated\n",
      "Time 1170.0 updated\n",
      "Time 1170.5 updated\n",
      "Time 1171.0 updated\n",
      "Time 1171.5 updated\n",
      "Time 1172.0 updated\n",
      "Time 1172.5 updated\n",
      "Time 1173.0 updated\n",
      "Time 1173.5 updated\n",
      "Time 1174.0 updated\n",
      "Time 1174.5 updated\n",
      "Time 1175.0 updated\n",
      "Time 1175.5 updated\n",
      "Time 1176.0 updated\n",
      "Time 1176.5 updated\n",
      "Time 1177.0 updated\n",
      "Time 1177.5 updated\n",
      "Time 1178.0 updated\n",
      "Time 1178.5 updated\n",
      "Time 1179.0 updated\n",
      "Time 1179.5 updated\n",
      "Time 1180.0 updated\n",
      "Time 1180.5 updated\n",
      "Time 1181.0 updated\n",
      "Time 1181.5 updated\n",
      "Time 1182.0 updated\n",
      "Time 1182.5 updated\n",
      "Time 1183.0 updated\n",
      "Time 1183.5 updated\n",
      "Time 1184.0 updated\n",
      "Time 1184.5 updated\n",
      "Time 1185.0 updated\n",
      "Time 1185.5 updated\n",
      "Time 1186.0 updated\n",
      "Time 1186.5 updated\n",
      "Time 1187.0 updated\n",
      "Time 1187.5 updated\n",
      "Time 1188.0 updated\n",
      "Time 1188.5 updated\n",
      "Time 1189.0 updated\n",
      "Time 1189.5 updated\n",
      "Time 1190.0 updated\n",
      "Time 1190.5 updated\n",
      "Time 1191.0 updated\n",
      "Time 1191.5 updated\n",
      "Time 1192.0 updated\n",
      "Time 1192.5 updated\n",
      "Time 1193.0 updated\n",
      "Time 1193.5 updated\n",
      "Time 1194.0 updated\n",
      "Time 1194.5 updated\n",
      "Time 1195.0 updated\n",
      "Time 1195.5 updated\n",
      "Time 1196.0 updated\n",
      "Time 1196.5 updated\n",
      "Time 1197.0 updated\n",
      "Time 1197.5 updated\n",
      "Time 1198.0 updated\n",
      "Time 1198.5 updated\n",
      "Time 1199.0 updated\n",
      "Time 1199.5 updated\n",
      "Time 1200.0 updated\n",
      "Time 1200.5 updated\n",
      "Time 1201.0 updated\n",
      "Time 1201.5 updated\n",
      "Time 1202.0 updated\n",
      "Time 1202.5 updated\n",
      "Time 1203.0 updated\n",
      "Time 1203.5 updated\n",
      "Time 1204.0 updated\n",
      "Time 1204.5 updated\n",
      "Time 1205.0 updated\n",
      "Time 1205.5 updated\n",
      "Time 1206.0 updated\n",
      "Time 1206.5 updated\n",
      "Time 1207.0 updated\n",
      "Time 1207.5 updated\n",
      "Time 1208.0 updated\n",
      "Time 1208.5 updated\n",
      "Time 1209.0 updated\n",
      "Time 1209.5 updated\n",
      "Time 1210.0 updated\n",
      "Time 1210.5 updated\n",
      "Time 1211.0 updated\n",
      "Time 1211.5 updated\n",
      "Time 1212.0 updated\n",
      "Time 1212.5 updated\n",
      "Time 1213.0 updated\n",
      "Time 1213.5 updated\n",
      "Time 1214.0 updated\n",
      "Time 1214.5 updated\n",
      "Time 1215.0 updated\n",
      "Time 1215.5 updated\n",
      "Time 1216.0 updated\n",
      "Time 1216.5 updated\n",
      "Time 1217.0 updated\n",
      "Time 1217.5 updated\n",
      "Time 1218.0 updated\n",
      "Time 1218.5 updated\n",
      "Time 1219.0 updated\n",
      "Time 1219.5 updated\n",
      "Time 1220.0 updated\n",
      "Time 1220.5 updated\n",
      "Time 1221.0 updated\n",
      "Time 1221.5 updated\n",
      "Time 1222.0 updated\n",
      "Time 1222.5 updated\n",
      "Time 1223.0 updated\n",
      "Time 1223.5 updated\n",
      "Time 1224.0 updated\n",
      "Time 1224.5 updated\n",
      "Time 1225.0 updated\n",
      "Time 1225.5 updated\n",
      "Time 1226.0 updated\n",
      "Time 1226.5 updated\n",
      "Time 1227.0 updated\n",
      "Time 1227.5 updated\n",
      "Time 1228.0 updated\n",
      "Time 1228.5 updated\n",
      "Time 1229.0 updated\n",
      "Time 1229.5 updated\n",
      "Time 1230.0 updated\n",
      "Time 1230.5 updated\n",
      "Time 1231.0 updated\n",
      "Time 1231.5 updated\n",
      "Time 1232.0 updated\n",
      "Time 1232.5 updated\n",
      "Time 1233.0 updated\n",
      "Time 1233.5 updated\n",
      "Time 1234.0 updated\n",
      "Time 1234.5 updated\n",
      "Time 1235.0 updated\n",
      "Time 1235.5 updated\n",
      "Time 1236.0 updated\n",
      "Time 1236.5 updated\n",
      "Time 1237.0 updated\n",
      "Time 1237.5 updated\n",
      "Time 1238.0 updated\n",
      "Time 1238.5 updated\n",
      "Time 1239.0 updated\n",
      "Time 1239.5 updated\n",
      "Time 1240.0 updated\n",
      "Time 1240.5 updated\n",
      "Time 1241.0 updated\n",
      "Time 1241.5 updated\n",
      "Time 1242.0 updated\n",
      "Time 1242.5 updated\n",
      "Time 1243.0 updated\n",
      "Time 1243.5 updated\n",
      "Time 1244.0 updated\n",
      "Time 1244.5 updated\n",
      "Time 1245.0 updated\n",
      "Time 1245.5 updated\n",
      "Time 1246.0 updated\n",
      "Time 1246.5 updated\n",
      "Time 1247.0 updated\n",
      "Time 1247.5 updated\n",
      "Time 1248.0 updated\n",
      "Time 1248.5 updated\n",
      "Time 1249.0 updated\n",
      "Time 1249.5 updated\n",
      "Time 1250.0 updated\n",
      "Time 1250.5 updated\n",
      "Time 1251.0 updated\n",
      "Time 1251.5 updated\n",
      "Time 1252.0 updated\n",
      "Time 1252.5 updated\n",
      "Time 1253.0 updated\n",
      "Time 1253.5 updated\n",
      "Time 1254.0 updated\n",
      "Time 1254.5 updated\n",
      "Time 1255.0 updated\n",
      "Time 1255.5 updated\n",
      "Time 1256.0 updated\n",
      "Time 1256.5 updated\n",
      "Time 1257.0 updated\n",
      "Time 1257.5 updated\n",
      "Time 1258.0 updated\n",
      "Time 1258.5 updated\n",
      "Time 1259.0 updated\n",
      "Time 1259.5 updated\n",
      "Time 1260.0 updated\n",
      "Time 1260.5 updated\n",
      "Time 1261.0 updated\n",
      "Time 1261.5 updated\n",
      "Time 1262.0 updated\n",
      "Time 1262.5 updated\n",
      "Time 1263.0 updated\n",
      "Time 1263.5 updated\n",
      "Time 1264.0 updated\n",
      "Time 1264.5 updated\n",
      "Time 1265.0 updated\n",
      "Time 1265.5 updated\n",
      "Time 1266.0 updated\n",
      "Time 1266.5 updated\n",
      "Time 1267.0 updated\n",
      "Time 1267.5 updated\n",
      "Time 1268.0 updated\n",
      "Time 1268.5 updated\n",
      "Time 1269.0 updated\n",
      "Time 1269.5 updated\n",
      "Time 1270.0 updated\n",
      "Time 1270.5 updated\n",
      "Time 1271.0 updated\n",
      "Time 1271.5 updated\n",
      "Time 1272.0 updated\n",
      "Time 1272.5 updated\n",
      "Time 1273.0 updated\n",
      "Time 1273.5 updated\n",
      "Time 1274.0 updated\n",
      "Time 1274.5 updated\n",
      "Time 1275.0 updated\n",
      "Time 1275.5 updated\n",
      "Time 1276.0 updated\n",
      "Time 1276.5 updated\n",
      "Time 1277.0 updated\n",
      "Time 1277.5 updated\n",
      "Time 1278.0 updated\n",
      "Time 1278.5 updated\n",
      "Time 1279.0 updated\n",
      "Time 1279.5 updated\n",
      "Time 1280.0 updated\n",
      "Time 1280.5 updated\n",
      "Time 1281.0 updated\n",
      "Time 1281.5 updated\n",
      "Time 1282.0 updated\n",
      "Time 1282.5 updated\n",
      "Time 1283.0 updated\n",
      "Time 1283.5 updated\n",
      "Time 1284.0 updated\n",
      "Time 1284.5 updated\n",
      "Time 1285.0 updated\n",
      "Time 1285.5 updated\n",
      "Time 1286.0 updated\n",
      "Time 1286.5 updated\n",
      "Time 1287.0 updated\n",
      "Time 1287.5 updated\n",
      "Time 1288.0 updated\n",
      "Time 1288.5 updated\n",
      "Time 1289.0 updated\n",
      "Time 1289.5 updated\n",
      "Time 1290.0 updated\n",
      "Time 1290.5 updated\n",
      "Time 1291.0 updated\n",
      "Time 1291.5 updated\n",
      "Time 1292.0 updated\n",
      "Time 1292.5 updated\n",
      "Time 1293.0 updated\n",
      "Time 1293.5 updated\n",
      "Time 1294.0 updated\n",
      "Time 1294.5 updated\n",
      "Time 1295.0 updated\n",
      "Time 1295.5 updated\n",
      "Time 1296.0 updated\n",
      "Time 1296.5 updated\n",
      "Time 1297.0 updated\n",
      "Time 1297.5 updated\n",
      "Time 1298.0 updated\n",
      "Time 1298.5 updated\n",
      "Time 1299.0 updated\n",
      "Time 1299.5 updated\n",
      "Time 1300.0 updated\n",
      "Time 1300.5 updated\n",
      "Time 1301.0 updated\n",
      "Time 1301.5 updated\n",
      "Time 1302.0 updated\n",
      "Time 1302.5 updated\n",
      "Time 1303.0 updated\n",
      "Time 1303.5 updated\n",
      "Time 1304.0 updated\n",
      "Time 1304.5 updated\n",
      "Time 1305.0 updated\n",
      "Time 1305.5 updated\n",
      "Time 1306.0 updated\n",
      "Time 1306.5 updated\n",
      "Time 1307.0 updated\n",
      "Time 1307.5 updated\n",
      "Time 1308.0 updated\n",
      "Time 1308.5 updated\n",
      "Time 1309.0 updated\n",
      "Time 1309.5 updated\n",
      "Time 1310.0 updated\n",
      "Time 1310.5 updated\n",
      "Time 1311.0 updated\n",
      "Time 1311.5 updated\n",
      "Time 1312.0 updated\n",
      "Time 1312.5 updated\n",
      "Time 1313.0 updated\n",
      "Time 1313.5 updated\n",
      "Time 1314.0 updated\n",
      "Time 1314.5 updated\n",
      "Time 1315.0 updated\n",
      "Time 1315.5 updated\n",
      "Time 1316.0 updated\n",
      "Time 1316.5 updated\n",
      "Time 1317.0 updated\n",
      "Time 1317.5 updated\n",
      "Time 1318.0 updated\n",
      "Time 1318.5 updated\n",
      "Time 1319.0 updated\n",
      "Time 1319.5 updated\n",
      "Time 1320.0 updated\n",
      "Time 1320.5 updated\n",
      "Time 1321.0 updated\n",
      "Time 1321.5 updated\n",
      "Time 1322.0 updated\n",
      "Time 1322.5 updated\n",
      "Time 1323.0 updated\n",
      "Time 1323.5 updated\n",
      "Time 1324.0 updated\n",
      "Time 1324.5 updated\n",
      "Time 1325.0 updated\n",
      "Time 1325.5 updated\n",
      "Time 1326.0 updated\n",
      "Time 1326.5 updated\n",
      "Time 1327.0 updated\n",
      "Time 1327.5 updated\n",
      "Time 1328.0 updated\n",
      "Time 1328.5 updated\n",
      "Time 1329.0 updated\n",
      "Time 1329.5 updated\n",
      "Time 1330.0 updated\n",
      "Time 1330.5 updated\n",
      "Time 1331.0 updated\n",
      "Time 1331.5 updated\n",
      "Time 1332.0 updated\n",
      "Time 1332.5 updated\n",
      "Time 1333.0 updated\n",
      "Time 1333.5 updated\n",
      "Time 1334.0 updated\n",
      "Time 1334.5 updated\n",
      "Time 1335.0 updated\n",
      "Time 1335.5 updated\n",
      "Time 1336.0 updated\n",
      "Time 1336.5 updated\n",
      "Time 1337.0 updated\n",
      "Time 1337.5 updated\n",
      "Time 1338.0 updated\n",
      "Time 1338.5 updated\n",
      "Time 1339.0 updated\n",
      "Time 1339.5 updated\n",
      "Time 1340.0 updated\n",
      "Time 1340.5 updated\n",
      "Time 1341.0 updated\n",
      "Time 1341.5 updated\n",
      "Time 1342.0 updated\n",
      "Time 1342.5 updated\n",
      "Time 1343.0 updated\n",
      "Time 1343.5 updated\n",
      "Time 1344.0 updated\n",
      "Time 1344.5 updated\n",
      "Time 1345.0 updated\n",
      "Time 1345.5 updated\n",
      "Time 1346.0 updated\n",
      "Time 1346.5 updated\n",
      "Time 1347.0 updated\n",
      "Time 1347.5 updated\n",
      "Time 1348.0 updated\n",
      "Time 1348.5 updated\n",
      "Time 1349.0 updated\n",
      "Time 1349.5 updated\n",
      "Time 1350.0 updated\n",
      "Time 1350.5 updated\n",
      "Time 1351.0 updated\n",
      "Time 1351.5 updated\n",
      "Time 1352.0 updated\n",
      "Time 1352.5 updated\n",
      "Time 1353.0 updated\n",
      "Time 1353.5 updated\n",
      "Time 1354.0 updated\n",
      "Time 1354.5 updated\n",
      "Time 1355.0 updated\n",
      "Time 1355.5 updated\n",
      "Time 1356.0 updated\n",
      "Time 1356.5 updated\n",
      "Time 1357.0 updated\n",
      "Time 1357.5 updated\n",
      "Time 1358.0 updated\n",
      "Time 1358.5 updated\n",
      "Time 1359.0 updated\n",
      "Time 1359.5 updated\n",
      "Time 1360.0 updated\n",
      "Time 1360.5 updated\n",
      "Time 1361.0 updated\n",
      "Time 1361.5 updated\n",
      "Time 1362.0 updated\n",
      "Time 1362.5 updated\n",
      "Time 1363.0 updated\n",
      "Time 1363.5 updated\n",
      "Time 1364.0 updated\n",
      "Time 1364.5 updated\n",
      "Time 1365.0 updated\n",
      "Time 1365.5 updated\n",
      "Time 1366.0 updated\n",
      "Time 1366.5 updated\n",
      "Time 1367.0 updated\n",
      "Time 1367.5 updated\n",
      "Time 1368.0 updated\n",
      "Time 1368.5 updated\n",
      "Time 1369.0 updated\n",
      "Time 1369.5 updated\n",
      "Time 1370.0 updated\n",
      "Time 1370.5 updated\n",
      "Time 1371.0 updated\n",
      "Time 1371.5 updated\n",
      "Time 1372.0 updated\n",
      "Time 1372.5 updated\n",
      "Time 1373.0 updated\n",
      "Time 1373.5 updated\n",
      "Time 1374.0 updated\n",
      "Time 1374.5 updated\n",
      "Time 1375.0 updated\n",
      "Time 1375.5 updated\n",
      "Time 1376.0 updated\n",
      "Time 1376.5 updated\n",
      "Time 1377.0 updated\n",
      "Time 1377.5 updated\n",
      "Time 1378.0 updated\n",
      "Time 1378.5 updated\n",
      "Time 1379.0 updated\n",
      "Time 1379.5 updated\n",
      "Time 1380.0 updated\n",
      "Time 1380.5 updated\n",
      "Time 1381.0 updated\n",
      "Time 1381.5 updated\n",
      "Time 1382.0 updated\n",
      "Time 1382.5 updated\n",
      "Time 1383.0 updated\n",
      "Time 1383.5 updated\n",
      "Time 1384.0 updated\n",
      "Time 1384.5 updated\n",
      "Time 1385.0 updated\n",
      "Time 1385.5 updated\n",
      "Time 1386.0 updated\n",
      "Time 1386.5 updated\n",
      "Time 1387.0 updated\n",
      "Time 1387.5 updated\n",
      "Time 1388.0 updated\n",
      "Time 1388.5 updated\n",
      "Time 1389.0 updated\n",
      "Time 1389.5 updated\n",
      "Time 1390.0 updated\n",
      "Time 1390.5 updated\n",
      "Time 1391.0 updated\n",
      "Time 1391.5 updated\n",
      "Time 1392.0 updated\n",
      "Time 1392.5 updated\n",
      "Time 1393.0 updated\n",
      "Time 1393.5 updated\n",
      "Time 1394.0 updated\n",
      "Time 1394.5 updated\n",
      "Time 1395.0 updated\n",
      "Time 1395.5 updated\n",
      "Time 1396.0 updated\n",
      "Time 1396.5 updated\n",
      "Time 1397.0 updated\n",
      "Time 1397.5 updated\n",
      "Time 1398.0 updated\n",
      "Time 1398.5 updated\n",
      "Time 1399.0 updated\n",
      "Time 1399.5 updated\n",
      "Time 1400.0 updated\n",
      "Time 1400.5 updated\n"
     ]
    }
   ],
   "source": [
    "diff_sample_2 = generation(model_test, 250, n, 0.5, 4 * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "304f1cff-ec9d-40a5-ae83-244e6494c606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnUAAAHWCAYAAAARl3+JAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPORJREFUeJzt3Xt8z/X///H7zgfz3rthB6e1z0ViOYVPvFMOWUaTij5qiiXxoUn4hI8SPoRSSHIoOXSy0KdUyKGJPjHyWVufZT4ORfs4vKdoZk5je/3+6Lf31zuHttl777eX2/Vy2aXt+Xq8Xs+DXbj3Or29DMMwBAAAgGuat7sHAAAAgKtHqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqCuFQYMGuXsIAAAAV0SoK4WDBw+6ewgAAABXRKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAK+7h4AAACSVFRUpHPnzrl7GMAV+fn5ycfHx93DuCRCHQDArQzDkN1uV15enruHApSK1WpVZGSkvLy83D0UJ4Q6AIBblQS68PBwBQcHe9w/lEAJwzB06tQpHTlyRJIUFRXl5hE5I9QBANymqKjIEeiqVavm7uEAfygoKEiSdOTIEYWHh3vUpVgelAAAuE3JPXTBwcFuHglQeiW/r552DyihDgDgdlxyxbXEU39fCXUAALiYl5eXVqxY4fj5v//9r1q3bq3AwEA1a9bssm1XY+PGjfLy8rpuH0B57LHHdP/997t7GJWKe+oAACiHxx57TG+//bYkydfXV2FhYWrSpIkSExP12GOPydv7/86bHD58WDfccIPj53HjxqlKlSratWuXQkJCLtt2NW6//XYdPnxYoaGhV30sXBsIdQAAjzTnzfnKKzhZaf1ZQ6royQH9y7RP586dtWjRIhUVFSk3N1dr1qzR008/rQ8//FCffvqpfH1/+2c2MjLSab8ffvhBCQkJio6OvmLb1fD397+oX5icgT907733unsIAGBKp0+fNrKzs43Tp09ftG3StBlG+uG8SvuaNG1GmcaelJRk3HfffRe1p6amGpKM+fPnO9okGR9//LHj+wu/xo0bd8m2L7/80pBk/Prrr47jZGRkGJKMffv2GYZhGPv37ze6du1qWK1WIzg42IiNjTVWrVplGIZxyf0//PBDIzY21vD39zeio6ONV155xWns0dHRxqRJk4y+ffsaISEhRp06dYw33njjiuuwfPlyo1GjRkZgYKARFhZmdOzY0SgoKDAMwzC++eYbIy4uzqhWrZphsViMtm3bGunp6U77SzLmzZtnJCQkGEFBQUaDBg2MLVu2GHv27DHatWtnBAcHGzabzdi7d69jn3HjxhlNmzY15s2bZ9SuXdsICgoy/vKXvxh5eXmX/fMpKioyJk+ebNx4441GYGCg0aRJE2P58uWO7ceOHTN69eplVK9e3QgMDDTq1atnLFy48JJzvtLvrTu57Z668ePHy8vLy+mrQYMGju1nzpxRcnKyqlWrppCQEPXo0UO5ublOx8jJyVFCQoKCg4MVHh6uESNG6Pz58041GzduVPPmzRUQEKB69epp8eLFlTE9AMB16q677lLTpk310UcfXXL74cOHdcstt+hvf/ubDh8+rGeeeeaSbaWRnJyss2fP6quvvlJWVpZeeumly166TU9PV8+ePfXwww8rKytL48eP1/PPP3/Rv4vTpk1Ty5YtlZGRoSeffFKDBg3Srl27LjuXxMREPf7449q5c6c2btyo7t27yzAMSdKJEyeUlJSkr7/+Wlu3btVNN92ke+65RydOnHA6zsSJE9WnTx9lZmaqQYMG6tWrl/76179q9OjR+ve//y3DMDR48GCnffbu3atly5bps88+05o1axzjvZwpU6bonXfe0bx587Rjxw4NGzZMjz76qDZt2iRJev7555Wdna3PP/9cO3fu1Ny5c1W9evUrrr+ncevl11tuuUVffPGF4+eS09SSNGzYMK1atUrLly9XaGioBg8erO7du2vz5s2Sfnu3UUJCgiIjI7VlyxYdPnxYffr0kZ+fnyZPnixJ2rdvnxISEjRw4EC9//77Sk1N1RNPPKGoqCjFx8dX7mQBANeNBg0a6D//+c8lt0VGRsrX11chISGOy6MhISEXtZVGTk6OevToocaNG0uS/vSnP122dvr06erYsaOef/55SVL9+vWVnZ2tl19+WY899pij7p577nGEo1GjRmnGjBn68ssvdfPNN190zMOHD+v8+fPq3r2747JxyVik3wLuhd58801ZrVZt2rRJXbt2dbT37dtXPXv2dPRps9n0/PPPO/6tfvrpp9W3b1+nY505c0bvvPOOatWqJUmaNWuWEhISNG3atIvW8OzZs5o8ebK++OIL2Ww2x1p9/fXXeuONN9SuXTvl5OTo1ltvVcuWLSVJN95442XX0lO59elXX19fRUZGOr5KEvHx48e1YMECTZ8+XXfddZdatGihRYsWacuWLdq6daskad26dcrOztZ7772nZs2aqUuXLpo4caJmz56twsJCSdK8efMUExOjadOmqWHDhho8eLAefPBBzZgxw21zBgCYn2EYlfLaiyFDhuiFF15QmzZtNG7cuMsGSUnauXOn2rRp49TWpk0b7dmzR0VFRY62Jk2aOL738vJSZGSk4xMUfq9p06bq2LGjGjdurL/85S+aP3++fv31V8f23Nxc9e/fXzfddJNCQ0NlsVhUUFCgnJwcp+Nc2GdERIQk53AYERGhM2fOKD8/39FWt25dR6CTJJvNpuLi4kueVdy7d69OnTqlu+++WyEhIY6vd955Rz/88IMkadCgQfrggw/UrFkzjRw5Ulu2bLnknD2ZW8/U7dmzRzVr1lRgYKBsNpumTJmiunXrKj09XefOnVNcXJyjtkGDBqpbt67S0tLUunVrpaWlqXHjxo4/fEmKj4/XoEGDtGPHDt16661KS0tzOkZJzdChQytrigAqQGXdMF+eG+WBS9m5c6diYmKu6hglT8+WXMqULn7Z7RNPPKH4+HitWrVK69at05QpUzRt2jQ99dRT5e7Xz8/P6WcvLy8VFxdfstbHx0fr16/Xli1btG7dOs2aNUvPPfectm3bppiYGCUlJeno0aOaOXOmoqOjFRAQIJvN5jj5cqk+S8LwpdouN44/UlBQIElatWqVUxCUpICAAElSly5d9NNPP2n16tVav369OnbsqOTkZL3yyivl6tMd3BbqWrVqpcWLF+vmm2/W4cOH9Y9//EN33nmnvv/+e9ntdvn7+8tqtTrtExERIbvdLum3zwq8MNCVbC/ZdqWa/Px8nT592vFRHwA8W17BSXXu1fePC6/SmiWLXN4HzG/Dhg3KysrSsGHDruo4NWrUkOT8OpTMzMyL6urUqaOBAwdq4MCBGj16tObPn3/JUNewYUPHLUwlNm/erPr161/VR115eXmpTZs2atOmjcaOHavo6Gh9/PHHGj58uDZv3qw5c+bonnvukST973//0y+//FLuvi6Uk5OjQ4cOqWbNmpKkrVu3ytvb+5KXiWNjYxUQEKCcnBy1a9fussesUaOGkpKSlJSUpDvvvFMjRowg1JVGly5dHN83adJErVq1UnR0tJYtW+bWsJWSkqKUlBSntoMHD7ppNAAAT3b27FnZ7XanV5pMmTJFXbt2VZ8+fa7q2PXq1VOdOnU0fvx4TZo0Sbt379a0adOcaoYOHaouXbqofv36+vXXX/Xll1+qYcOGlzze3/72N/35z3/WxIkT9dBDDyktLU2vv/665syZU+4xbtu2TampqerUqZPCw8O1bds2/fzzz44x3HTTTXr33XfVsmVL5efna8SIERX2b3xgYKCSkpL0yiuvKD8/X0OGDFHPnj0veU9i1apV9cwzz2jYsGEqLi7WHXfcoePHj2vz5s2yWCxKSkrS2LFj1aJFC91yyy06e/asVq5cedm19FQe8546q9Wq+vXra+/evbr77rtVWFiovLw8p7N1ubm5jj+syMhIffPNN07HKHk69sKa3z8xm5ubK4vFctlfqsTERCUmJjq1devW7armBgAwpzVr1igqKkq+vr664YYb1LRpU7322mtKSkpyevlwefj5+SklJUWDBg1SkyZN9Oc//1kvvPCC/vKXvzhqioqKlJycrAMHDshisahz586XvW+8efPmWrZsmcaOHauJEycqKipKEyZMcHpIoqwsFou++uorvfrqq8rPz1d0dLSmTZvmOHGzYMECDRgwQM2bN1edOnU0efLkUj/Z+0fq1aun7t2765577tGxY8fUtWvXKwbUiRMnqkaNGpoyZYp+/PFHWa1WNW/eXM8++6yk397rN3r0aO3fv19BQUG688479cEHH1TIWCuLl3HhxXo3KigoUN26dTV+/HglJSWpRo0aSklJUY8ePSRJu3btUoMGDRz31H3++efq2rWrDh8+rPDwcEm/PVUzYsQIHTlyRAEBARo1apRWr16trKwsRz+9evXSsWPHtGbNmlKPrVu3bvr0008rdsIASm3y9Fcr7fLrs8OHurwf/J8zZ85o3759iomJUWBgoNO2yvpzL8Gf/7Vj/PjxWrFixSUvR1eGK/3eupPbztQ988wzuvfeexUdHa1Dhw5p3Lhx8vHxUWJiokJDQ9WvXz8NHz5cYWFhslgseuqpp2Sz2dS6dWtJUqdOnRQbG6vevXtr6tSpstvtGjNmjJKTkx03PQ4cOFCvv/66Ro4cqccff1wbNmzQsmXLtGrVKndNGwBQStaQKpV6n6M1pEql9QW4gttC3YEDB5SYmKijR4+qRo0auuOOO7R161bHjaEzZsyQt7e3evToobNnzyo+Pt7ptKqPj49WrlypQYMGyWazqUqVKkpKStKECRMcNTExMVq1apWGDRummTNnqnbt2nrrrbd4Rx0AXAN4EhkoG4+5/OrJuPwKuBeXX83LUy9jAVfiqb+3bn35MAAAACoGoQ4AAMAECHUAALfjTiBcSzz195VQBwBwm5KPgjp16pSbRwKUXsnv6+8/Us3dPOblwwCA64+Pj4+sVqvjA+ODg4Mdn/MJeBrDMHTq1CkdOXJEVqv1qj5ezRUIdQAAtyr5FKCSYAd4OqvVesmPI3M3Qh0AwK28vLwUFRWl8PBwnTt3zt3DAa7Iz8/P487QlSDUAQA8go+Pj8f+YwlcC3hQAgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAr7uHgCAa9ucN+crr+CkS/vYkb1TnV3aAwBc+wh1AK5KXsFJde7V16V9ZIwa7tLjA4AZcPkVAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAGPCHUvvviivLy8NHToUEfbmTNnlJycrGrVqikkJEQ9evRQbm6u0345OTlKSEhQcHCwwsPDNWLECJ0/f96pZuPGjWrevLkCAgJUr149LV68uBJmBAAAULncHuq2b9+uN954Q02aNHFqHzZsmD777DMtX75cmzZt0qFDh9S9e3fH9qKiIiUkJKiwsFBbtmzR22+/rcWLF2vs2LGOmn379ikhIUEdOnRQZmamhg4dqieeeEJr166ttPkBAABUBreGuoKCAj3yyCOaP3++brjhBkf78ePHtWDBAk2fPl133XWXWrRooUWLFmnLli3aunWrJGndunXKzs7We++9p2bNmqlLly6aOHGiZs+ercLCQknSvHnzFBMTo2nTpqlhw4YaPHiwHnzwQc2YMcMt8wUAAHAVt4a65ORkJSQkKC4uzqk9PT1d586dc2pv0KCB6tatq7S0NElSWlqaGjdurIiICEdNfHy88vPztWPHDkfN748dHx/vOAYAAIBZ+Lqr4w8++EDffvuttm/fftE2u90uf39/Wa1Wp/aIiAjZ7XZHzYWBrmR7ybYr1eTn5+v06dMKCgq6qO+UlBSlpKQ4tR08eLBskwMAAKhkbgl1//vf//T0009r/fr1CgwMdMcQLisxMVGJiYlObd26dXPTaAAAAErHLZdf09PTdeTIETVv3ly+vr7y9fXVpk2b9Nprr8nX11cREREqLCxUXl6e0365ubmKjIyUJEVGRl70NGzJz39UY7FYLnmWDgAA4FrlllDXsWNHZWVlKTMz0/HVsmVLPfLII47v/fz8lJqa6thn165dysnJkc1mkyTZbDZlZWXpyJEjjpr169fLYrEoNjbWUXPhMUpqSo4BAABgFm65/Fq1alU1atTIqa1KlSqqVq2ao71fv34aPny4wsLCZLFY9NRTT8lms6l169aSpE6dOik2Nla9e/fW1KlTZbfbNWbMGCUnJysgIECSNHDgQL3++usaOXKkHn/8cW3YsEHLli3TqlWrKnfCAAAALua2ByX+yIwZM+Tt7a0ePXro7Nmzio+P15w5cxzbfXx8tHLlSg0aNEg2m01VqlRRUlKSJkyY4KiJiYnRqlWrNGzYMM2cOVO1a9fWW2+9pfj4eHdMCQAAwGU8JtRt3LjR6efAwEDNnj1bs2fPvuw+0dHRWr169RWP2759e2VkZFTEEAEAADyW2z9RAgAAAFePUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAAT8JjPfgVQ8ea8OV95BSdd2seO7J3q7NIeAAClQagDTCyv4KQ69+rr0j4yRg136fEBAKXD5VcAAAATINQBAACYAKEOAADABAh1AAAAJsCDEgDw/2VmZmry9Fdd2oc1pIqeHNDfpX0AuD4R6gDg/zO8fFz+tPCaJYtcenwA1y8uvwIAAJgAoQ4AAMAECHUAAAAmwD11AFCJeBgDgKsQ6gCgEvEwBgBX4fIrAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATMBtoW7u3Llq0qSJLBaLLBaLbDabPv/8c8f2M2fOKDk5WdWqVVNISIh69Oih3Nxcp2Pk5OQoISFBwcHBCg8P14gRI3T+/Hmnmo0bN6p58+YKCAhQvXr1tHjx4sqYHgAAQKVyW6irXbu2XnzxRaWnp+vf//637rrrLt13333asWOHJGnYsGH67LPPtHz5cm3atEmHDh1S9+7dHfsXFRUpISFBhYWF2rJli95++20tXrxYY8eOddTs27dPCQkJ6tChgzIzMzV06FA98cQTWrt2baXPFwAAwJV83dXxvffe6/TzpEmTNHfuXG3dulW1a9fWggULtGTJEt11112SpEWLFqlhw4baunWrWrdurXXr1ik7O1tffPGFIiIi1KxZM02cOFGjRo3S+PHj5e/vr3nz5ikmJkbTpk2TJDVs2FBff/21ZsyYofj4+EqfMwAAgKt4xD11RUVF+uCDD3Ty5EnZbDalp6fr3LlziouLc9Q0aNBAdevWVVpamiQpLS1NjRs3VkREhKMmPj5e+fn5jrN9aWlpTscoqSk5BgAAgFm47UydJGVlZclms+nMmTMKCQnRxx9/rNjYWGVmZsrf319Wq9WpPiIiQna7XZJkt9udAl3J9pJtV6rJz8/X6dOnFRQUdNGYUlJSlJKS4tR28ODBq5onAACAq7k11N18883KzMzU8ePH9eGHHyopKUmbNm1y55CUmJioxMREp7Zu3bq5aTQwszlvzldewUmX9rEje6c6u7QHAICncGuo8/f3V7169SRJLVq00Pbt2zVz5kw99NBDKiwsVF5entPZutzcXEVGRkqSIiMj9c033zgdr+Tp2Atrfv/EbG5uriwWyyXP0gGVKa/gpDr36uvSPjJGDXfp8QEAnsMj7qkrUVxcrLNnz6pFixby8/NTamqqY9uuXbuUk5Mjm80mSbLZbMrKytKRI0ccNevXr5fFYlFsbKyj5sJjlNSUHAMAAMAs3HambvTo0erSpYvq1q2rEydOaMmSJdq4caPWrl2r0NBQ9evXT8OHD1dYWJgsFoueeuop2Ww2tW7dWpLUqVMnxcbGqnfv3po6darsdrvGjBmj5ORkBQQESJIGDhyo119/XSNHjtTjjz+uDRs2aNmyZVq1apW7pg0AAOASbgt1R44cUZ8+fXT48GGFhoaqSZMmWrt2re6++25J0owZM+Tt7a0ePXro7Nmzio+P15w5cxz7+/j4aOXKlRo0aJBsNpuqVKmipKQkTZgwwVETExOjVatWadiwYZo5c6Zq166tt956i9eZAAAA03FbqFuwYMEVtwcGBmr27NmaPXv2ZWuio6O1evXqKx6nffv2ysjIKNcYAQAArhUedU8dAAAAyqdcoW758uU6d+5cRY8FAAAA5VSuUDdu3DhFRUVpyJAh+u677yp6TAAAACijcoW67OxsffbZZyosLFT79u116623atasWTp27FhFjw8AAAClUO576mw2m+bNmye73a5Ro0ZpwYIFqlWrlnr27KmNGzdW4BABAADwR67qQYlTp05p6dKleuONN/Tjjz+qV69eaty4sfr06aPk5OSKGiMAAAD+QLleafLVV19p0aJF+uc//6nGjRvr8ccf12effaaQkBBJv730NyYm5oqvIwEAAEDFKVeo69mzp/r06aNvvvlGDRo0uGh7jRo19Oyzz1714AAAAFA65Qp1H374oe64446L2rds2aLbb79dkgh1AAAAlahc99Tdc889l2zv2rXrVQ0GAAAA5VOuUGcYxkVtP//8s3x8fK56QAAAACi7Ml1+veGGG+Tl5aVTp04pLCzMaduJEyfUr1+/Ch0cAAAASqdMoW7FihUyDEP33HOPPv74Y0e7t7e3IiIiVL9+/QofIAAAAP5YmUJdu3btJEkHDhy46EwdAAAA3KfUoW7evHkaOHCgJOm99967bN2QIUOuflQAAAAok1KHuk8//dQR6i689HohLy8vQh0AAIAblDrUrV692vH9l19+6ZLBAAAAoHzK9UqT//3vf8rLy5MknT17VjNmzNDrr7+u8+fPV+TYAAAAUErl+kSJ7t27a8GCBbJarRo1apRSU1Pl5+enXbt2adasWRU9RgAAAPyBcoW6vXv3qnHjxpKkZcuWKS0tTSEhIWrUqBGhDgAAwA3KFeq8vLxUWFioXbt2yWKxKDo6WoZhqKCgoKLHBwAAgFIoV6hr3769evbsqaNHj+qBBx6Q9NvZu/Dw8AodHAAAAEqnXA9KLFiwQI0aNVJcXJzGjBkjSdq9ezevMwEAAHCTcp2pu+GGGzRp0iSntoSEhAoZEAAAAMquXKGuqKhI77//vtLT03XixAmnbQsXLqyQgQEAAKD0yhXqnnjiCaWmpqpz584KDQ2t6DEBAACgjMoV6j755BNlZ2crMjKyoscDAACAcijXgxLVqlWTxWKp6LEAAACgnMoV6saNG6cBAwZoz549ys/Pd/oCAABA5SvX5dc+ffpIkpYsWSIvLy9JkmEY8vLyUlFRUcWNDgAAAKVSrlC3b9++ih4HAAAArkK5Ql10dLSk387O2e12RUVFVeigAAAAUDbluqcuPz9fffr0UWBgoOrVqydJWrFihcaOHVuhgwMAAEDplCvUDRkyREVFRfr+++/l7+8vSWrdurWWLl1aoYMDAABA6ZTr8uuaNWu0b98+BQUFOR6UiIyMVG5uboUODgAAAKVTrjN1AQEBOn/+vFPb0aNHFRYWViGDAgAAQNmUK9Tde++9evLJJ3X8+HFJ0tmzZzVy5Eg98MADFTo4AAAAlE65Lr++9NJL6tu3r6pVq6bi4mKFhITo/vvv18yZMyt6fIBbzHlzvvIKTrq0jx3ZO9XZpT0AAK4n5Qp1VapU0bJly/TLL79o//79qlOnjiIiIip6bIDb5BWcVOdefV3aR8ao4S49PgDg+lLqUBcTE+N4KOJKfvzxx6saEAAAAMqu1KHu1VdfdXyfnZ2t+fPna+DAgYqOjtZPP/2kN998U/369XPFGAEAAPAHSh3q7rvvPsf3L7zwgtasWaP69es7bX/00Uc1evToih0hAAAA/lC5nn7dvXu346PCSkRHR2v37t0VMigAAACUTblCXevWrTVo0CAdPXpUkvTLL79o8ODBatWqVYUODgAAAKVTrlC3cOFC7d27V+Hh4apSpYoiIiK0e/duLVy4sKLHBwAAgFIo1ytNatWqpa+++koHDhzQoUOHVLNmTdWuXbuixwYAAIBSKleoK1G7dm3CHAAAgAco1+VXAAAAeBZCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEzAbaFuypQp+vOf/6yqVasqPDxc999/v3bt2uVUc+bMGSUnJ6tatWoKCQlRjx49lJub61STk5OjhIQEBQcHKzw8XCNGjND58+edajZu3KjmzZsrICBA9erV0+LFi109PQAAgErltlC3adMmJScna+vWrVq/fr3OnTunTp066eTJk46aYcOG6bPPPtPy5cu1adMmHTp0SN27d3dsLyoqUkJCggoLC7Vlyxa9/fbbWrx4scaOHeuo2bdvnxISEtShQwdlZmZq6NCheuKJJ7R27dpKnS8AAIAr+bqr4zVr1jj9vHjxYoWHhys9PV1t27bV8ePHtWDBAi1ZskR33XWXJGnRokVq2LChtm7dqtatW2vdunXKzs7WF198oYiICDVr1kwTJ07UqFGjNH78ePn7+2vevHmKiYnRtGnTJEkNGzbU119/rRkzZig+Pr7S5w0AAOAKHnNP3fHjxyVJYWFhkqT09HSdO3dOcXFxjpoGDRqobt26SktLkySlpaWpcePGioiIcNTEx8crPz9fO3bscNRceIySmpJjAAAAmIFHhLri4mINHTpUbdq0UaNGjSRJdrtd/v7+slqtTrURERGy2+2OmgsDXcn2km1XqsnPz9fp06ddMR0AAIBK57bLrxdKTk7W999/r6+//trdQ1FKSopSUlKc2g4ePOim0QAAAJSO20Pd4MGDtXLlSn311VeqXbu2oz0yMlKFhYXKy8tzOluXm5uryMhIR80333zjdLySp2MvrPn9E7O5ubmyWCwKCgq6aDyJiYlKTEx0auvWrVv5JwgAAFAJ3Hb51TAMDR48WB9//LE2bNigmJgYp+0tWrSQn5+fUlNTHW27du1STk6ObDabJMlmsykrK0tHjhxx1Kxfv14Wi0WxsbGOmguPUVJTcgwAAAAzcNuZuuTkZC1ZskSffPKJqlat6rgHLjQ0VEFBQQoNDVW/fv00fPhwhYWFyWKx6KmnnpLNZlPr1q0lSZ06dVJsbKx69+6tqVOnym63a8yYMUpOTlZAQIAkaeDAgXr99dc1cuRIPf7449qwYYOWLVumVatWuWvqAAAAFc5tZ+rmzp2r48ePq3379oqKinJ8LV261FEzY8YMde3aVT169FDbtm0VGRmpjz76yLHdx8dHK1eulI+Pj2w2mx599FH16dNHEyZMcNTExMRo1apVWr9+vZo2bapp06bprbfe4nUmAADAVNx2ps4wjD+sCQwM1OzZszV79uzL1kRHR2v16tVXPE779u2VkZFR5jECAABcKzzilSYAAAC4OoQ6AAAAEyDUAQAAmIDb31MHAKhYmZmZmjz9VZf2YQ2poicH9HdpHwDKhlAHACZjePmoc6++Lu1jzZJFLj0+gLLj8isAAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAR83T0AAMC1JzMzU5Onv+rSPqwhVfTkgP4u7QMwE0IdAKDMDC8fde7V16V9rFmyyKXHB8yGy68AAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABPwdfcAgLKa8+Z85RWcdGkfO7J3qrNLewAAoGIR6nDNySs4qc69+rq0j4xRw116fAAAKhqXXwEAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJuC2UPfVV1/p3nvvVc2aNeXl5aUVK1Y4bTcMQ2PHjlVUVJSCgoIUFxenPXv2ONUcO3ZMjzzyiCwWi6xWq/r166eCggKnmv/85z+68847FRgYqDp16mjq1KmunhoAAEClc1uoO3nypJo2barZs2dfcvvUqVP12muvad68edq2bZuqVKmi+Ph4nTlzxlHzyCOPaMeOHVq/fr1Wrlypr776SgMGDHBsz8/PV6dOnRQdHa309HS9/PLLGj9+vN58802Xzw8AAKAy+bqr4y5duqhLly6X3GYYhl599VWNGTNG9913nyTpnXfeUUREhFasWKGHH35YO3fu1Jo1a7R9+3a1bNlSkjRr1izdc889euWVV1SzZk29//77Kiws1MKFC+Xv769bbrlFmZmZmj59ulP4AwAAuNZ55D11+/btk91uV1xcnKMtNDRUrVq1UlpamiQpLS1NVqvVEegkKS4uTt7e3tq2bZujpm3btvL393fUxMfHa9euXfr1118raTYAAACu57YzdVdit9slSREREU7tERERjm12u13h4eFO2319fRUWFuZUExMTc9ExSrbdcMMNF/WdkpKilJQUp7aDBw9exWwAAABczyNDnTslJiYqMTHRqa1bt25uGg0AAEDpeGSoi4yMlCTl5uYqKirK0Z6bm6tmzZo5ao4cOeK03/nz53Xs2DHH/pGRkcrNzXWqKfm5pAYA4JkyMzM1efqrLu3DGlJFTw7o79I+gMrikaEuJiZGkZGRSk1NdYS4/Px8bdu2TYMGDZIk2Ww25eXlKT09XS1atJAkbdiwQcXFxWrVqpWj5rnnntO5c+fk5+cnSVq/fr1uvvnmS156BQB4DsPLR5179XVpH2uWLHLp8YHK5LYHJQoKCpSZmanMzExJvz0ckZmZqZycHHl5eWno0KF64YUX9OmnnyorK0t9+vRRzZo1df/990uSGjZsqM6dO6t///765ptvtHnzZg0ePFgPP/ywatasKUnq1auX/P391a9fP+3YsUNLly7VzJkzNXz4cDfNGgAAwDXcdqbu3//+tzp06OD4uSRoJSUlafHixRo5cqROnjypAQMGKC8vT3fccYfWrFmjwMBAxz7vv/++Bg8erI4dO8rb21s9evTQa6+95tgeGhqqdevWKTk5WS1atFD16tU1duxYXmcCAABMx22hrn379jIM47Lbvby8NGHCBE2YMOGyNWFhYVqyZMkV+2nSpIn+9a9/lXucAAAA1wKPfE8dAAAAysYjH5TAtWvOm/OVV3DSpX3syN6pzi7tAQCAaw+hDhUqr+Cky59WyxjFgy4AAPwel18BAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJ8PJhAMB1KzMzU5Onv+rSPqwhVfTkgP4u7QOQCHUAgOuY4eXj8k/BWbNkkUuPD5Tg8isAAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAEyDUAQAAmAChDgAAwAQIdQAAACZAqAMAADABQh0AAIAJEOoAAABMgFAHAABgAoQ6AAAAE/B19wBQOea8OV95BSdd3s+O7J3q7PJeAADA7xHqrhN5BSfVuVdfl/eTMWq4y/sAAAAX4/IrAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ4AAMAEfN09AEhz3pyvvIKTLu1jR/ZOdXZpDwCAS8nMzNTk6a+6tA9rSBU9OaC/S/uA5yPUeYC8gpPq3KuvS/vIGDXcpccHAFya4eXj8r/j1yxZ5NLj49rA5VcAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAn4unsAAAAAJea8OV95BSdd2oc1pIqeHNDfpX24w3UT6mbPnq2XX35ZdrtdTZs21axZs3Tbbbe5e1gAAOACeQUn1blXX5f2sWbJIpce312ui8uvS5cu1fDhwzVu3Dh9++23atq0qeLj43XkyBF3Dw0AAKBCXBehbvr06erfv7/69u2r2NhYzZs3T8HBwVq4cKG7hwYAAFAhTB/qCgsLlZ6erri4OEebt7e34uLilJaW5saRAQAAVBzTh7pffvlFRUVFioiIcGqPiIiQ3W5306gAAAAqlpdhGIa7B+FKhw4dUq1atbRlyxbZbDZH+8iRI7Vp0yZt27bNqT4lJUUpKSlObd9++62aN29eKeO9Xhw8eFC1atVy9zBMibV1LdbXtVhf12FtXevC9a1Vq5bmzp1b6WMw/dOv1atXl4+Pj3Jzc53ac3NzFRkZeVF9YmKiEhMTK2t4161u3brp008/dfcwTIm1dS3W17VYX9dhbV3LE9bX9Jdf/f391aJFC6WmpjraiouLlZqa6nTmDgAA4Fpm+jN1kjR8+HAlJSWpZcuWuu222/Tqq6/q5MmT6tvXte/BAQAAqCzXRah76KGH9PPPP2vs2LGy2+1q1qyZ1qxZc9HDEwAAANeq6yLUSdLgwYM1ePBgdw8D/x/3LboOa+tarK9rsb6uw9q6liesr+mffgUAALgemP5BCQAAgOsBoQ4AAMAECHUAAAAmQKjDH5o9e7ZuvPFGBQYGqlWrVvrmm2+uWL98+XI1aNBAgYGBaty4sVavXu203TAMjR07VlFRUQoKClJcXJz27NnjVHPs2DE98sgjslgsslqt6tevnwoKCpxq1q5dq9atW6tq1aqqUaOGevToof3791fInCuLp67tsmXL1KxZMwUHBys6Olovv/xyxUy4krljfSdNmqTbb79dwcHBslqtl+wnJydHCQkJCg4OVnh4uEaMGKHz589f1Vwrm6eu7ZAhQ9SiRQsFBASoWbNmVzNFt/LE9f3uu++UmJioOnXqKCgoSA0bNtTMmTOveq6VzRPX9ujRo+rcubNq1qypgIAA1alTR4MHD1Z+fn7ZJmcAV/DBBx8Y/v7+xsKFC40dO3YY/fv3N6xWq5Gbm3vJ+s2bNxs+Pj7G1KlTjezsbGPMmDGGn5+fkZWV5ah58cUXjdDQUGPFihXGd999Z3Tr1s2IiYkxTp8+7ajp3Lmz0bRpU2Pr1q3Gv/71L6NevXpGYmKiY/uPP/5oBAQEGKNHjzb27t1rpKenG23btjVuvfVW1y1GBfPUtV29erXh6+trzJ071/jhhx+MlStXGlFRUcasWbNctxgu4K71HTt2rDF9+nRj+PDhRmho6EX9nD9/3mjUqJERFxdnZGRkGKtXrzaqV69ujB49usLXwFU8dW0NwzCeeuop4/XXXzd69+5tNG3atCKnXWk8dX0XLFhgDBkyxNi4caPxww8/GO+++64RFBR0Tf3d4Klre+zYMWPOnDnG9u3bjf379xtffPGFcfPNNzv93VwahDpc0W233WYkJyc7fi4qKjJq1qxpTJky5ZL1PXv2NBISEpzaWrVqZfz1r381DMMwiouLjcjISOPll192bM/LyzMCAgKMlJQUwzAMIzs725BkbN++3VHz+eefG15eXsbBgwcNwzCM5cuXG76+vkZRUZGj5tNPPzW8vLyMwsLCq5x15fDUtU1MTDQefPBBp35ee+01o3bt2kZxcfFVzLhyuWN9L7Ro0aJL/uW9evVqw9vb27Db7Y62uXPnGhaLxTh79myZ5ugunrq2Fxo3btw1G+quhfUt8eSTTxodOnQoVa0nuJbWdubMmUbt2rVLVVuCy6+4rMLCQqWnpysuLs7R5u3trbi4OKWlpV1yn7S0NKd6SYqPj3fU79u3T3a73akmNDRUrVq1ctSkpaXJarWqZcuWjpq4uDh5e3tr27ZtkqQWLVrI29tbixYtUlFRkY4fP653331XcXFx8vPzq5gFcCFPXtuzZ88qMDDQqZ+goCAdOHBAP/3001XMuvK4a31LIy0tTY0bN3Z6+Xl8fLzy8/O1Y8eOUh/HXTx5bc3gWlvf48ePKyws7KqOUVmupbU9dOiQPvroI7Vr165M+xHqcFm//PKLioqKLvrkjYiICNnt9kvuY7fbr1hf8t8/qgkPD3fa7uvrq7CwMEdNTEyM1q1bp2effVYBAQGyWq06cOCAli1bVs7ZVi5PXtv4+Hh99NFHSk1NVXFxsXbv3q1p06ZJkg4fPlye6VY6d61vaVyunwv78GSevLZmcC2t75YtW7R06VINGDCg3MeoTNfC2iYmJio4OFi1atWSxWLRW2+9Vab9CXW4JtntdvXv319JSUnavn27Nm3aJH9/fz344IMyeJ/2Venfv78GDx6srl27yt/fX61bt9bDDz8s6bf/qwWA77//Xvfdd5/GjRunTp06uXs4pjFjxgx9++23+uSTT/TDDz9o+PDhZdqfv6FxWdWrV5ePj49yc3Od2nNzcxUZGXnJfSIjI69YX/LfP6o5cuSI0/bz58/r2LFjjprZs2crNDRUU6dO1a233qq2bdvqvffeU2pqquMyoifz5LX18vLSSy+9pIKCAv3000+y2+267bbbJEl/+tOfyjPdSueu9S2Ny/VzYR+ezJPX1gyuhfXNzs5Wx44dNWDAAI0ZM6bM+7vLtbC2kZGRatCggbp166Y33nhDc+fOLdMVEkIdLsvf318tWrRQamqqo624uFipqamy2WyX3MdmsznVS9L69esd9TExMYqMjHSqyc/P17Zt2xw1NptNeXl5Sk9Pd9Rs2LBBxcXFatWqlSTp1KlTF5018vHxcYzR03ny2pbw8fFRrVq15O/vr5SUFNlsNtWoUePqJl5J3LW+pWGz2ZSVleUUrtevXy+LxaLY2NhSH8ddPHltzcDT13fHjh3q0KGDkpKSNGnSpDLt626evra/V/Jv2dmzZ0u/U5keq8B154MPPjACAgKMxYsXG9nZ2caAAQMMq9XqeHKvd+/ext///ndH/ebNmw1fX1/jlVdeMXbu3GmMGzfuko9/W61W45NPPjH+85//GPfdd98lX7tx6623Gtu2bTO+/vpr46abbnJ6tDs1NdXw8vIy/vGPfxi7d+820tPTjfj4eCM6Oto4depUJazM1fPUtf3555+NuXPnGjt37jQyMjKMIUOGGIGBgca2bdsqYVUqjrvW96effjIyMjKMf/zjH0ZISIiRkZFhZGRkGCdOnDAM4/9eadKpUycjMzPTWLNmjVGjRo1r7pUmnri2hmEYe/bsMTIyMoy//vWvRv369R0118qTxYbhueublZVl1KhRw3j00UeNw4cPO76OHDlSSStz9Tx1bVetWmUsXLjQyMrKMvbt22esXLnSaNiwodGmTZsyzY9Qhz80a9Yso27duoa/v79x2223GVu3bnVsa9eunZGUlORUv2zZMqN+/fqGv7+/ccsttxirVq1y2l5cXGw8//zzRkREhBEQEGB07NjR2LVrl1PN0aNHjcTERCMkJMSwWCxG3759nf7iNgzDSElJMW699VajSpUqRo0aNYxu3boZO3furNjJu5gnru3PP/9stG7d2qhSpYoRHBxsdOzY0Wlc1xJ3rG9SUpIh6aKvL7/80lGzf/9+o0uXLkZQUJBRvXp1429/+5tx7ty5Cp+/K3nq2rZr1+6SNfv27avoJXApT1zfcePGXXJ7dHS0K5bAZTxxbTds2GDYbDYjNDTUCAwMNG666SZj1KhRxq+//lqmuXkZBneVAwAAXOu4pw4AAMAECHUAAAAmQKgDAAAwAUIdAACACRDqAAAATIBQBwAAYAKEOgAAABMg1AEAAJgAoQ6AKezfv19eXl7Ky8tzyfEHDhyoUaNGueTYAFARCHUA3Kp9+/YKCAhQSEiI42vOnDnuHtZF5s2bp5deesndwwCAy/J19wAA4KWXXtLQoUOvWHPu3Dn5+flVzoCuUawRcH3jTB0Aj/TYY4+pX79+6tmzpywWi+bNm6cTJ05owIABioqKUlRUlAYOHKiTJ0867bd8+XLdeOONqlatmp588kkVFhY6tn377bfq0KGDwsLCVK9ePc2fP9+xbfz48br33ns1ePBgWa1W1a1bV0uXLnUaT0nw3Lhxo6xWq1O/999/v8aPHy9JOnbsmB544AHdcMMNslqtatGihX766adLzrN9+/YaPXq04uPjVbVqVTVv3lxZWVmO7bm5uerZs6dq1KihunXr6rnnntP58+edxjF37lzVrVtXt99+uxYvXqxmzZpp7Nixql69uiIjI7V06VJt3rxZjRo1UmhoqPr166fi4uIy/5kA8GyEOgAeKyUlRf369VNeXp769eunp59+Wnv37tX333+vrKws/fe//9WwYcOc9vn444+VmZmprKwsbdmyRVOmTJEk2e123X333Ro0aJB+/vlnrVixQuPGjVNqaqpj37Vr16pt27Y6evSoXnjhBT3xxBM6ceJEmcf9yiuv6Pz58zp48KCOHj2qBQsWqGrVqpetf/fddzV16lT9+uuvatmypZ566inHtl69esnPz0/79u3Tv/71L61YsUJTp051bD9x4oS+++47/fe//9WmTZskSd9//72qV68uu92uSZMmacCAAZo5c6Y2bdqknTt3auXKlVqxYkWZ5wXAsxHqALjd6NGjZbVaHV8lZ986deqk+Ph4eXt7KzAwUO+//76mTJmiatWqqXr16po8ebLeeecdp7NO48ePl9VqVc2aNTV69Gi9++67kn4LTm3btlXPnj3l4+OjRo0aqW/fvlqyZIlj3+bNmzu29+7dW4WFhdq9e3eZ5+Pn56ejR49qz5498vHxUbNmzRQWFnbZ+kcffVRNmzaVr6+vkpKSlJ6eLkk6ePCgNmzYoOnTpyskJETR0dF67rnntHjxYse+xcXFevHFFxUcHKzg4GBJUo0aNTRkyBD5+voqMTFR+fn56tevn6pVq6aaNWuqXbt2+vbbb8s8LwCejXvqALjdlClTLnlPXd26dR3f//zzzyosLNSNN97oaPvTn/6ks2fP6pdffnG0RUdHO31/8OBBSb89Hbt69Wqny6ZFRUW68847HT9HRkY6vvfy8lJQUFC5ztSNGDFCZ86cUc+ePXX8+HE99NBDevHFFxUUFHTJ+gv7rVKligoKCiRJBw4cUGBgoCIiIpzmfODAAcfPVatWvehS8IX1JUHv920lfQAwD87UAfBY3t7/91dUjRo15O/vr/379zva9u/fr4CAAFWvXt3RduG9azk5OapVq5YkqU6dOnrggQeUl5fn+Dpx4oRWr15d5nGFhITo9OnTMgzD0Xb48GGn7S+99JJ27dqltLQ0paamluuJ3tq1a+vMmTPKzc11tO3fv1+1a9d2/HzhGgG4vvG3AYBrgre3t3r16qXnnntOx44d09GjR/Xss8+qd+/eTsFmwoQJysvL06FDhzRlyhQ98sgjkqTevXtrw4YN+uc//6lz587p3LlzyszM1Pbt28s8lvr168vPz09LlixRUVGRUlJSlJGR4di+cuVK7d69W8XFxbJYLPLz85Ovb9kvjNSqVUsdOnTQM888o5MnTyonJ0eTJk1SUlJSmY8FwPwIdQCuGTNnztSNN96o2NhY3XLLLapXr56mT5/uVHPfffepWbNmatSokVq1aqVnn31W0m8Bae3atXrjjTcUFRWliIgIJScnKz8/v8zjsFgsmj9/vv7+97+rWrVq2rx5s+Lj4x3b9+7dq86dO6tq1aqKjY2VzWbToEGDyjXnJUuW6PTp04qOjlabNm2UkJCgkSNHlutYAMzNy7jw+gEAAACuSZypAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAChDoAAAATINQBAACYAKEOAADABAh1AAAAJkCoAwAAMAFCHQAAgAkQ6gAAAEyAUAcAAGAC/w+DGFl7nqrQlQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "diff_sample = torch.concat([diff_sample_1, diff_sample_2], dim=0)\n",
    "fn = torch.norm(diff_sample, p='fro', dim=(1, 2))\n",
    "\n",
    "fig, ax = plt.subplots()  # width, height in inches\n",
    "# Plot histogram\n",
    "ax.hist(fn.detach().cpu().numpy(),\n",
    "        bins='fd',                     # Freedman–Diaconis rule\n",
    "        density=True,                  # or density=True for area=1\n",
    "        facecolor='lightblue',                # light gray\n",
    "        edgecolor='black', \n",
    "        alpha=0.6,\n",
    "        linewidth=0.5, label=\"Diffusion samples\")\n",
    "\n",
    "\"\"\"\n",
    "ax.hist(fn_true.detach().cpu().numpy(),\n",
    "        bins='fd',                     # Freedman–Diaconis rule\n",
    "        density=True,                  # or density=True for area=1\n",
    "        facecolor='lightgreen',                # light gray\n",
    "        edgecolor='black', \n",
    "        alpha=0.6,\n",
    "        linewidth=0.5, label=\"True samples\")\n",
    "\"\"\"\n",
    "\n",
    "# Clean up spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['left'].set_linewidth(0.5)\n",
    "ax.spines['bottom'].set_linewidth(0.5)\n",
    "\n",
    "# Ticks outside\n",
    "ax.tick_params(direction='out', width=0.5)\n",
    "\n",
    "# Labels (match your manuscript font & size)\n",
    "ax.set_xlabel('Frobenius norm', fontsize=9)\n",
    "ax.set_ylabel('density', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.savefig('n=350 histogram.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d304a5-d0c5-437e-82ed-117ac4cd6883",
   "metadata": {},
   "source": [
    "# Inspect the model on noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4f12c419-d3b1-45c4-bc9e-335f5f6c731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_noise(model, n, time_step, terminal_time, num_samples=50, device=device):\n",
    "    t = 0\n",
    "    norm_ls = []\n",
    "    while t <= terminal_time:\n",
    "        if t == 0:\n",
    "            t += time_step\n",
    "\n",
    "        else:\n",
    "            # Generate scaled noise\n",
    "            noise_scaled = torch.randn((num_samples, n, n), device=device) * (t ** (-1/2))\n",
    "            noise_scaled_symm = (1/2) * (noise_scaled + noise_scaled.transpose(-1, -2))\n",
    "            noise_scaled_symm_expand = noise_scaled_symm.unsqueeze(-1)\n",
    "\n",
    "            # Create time array\n",
    "            time_array = torch.ones((num_samples, 1, 1), device=device) * t\n",
    "\n",
    "            # Generate random embedding\n",
    "            # Generate random node embedding\n",
    "            x_init = torch.randn((num_samples, n), device=device)\n",
    "            x_init = x_init / x_init.norm(dim=1, keepdim=True)\n",
    "            x_init_expand = x_init.unsqueeze(-1)\n",
    "\n",
    "            # Get output\n",
    "            out = model.predict(x_init_expand, noise_scaled_symm_expand, time_array)\n",
    "\n",
    "            # Get Frobenius norm\n",
    "            norms_sq = torch.norm(out, p=\"fro\", dim=(1, 2)) ** 2\n",
    "\n",
    "            # Get average\n",
    "            norms_sq_avg = torch.mean(norms_sq)\n",
    "\n",
    "            # Store value\n",
    "            norm_ls.append(norms_sq_avg.item())\n",
    "\n",
    "            # Increment t\n",
    "            t += time_step\n",
    "\n",
    "\n",
    "        # Print update\n",
    "        print(\"Time {} updated!\".format(t))\n",
    "\n",
    "\n",
    "    return norm_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8992082-7b93-4bfc-8e5c-57a0e8e8b526",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.5 updated!\n",
      "Time 1.0 updated!\n",
      "Time 1.5 updated!\n",
      "Time 2.0 updated!\n",
      "Time 2.5 updated!\n",
      "Time 3.0 updated!\n",
      "Time 3.5 updated!\n",
      "Time 4.0 updated!\n",
      "Time 4.5 updated!\n",
      "Time 5.0 updated!\n",
      "Time 5.5 updated!\n",
      "Time 6.0 updated!\n",
      "Time 6.5 updated!\n",
      "Time 7.0 updated!\n",
      "Time 7.5 updated!\n",
      "Time 8.0 updated!\n",
      "Time 8.5 updated!\n",
      "Time 9.0 updated!\n",
      "Time 9.5 updated!\n",
      "Time 10.0 updated!\n",
      "Time 10.5 updated!\n",
      "Time 11.0 updated!\n",
      "Time 11.5 updated!\n",
      "Time 12.0 updated!\n",
      "Time 12.5 updated!\n",
      "Time 13.0 updated!\n",
      "Time 13.5 updated!\n",
      "Time 14.0 updated!\n",
      "Time 14.5 updated!\n",
      "Time 15.0 updated!\n",
      "Time 15.5 updated!\n",
      "Time 16.0 updated!\n",
      "Time 16.5 updated!\n",
      "Time 17.0 updated!\n",
      "Time 17.5 updated!\n",
      "Time 18.0 updated!\n",
      "Time 18.5 updated!\n",
      "Time 19.0 updated!\n",
      "Time 19.5 updated!\n",
      "Time 20.0 updated!\n",
      "Time 20.5 updated!\n",
      "Time 21.0 updated!\n",
      "Time 21.5 updated!\n",
      "Time 22.0 updated!\n",
      "Time 22.5 updated!\n",
      "Time 23.0 updated!\n",
      "Time 23.5 updated!\n",
      "Time 24.0 updated!\n",
      "Time 24.5 updated!\n",
      "Time 25.0 updated!\n",
      "Time 25.5 updated!\n",
      "Time 26.0 updated!\n",
      "Time 26.5 updated!\n",
      "Time 27.0 updated!\n",
      "Time 27.5 updated!\n",
      "Time 28.0 updated!\n",
      "Time 28.5 updated!\n",
      "Time 29.0 updated!\n",
      "Time 29.5 updated!\n",
      "Time 30.0 updated!\n",
      "Time 30.5 updated!\n",
      "Time 31.0 updated!\n",
      "Time 31.5 updated!\n",
      "Time 32.0 updated!\n",
      "Time 32.5 updated!\n",
      "Time 33.0 updated!\n",
      "Time 33.5 updated!\n",
      "Time 34.0 updated!\n",
      "Time 34.5 updated!\n",
      "Time 35.0 updated!\n",
      "Time 35.5 updated!\n",
      "Time 36.0 updated!\n",
      "Time 36.5 updated!\n",
      "Time 37.0 updated!\n",
      "Time 37.5 updated!\n",
      "Time 38.0 updated!\n",
      "Time 38.5 updated!\n",
      "Time 39.0 updated!\n",
      "Time 39.5 updated!\n",
      "Time 40.0 updated!\n",
      "Time 40.5 updated!\n",
      "Time 41.0 updated!\n",
      "Time 41.5 updated!\n",
      "Time 42.0 updated!\n",
      "Time 42.5 updated!\n",
      "Time 43.0 updated!\n",
      "Time 43.5 updated!\n",
      "Time 44.0 updated!\n",
      "Time 44.5 updated!\n",
      "Time 45.0 updated!\n",
      "Time 45.5 updated!\n",
      "Time 46.0 updated!\n",
      "Time 46.5 updated!\n",
      "Time 47.0 updated!\n",
      "Time 47.5 updated!\n",
      "Time 48.0 updated!\n",
      "Time 48.5 updated!\n",
      "Time 49.0 updated!\n",
      "Time 49.5 updated!\n",
      "Time 50.0 updated!\n",
      "Time 50.5 updated!\n",
      "Time 51.0 updated!\n",
      "Time 51.5 updated!\n",
      "Time 52.0 updated!\n",
      "Time 52.5 updated!\n",
      "Time 53.0 updated!\n",
      "Time 53.5 updated!\n",
      "Time 54.0 updated!\n",
      "Time 54.5 updated!\n",
      "Time 55.0 updated!\n",
      "Time 55.5 updated!\n",
      "Time 56.0 updated!\n",
      "Time 56.5 updated!\n",
      "Time 57.0 updated!\n",
      "Time 57.5 updated!\n",
      "Time 58.0 updated!\n",
      "Time 58.5 updated!\n",
      "Time 59.0 updated!\n",
      "Time 59.5 updated!\n",
      "Time 60.0 updated!\n",
      "Time 60.5 updated!\n",
      "Time 61.0 updated!\n",
      "Time 61.5 updated!\n",
      "Time 62.0 updated!\n",
      "Time 62.5 updated!\n",
      "Time 63.0 updated!\n",
      "Time 63.5 updated!\n",
      "Time 64.0 updated!\n",
      "Time 64.5 updated!\n",
      "Time 65.0 updated!\n",
      "Time 65.5 updated!\n",
      "Time 66.0 updated!\n",
      "Time 66.5 updated!\n",
      "Time 67.0 updated!\n",
      "Time 67.5 updated!\n",
      "Time 68.0 updated!\n",
      "Time 68.5 updated!\n",
      "Time 69.0 updated!\n",
      "Time 69.5 updated!\n",
      "Time 70.0 updated!\n",
      "Time 70.5 updated!\n",
      "Time 71.0 updated!\n",
      "Time 71.5 updated!\n",
      "Time 72.0 updated!\n",
      "Time 72.5 updated!\n",
      "Time 73.0 updated!\n",
      "Time 73.5 updated!\n",
      "Time 74.0 updated!\n",
      "Time 74.5 updated!\n",
      "Time 75.0 updated!\n",
      "Time 75.5 updated!\n",
      "Time 76.0 updated!\n",
      "Time 76.5 updated!\n",
      "Time 77.0 updated!\n",
      "Time 77.5 updated!\n",
      "Time 78.0 updated!\n",
      "Time 78.5 updated!\n",
      "Time 79.0 updated!\n",
      "Time 79.5 updated!\n",
      "Time 80.0 updated!\n",
      "Time 80.5 updated!\n",
      "Time 81.0 updated!\n",
      "Time 81.5 updated!\n",
      "Time 82.0 updated!\n",
      "Time 82.5 updated!\n",
      "Time 83.0 updated!\n",
      "Time 83.5 updated!\n",
      "Time 84.0 updated!\n",
      "Time 84.5 updated!\n",
      "Time 85.0 updated!\n",
      "Time 85.5 updated!\n",
      "Time 86.0 updated!\n",
      "Time 86.5 updated!\n",
      "Time 87.0 updated!\n",
      "Time 87.5 updated!\n",
      "Time 88.0 updated!\n",
      "Time 88.5 updated!\n",
      "Time 89.0 updated!\n",
      "Time 89.5 updated!\n",
      "Time 90.0 updated!\n",
      "Time 90.5 updated!\n",
      "Time 91.0 updated!\n",
      "Time 91.5 updated!\n",
      "Time 92.0 updated!\n",
      "Time 92.5 updated!\n",
      "Time 93.0 updated!\n",
      "Time 93.5 updated!\n",
      "Time 94.0 updated!\n",
      "Time 94.5 updated!\n",
      "Time 95.0 updated!\n",
      "Time 95.5 updated!\n",
      "Time 96.0 updated!\n",
      "Time 96.5 updated!\n",
      "Time 97.0 updated!\n",
      "Time 97.5 updated!\n",
      "Time 98.0 updated!\n",
      "Time 98.5 updated!\n",
      "Time 99.0 updated!\n",
      "Time 99.5 updated!\n",
      "Time 100.0 updated!\n",
      "Time 100.5 updated!\n",
      "Time 101.0 updated!\n",
      "Time 101.5 updated!\n",
      "Time 102.0 updated!\n",
      "Time 102.5 updated!\n",
      "Time 103.0 updated!\n",
      "Time 103.5 updated!\n",
      "Time 104.0 updated!\n",
      "Time 104.5 updated!\n",
      "Time 105.0 updated!\n",
      "Time 105.5 updated!\n",
      "Time 106.0 updated!\n",
      "Time 106.5 updated!\n",
      "Time 107.0 updated!\n",
      "Time 107.5 updated!\n",
      "Time 108.0 updated!\n",
      "Time 108.5 updated!\n",
      "Time 109.0 updated!\n",
      "Time 109.5 updated!\n",
      "Time 110.0 updated!\n",
      "Time 110.5 updated!\n",
      "Time 111.0 updated!\n",
      "Time 111.5 updated!\n",
      "Time 112.0 updated!\n",
      "Time 112.5 updated!\n",
      "Time 113.0 updated!\n",
      "Time 113.5 updated!\n",
      "Time 114.0 updated!\n",
      "Time 114.5 updated!\n",
      "Time 115.0 updated!\n",
      "Time 115.5 updated!\n",
      "Time 116.0 updated!\n",
      "Time 116.5 updated!\n",
      "Time 117.0 updated!\n",
      "Time 117.5 updated!\n",
      "Time 118.0 updated!\n",
      "Time 118.5 updated!\n",
      "Time 119.0 updated!\n",
      "Time 119.5 updated!\n",
      "Time 120.0 updated!\n",
      "Time 120.5 updated!\n",
      "Time 121.0 updated!\n",
      "Time 121.5 updated!\n",
      "Time 122.0 updated!\n",
      "Time 122.5 updated!\n",
      "Time 123.0 updated!\n",
      "Time 123.5 updated!\n",
      "Time 124.0 updated!\n",
      "Time 124.5 updated!\n",
      "Time 125.0 updated!\n",
      "Time 125.5 updated!\n",
      "Time 126.0 updated!\n",
      "Time 126.5 updated!\n",
      "Time 127.0 updated!\n",
      "Time 127.5 updated!\n",
      "Time 128.0 updated!\n",
      "Time 128.5 updated!\n",
      "Time 129.0 updated!\n",
      "Time 129.5 updated!\n",
      "Time 130.0 updated!\n",
      "Time 130.5 updated!\n",
      "Time 131.0 updated!\n",
      "Time 131.5 updated!\n",
      "Time 132.0 updated!\n",
      "Time 132.5 updated!\n",
      "Time 133.0 updated!\n",
      "Time 133.5 updated!\n",
      "Time 134.0 updated!\n",
      "Time 134.5 updated!\n",
      "Time 135.0 updated!\n",
      "Time 135.5 updated!\n",
      "Time 136.0 updated!\n",
      "Time 136.5 updated!\n",
      "Time 137.0 updated!\n",
      "Time 137.5 updated!\n",
      "Time 138.0 updated!\n",
      "Time 138.5 updated!\n",
      "Time 139.0 updated!\n",
      "Time 139.5 updated!\n",
      "Time 140.0 updated!\n",
      "Time 140.5 updated!\n",
      "Time 141.0 updated!\n",
      "Time 141.5 updated!\n",
      "Time 142.0 updated!\n",
      "Time 142.5 updated!\n",
      "Time 143.0 updated!\n",
      "Time 143.5 updated!\n",
      "Time 144.0 updated!\n",
      "Time 144.5 updated!\n",
      "Time 145.0 updated!\n",
      "Time 145.5 updated!\n",
      "Time 146.0 updated!\n",
      "Time 146.5 updated!\n",
      "Time 147.0 updated!\n",
      "Time 147.5 updated!\n",
      "Time 148.0 updated!\n",
      "Time 148.5 updated!\n",
      "Time 149.0 updated!\n",
      "Time 149.5 updated!\n",
      "Time 150.0 updated!\n",
      "Time 150.5 updated!\n",
      "Time 151.0 updated!\n",
      "Time 151.5 updated!\n",
      "Time 152.0 updated!\n",
      "Time 152.5 updated!\n",
      "Time 153.0 updated!\n",
      "Time 153.5 updated!\n",
      "Time 154.0 updated!\n",
      "Time 154.5 updated!\n",
      "Time 155.0 updated!\n",
      "Time 155.5 updated!\n",
      "Time 156.0 updated!\n",
      "Time 156.5 updated!\n",
      "Time 157.0 updated!\n",
      "Time 157.5 updated!\n",
      "Time 158.0 updated!\n",
      "Time 158.5 updated!\n",
      "Time 159.0 updated!\n",
      "Time 159.5 updated!\n",
      "Time 160.0 updated!\n",
      "Time 160.5 updated!\n",
      "Time 161.0 updated!\n",
      "Time 161.5 updated!\n",
      "Time 162.0 updated!\n",
      "Time 162.5 updated!\n",
      "Time 163.0 updated!\n",
      "Time 163.5 updated!\n",
      "Time 164.0 updated!\n",
      "Time 164.5 updated!\n",
      "Time 165.0 updated!\n",
      "Time 165.5 updated!\n",
      "Time 166.0 updated!\n",
      "Time 166.5 updated!\n",
      "Time 167.0 updated!\n",
      "Time 167.5 updated!\n",
      "Time 168.0 updated!\n",
      "Time 168.5 updated!\n",
      "Time 169.0 updated!\n",
      "Time 169.5 updated!\n",
      "Time 170.0 updated!\n",
      "Time 170.5 updated!\n",
      "Time 171.0 updated!\n",
      "Time 171.5 updated!\n",
      "Time 172.0 updated!\n",
      "Time 172.5 updated!\n",
      "Time 173.0 updated!\n",
      "Time 173.5 updated!\n",
      "Time 174.0 updated!\n",
      "Time 174.5 updated!\n",
      "Time 175.0 updated!\n",
      "Time 175.5 updated!\n",
      "Time 176.0 updated!\n",
      "Time 176.5 updated!\n",
      "Time 177.0 updated!\n",
      "Time 177.5 updated!\n",
      "Time 178.0 updated!\n",
      "Time 178.5 updated!\n",
      "Time 179.0 updated!\n",
      "Time 179.5 updated!\n",
      "Time 180.0 updated!\n",
      "Time 180.5 updated!\n",
      "Time 181.0 updated!\n",
      "Time 181.5 updated!\n",
      "Time 182.0 updated!\n",
      "Time 182.5 updated!\n",
      "Time 183.0 updated!\n",
      "Time 183.5 updated!\n",
      "Time 184.0 updated!\n",
      "Time 184.5 updated!\n",
      "Time 185.0 updated!\n",
      "Time 185.5 updated!\n",
      "Time 186.0 updated!\n",
      "Time 186.5 updated!\n",
      "Time 187.0 updated!\n",
      "Time 187.5 updated!\n",
      "Time 188.0 updated!\n",
      "Time 188.5 updated!\n",
      "Time 189.0 updated!\n",
      "Time 189.5 updated!\n",
      "Time 190.0 updated!\n",
      "Time 190.5 updated!\n",
      "Time 191.0 updated!\n",
      "Time 191.5 updated!\n",
      "Time 192.0 updated!\n",
      "Time 192.5 updated!\n",
      "Time 193.0 updated!\n",
      "Time 193.5 updated!\n",
      "Time 194.0 updated!\n",
      "Time 194.5 updated!\n",
      "Time 195.0 updated!\n",
      "Time 195.5 updated!\n",
      "Time 196.0 updated!\n",
      "Time 196.5 updated!\n",
      "Time 197.0 updated!\n",
      "Time 197.5 updated!\n",
      "Time 198.0 updated!\n",
      "Time 198.5 updated!\n",
      "Time 199.0 updated!\n",
      "Time 199.5 updated!\n",
      "Time 200.0 updated!\n",
      "Time 200.5 updated!\n",
      "Time 201.0 updated!\n",
      "Time 201.5 updated!\n",
      "Time 202.0 updated!\n",
      "Time 202.5 updated!\n",
      "Time 203.0 updated!\n",
      "Time 203.5 updated!\n",
      "Time 204.0 updated!\n",
      "Time 204.5 updated!\n",
      "Time 205.0 updated!\n",
      "Time 205.5 updated!\n",
      "Time 206.0 updated!\n",
      "Time 206.5 updated!\n",
      "Time 207.0 updated!\n",
      "Time 207.5 updated!\n",
      "Time 208.0 updated!\n",
      "Time 208.5 updated!\n",
      "Time 209.0 updated!\n",
      "Time 209.5 updated!\n",
      "Time 210.0 updated!\n",
      "Time 210.5 updated!\n",
      "Time 211.0 updated!\n",
      "Time 211.5 updated!\n",
      "Time 212.0 updated!\n",
      "Time 212.5 updated!\n",
      "Time 213.0 updated!\n",
      "Time 213.5 updated!\n",
      "Time 214.0 updated!\n",
      "Time 214.5 updated!\n",
      "Time 215.0 updated!\n",
      "Time 215.5 updated!\n",
      "Time 216.0 updated!\n",
      "Time 216.5 updated!\n",
      "Time 217.0 updated!\n",
      "Time 217.5 updated!\n",
      "Time 218.0 updated!\n",
      "Time 218.5 updated!\n",
      "Time 219.0 updated!\n",
      "Time 219.5 updated!\n",
      "Time 220.0 updated!\n",
      "Time 220.5 updated!\n",
      "Time 221.0 updated!\n",
      "Time 221.5 updated!\n",
      "Time 222.0 updated!\n",
      "Time 222.5 updated!\n",
      "Time 223.0 updated!\n",
      "Time 223.5 updated!\n",
      "Time 224.0 updated!\n",
      "Time 224.5 updated!\n",
      "Time 225.0 updated!\n",
      "Time 225.5 updated!\n",
      "Time 226.0 updated!\n",
      "Time 226.5 updated!\n",
      "Time 227.0 updated!\n",
      "Time 227.5 updated!\n",
      "Time 228.0 updated!\n",
      "Time 228.5 updated!\n",
      "Time 229.0 updated!\n",
      "Time 229.5 updated!\n",
      "Time 230.0 updated!\n",
      "Time 230.5 updated!\n",
      "Time 231.0 updated!\n",
      "Time 231.5 updated!\n",
      "Time 232.0 updated!\n",
      "Time 232.5 updated!\n",
      "Time 233.0 updated!\n",
      "Time 233.5 updated!\n",
      "Time 234.0 updated!\n",
      "Time 234.5 updated!\n",
      "Time 235.0 updated!\n",
      "Time 235.5 updated!\n",
      "Time 236.0 updated!\n",
      "Time 236.5 updated!\n",
      "Time 237.0 updated!\n",
      "Time 237.5 updated!\n",
      "Time 238.0 updated!\n",
      "Time 238.5 updated!\n",
      "Time 239.0 updated!\n",
      "Time 239.5 updated!\n",
      "Time 240.0 updated!\n",
      "Time 240.5 updated!\n",
      "Time 241.0 updated!\n",
      "Time 241.5 updated!\n",
      "Time 242.0 updated!\n",
      "Time 242.5 updated!\n",
      "Time 243.0 updated!\n",
      "Time 243.5 updated!\n",
      "Time 244.0 updated!\n",
      "Time 244.5 updated!\n",
      "Time 245.0 updated!\n",
      "Time 245.5 updated!\n",
      "Time 246.0 updated!\n",
      "Time 246.5 updated!\n",
      "Time 247.0 updated!\n",
      "Time 247.5 updated!\n",
      "Time 248.0 updated!\n",
      "Time 248.5 updated!\n",
      "Time 249.0 updated!\n",
      "Time 249.5 updated!\n",
      "Time 250.0 updated!\n",
      "Time 250.5 updated!\n",
      "Time 251.0 updated!\n",
      "Time 251.5 updated!\n",
      "Time 252.0 updated!\n",
      "Time 252.5 updated!\n",
      "Time 253.0 updated!\n",
      "Time 253.5 updated!\n",
      "Time 254.0 updated!\n",
      "Time 254.5 updated!\n",
      "Time 255.0 updated!\n",
      "Time 255.5 updated!\n",
      "Time 256.0 updated!\n",
      "Time 256.5 updated!\n",
      "Time 257.0 updated!\n",
      "Time 257.5 updated!\n",
      "Time 258.0 updated!\n",
      "Time 258.5 updated!\n",
      "Time 259.0 updated!\n",
      "Time 259.5 updated!\n",
      "Time 260.0 updated!\n",
      "Time 260.5 updated!\n",
      "Time 261.0 updated!\n",
      "Time 261.5 updated!\n",
      "Time 262.0 updated!\n",
      "Time 262.5 updated!\n",
      "Time 263.0 updated!\n",
      "Time 263.5 updated!\n",
      "Time 264.0 updated!\n",
      "Time 264.5 updated!\n",
      "Time 265.0 updated!\n",
      "Time 265.5 updated!\n",
      "Time 266.0 updated!\n",
      "Time 266.5 updated!\n",
      "Time 267.0 updated!\n",
      "Time 267.5 updated!\n",
      "Time 268.0 updated!\n",
      "Time 268.5 updated!\n",
      "Time 269.0 updated!\n",
      "Time 269.5 updated!\n",
      "Time 270.0 updated!\n",
      "Time 270.5 updated!\n",
      "Time 271.0 updated!\n",
      "Time 271.5 updated!\n",
      "Time 272.0 updated!\n",
      "Time 272.5 updated!\n",
      "Time 273.0 updated!\n",
      "Time 273.5 updated!\n",
      "Time 274.0 updated!\n",
      "Time 274.5 updated!\n",
      "Time 275.0 updated!\n",
      "Time 275.5 updated!\n",
      "Time 276.0 updated!\n",
      "Time 276.5 updated!\n",
      "Time 277.0 updated!\n",
      "Time 277.5 updated!\n",
      "Time 278.0 updated!\n",
      "Time 278.5 updated!\n",
      "Time 279.0 updated!\n",
      "Time 279.5 updated!\n",
      "Time 280.0 updated!\n",
      "Time 280.5 updated!\n",
      "Time 281.0 updated!\n",
      "Time 281.5 updated!\n",
      "Time 282.0 updated!\n",
      "Time 282.5 updated!\n",
      "Time 283.0 updated!\n",
      "Time 283.5 updated!\n",
      "Time 284.0 updated!\n",
      "Time 284.5 updated!\n",
      "Time 285.0 updated!\n",
      "Time 285.5 updated!\n",
      "Time 286.0 updated!\n",
      "Time 286.5 updated!\n",
      "Time 287.0 updated!\n",
      "Time 287.5 updated!\n",
      "Time 288.0 updated!\n",
      "Time 288.5 updated!\n",
      "Time 289.0 updated!\n",
      "Time 289.5 updated!\n",
      "Time 290.0 updated!\n",
      "Time 290.5 updated!\n",
      "Time 291.0 updated!\n",
      "Time 291.5 updated!\n",
      "Time 292.0 updated!\n",
      "Time 292.5 updated!\n",
      "Time 293.0 updated!\n",
      "Time 293.5 updated!\n",
      "Time 294.0 updated!\n",
      "Time 294.5 updated!\n",
      "Time 295.0 updated!\n",
      "Time 295.5 updated!\n",
      "Time 296.0 updated!\n",
      "Time 296.5 updated!\n",
      "Time 297.0 updated!\n",
      "Time 297.5 updated!\n",
      "Time 298.0 updated!\n",
      "Time 298.5 updated!\n",
      "Time 299.0 updated!\n",
      "Time 299.5 updated!\n",
      "Time 300.0 updated!\n",
      "Time 300.5 updated!\n",
      "Time 301.0 updated!\n",
      "Time 301.5 updated!\n",
      "Time 302.0 updated!\n",
      "Time 302.5 updated!\n",
      "Time 303.0 updated!\n",
      "Time 303.5 updated!\n",
      "Time 304.0 updated!\n",
      "Time 304.5 updated!\n",
      "Time 305.0 updated!\n",
      "Time 305.5 updated!\n",
      "Time 306.0 updated!\n",
      "Time 306.5 updated!\n",
      "Time 307.0 updated!\n",
      "Time 307.5 updated!\n",
      "Time 308.0 updated!\n",
      "Time 308.5 updated!\n",
      "Time 309.0 updated!\n",
      "Time 309.5 updated!\n",
      "Time 310.0 updated!\n",
      "Time 310.5 updated!\n",
      "Time 311.0 updated!\n",
      "Time 311.5 updated!\n",
      "Time 312.0 updated!\n",
      "Time 312.5 updated!\n",
      "Time 313.0 updated!\n",
      "Time 313.5 updated!\n",
      "Time 314.0 updated!\n",
      "Time 314.5 updated!\n",
      "Time 315.0 updated!\n",
      "Time 315.5 updated!\n",
      "Time 316.0 updated!\n",
      "Time 316.5 updated!\n",
      "Time 317.0 updated!\n",
      "Time 317.5 updated!\n",
      "Time 318.0 updated!\n",
      "Time 318.5 updated!\n",
      "Time 319.0 updated!\n",
      "Time 319.5 updated!\n",
      "Time 320.0 updated!\n",
      "Time 320.5 updated!\n",
      "Time 321.0 updated!\n",
      "Time 321.5 updated!\n",
      "Time 322.0 updated!\n",
      "Time 322.5 updated!\n",
      "Time 323.0 updated!\n",
      "Time 323.5 updated!\n",
      "Time 324.0 updated!\n",
      "Time 324.5 updated!\n",
      "Time 325.0 updated!\n",
      "Time 325.5 updated!\n",
      "Time 326.0 updated!\n",
      "Time 326.5 updated!\n",
      "Time 327.0 updated!\n",
      "Time 327.5 updated!\n",
      "Time 328.0 updated!\n",
      "Time 328.5 updated!\n",
      "Time 329.0 updated!\n",
      "Time 329.5 updated!\n",
      "Time 330.0 updated!\n",
      "Time 330.5 updated!\n",
      "Time 331.0 updated!\n",
      "Time 331.5 updated!\n",
      "Time 332.0 updated!\n",
      "Time 332.5 updated!\n",
      "Time 333.0 updated!\n",
      "Time 333.5 updated!\n",
      "Time 334.0 updated!\n",
      "Time 334.5 updated!\n",
      "Time 335.0 updated!\n",
      "Time 335.5 updated!\n",
      "Time 336.0 updated!\n",
      "Time 336.5 updated!\n",
      "Time 337.0 updated!\n",
      "Time 337.5 updated!\n",
      "Time 338.0 updated!\n",
      "Time 338.5 updated!\n",
      "Time 339.0 updated!\n",
      "Time 339.5 updated!\n",
      "Time 340.0 updated!\n",
      "Time 340.5 updated!\n",
      "Time 341.0 updated!\n",
      "Time 341.5 updated!\n",
      "Time 342.0 updated!\n",
      "Time 342.5 updated!\n",
      "Time 343.0 updated!\n",
      "Time 343.5 updated!\n",
      "Time 344.0 updated!\n",
      "Time 344.5 updated!\n",
      "Time 345.0 updated!\n",
      "Time 345.5 updated!\n",
      "Time 346.0 updated!\n",
      "Time 346.5 updated!\n",
      "Time 347.0 updated!\n",
      "Time 347.5 updated!\n",
      "Time 348.0 updated!\n",
      "Time 348.5 updated!\n",
      "Time 349.0 updated!\n",
      "Time 349.5 updated!\n",
      "Time 350.0 updated!\n",
      "Time 350.5 updated!\n",
      "Time 351.0 updated!\n",
      "Time 351.5 updated!\n",
      "Time 352.0 updated!\n",
      "Time 352.5 updated!\n",
      "Time 353.0 updated!\n",
      "Time 353.5 updated!\n",
      "Time 354.0 updated!\n",
      "Time 354.5 updated!\n",
      "Time 355.0 updated!\n",
      "Time 355.5 updated!\n",
      "Time 356.0 updated!\n",
      "Time 356.5 updated!\n",
      "Time 357.0 updated!\n",
      "Time 357.5 updated!\n",
      "Time 358.0 updated!\n",
      "Time 358.5 updated!\n",
      "Time 359.0 updated!\n",
      "Time 359.5 updated!\n",
      "Time 360.0 updated!\n",
      "Time 360.5 updated!\n",
      "Time 361.0 updated!\n",
      "Time 361.5 updated!\n",
      "Time 362.0 updated!\n",
      "Time 362.5 updated!\n",
      "Time 363.0 updated!\n",
      "Time 363.5 updated!\n",
      "Time 364.0 updated!\n",
      "Time 364.5 updated!\n",
      "Time 365.0 updated!\n",
      "Time 365.5 updated!\n",
      "Time 366.0 updated!\n",
      "Time 366.5 updated!\n",
      "Time 367.0 updated!\n",
      "Time 367.5 updated!\n",
      "Time 368.0 updated!\n",
      "Time 368.5 updated!\n",
      "Time 369.0 updated!\n",
      "Time 369.5 updated!\n",
      "Time 370.0 updated!\n",
      "Time 370.5 updated!\n",
      "Time 371.0 updated!\n",
      "Time 371.5 updated!\n",
      "Time 372.0 updated!\n",
      "Time 372.5 updated!\n",
      "Time 373.0 updated!\n",
      "Time 373.5 updated!\n",
      "Time 374.0 updated!\n",
      "Time 374.5 updated!\n",
      "Time 375.0 updated!\n",
      "Time 375.5 updated!\n",
      "Time 376.0 updated!\n",
      "Time 376.5 updated!\n",
      "Time 377.0 updated!\n",
      "Time 377.5 updated!\n",
      "Time 378.0 updated!\n",
      "Time 378.5 updated!\n",
      "Time 379.0 updated!\n",
      "Time 379.5 updated!\n",
      "Time 380.0 updated!\n",
      "Time 380.5 updated!\n",
      "Time 381.0 updated!\n",
      "Time 381.5 updated!\n",
      "Time 382.0 updated!\n",
      "Time 382.5 updated!\n",
      "Time 383.0 updated!\n",
      "Time 383.5 updated!\n",
      "Time 384.0 updated!\n",
      "Time 384.5 updated!\n",
      "Time 385.0 updated!\n",
      "Time 385.5 updated!\n",
      "Time 386.0 updated!\n",
      "Time 386.5 updated!\n",
      "Time 387.0 updated!\n",
      "Time 387.5 updated!\n",
      "Time 388.0 updated!\n",
      "Time 388.5 updated!\n",
      "Time 389.0 updated!\n",
      "Time 389.5 updated!\n",
      "Time 390.0 updated!\n",
      "Time 390.5 updated!\n",
      "Time 391.0 updated!\n",
      "Time 391.5 updated!\n",
      "Time 392.0 updated!\n",
      "Time 392.5 updated!\n",
      "Time 393.0 updated!\n",
      "Time 393.5 updated!\n",
      "Time 394.0 updated!\n",
      "Time 394.5 updated!\n",
      "Time 395.0 updated!\n",
      "Time 395.5 updated!\n",
      "Time 396.0 updated!\n",
      "Time 396.5 updated!\n",
      "Time 397.0 updated!\n",
      "Time 397.5 updated!\n",
      "Time 398.0 updated!\n",
      "Time 398.5 updated!\n",
      "Time 399.0 updated!\n",
      "Time 399.5 updated!\n",
      "Time 400.0 updated!\n",
      "Time 400.5 updated!\n",
      "Time 401.0 updated!\n",
      "Time 401.5 updated!\n",
      "Time 402.0 updated!\n",
      "Time 402.5 updated!\n",
      "Time 403.0 updated!\n",
      "Time 403.5 updated!\n",
      "Time 404.0 updated!\n",
      "Time 404.5 updated!\n",
      "Time 405.0 updated!\n",
      "Time 405.5 updated!\n",
      "Time 406.0 updated!\n",
      "Time 406.5 updated!\n",
      "Time 407.0 updated!\n",
      "Time 407.5 updated!\n",
      "Time 408.0 updated!\n",
      "Time 408.5 updated!\n",
      "Time 409.0 updated!\n",
      "Time 409.5 updated!\n",
      "Time 410.0 updated!\n",
      "Time 410.5 updated!\n",
      "Time 411.0 updated!\n",
      "Time 411.5 updated!\n",
      "Time 412.0 updated!\n",
      "Time 412.5 updated!\n",
      "Time 413.0 updated!\n",
      "Time 413.5 updated!\n",
      "Time 414.0 updated!\n",
      "Time 414.5 updated!\n",
      "Time 415.0 updated!\n",
      "Time 415.5 updated!\n",
      "Time 416.0 updated!\n",
      "Time 416.5 updated!\n",
      "Time 417.0 updated!\n",
      "Time 417.5 updated!\n",
      "Time 418.0 updated!\n",
      "Time 418.5 updated!\n",
      "Time 419.0 updated!\n",
      "Time 419.5 updated!\n",
      "Time 420.0 updated!\n",
      "Time 420.5 updated!\n",
      "Time 421.0 updated!\n",
      "Time 421.5 updated!\n",
      "Time 422.0 updated!\n",
      "Time 422.5 updated!\n",
      "Time 423.0 updated!\n",
      "Time 423.5 updated!\n",
      "Time 424.0 updated!\n",
      "Time 424.5 updated!\n",
      "Time 425.0 updated!\n",
      "Time 425.5 updated!\n",
      "Time 426.0 updated!\n",
      "Time 426.5 updated!\n",
      "Time 427.0 updated!\n",
      "Time 427.5 updated!\n",
      "Time 428.0 updated!\n",
      "Time 428.5 updated!\n",
      "Time 429.0 updated!\n",
      "Time 429.5 updated!\n",
      "Time 430.0 updated!\n",
      "Time 430.5 updated!\n",
      "Time 431.0 updated!\n",
      "Time 431.5 updated!\n",
      "Time 432.0 updated!\n",
      "Time 432.5 updated!\n",
      "Time 433.0 updated!\n",
      "Time 433.5 updated!\n",
      "Time 434.0 updated!\n",
      "Time 434.5 updated!\n",
      "Time 435.0 updated!\n",
      "Time 435.5 updated!\n",
      "Time 436.0 updated!\n",
      "Time 436.5 updated!\n",
      "Time 437.0 updated!\n",
      "Time 437.5 updated!\n",
      "Time 438.0 updated!\n",
      "Time 438.5 updated!\n",
      "Time 439.0 updated!\n",
      "Time 439.5 updated!\n",
      "Time 440.0 updated!\n",
      "Time 440.5 updated!\n",
      "Time 441.0 updated!\n",
      "Time 441.5 updated!\n",
      "Time 442.0 updated!\n",
      "Time 442.5 updated!\n",
      "Time 443.0 updated!\n",
      "Time 443.5 updated!\n",
      "Time 444.0 updated!\n",
      "Time 444.5 updated!\n",
      "Time 445.0 updated!\n",
      "Time 445.5 updated!\n",
      "Time 446.0 updated!\n",
      "Time 446.5 updated!\n",
      "Time 447.0 updated!\n",
      "Time 447.5 updated!\n",
      "Time 448.0 updated!\n",
      "Time 448.5 updated!\n",
      "Time 449.0 updated!\n",
      "Time 449.5 updated!\n",
      "Time 450.0 updated!\n",
      "Time 450.5 updated!\n",
      "Time 451.0 updated!\n",
      "Time 451.5 updated!\n",
      "Time 452.0 updated!\n",
      "Time 452.5 updated!\n",
      "Time 453.0 updated!\n",
      "Time 453.5 updated!\n",
      "Time 454.0 updated!\n",
      "Time 454.5 updated!\n",
      "Time 455.0 updated!\n",
      "Time 455.5 updated!\n",
      "Time 456.0 updated!\n",
      "Time 456.5 updated!\n",
      "Time 457.0 updated!\n",
      "Time 457.5 updated!\n",
      "Time 458.0 updated!\n",
      "Time 458.5 updated!\n",
      "Time 459.0 updated!\n",
      "Time 459.5 updated!\n",
      "Time 460.0 updated!\n",
      "Time 460.5 updated!\n",
      "Time 461.0 updated!\n",
      "Time 461.5 updated!\n",
      "Time 462.0 updated!\n",
      "Time 462.5 updated!\n",
      "Time 463.0 updated!\n",
      "Time 463.5 updated!\n",
      "Time 464.0 updated!\n",
      "Time 464.5 updated!\n",
      "Time 465.0 updated!\n",
      "Time 465.5 updated!\n",
      "Time 466.0 updated!\n",
      "Time 466.5 updated!\n",
      "Time 467.0 updated!\n",
      "Time 467.5 updated!\n",
      "Time 468.0 updated!\n",
      "Time 468.5 updated!\n",
      "Time 469.0 updated!\n",
      "Time 469.5 updated!\n",
      "Time 470.0 updated!\n",
      "Time 470.5 updated!\n",
      "Time 471.0 updated!\n",
      "Time 471.5 updated!\n",
      "Time 472.0 updated!\n",
      "Time 472.5 updated!\n",
      "Time 473.0 updated!\n",
      "Time 473.5 updated!\n",
      "Time 474.0 updated!\n",
      "Time 474.5 updated!\n",
      "Time 475.0 updated!\n",
      "Time 475.5 updated!\n",
      "Time 476.0 updated!\n",
      "Time 476.5 updated!\n",
      "Time 477.0 updated!\n",
      "Time 477.5 updated!\n",
      "Time 478.0 updated!\n",
      "Time 478.5 updated!\n",
      "Time 479.0 updated!\n",
      "Time 479.5 updated!\n",
      "Time 480.0 updated!\n",
      "Time 480.5 updated!\n",
      "Time 481.0 updated!\n",
      "Time 481.5 updated!\n",
      "Time 482.0 updated!\n",
      "Time 482.5 updated!\n",
      "Time 483.0 updated!\n",
      "Time 483.5 updated!\n",
      "Time 484.0 updated!\n",
      "Time 484.5 updated!\n",
      "Time 485.0 updated!\n",
      "Time 485.5 updated!\n",
      "Time 486.0 updated!\n",
      "Time 486.5 updated!\n",
      "Time 487.0 updated!\n",
      "Time 487.5 updated!\n",
      "Time 488.0 updated!\n",
      "Time 488.5 updated!\n",
      "Time 489.0 updated!\n",
      "Time 489.5 updated!\n",
      "Time 490.0 updated!\n",
      "Time 490.5 updated!\n",
      "Time 491.0 updated!\n",
      "Time 491.5 updated!\n",
      "Time 492.0 updated!\n",
      "Time 492.5 updated!\n",
      "Time 493.0 updated!\n",
      "Time 493.5 updated!\n",
      "Time 494.0 updated!\n",
      "Time 494.5 updated!\n",
      "Time 495.0 updated!\n",
      "Time 495.5 updated!\n",
      "Time 496.0 updated!\n",
      "Time 496.5 updated!\n",
      "Time 497.0 updated!\n",
      "Time 497.5 updated!\n",
      "Time 498.0 updated!\n",
      "Time 498.5 updated!\n",
      "Time 499.0 updated!\n",
      "Time 499.5 updated!\n",
      "Time 500.0 updated!\n",
      "Time 500.5 updated!\n",
      "Time 501.0 updated!\n",
      "Time 501.5 updated!\n",
      "Time 502.0 updated!\n",
      "Time 502.5 updated!\n",
      "Time 503.0 updated!\n",
      "Time 503.5 updated!\n",
      "Time 504.0 updated!\n",
      "Time 504.5 updated!\n",
      "Time 505.0 updated!\n",
      "Time 505.5 updated!\n",
      "Time 506.0 updated!\n",
      "Time 506.5 updated!\n",
      "Time 507.0 updated!\n",
      "Time 507.5 updated!\n",
      "Time 508.0 updated!\n",
      "Time 508.5 updated!\n",
      "Time 509.0 updated!\n",
      "Time 509.5 updated!\n",
      "Time 510.0 updated!\n",
      "Time 510.5 updated!\n",
      "Time 511.0 updated!\n",
      "Time 511.5 updated!\n",
      "Time 512.0 updated!\n",
      "Time 512.5 updated!\n",
      "Time 513.0 updated!\n",
      "Time 513.5 updated!\n",
      "Time 514.0 updated!\n",
      "Time 514.5 updated!\n",
      "Time 515.0 updated!\n",
      "Time 515.5 updated!\n",
      "Time 516.0 updated!\n",
      "Time 516.5 updated!\n",
      "Time 517.0 updated!\n",
      "Time 517.5 updated!\n",
      "Time 518.0 updated!\n",
      "Time 518.5 updated!\n",
      "Time 519.0 updated!\n",
      "Time 519.5 updated!\n",
      "Time 520.0 updated!\n",
      "Time 520.5 updated!\n",
      "Time 521.0 updated!\n",
      "Time 521.5 updated!\n",
      "Time 522.0 updated!\n",
      "Time 522.5 updated!\n",
      "Time 523.0 updated!\n",
      "Time 523.5 updated!\n",
      "Time 524.0 updated!\n",
      "Time 524.5 updated!\n",
      "Time 525.0 updated!\n",
      "Time 525.5 updated!\n",
      "Time 526.0 updated!\n",
      "Time 526.5 updated!\n",
      "Time 527.0 updated!\n",
      "Time 527.5 updated!\n",
      "Time 528.0 updated!\n",
      "Time 528.5 updated!\n",
      "Time 529.0 updated!\n",
      "Time 529.5 updated!\n",
      "Time 530.0 updated!\n",
      "Time 530.5 updated!\n",
      "Time 531.0 updated!\n",
      "Time 531.5 updated!\n",
      "Time 532.0 updated!\n",
      "Time 532.5 updated!\n",
      "Time 533.0 updated!\n",
      "Time 533.5 updated!\n",
      "Time 534.0 updated!\n",
      "Time 534.5 updated!\n",
      "Time 535.0 updated!\n",
      "Time 535.5 updated!\n",
      "Time 536.0 updated!\n",
      "Time 536.5 updated!\n",
      "Time 537.0 updated!\n",
      "Time 537.5 updated!\n",
      "Time 538.0 updated!\n",
      "Time 538.5 updated!\n",
      "Time 539.0 updated!\n",
      "Time 539.5 updated!\n",
      "Time 540.0 updated!\n",
      "Time 540.5 updated!\n",
      "Time 541.0 updated!\n",
      "Time 541.5 updated!\n",
      "Time 542.0 updated!\n",
      "Time 542.5 updated!\n",
      "Time 543.0 updated!\n",
      "Time 543.5 updated!\n",
      "Time 544.0 updated!\n",
      "Time 544.5 updated!\n",
      "Time 545.0 updated!\n",
      "Time 545.5 updated!\n",
      "Time 546.0 updated!\n",
      "Time 546.5 updated!\n",
      "Time 547.0 updated!\n",
      "Time 547.5 updated!\n",
      "Time 548.0 updated!\n",
      "Time 548.5 updated!\n",
      "Time 549.0 updated!\n",
      "Time 549.5 updated!\n",
      "Time 550.0 updated!\n",
      "Time 550.5 updated!\n",
      "Time 551.0 updated!\n",
      "Time 551.5 updated!\n",
      "Time 552.0 updated!\n",
      "Time 552.5 updated!\n",
      "Time 553.0 updated!\n",
      "Time 553.5 updated!\n",
      "Time 554.0 updated!\n",
      "Time 554.5 updated!\n",
      "Time 555.0 updated!\n",
      "Time 555.5 updated!\n",
      "Time 556.0 updated!\n",
      "Time 556.5 updated!\n",
      "Time 557.0 updated!\n",
      "Time 557.5 updated!\n",
      "Time 558.0 updated!\n",
      "Time 558.5 updated!\n",
      "Time 559.0 updated!\n",
      "Time 559.5 updated!\n",
      "Time 560.0 updated!\n",
      "Time 560.5 updated!\n",
      "Time 561.0 updated!\n",
      "Time 561.5 updated!\n",
      "Time 562.0 updated!\n",
      "Time 562.5 updated!\n",
      "Time 563.0 updated!\n",
      "Time 563.5 updated!\n",
      "Time 564.0 updated!\n",
      "Time 564.5 updated!\n",
      "Time 565.0 updated!\n",
      "Time 565.5 updated!\n",
      "Time 566.0 updated!\n",
      "Time 566.5 updated!\n",
      "Time 567.0 updated!\n",
      "Time 567.5 updated!\n",
      "Time 568.0 updated!\n",
      "Time 568.5 updated!\n",
      "Time 569.0 updated!\n",
      "Time 569.5 updated!\n",
      "Time 570.0 updated!\n",
      "Time 570.5 updated!\n",
      "Time 571.0 updated!\n",
      "Time 571.5 updated!\n",
      "Time 572.0 updated!\n",
      "Time 572.5 updated!\n",
      "Time 573.0 updated!\n",
      "Time 573.5 updated!\n",
      "Time 574.0 updated!\n",
      "Time 574.5 updated!\n",
      "Time 575.0 updated!\n",
      "Time 575.5 updated!\n",
      "Time 576.0 updated!\n",
      "Time 576.5 updated!\n",
      "Time 577.0 updated!\n",
      "Time 577.5 updated!\n",
      "Time 578.0 updated!\n",
      "Time 578.5 updated!\n",
      "Time 579.0 updated!\n",
      "Time 579.5 updated!\n",
      "Time 580.0 updated!\n",
      "Time 580.5 updated!\n",
      "Time 581.0 updated!\n",
      "Time 581.5 updated!\n",
      "Time 582.0 updated!\n",
      "Time 582.5 updated!\n",
      "Time 583.0 updated!\n",
      "Time 583.5 updated!\n",
      "Time 584.0 updated!\n",
      "Time 584.5 updated!\n",
      "Time 585.0 updated!\n",
      "Time 585.5 updated!\n",
      "Time 586.0 updated!\n",
      "Time 586.5 updated!\n",
      "Time 587.0 updated!\n",
      "Time 587.5 updated!\n",
      "Time 588.0 updated!\n",
      "Time 588.5 updated!\n",
      "Time 589.0 updated!\n",
      "Time 589.5 updated!\n",
      "Time 590.0 updated!\n",
      "Time 590.5 updated!\n",
      "Time 591.0 updated!\n",
      "Time 591.5 updated!\n",
      "Time 592.0 updated!\n",
      "Time 592.5 updated!\n",
      "Time 593.0 updated!\n",
      "Time 593.5 updated!\n",
      "Time 594.0 updated!\n",
      "Time 594.5 updated!\n",
      "Time 595.0 updated!\n",
      "Time 595.5 updated!\n",
      "Time 596.0 updated!\n",
      "Time 596.5 updated!\n",
      "Time 597.0 updated!\n",
      "Time 597.5 updated!\n",
      "Time 598.0 updated!\n",
      "Time 598.5 updated!\n",
      "Time 599.0 updated!\n",
      "Time 599.5 updated!\n",
      "Time 600.0 updated!\n",
      "Time 600.5 updated!\n",
      "Time 601.0 updated!\n",
      "Time 601.5 updated!\n",
      "Time 602.0 updated!\n",
      "Time 602.5 updated!\n",
      "Time 603.0 updated!\n",
      "Time 603.5 updated!\n",
      "Time 604.0 updated!\n",
      "Time 604.5 updated!\n",
      "Time 605.0 updated!\n",
      "Time 605.5 updated!\n",
      "Time 606.0 updated!\n",
      "Time 606.5 updated!\n",
      "Time 607.0 updated!\n",
      "Time 607.5 updated!\n",
      "Time 608.0 updated!\n",
      "Time 608.5 updated!\n",
      "Time 609.0 updated!\n",
      "Time 609.5 updated!\n",
      "Time 610.0 updated!\n",
      "Time 610.5 updated!\n",
      "Time 611.0 updated!\n",
      "Time 611.5 updated!\n",
      "Time 612.0 updated!\n",
      "Time 612.5 updated!\n",
      "Time 613.0 updated!\n",
      "Time 613.5 updated!\n",
      "Time 614.0 updated!\n",
      "Time 614.5 updated!\n",
      "Time 615.0 updated!\n",
      "Time 615.5 updated!\n",
      "Time 616.0 updated!\n",
      "Time 616.5 updated!\n",
      "Time 617.0 updated!\n",
      "Time 617.5 updated!\n",
      "Time 618.0 updated!\n",
      "Time 618.5 updated!\n",
      "Time 619.0 updated!\n",
      "Time 619.5 updated!\n",
      "Time 620.0 updated!\n",
      "Time 620.5 updated!\n",
      "Time 621.0 updated!\n",
      "Time 621.5 updated!\n",
      "Time 622.0 updated!\n",
      "Time 622.5 updated!\n",
      "Time 623.0 updated!\n",
      "Time 623.5 updated!\n",
      "Time 624.0 updated!\n",
      "Time 624.5 updated!\n",
      "Time 625.0 updated!\n",
      "Time 625.5 updated!\n",
      "Time 626.0 updated!\n",
      "Time 626.5 updated!\n",
      "Time 627.0 updated!\n",
      "Time 627.5 updated!\n",
      "Time 628.0 updated!\n",
      "Time 628.5 updated!\n",
      "Time 629.0 updated!\n",
      "Time 629.5 updated!\n",
      "Time 630.0 updated!\n",
      "Time 630.5 updated!\n",
      "Time 631.0 updated!\n",
      "Time 631.5 updated!\n",
      "Time 632.0 updated!\n",
      "Time 632.5 updated!\n",
      "Time 633.0 updated!\n",
      "Time 633.5 updated!\n",
      "Time 634.0 updated!\n",
      "Time 634.5 updated!\n",
      "Time 635.0 updated!\n",
      "Time 635.5 updated!\n",
      "Time 636.0 updated!\n",
      "Time 636.5 updated!\n",
      "Time 637.0 updated!\n",
      "Time 637.5 updated!\n",
      "Time 638.0 updated!\n",
      "Time 638.5 updated!\n",
      "Time 639.0 updated!\n",
      "Time 639.5 updated!\n",
      "Time 640.0 updated!\n",
      "Time 640.5 updated!\n",
      "Time 641.0 updated!\n",
      "Time 641.5 updated!\n",
      "Time 642.0 updated!\n",
      "Time 642.5 updated!\n",
      "Time 643.0 updated!\n",
      "Time 643.5 updated!\n",
      "Time 644.0 updated!\n",
      "Time 644.5 updated!\n",
      "Time 645.0 updated!\n",
      "Time 645.5 updated!\n",
      "Time 646.0 updated!\n",
      "Time 646.5 updated!\n",
      "Time 647.0 updated!\n",
      "Time 647.5 updated!\n",
      "Time 648.0 updated!\n",
      "Time 648.5 updated!\n",
      "Time 649.0 updated!\n",
      "Time 649.5 updated!\n",
      "Time 650.0 updated!\n",
      "Time 650.5 updated!\n",
      "Time 651.0 updated!\n",
      "Time 651.5 updated!\n",
      "Time 652.0 updated!\n",
      "Time 652.5 updated!\n",
      "Time 653.0 updated!\n",
      "Time 653.5 updated!\n",
      "Time 654.0 updated!\n",
      "Time 654.5 updated!\n",
      "Time 655.0 updated!\n",
      "Time 655.5 updated!\n",
      "Time 656.0 updated!\n",
      "Time 656.5 updated!\n",
      "Time 657.0 updated!\n",
      "Time 657.5 updated!\n",
      "Time 658.0 updated!\n",
      "Time 658.5 updated!\n",
      "Time 659.0 updated!\n",
      "Time 659.5 updated!\n",
      "Time 660.0 updated!\n",
      "Time 660.5 updated!\n",
      "Time 661.0 updated!\n",
      "Time 661.5 updated!\n",
      "Time 662.0 updated!\n",
      "Time 662.5 updated!\n",
      "Time 663.0 updated!\n",
      "Time 663.5 updated!\n",
      "Time 664.0 updated!\n",
      "Time 664.5 updated!\n",
      "Time 665.0 updated!\n",
      "Time 665.5 updated!\n",
      "Time 666.0 updated!\n",
      "Time 666.5 updated!\n",
      "Time 667.0 updated!\n",
      "Time 667.5 updated!\n",
      "Time 668.0 updated!\n",
      "Time 668.5 updated!\n",
      "Time 669.0 updated!\n",
      "Time 669.5 updated!\n",
      "Time 670.0 updated!\n",
      "Time 670.5 updated!\n",
      "Time 671.0 updated!\n",
      "Time 671.5 updated!\n",
      "Time 672.0 updated!\n",
      "Time 672.5 updated!\n",
      "Time 673.0 updated!\n",
      "Time 673.5 updated!\n",
      "Time 674.0 updated!\n",
      "Time 674.5 updated!\n",
      "Time 675.0 updated!\n",
      "Time 675.5 updated!\n",
      "Time 676.0 updated!\n",
      "Time 676.5 updated!\n",
      "Time 677.0 updated!\n",
      "Time 677.5 updated!\n",
      "Time 678.0 updated!\n",
      "Time 678.5 updated!\n",
      "Time 679.0 updated!\n",
      "Time 679.5 updated!\n",
      "Time 680.0 updated!\n",
      "Time 680.5 updated!\n",
      "Time 681.0 updated!\n",
      "Time 681.5 updated!\n",
      "Time 682.0 updated!\n",
      "Time 682.5 updated!\n",
      "Time 683.0 updated!\n",
      "Time 683.5 updated!\n",
      "Time 684.0 updated!\n",
      "Time 684.5 updated!\n",
      "Time 685.0 updated!\n",
      "Time 685.5 updated!\n",
      "Time 686.0 updated!\n",
      "Time 686.5 updated!\n",
      "Time 687.0 updated!\n",
      "Time 687.5 updated!\n",
      "Time 688.0 updated!\n",
      "Time 688.5 updated!\n",
      "Time 689.0 updated!\n",
      "Time 689.5 updated!\n",
      "Time 690.0 updated!\n",
      "Time 690.5 updated!\n",
      "Time 691.0 updated!\n",
      "Time 691.5 updated!\n",
      "Time 692.0 updated!\n",
      "Time 692.5 updated!\n",
      "Time 693.0 updated!\n",
      "Time 693.5 updated!\n",
      "Time 694.0 updated!\n",
      "Time 694.5 updated!\n",
      "Time 695.0 updated!\n",
      "Time 695.5 updated!\n",
      "Time 696.0 updated!\n",
      "Time 696.5 updated!\n",
      "Time 697.0 updated!\n",
      "Time 697.5 updated!\n",
      "Time 698.0 updated!\n",
      "Time 698.5 updated!\n",
      "Time 699.0 updated!\n",
      "Time 699.5 updated!\n",
      "Time 700.0 updated!\n",
      "Time 700.5 updated!\n",
      "Time 701.0 updated!\n",
      "Time 701.5 updated!\n",
      "Time 702.0 updated!\n",
      "Time 702.5 updated!\n",
      "Time 703.0 updated!\n",
      "Time 703.5 updated!\n",
      "Time 704.0 updated!\n",
      "Time 704.5 updated!\n",
      "Time 705.0 updated!\n",
      "Time 705.5 updated!\n",
      "Time 706.0 updated!\n",
      "Time 706.5 updated!\n",
      "Time 707.0 updated!\n",
      "Time 707.5 updated!\n",
      "Time 708.0 updated!\n",
      "Time 708.5 updated!\n",
      "Time 709.0 updated!\n",
      "Time 709.5 updated!\n",
      "Time 710.0 updated!\n",
      "Time 710.5 updated!\n",
      "Time 711.0 updated!\n",
      "Time 711.5 updated!\n",
      "Time 712.0 updated!\n",
      "Time 712.5 updated!\n",
      "Time 713.0 updated!\n",
      "Time 713.5 updated!\n",
      "Time 714.0 updated!\n",
      "Time 714.5 updated!\n",
      "Time 715.0 updated!\n",
      "Time 715.5 updated!\n",
      "Time 716.0 updated!\n",
      "Time 716.5 updated!\n",
      "Time 717.0 updated!\n",
      "Time 717.5 updated!\n",
      "Time 718.0 updated!\n",
      "Time 718.5 updated!\n",
      "Time 719.0 updated!\n",
      "Time 719.5 updated!\n",
      "Time 720.0 updated!\n",
      "Time 720.5 updated!\n",
      "Time 721.0 updated!\n",
      "Time 721.5 updated!\n",
      "Time 722.0 updated!\n",
      "Time 722.5 updated!\n",
      "Time 723.0 updated!\n",
      "Time 723.5 updated!\n",
      "Time 724.0 updated!\n",
      "Time 724.5 updated!\n",
      "Time 725.0 updated!\n",
      "Time 725.5 updated!\n",
      "Time 726.0 updated!\n",
      "Time 726.5 updated!\n",
      "Time 727.0 updated!\n",
      "Time 727.5 updated!\n",
      "Time 728.0 updated!\n",
      "Time 728.5 updated!\n",
      "Time 729.0 updated!\n",
      "Time 729.5 updated!\n",
      "Time 730.0 updated!\n",
      "Time 730.5 updated!\n",
      "Time 731.0 updated!\n",
      "Time 731.5 updated!\n",
      "Time 732.0 updated!\n",
      "Time 732.5 updated!\n",
      "Time 733.0 updated!\n",
      "Time 733.5 updated!\n",
      "Time 734.0 updated!\n",
      "Time 734.5 updated!\n",
      "Time 735.0 updated!\n",
      "Time 735.5 updated!\n",
      "Time 736.0 updated!\n",
      "Time 736.5 updated!\n",
      "Time 737.0 updated!\n",
      "Time 737.5 updated!\n",
      "Time 738.0 updated!\n",
      "Time 738.5 updated!\n",
      "Time 739.0 updated!\n",
      "Time 739.5 updated!\n",
      "Time 740.0 updated!\n",
      "Time 740.5 updated!\n",
      "Time 741.0 updated!\n",
      "Time 741.5 updated!\n",
      "Time 742.0 updated!\n",
      "Time 742.5 updated!\n",
      "Time 743.0 updated!\n",
      "Time 743.5 updated!\n",
      "Time 744.0 updated!\n",
      "Time 744.5 updated!\n",
      "Time 745.0 updated!\n",
      "Time 745.5 updated!\n",
      "Time 746.0 updated!\n",
      "Time 746.5 updated!\n",
      "Time 747.0 updated!\n",
      "Time 747.5 updated!\n",
      "Time 748.0 updated!\n",
      "Time 748.5 updated!\n",
      "Time 749.0 updated!\n",
      "Time 749.5 updated!\n",
      "Time 750.0 updated!\n",
      "Time 750.5 updated!\n",
      "Time 751.0 updated!\n",
      "Time 751.5 updated!\n",
      "Time 752.0 updated!\n",
      "Time 752.5 updated!\n",
      "Time 753.0 updated!\n",
      "Time 753.5 updated!\n",
      "Time 754.0 updated!\n",
      "Time 754.5 updated!\n",
      "Time 755.0 updated!\n",
      "Time 755.5 updated!\n",
      "Time 756.0 updated!\n",
      "Time 756.5 updated!\n",
      "Time 757.0 updated!\n",
      "Time 757.5 updated!\n",
      "Time 758.0 updated!\n",
      "Time 758.5 updated!\n",
      "Time 759.0 updated!\n",
      "Time 759.5 updated!\n",
      "Time 760.0 updated!\n",
      "Time 760.5 updated!\n",
      "Time 761.0 updated!\n",
      "Time 761.5 updated!\n",
      "Time 762.0 updated!\n",
      "Time 762.5 updated!\n",
      "Time 763.0 updated!\n",
      "Time 763.5 updated!\n",
      "Time 764.0 updated!\n",
      "Time 764.5 updated!\n",
      "Time 765.0 updated!\n",
      "Time 765.5 updated!\n",
      "Time 766.0 updated!\n",
      "Time 766.5 updated!\n",
      "Time 767.0 updated!\n",
      "Time 767.5 updated!\n",
      "Time 768.0 updated!\n",
      "Time 768.5 updated!\n",
      "Time 769.0 updated!\n",
      "Time 769.5 updated!\n",
      "Time 770.0 updated!\n",
      "Time 770.5 updated!\n",
      "Time 771.0 updated!\n",
      "Time 771.5 updated!\n",
      "Time 772.0 updated!\n",
      "Time 772.5 updated!\n",
      "Time 773.0 updated!\n",
      "Time 773.5 updated!\n",
      "Time 774.0 updated!\n",
      "Time 774.5 updated!\n",
      "Time 775.0 updated!\n",
      "Time 775.5 updated!\n",
      "Time 776.0 updated!\n",
      "Time 776.5 updated!\n",
      "Time 777.0 updated!\n",
      "Time 777.5 updated!\n",
      "Time 778.0 updated!\n",
      "Time 778.5 updated!\n",
      "Time 779.0 updated!\n",
      "Time 779.5 updated!\n",
      "Time 780.0 updated!\n",
      "Time 780.5 updated!\n",
      "Time 781.0 updated!\n",
      "Time 781.5 updated!\n",
      "Time 782.0 updated!\n",
      "Time 782.5 updated!\n",
      "Time 783.0 updated!\n",
      "Time 783.5 updated!\n",
      "Time 784.0 updated!\n",
      "Time 784.5 updated!\n",
      "Time 785.0 updated!\n",
      "Time 785.5 updated!\n",
      "Time 786.0 updated!\n",
      "Time 786.5 updated!\n",
      "Time 787.0 updated!\n",
      "Time 787.5 updated!\n",
      "Time 788.0 updated!\n",
      "Time 788.5 updated!\n",
      "Time 789.0 updated!\n",
      "Time 789.5 updated!\n",
      "Time 790.0 updated!\n",
      "Time 790.5 updated!\n",
      "Time 791.0 updated!\n",
      "Time 791.5 updated!\n",
      "Time 792.0 updated!\n",
      "Time 792.5 updated!\n",
      "Time 793.0 updated!\n",
      "Time 793.5 updated!\n",
      "Time 794.0 updated!\n",
      "Time 794.5 updated!\n",
      "Time 795.0 updated!\n",
      "Time 795.5 updated!\n",
      "Time 796.0 updated!\n",
      "Time 796.5 updated!\n",
      "Time 797.0 updated!\n",
      "Time 797.5 updated!\n",
      "Time 798.0 updated!\n",
      "Time 798.5 updated!\n",
      "Time 799.0 updated!\n",
      "Time 799.5 updated!\n",
      "Time 800.0 updated!\n",
      "Time 800.5 updated!\n",
      "Time 801.0 updated!\n",
      "Time 801.5 updated!\n",
      "Time 802.0 updated!\n",
      "Time 802.5 updated!\n",
      "Time 803.0 updated!\n",
      "Time 803.5 updated!\n",
      "Time 804.0 updated!\n",
      "Time 804.5 updated!\n",
      "Time 805.0 updated!\n",
      "Time 805.5 updated!\n",
      "Time 806.0 updated!\n",
      "Time 806.5 updated!\n",
      "Time 807.0 updated!\n",
      "Time 807.5 updated!\n",
      "Time 808.0 updated!\n",
      "Time 808.5 updated!\n",
      "Time 809.0 updated!\n",
      "Time 809.5 updated!\n",
      "Time 810.0 updated!\n",
      "Time 810.5 updated!\n",
      "Time 811.0 updated!\n",
      "Time 811.5 updated!\n",
      "Time 812.0 updated!\n",
      "Time 812.5 updated!\n",
      "Time 813.0 updated!\n",
      "Time 813.5 updated!\n",
      "Time 814.0 updated!\n",
      "Time 814.5 updated!\n",
      "Time 815.0 updated!\n",
      "Time 815.5 updated!\n",
      "Time 816.0 updated!\n",
      "Time 816.5 updated!\n",
      "Time 817.0 updated!\n",
      "Time 817.5 updated!\n",
      "Time 818.0 updated!\n",
      "Time 818.5 updated!\n",
      "Time 819.0 updated!\n",
      "Time 819.5 updated!\n",
      "Time 820.0 updated!\n",
      "Time 820.5 updated!\n",
      "Time 821.0 updated!\n",
      "Time 821.5 updated!\n",
      "Time 822.0 updated!\n",
      "Time 822.5 updated!\n",
      "Time 823.0 updated!\n",
      "Time 823.5 updated!\n",
      "Time 824.0 updated!\n",
      "Time 824.5 updated!\n",
      "Time 825.0 updated!\n",
      "Time 825.5 updated!\n",
      "Time 826.0 updated!\n",
      "Time 826.5 updated!\n",
      "Time 827.0 updated!\n",
      "Time 827.5 updated!\n",
      "Time 828.0 updated!\n",
      "Time 828.5 updated!\n",
      "Time 829.0 updated!\n",
      "Time 829.5 updated!\n",
      "Time 830.0 updated!\n",
      "Time 830.5 updated!\n",
      "Time 831.0 updated!\n",
      "Time 831.5 updated!\n",
      "Time 832.0 updated!\n",
      "Time 832.5 updated!\n",
      "Time 833.0 updated!\n",
      "Time 833.5 updated!\n",
      "Time 834.0 updated!\n",
      "Time 834.5 updated!\n",
      "Time 835.0 updated!\n",
      "Time 835.5 updated!\n",
      "Time 836.0 updated!\n",
      "Time 836.5 updated!\n",
      "Time 837.0 updated!\n",
      "Time 837.5 updated!\n",
      "Time 838.0 updated!\n",
      "Time 838.5 updated!\n",
      "Time 839.0 updated!\n",
      "Time 839.5 updated!\n",
      "Time 840.0 updated!\n",
      "Time 840.5 updated!\n",
      "Time 841.0 updated!\n",
      "Time 841.5 updated!\n",
      "Time 842.0 updated!\n",
      "Time 842.5 updated!\n",
      "Time 843.0 updated!\n",
      "Time 843.5 updated!\n",
      "Time 844.0 updated!\n",
      "Time 844.5 updated!\n",
      "Time 845.0 updated!\n",
      "Time 845.5 updated!\n",
      "Time 846.0 updated!\n",
      "Time 846.5 updated!\n",
      "Time 847.0 updated!\n",
      "Time 847.5 updated!\n",
      "Time 848.0 updated!\n",
      "Time 848.5 updated!\n",
      "Time 849.0 updated!\n",
      "Time 849.5 updated!\n",
      "Time 850.0 updated!\n",
      "Time 850.5 updated!\n",
      "Time 851.0 updated!\n",
      "Time 851.5 updated!\n",
      "Time 852.0 updated!\n",
      "Time 852.5 updated!\n",
      "Time 853.0 updated!\n",
      "Time 853.5 updated!\n",
      "Time 854.0 updated!\n",
      "Time 854.5 updated!\n",
      "Time 855.0 updated!\n",
      "Time 855.5 updated!\n",
      "Time 856.0 updated!\n",
      "Time 856.5 updated!\n",
      "Time 857.0 updated!\n",
      "Time 857.5 updated!\n",
      "Time 858.0 updated!\n",
      "Time 858.5 updated!\n",
      "Time 859.0 updated!\n",
      "Time 859.5 updated!\n",
      "Time 860.0 updated!\n",
      "Time 860.5 updated!\n",
      "Time 861.0 updated!\n",
      "Time 861.5 updated!\n",
      "Time 862.0 updated!\n",
      "Time 862.5 updated!\n",
      "Time 863.0 updated!\n",
      "Time 863.5 updated!\n",
      "Time 864.0 updated!\n",
      "Time 864.5 updated!\n",
      "Time 865.0 updated!\n",
      "Time 865.5 updated!\n",
      "Time 866.0 updated!\n",
      "Time 866.5 updated!\n",
      "Time 867.0 updated!\n",
      "Time 867.5 updated!\n",
      "Time 868.0 updated!\n",
      "Time 868.5 updated!\n",
      "Time 869.0 updated!\n",
      "Time 869.5 updated!\n",
      "Time 870.0 updated!\n",
      "Time 870.5 updated!\n",
      "Time 871.0 updated!\n",
      "Time 871.5 updated!\n",
      "Time 872.0 updated!\n",
      "Time 872.5 updated!\n",
      "Time 873.0 updated!\n",
      "Time 873.5 updated!\n",
      "Time 874.0 updated!\n",
      "Time 874.5 updated!\n",
      "Time 875.0 updated!\n",
      "Time 875.5 updated!\n",
      "Time 876.0 updated!\n",
      "Time 876.5 updated!\n",
      "Time 877.0 updated!\n",
      "Time 877.5 updated!\n",
      "Time 878.0 updated!\n",
      "Time 878.5 updated!\n",
      "Time 879.0 updated!\n",
      "Time 879.5 updated!\n",
      "Time 880.0 updated!\n",
      "Time 880.5 updated!\n",
      "Time 881.0 updated!\n",
      "Time 881.5 updated!\n",
      "Time 882.0 updated!\n",
      "Time 882.5 updated!\n",
      "Time 883.0 updated!\n",
      "Time 883.5 updated!\n",
      "Time 884.0 updated!\n",
      "Time 884.5 updated!\n",
      "Time 885.0 updated!\n",
      "Time 885.5 updated!\n",
      "Time 886.0 updated!\n",
      "Time 886.5 updated!\n",
      "Time 887.0 updated!\n",
      "Time 887.5 updated!\n",
      "Time 888.0 updated!\n",
      "Time 888.5 updated!\n",
      "Time 889.0 updated!\n",
      "Time 889.5 updated!\n",
      "Time 890.0 updated!\n",
      "Time 890.5 updated!\n",
      "Time 891.0 updated!\n",
      "Time 891.5 updated!\n",
      "Time 892.0 updated!\n",
      "Time 892.5 updated!\n",
      "Time 893.0 updated!\n",
      "Time 893.5 updated!\n",
      "Time 894.0 updated!\n",
      "Time 894.5 updated!\n",
      "Time 895.0 updated!\n",
      "Time 895.5 updated!\n",
      "Time 896.0 updated!\n",
      "Time 896.5 updated!\n",
      "Time 897.0 updated!\n",
      "Time 897.5 updated!\n",
      "Time 898.0 updated!\n",
      "Time 898.5 updated!\n",
      "Time 899.0 updated!\n",
      "Time 899.5 updated!\n",
      "Time 900.0 updated!\n",
      "Time 900.5 updated!\n",
      "Time 901.0 updated!\n",
      "Time 901.5 updated!\n",
      "Time 902.0 updated!\n",
      "Time 902.5 updated!\n",
      "Time 903.0 updated!\n",
      "Time 903.5 updated!\n",
      "Time 904.0 updated!\n",
      "Time 904.5 updated!\n",
      "Time 905.0 updated!\n",
      "Time 905.5 updated!\n",
      "Time 906.0 updated!\n",
      "Time 906.5 updated!\n",
      "Time 907.0 updated!\n",
      "Time 907.5 updated!\n",
      "Time 908.0 updated!\n",
      "Time 908.5 updated!\n",
      "Time 909.0 updated!\n",
      "Time 909.5 updated!\n",
      "Time 910.0 updated!\n",
      "Time 910.5 updated!\n",
      "Time 911.0 updated!\n",
      "Time 911.5 updated!\n",
      "Time 912.0 updated!\n",
      "Time 912.5 updated!\n",
      "Time 913.0 updated!\n",
      "Time 913.5 updated!\n",
      "Time 914.0 updated!\n",
      "Time 914.5 updated!\n",
      "Time 915.0 updated!\n",
      "Time 915.5 updated!\n",
      "Time 916.0 updated!\n",
      "Time 916.5 updated!\n",
      "Time 917.0 updated!\n",
      "Time 917.5 updated!\n",
      "Time 918.0 updated!\n",
      "Time 918.5 updated!\n",
      "Time 919.0 updated!\n",
      "Time 919.5 updated!\n",
      "Time 920.0 updated!\n",
      "Time 920.5 updated!\n",
      "Time 921.0 updated!\n",
      "Time 921.5 updated!\n",
      "Time 922.0 updated!\n",
      "Time 922.5 updated!\n",
      "Time 923.0 updated!\n",
      "Time 923.5 updated!\n",
      "Time 924.0 updated!\n",
      "Time 924.5 updated!\n",
      "Time 925.0 updated!\n",
      "Time 925.5 updated!\n",
      "Time 926.0 updated!\n",
      "Time 926.5 updated!\n",
      "Time 927.0 updated!\n",
      "Time 927.5 updated!\n",
      "Time 928.0 updated!\n",
      "Time 928.5 updated!\n",
      "Time 929.0 updated!\n",
      "Time 929.5 updated!\n",
      "Time 930.0 updated!\n",
      "Time 930.5 updated!\n",
      "Time 931.0 updated!\n",
      "Time 931.5 updated!\n",
      "Time 932.0 updated!\n",
      "Time 932.5 updated!\n",
      "Time 933.0 updated!\n",
      "Time 933.5 updated!\n",
      "Time 934.0 updated!\n",
      "Time 934.5 updated!\n",
      "Time 935.0 updated!\n",
      "Time 935.5 updated!\n",
      "Time 936.0 updated!\n",
      "Time 936.5 updated!\n",
      "Time 937.0 updated!\n",
      "Time 937.5 updated!\n",
      "Time 938.0 updated!\n",
      "Time 938.5 updated!\n",
      "Time 939.0 updated!\n",
      "Time 939.5 updated!\n",
      "Time 940.0 updated!\n",
      "Time 940.5 updated!\n",
      "Time 941.0 updated!\n",
      "Time 941.5 updated!\n",
      "Time 942.0 updated!\n",
      "Time 942.5 updated!\n",
      "Time 943.0 updated!\n",
      "Time 943.5 updated!\n",
      "Time 944.0 updated!\n",
      "Time 944.5 updated!\n",
      "Time 945.0 updated!\n",
      "Time 945.5 updated!\n",
      "Time 946.0 updated!\n",
      "Time 946.5 updated!\n",
      "Time 947.0 updated!\n",
      "Time 947.5 updated!\n",
      "Time 948.0 updated!\n",
      "Time 948.5 updated!\n",
      "Time 949.0 updated!\n",
      "Time 949.5 updated!\n",
      "Time 950.0 updated!\n",
      "Time 950.5 updated!\n",
      "Time 951.0 updated!\n",
      "Time 951.5 updated!\n",
      "Time 952.0 updated!\n",
      "Time 952.5 updated!\n",
      "Time 953.0 updated!\n",
      "Time 953.5 updated!\n",
      "Time 954.0 updated!\n",
      "Time 954.5 updated!\n",
      "Time 955.0 updated!\n",
      "Time 955.5 updated!\n",
      "Time 956.0 updated!\n",
      "Time 956.5 updated!\n",
      "Time 957.0 updated!\n",
      "Time 957.5 updated!\n",
      "Time 958.0 updated!\n",
      "Time 958.5 updated!\n",
      "Time 959.0 updated!\n",
      "Time 959.5 updated!\n",
      "Time 960.0 updated!\n",
      "Time 960.5 updated!\n",
      "Time 961.0 updated!\n",
      "Time 961.5 updated!\n",
      "Time 962.0 updated!\n",
      "Time 962.5 updated!\n",
      "Time 963.0 updated!\n",
      "Time 963.5 updated!\n",
      "Time 964.0 updated!\n",
      "Time 964.5 updated!\n",
      "Time 965.0 updated!\n",
      "Time 965.5 updated!\n",
      "Time 966.0 updated!\n",
      "Time 966.5 updated!\n",
      "Time 967.0 updated!\n",
      "Time 967.5 updated!\n",
      "Time 968.0 updated!\n",
      "Time 968.5 updated!\n",
      "Time 969.0 updated!\n",
      "Time 969.5 updated!\n",
      "Time 970.0 updated!\n",
      "Time 970.5 updated!\n",
      "Time 971.0 updated!\n",
      "Time 971.5 updated!\n",
      "Time 972.0 updated!\n",
      "Time 972.5 updated!\n",
      "Time 973.0 updated!\n",
      "Time 973.5 updated!\n",
      "Time 974.0 updated!\n",
      "Time 974.5 updated!\n",
      "Time 975.0 updated!\n",
      "Time 975.5 updated!\n",
      "Time 976.0 updated!\n",
      "Time 976.5 updated!\n",
      "Time 977.0 updated!\n",
      "Time 977.5 updated!\n",
      "Time 978.0 updated!\n",
      "Time 978.5 updated!\n",
      "Time 979.0 updated!\n",
      "Time 979.5 updated!\n",
      "Time 980.0 updated!\n",
      "Time 980.5 updated!\n",
      "Time 981.0 updated!\n",
      "Time 981.5 updated!\n",
      "Time 982.0 updated!\n",
      "Time 982.5 updated!\n",
      "Time 983.0 updated!\n",
      "Time 983.5 updated!\n",
      "Time 984.0 updated!\n",
      "Time 984.5 updated!\n",
      "Time 985.0 updated!\n",
      "Time 985.5 updated!\n",
      "Time 986.0 updated!\n",
      "Time 986.5 updated!\n",
      "Time 987.0 updated!\n",
      "Time 987.5 updated!\n",
      "Time 988.0 updated!\n",
      "Time 988.5 updated!\n",
      "Time 989.0 updated!\n",
      "Time 989.5 updated!\n",
      "Time 990.0 updated!\n",
      "Time 990.5 updated!\n",
      "Time 991.0 updated!\n",
      "Time 991.5 updated!\n",
      "Time 992.0 updated!\n",
      "Time 992.5 updated!\n",
      "Time 993.0 updated!\n",
      "Time 993.5 updated!\n",
      "Time 994.0 updated!\n",
      "Time 994.5 updated!\n",
      "Time 995.0 updated!\n",
      "Time 995.5 updated!\n",
      "Time 996.0 updated!\n",
      "Time 996.5 updated!\n",
      "Time 997.0 updated!\n",
      "Time 997.5 updated!\n",
      "Time 998.0 updated!\n",
      "Time 998.5 updated!\n",
      "Time 999.0 updated!\n",
      "Time 999.5 updated!\n",
      "Time 1000.0 updated!\n",
      "Time 1000.5 updated!\n",
      "Time 1001.0 updated!\n",
      "Time 1001.5 updated!\n",
      "Time 1002.0 updated!\n",
      "Time 1002.5 updated!\n",
      "Time 1003.0 updated!\n",
      "Time 1003.5 updated!\n",
      "Time 1004.0 updated!\n",
      "Time 1004.5 updated!\n",
      "Time 1005.0 updated!\n",
      "Time 1005.5 updated!\n",
      "Time 1006.0 updated!\n",
      "Time 1006.5 updated!\n",
      "Time 1007.0 updated!\n",
      "Time 1007.5 updated!\n",
      "Time 1008.0 updated!\n",
      "Time 1008.5 updated!\n",
      "Time 1009.0 updated!\n",
      "Time 1009.5 updated!\n",
      "Time 1010.0 updated!\n",
      "Time 1010.5 updated!\n",
      "Time 1011.0 updated!\n",
      "Time 1011.5 updated!\n",
      "Time 1012.0 updated!\n",
      "Time 1012.5 updated!\n",
      "Time 1013.0 updated!\n",
      "Time 1013.5 updated!\n",
      "Time 1014.0 updated!\n",
      "Time 1014.5 updated!\n",
      "Time 1015.0 updated!\n",
      "Time 1015.5 updated!\n",
      "Time 1016.0 updated!\n",
      "Time 1016.5 updated!\n",
      "Time 1017.0 updated!\n",
      "Time 1017.5 updated!\n",
      "Time 1018.0 updated!\n",
      "Time 1018.5 updated!\n",
      "Time 1019.0 updated!\n",
      "Time 1019.5 updated!\n",
      "Time 1020.0 updated!\n",
      "Time 1020.5 updated!\n",
      "Time 1021.0 updated!\n",
      "Time 1021.5 updated!\n",
      "Time 1022.0 updated!\n",
      "Time 1022.5 updated!\n",
      "Time 1023.0 updated!\n",
      "Time 1023.5 updated!\n",
      "Time 1024.0 updated!\n",
      "Time 1024.5 updated!\n",
      "Time 1025.0 updated!\n",
      "Time 1025.5 updated!\n",
      "Time 1026.0 updated!\n",
      "Time 1026.5 updated!\n",
      "Time 1027.0 updated!\n",
      "Time 1027.5 updated!\n",
      "Time 1028.0 updated!\n",
      "Time 1028.5 updated!\n",
      "Time 1029.0 updated!\n",
      "Time 1029.5 updated!\n",
      "Time 1030.0 updated!\n",
      "Time 1030.5 updated!\n",
      "Time 1031.0 updated!\n",
      "Time 1031.5 updated!\n",
      "Time 1032.0 updated!\n",
      "Time 1032.5 updated!\n",
      "Time 1033.0 updated!\n",
      "Time 1033.5 updated!\n",
      "Time 1034.0 updated!\n",
      "Time 1034.5 updated!\n",
      "Time 1035.0 updated!\n",
      "Time 1035.5 updated!\n",
      "Time 1036.0 updated!\n",
      "Time 1036.5 updated!\n",
      "Time 1037.0 updated!\n",
      "Time 1037.5 updated!\n",
      "Time 1038.0 updated!\n",
      "Time 1038.5 updated!\n",
      "Time 1039.0 updated!\n",
      "Time 1039.5 updated!\n",
      "Time 1040.0 updated!\n",
      "Time 1040.5 updated!\n",
      "Time 1041.0 updated!\n",
      "Time 1041.5 updated!\n",
      "Time 1042.0 updated!\n",
      "Time 1042.5 updated!\n",
      "Time 1043.0 updated!\n",
      "Time 1043.5 updated!\n",
      "Time 1044.0 updated!\n",
      "Time 1044.5 updated!\n",
      "Time 1045.0 updated!\n",
      "Time 1045.5 updated!\n",
      "Time 1046.0 updated!\n",
      "Time 1046.5 updated!\n",
      "Time 1047.0 updated!\n",
      "Time 1047.5 updated!\n",
      "Time 1048.0 updated!\n",
      "Time 1048.5 updated!\n",
      "Time 1049.0 updated!\n",
      "Time 1049.5 updated!\n",
      "Time 1050.0 updated!\n",
      "Time 1050.5 updated!\n",
      "Time 1051.0 updated!\n",
      "Time 1051.5 updated!\n",
      "Time 1052.0 updated!\n",
      "Time 1052.5 updated!\n",
      "Time 1053.0 updated!\n",
      "Time 1053.5 updated!\n",
      "Time 1054.0 updated!\n",
      "Time 1054.5 updated!\n",
      "Time 1055.0 updated!\n",
      "Time 1055.5 updated!\n",
      "Time 1056.0 updated!\n",
      "Time 1056.5 updated!\n",
      "Time 1057.0 updated!\n",
      "Time 1057.5 updated!\n",
      "Time 1058.0 updated!\n",
      "Time 1058.5 updated!\n",
      "Time 1059.0 updated!\n",
      "Time 1059.5 updated!\n",
      "Time 1060.0 updated!\n",
      "Time 1060.5 updated!\n",
      "Time 1061.0 updated!\n",
      "Time 1061.5 updated!\n",
      "Time 1062.0 updated!\n",
      "Time 1062.5 updated!\n",
      "Time 1063.0 updated!\n",
      "Time 1063.5 updated!\n",
      "Time 1064.0 updated!\n",
      "Time 1064.5 updated!\n",
      "Time 1065.0 updated!\n",
      "Time 1065.5 updated!\n",
      "Time 1066.0 updated!\n",
      "Time 1066.5 updated!\n",
      "Time 1067.0 updated!\n",
      "Time 1067.5 updated!\n",
      "Time 1068.0 updated!\n",
      "Time 1068.5 updated!\n",
      "Time 1069.0 updated!\n",
      "Time 1069.5 updated!\n",
      "Time 1070.0 updated!\n",
      "Time 1070.5 updated!\n",
      "Time 1071.0 updated!\n",
      "Time 1071.5 updated!\n",
      "Time 1072.0 updated!\n",
      "Time 1072.5 updated!\n",
      "Time 1073.0 updated!\n",
      "Time 1073.5 updated!\n",
      "Time 1074.0 updated!\n",
      "Time 1074.5 updated!\n",
      "Time 1075.0 updated!\n",
      "Time 1075.5 updated!\n",
      "Time 1076.0 updated!\n",
      "Time 1076.5 updated!\n",
      "Time 1077.0 updated!\n",
      "Time 1077.5 updated!\n",
      "Time 1078.0 updated!\n",
      "Time 1078.5 updated!\n",
      "Time 1079.0 updated!\n",
      "Time 1079.5 updated!\n",
      "Time 1080.0 updated!\n",
      "Time 1080.5 updated!\n",
      "Time 1081.0 updated!\n",
      "Time 1081.5 updated!\n",
      "Time 1082.0 updated!\n",
      "Time 1082.5 updated!\n",
      "Time 1083.0 updated!\n",
      "Time 1083.5 updated!\n",
      "Time 1084.0 updated!\n",
      "Time 1084.5 updated!\n",
      "Time 1085.0 updated!\n",
      "Time 1085.5 updated!\n",
      "Time 1086.0 updated!\n",
      "Time 1086.5 updated!\n",
      "Time 1087.0 updated!\n",
      "Time 1087.5 updated!\n",
      "Time 1088.0 updated!\n",
      "Time 1088.5 updated!\n",
      "Time 1089.0 updated!\n",
      "Time 1089.5 updated!\n",
      "Time 1090.0 updated!\n",
      "Time 1090.5 updated!\n",
      "Time 1091.0 updated!\n",
      "Time 1091.5 updated!\n",
      "Time 1092.0 updated!\n",
      "Time 1092.5 updated!\n",
      "Time 1093.0 updated!\n",
      "Time 1093.5 updated!\n",
      "Time 1094.0 updated!\n",
      "Time 1094.5 updated!\n",
      "Time 1095.0 updated!\n",
      "Time 1095.5 updated!\n",
      "Time 1096.0 updated!\n",
      "Time 1096.5 updated!\n",
      "Time 1097.0 updated!\n",
      "Time 1097.5 updated!\n",
      "Time 1098.0 updated!\n",
      "Time 1098.5 updated!\n",
      "Time 1099.0 updated!\n",
      "Time 1099.5 updated!\n",
      "Time 1100.0 updated!\n",
      "Time 1100.5 updated!\n",
      "Time 1101.0 updated!\n",
      "Time 1101.5 updated!\n",
      "Time 1102.0 updated!\n",
      "Time 1102.5 updated!\n",
      "Time 1103.0 updated!\n",
      "Time 1103.5 updated!\n",
      "Time 1104.0 updated!\n",
      "Time 1104.5 updated!\n",
      "Time 1105.0 updated!\n",
      "Time 1105.5 updated!\n",
      "Time 1106.0 updated!\n",
      "Time 1106.5 updated!\n",
      "Time 1107.0 updated!\n",
      "Time 1107.5 updated!\n",
      "Time 1108.0 updated!\n",
      "Time 1108.5 updated!\n",
      "Time 1109.0 updated!\n",
      "Time 1109.5 updated!\n",
      "Time 1110.0 updated!\n",
      "Time 1110.5 updated!\n",
      "Time 1111.0 updated!\n",
      "Time 1111.5 updated!\n",
      "Time 1112.0 updated!\n",
      "Time 1112.5 updated!\n",
      "Time 1113.0 updated!\n",
      "Time 1113.5 updated!\n",
      "Time 1114.0 updated!\n",
      "Time 1114.5 updated!\n",
      "Time 1115.0 updated!\n",
      "Time 1115.5 updated!\n",
      "Time 1116.0 updated!\n",
      "Time 1116.5 updated!\n",
      "Time 1117.0 updated!\n",
      "Time 1117.5 updated!\n",
      "Time 1118.0 updated!\n",
      "Time 1118.5 updated!\n",
      "Time 1119.0 updated!\n",
      "Time 1119.5 updated!\n",
      "Time 1120.0 updated!\n",
      "Time 1120.5 updated!\n",
      "Time 1121.0 updated!\n",
      "Time 1121.5 updated!\n",
      "Time 1122.0 updated!\n",
      "Time 1122.5 updated!\n",
      "Time 1123.0 updated!\n",
      "Time 1123.5 updated!\n",
      "Time 1124.0 updated!\n",
      "Time 1124.5 updated!\n",
      "Time 1125.0 updated!\n",
      "Time 1125.5 updated!\n",
      "Time 1126.0 updated!\n",
      "Time 1126.5 updated!\n",
      "Time 1127.0 updated!\n",
      "Time 1127.5 updated!\n",
      "Time 1128.0 updated!\n",
      "Time 1128.5 updated!\n",
      "Time 1129.0 updated!\n",
      "Time 1129.5 updated!\n",
      "Time 1130.0 updated!\n",
      "Time 1130.5 updated!\n",
      "Time 1131.0 updated!\n",
      "Time 1131.5 updated!\n",
      "Time 1132.0 updated!\n",
      "Time 1132.5 updated!\n",
      "Time 1133.0 updated!\n",
      "Time 1133.5 updated!\n",
      "Time 1134.0 updated!\n",
      "Time 1134.5 updated!\n",
      "Time 1135.0 updated!\n",
      "Time 1135.5 updated!\n",
      "Time 1136.0 updated!\n",
      "Time 1136.5 updated!\n",
      "Time 1137.0 updated!\n",
      "Time 1137.5 updated!\n",
      "Time 1138.0 updated!\n",
      "Time 1138.5 updated!\n",
      "Time 1139.0 updated!\n",
      "Time 1139.5 updated!\n",
      "Time 1140.0 updated!\n",
      "Time 1140.5 updated!\n",
      "Time 1141.0 updated!\n",
      "Time 1141.5 updated!\n",
      "Time 1142.0 updated!\n",
      "Time 1142.5 updated!\n",
      "Time 1143.0 updated!\n",
      "Time 1143.5 updated!\n",
      "Time 1144.0 updated!\n",
      "Time 1144.5 updated!\n",
      "Time 1145.0 updated!\n",
      "Time 1145.5 updated!\n",
      "Time 1146.0 updated!\n",
      "Time 1146.5 updated!\n",
      "Time 1147.0 updated!\n",
      "Time 1147.5 updated!\n",
      "Time 1148.0 updated!\n",
      "Time 1148.5 updated!\n",
      "Time 1149.0 updated!\n",
      "Time 1149.5 updated!\n",
      "Time 1150.0 updated!\n",
      "Time 1150.5 updated!\n",
      "Time 1151.0 updated!\n",
      "Time 1151.5 updated!\n",
      "Time 1152.0 updated!\n",
      "Time 1152.5 updated!\n",
      "Time 1153.0 updated!\n",
      "Time 1153.5 updated!\n",
      "Time 1154.0 updated!\n",
      "Time 1154.5 updated!\n",
      "Time 1155.0 updated!\n",
      "Time 1155.5 updated!\n",
      "Time 1156.0 updated!\n",
      "Time 1156.5 updated!\n",
      "Time 1157.0 updated!\n",
      "Time 1157.5 updated!\n",
      "Time 1158.0 updated!\n",
      "Time 1158.5 updated!\n",
      "Time 1159.0 updated!\n",
      "Time 1159.5 updated!\n",
      "Time 1160.0 updated!\n",
      "Time 1160.5 updated!\n",
      "Time 1161.0 updated!\n",
      "Time 1161.5 updated!\n",
      "Time 1162.0 updated!\n",
      "Time 1162.5 updated!\n",
      "Time 1163.0 updated!\n",
      "Time 1163.5 updated!\n",
      "Time 1164.0 updated!\n",
      "Time 1164.5 updated!\n",
      "Time 1165.0 updated!\n",
      "Time 1165.5 updated!\n",
      "Time 1166.0 updated!\n",
      "Time 1166.5 updated!\n",
      "Time 1167.0 updated!\n",
      "Time 1167.5 updated!\n",
      "Time 1168.0 updated!\n",
      "Time 1168.5 updated!\n",
      "Time 1169.0 updated!\n",
      "Time 1169.5 updated!\n",
      "Time 1170.0 updated!\n",
      "Time 1170.5 updated!\n",
      "Time 1171.0 updated!\n",
      "Time 1171.5 updated!\n",
      "Time 1172.0 updated!\n",
      "Time 1172.5 updated!\n",
      "Time 1173.0 updated!\n",
      "Time 1173.5 updated!\n",
      "Time 1174.0 updated!\n",
      "Time 1174.5 updated!\n",
      "Time 1175.0 updated!\n",
      "Time 1175.5 updated!\n",
      "Time 1176.0 updated!\n",
      "Time 1176.5 updated!\n",
      "Time 1177.0 updated!\n",
      "Time 1177.5 updated!\n",
      "Time 1178.0 updated!\n",
      "Time 1178.5 updated!\n",
      "Time 1179.0 updated!\n",
      "Time 1179.5 updated!\n",
      "Time 1180.0 updated!\n",
      "Time 1180.5 updated!\n",
      "Time 1181.0 updated!\n",
      "Time 1181.5 updated!\n",
      "Time 1182.0 updated!\n",
      "Time 1182.5 updated!\n",
      "Time 1183.0 updated!\n",
      "Time 1183.5 updated!\n",
      "Time 1184.0 updated!\n",
      "Time 1184.5 updated!\n",
      "Time 1185.0 updated!\n",
      "Time 1185.5 updated!\n",
      "Time 1186.0 updated!\n",
      "Time 1186.5 updated!\n",
      "Time 1187.0 updated!\n",
      "Time 1187.5 updated!\n",
      "Time 1188.0 updated!\n",
      "Time 1188.5 updated!\n",
      "Time 1189.0 updated!\n",
      "Time 1189.5 updated!\n",
      "Time 1190.0 updated!\n",
      "Time 1190.5 updated!\n",
      "Time 1191.0 updated!\n",
      "Time 1191.5 updated!\n",
      "Time 1192.0 updated!\n",
      "Time 1192.5 updated!\n",
      "Time 1193.0 updated!\n",
      "Time 1193.5 updated!\n",
      "Time 1194.0 updated!\n",
      "Time 1194.5 updated!\n",
      "Time 1195.0 updated!\n",
      "Time 1195.5 updated!\n",
      "Time 1196.0 updated!\n",
      "Time 1196.5 updated!\n",
      "Time 1197.0 updated!\n",
      "Time 1197.5 updated!\n",
      "Time 1198.0 updated!\n",
      "Time 1198.5 updated!\n",
      "Time 1199.0 updated!\n",
      "Time 1199.5 updated!\n",
      "Time 1200.0 updated!\n",
      "Time 1200.5 updated!\n",
      "Time 1201.0 updated!\n",
      "Time 1201.5 updated!\n",
      "Time 1202.0 updated!\n",
      "Time 1202.5 updated!\n",
      "Time 1203.0 updated!\n",
      "Time 1203.5 updated!\n",
      "Time 1204.0 updated!\n",
      "Time 1204.5 updated!\n",
      "Time 1205.0 updated!\n",
      "Time 1205.5 updated!\n",
      "Time 1206.0 updated!\n",
      "Time 1206.5 updated!\n",
      "Time 1207.0 updated!\n",
      "Time 1207.5 updated!\n",
      "Time 1208.0 updated!\n",
      "Time 1208.5 updated!\n",
      "Time 1209.0 updated!\n",
      "Time 1209.5 updated!\n",
      "Time 1210.0 updated!\n",
      "Time 1210.5 updated!\n",
      "Time 1211.0 updated!\n",
      "Time 1211.5 updated!\n",
      "Time 1212.0 updated!\n",
      "Time 1212.5 updated!\n",
      "Time 1213.0 updated!\n",
      "Time 1213.5 updated!\n",
      "Time 1214.0 updated!\n",
      "Time 1214.5 updated!\n",
      "Time 1215.0 updated!\n",
      "Time 1215.5 updated!\n",
      "Time 1216.0 updated!\n",
      "Time 1216.5 updated!\n",
      "Time 1217.0 updated!\n",
      "Time 1217.5 updated!\n",
      "Time 1218.0 updated!\n",
      "Time 1218.5 updated!\n",
      "Time 1219.0 updated!\n",
      "Time 1219.5 updated!\n",
      "Time 1220.0 updated!\n",
      "Time 1220.5 updated!\n",
      "Time 1221.0 updated!\n",
      "Time 1221.5 updated!\n",
      "Time 1222.0 updated!\n",
      "Time 1222.5 updated!\n",
      "Time 1223.0 updated!\n",
      "Time 1223.5 updated!\n",
      "Time 1224.0 updated!\n",
      "Time 1224.5 updated!\n",
      "Time 1225.0 updated!\n",
      "Time 1225.5 updated!\n",
      "Time 1226.0 updated!\n",
      "Time 1226.5 updated!\n",
      "Time 1227.0 updated!\n",
      "Time 1227.5 updated!\n",
      "Time 1228.0 updated!\n",
      "Time 1228.5 updated!\n",
      "Time 1229.0 updated!\n",
      "Time 1229.5 updated!\n",
      "Time 1230.0 updated!\n",
      "Time 1230.5 updated!\n",
      "Time 1231.0 updated!\n",
      "Time 1231.5 updated!\n",
      "Time 1232.0 updated!\n",
      "Time 1232.5 updated!\n",
      "Time 1233.0 updated!\n",
      "Time 1233.5 updated!\n",
      "Time 1234.0 updated!\n",
      "Time 1234.5 updated!\n",
      "Time 1235.0 updated!\n",
      "Time 1235.5 updated!\n",
      "Time 1236.0 updated!\n",
      "Time 1236.5 updated!\n",
      "Time 1237.0 updated!\n",
      "Time 1237.5 updated!\n",
      "Time 1238.0 updated!\n",
      "Time 1238.5 updated!\n",
      "Time 1239.0 updated!\n",
      "Time 1239.5 updated!\n",
      "Time 1240.0 updated!\n",
      "Time 1240.5 updated!\n",
      "Time 1241.0 updated!\n",
      "Time 1241.5 updated!\n",
      "Time 1242.0 updated!\n",
      "Time 1242.5 updated!\n",
      "Time 1243.0 updated!\n",
      "Time 1243.5 updated!\n",
      "Time 1244.0 updated!\n",
      "Time 1244.5 updated!\n",
      "Time 1245.0 updated!\n",
      "Time 1245.5 updated!\n",
      "Time 1246.0 updated!\n",
      "Time 1246.5 updated!\n",
      "Time 1247.0 updated!\n",
      "Time 1247.5 updated!\n",
      "Time 1248.0 updated!\n",
      "Time 1248.5 updated!\n",
      "Time 1249.0 updated!\n",
      "Time 1249.5 updated!\n",
      "Time 1250.0 updated!\n",
      "Time 1250.5 updated!\n",
      "Time 1251.0 updated!\n",
      "Time 1251.5 updated!\n",
      "Time 1252.0 updated!\n",
      "Time 1252.5 updated!\n",
      "Time 1253.0 updated!\n",
      "Time 1253.5 updated!\n",
      "Time 1254.0 updated!\n",
      "Time 1254.5 updated!\n",
      "Time 1255.0 updated!\n",
      "Time 1255.5 updated!\n",
      "Time 1256.0 updated!\n",
      "Time 1256.5 updated!\n",
      "Time 1257.0 updated!\n",
      "Time 1257.5 updated!\n",
      "Time 1258.0 updated!\n",
      "Time 1258.5 updated!\n",
      "Time 1259.0 updated!\n",
      "Time 1259.5 updated!\n",
      "Time 1260.0 updated!\n",
      "Time 1260.5 updated!\n",
      "Time 1261.0 updated!\n",
      "Time 1261.5 updated!\n",
      "Time 1262.0 updated!\n",
      "Time 1262.5 updated!\n",
      "Time 1263.0 updated!\n",
      "Time 1263.5 updated!\n",
      "Time 1264.0 updated!\n",
      "Time 1264.5 updated!\n",
      "Time 1265.0 updated!\n",
      "Time 1265.5 updated!\n",
      "Time 1266.0 updated!\n",
      "Time 1266.5 updated!\n",
      "Time 1267.0 updated!\n",
      "Time 1267.5 updated!\n",
      "Time 1268.0 updated!\n",
      "Time 1268.5 updated!\n",
      "Time 1269.0 updated!\n",
      "Time 1269.5 updated!\n",
      "Time 1270.0 updated!\n",
      "Time 1270.5 updated!\n",
      "Time 1271.0 updated!\n",
      "Time 1271.5 updated!\n",
      "Time 1272.0 updated!\n",
      "Time 1272.5 updated!\n",
      "Time 1273.0 updated!\n",
      "Time 1273.5 updated!\n",
      "Time 1274.0 updated!\n",
      "Time 1274.5 updated!\n",
      "Time 1275.0 updated!\n",
      "Time 1275.5 updated!\n",
      "Time 1276.0 updated!\n",
      "Time 1276.5 updated!\n",
      "Time 1277.0 updated!\n",
      "Time 1277.5 updated!\n",
      "Time 1278.0 updated!\n",
      "Time 1278.5 updated!\n",
      "Time 1279.0 updated!\n",
      "Time 1279.5 updated!\n",
      "Time 1280.0 updated!\n",
      "Time 1280.5 updated!\n",
      "Time 1281.0 updated!\n",
      "Time 1281.5 updated!\n",
      "Time 1282.0 updated!\n",
      "Time 1282.5 updated!\n",
      "Time 1283.0 updated!\n",
      "Time 1283.5 updated!\n",
      "Time 1284.0 updated!\n",
      "Time 1284.5 updated!\n",
      "Time 1285.0 updated!\n",
      "Time 1285.5 updated!\n",
      "Time 1286.0 updated!\n",
      "Time 1286.5 updated!\n",
      "Time 1287.0 updated!\n",
      "Time 1287.5 updated!\n",
      "Time 1288.0 updated!\n",
      "Time 1288.5 updated!\n",
      "Time 1289.0 updated!\n",
      "Time 1289.5 updated!\n",
      "Time 1290.0 updated!\n",
      "Time 1290.5 updated!\n",
      "Time 1291.0 updated!\n",
      "Time 1291.5 updated!\n",
      "Time 1292.0 updated!\n",
      "Time 1292.5 updated!\n",
      "Time 1293.0 updated!\n",
      "Time 1293.5 updated!\n",
      "Time 1294.0 updated!\n",
      "Time 1294.5 updated!\n",
      "Time 1295.0 updated!\n",
      "Time 1295.5 updated!\n",
      "Time 1296.0 updated!\n",
      "Time 1296.5 updated!\n",
      "Time 1297.0 updated!\n",
      "Time 1297.5 updated!\n",
      "Time 1298.0 updated!\n",
      "Time 1298.5 updated!\n",
      "Time 1299.0 updated!\n",
      "Time 1299.5 updated!\n",
      "Time 1300.0 updated!\n",
      "Time 1300.5 updated!\n",
      "Time 1301.0 updated!\n",
      "Time 1301.5 updated!\n",
      "Time 1302.0 updated!\n",
      "Time 1302.5 updated!\n",
      "Time 1303.0 updated!\n",
      "Time 1303.5 updated!\n",
      "Time 1304.0 updated!\n",
      "Time 1304.5 updated!\n",
      "Time 1305.0 updated!\n",
      "Time 1305.5 updated!\n",
      "Time 1306.0 updated!\n",
      "Time 1306.5 updated!\n",
      "Time 1307.0 updated!\n",
      "Time 1307.5 updated!\n",
      "Time 1308.0 updated!\n",
      "Time 1308.5 updated!\n",
      "Time 1309.0 updated!\n",
      "Time 1309.5 updated!\n",
      "Time 1310.0 updated!\n",
      "Time 1310.5 updated!\n",
      "Time 1311.0 updated!\n",
      "Time 1311.5 updated!\n",
      "Time 1312.0 updated!\n",
      "Time 1312.5 updated!\n",
      "Time 1313.0 updated!\n",
      "Time 1313.5 updated!\n",
      "Time 1314.0 updated!\n",
      "Time 1314.5 updated!\n",
      "Time 1315.0 updated!\n",
      "Time 1315.5 updated!\n",
      "Time 1316.0 updated!\n",
      "Time 1316.5 updated!\n",
      "Time 1317.0 updated!\n",
      "Time 1317.5 updated!\n",
      "Time 1318.0 updated!\n",
      "Time 1318.5 updated!\n",
      "Time 1319.0 updated!\n",
      "Time 1319.5 updated!\n",
      "Time 1320.0 updated!\n",
      "Time 1320.5 updated!\n",
      "Time 1321.0 updated!\n",
      "Time 1321.5 updated!\n",
      "Time 1322.0 updated!\n",
      "Time 1322.5 updated!\n",
      "Time 1323.0 updated!\n",
      "Time 1323.5 updated!\n",
      "Time 1324.0 updated!\n",
      "Time 1324.5 updated!\n",
      "Time 1325.0 updated!\n",
      "Time 1325.5 updated!\n",
      "Time 1326.0 updated!\n",
      "Time 1326.5 updated!\n",
      "Time 1327.0 updated!\n",
      "Time 1327.5 updated!\n",
      "Time 1328.0 updated!\n",
      "Time 1328.5 updated!\n",
      "Time 1329.0 updated!\n",
      "Time 1329.5 updated!\n",
      "Time 1330.0 updated!\n",
      "Time 1330.5 updated!\n",
      "Time 1331.0 updated!\n",
      "Time 1331.5 updated!\n",
      "Time 1332.0 updated!\n",
      "Time 1332.5 updated!\n",
      "Time 1333.0 updated!\n",
      "Time 1333.5 updated!\n",
      "Time 1334.0 updated!\n",
      "Time 1334.5 updated!\n",
      "Time 1335.0 updated!\n",
      "Time 1335.5 updated!\n",
      "Time 1336.0 updated!\n",
      "Time 1336.5 updated!\n",
      "Time 1337.0 updated!\n",
      "Time 1337.5 updated!\n",
      "Time 1338.0 updated!\n",
      "Time 1338.5 updated!\n",
      "Time 1339.0 updated!\n",
      "Time 1339.5 updated!\n",
      "Time 1340.0 updated!\n",
      "Time 1340.5 updated!\n",
      "Time 1341.0 updated!\n",
      "Time 1341.5 updated!\n",
      "Time 1342.0 updated!\n",
      "Time 1342.5 updated!\n",
      "Time 1343.0 updated!\n",
      "Time 1343.5 updated!\n",
      "Time 1344.0 updated!\n",
      "Time 1344.5 updated!\n",
      "Time 1345.0 updated!\n",
      "Time 1345.5 updated!\n",
      "Time 1346.0 updated!\n",
      "Time 1346.5 updated!\n",
      "Time 1347.0 updated!\n",
      "Time 1347.5 updated!\n",
      "Time 1348.0 updated!\n",
      "Time 1348.5 updated!\n",
      "Time 1349.0 updated!\n",
      "Time 1349.5 updated!\n",
      "Time 1350.0 updated!\n",
      "Time 1350.5 updated!\n",
      "Time 1351.0 updated!\n",
      "Time 1351.5 updated!\n",
      "Time 1352.0 updated!\n",
      "Time 1352.5 updated!\n",
      "Time 1353.0 updated!\n",
      "Time 1353.5 updated!\n",
      "Time 1354.0 updated!\n",
      "Time 1354.5 updated!\n",
      "Time 1355.0 updated!\n",
      "Time 1355.5 updated!\n",
      "Time 1356.0 updated!\n",
      "Time 1356.5 updated!\n",
      "Time 1357.0 updated!\n",
      "Time 1357.5 updated!\n",
      "Time 1358.0 updated!\n",
      "Time 1358.5 updated!\n",
      "Time 1359.0 updated!\n",
      "Time 1359.5 updated!\n",
      "Time 1360.0 updated!\n",
      "Time 1360.5 updated!\n",
      "Time 1361.0 updated!\n",
      "Time 1361.5 updated!\n",
      "Time 1362.0 updated!\n",
      "Time 1362.5 updated!\n",
      "Time 1363.0 updated!\n",
      "Time 1363.5 updated!\n",
      "Time 1364.0 updated!\n",
      "Time 1364.5 updated!\n",
      "Time 1365.0 updated!\n",
      "Time 1365.5 updated!\n",
      "Time 1366.0 updated!\n",
      "Time 1366.5 updated!\n",
      "Time 1367.0 updated!\n",
      "Time 1367.5 updated!\n",
      "Time 1368.0 updated!\n",
      "Time 1368.5 updated!\n",
      "Time 1369.0 updated!\n",
      "Time 1369.5 updated!\n",
      "Time 1370.0 updated!\n",
      "Time 1370.5 updated!\n",
      "Time 1371.0 updated!\n",
      "Time 1371.5 updated!\n",
      "Time 1372.0 updated!\n",
      "Time 1372.5 updated!\n",
      "Time 1373.0 updated!\n",
      "Time 1373.5 updated!\n",
      "Time 1374.0 updated!\n",
      "Time 1374.5 updated!\n",
      "Time 1375.0 updated!\n",
      "Time 1375.5 updated!\n",
      "Time 1376.0 updated!\n",
      "Time 1376.5 updated!\n",
      "Time 1377.0 updated!\n",
      "Time 1377.5 updated!\n",
      "Time 1378.0 updated!\n",
      "Time 1378.5 updated!\n",
      "Time 1379.0 updated!\n",
      "Time 1379.5 updated!\n",
      "Time 1380.0 updated!\n",
      "Time 1380.5 updated!\n",
      "Time 1381.0 updated!\n",
      "Time 1381.5 updated!\n",
      "Time 1382.0 updated!\n",
      "Time 1382.5 updated!\n",
      "Time 1383.0 updated!\n",
      "Time 1383.5 updated!\n",
      "Time 1384.0 updated!\n",
      "Time 1384.5 updated!\n",
      "Time 1385.0 updated!\n",
      "Time 1385.5 updated!\n",
      "Time 1386.0 updated!\n",
      "Time 1386.5 updated!\n",
      "Time 1387.0 updated!\n",
      "Time 1387.5 updated!\n",
      "Time 1388.0 updated!\n",
      "Time 1388.5 updated!\n",
      "Time 1389.0 updated!\n",
      "Time 1389.5 updated!\n",
      "Time 1390.0 updated!\n",
      "Time 1390.5 updated!\n",
      "Time 1391.0 updated!\n",
      "Time 1391.5 updated!\n",
      "Time 1392.0 updated!\n",
      "Time 1392.5 updated!\n",
      "Time 1393.0 updated!\n",
      "Time 1393.5 updated!\n",
      "Time 1394.0 updated!\n",
      "Time 1394.5 updated!\n",
      "Time 1395.0 updated!\n",
      "Time 1395.5 updated!\n",
      "Time 1396.0 updated!\n",
      "Time 1396.5 updated!\n",
      "Time 1397.0 updated!\n",
      "Time 1397.5 updated!\n",
      "Time 1398.0 updated!\n",
      "Time 1398.5 updated!\n",
      "Time 1399.0 updated!\n",
      "Time 1399.5 updated!\n",
      "Time 1400.0 updated!\n",
      "Time 1400.5 updated!\n"
     ]
    }
   ],
   "source": [
    "norm_ls = check_noise(model_test, n, 0.5, 4 * n, num_samples=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7d85723-836f-4415-b512-efd3e47e098d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time_points = torch.linspace(0.5, 4 * n, 8 * n) / n\n",
    "plt.plot(time_points.detach().cpu().numpy(), norm_ls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78a59806-f1a0-490e-ac83-b838bebabcd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation_detailed(model, num_samples, n, time_step, terminal_time, device=device):\n",
    "    # Create current values\n",
    "    y = torch.zeros(num_samples, n, n, device=device)\n",
    "    diff_sample = torch.zeros(num_samples, n, n, device=device)\n",
    "    t = 0\n",
    "\n",
    "    # List of Frobenius norms at each time\n",
    "    norm_ls = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        while t <= terminal_time:\n",
    "            if t == 0:\n",
    "               t += time_step\n",
    "    \n",
    "            else:\n",
    "                # Update y\n",
    "                y = y + time_step * diff_sample + torch.randn_like(y, device=device) * (time_step ** (1/2))\n",
    "    \n",
    "                # Symmetrize y as input\n",
    "                y_symm = (1/2) * (y + y.transpose(-1, -2))\n",
    "                y_symm_expand = y_symm.unsqueeze(-1)\n",
    "    \n",
    "                # Update diffusion sample\n",
    "                # Generate random node embedding\n",
    "                x_init = torch.randn((num_samples, n), device=device)\n",
    "                x_init = x_init / x_init.norm(dim=1, keepdim=True)\n",
    "                x_init_expand = x_init.unsqueeze(-1)\n",
    "    \n",
    "                # Create time array\n",
    "                time_array = torch.ones((num_samples, 1, 1), device=device) * t\n",
    "    \n",
    "                # Evaluate the diffusion sample\n",
    "                diff_sample = model.predict(x_init_expand, y_symm_expand / t, time_array)\n",
    "\n",
    "                # Compute Frobenius norms\n",
    "                fn_sample = torch.norm(diff_sample, p=\"fro\", dim=(1, 2))\n",
    "\n",
    "                norm_ls.append(torch.mean(fn_sample).item())\n",
    "    \n",
    "                # Increment t\n",
    "                t += time_step\n",
    "\n",
    "            # Print out t\n",
    "            print(\"Time {} updated\".format(t))\n",
    "\n",
    "    return diff_sample, norm_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ad6b051-27a9-43b6-8dd3-0f25fbf3f660",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.5 updated\n",
      "Time 1.0 updated\n",
      "Time 1.5 updated\n",
      "Time 2.0 updated\n",
      "Time 2.5 updated\n",
      "Time 3.0 updated\n",
      "Time 3.5 updated\n",
      "Time 4.0 updated\n",
      "Time 4.5 updated\n",
      "Time 5.0 updated\n",
      "Time 5.5 updated\n",
      "Time 6.0 updated\n",
      "Time 6.5 updated\n",
      "Time 7.0 updated\n",
      "Time 7.5 updated\n",
      "Time 8.0 updated\n",
      "Time 8.5 updated\n",
      "Time 9.0 updated\n",
      "Time 9.5 updated\n",
      "Time 10.0 updated\n",
      "Time 10.5 updated\n",
      "Time 11.0 updated\n",
      "Time 11.5 updated\n",
      "Time 12.0 updated\n",
      "Time 12.5 updated\n",
      "Time 13.0 updated\n",
      "Time 13.5 updated\n",
      "Time 14.0 updated\n",
      "Time 14.5 updated\n",
      "Time 15.0 updated\n",
      "Time 15.5 updated\n",
      "Time 16.0 updated\n",
      "Time 16.5 updated\n",
      "Time 17.0 updated\n",
      "Time 17.5 updated\n",
      "Time 18.0 updated\n",
      "Time 18.5 updated\n",
      "Time 19.0 updated\n",
      "Time 19.5 updated\n",
      "Time 20.0 updated\n",
      "Time 20.5 updated\n",
      "Time 21.0 updated\n",
      "Time 21.5 updated\n",
      "Time 22.0 updated\n",
      "Time 22.5 updated\n",
      "Time 23.0 updated\n",
      "Time 23.5 updated\n",
      "Time 24.0 updated\n",
      "Time 24.5 updated\n",
      "Time 25.0 updated\n",
      "Time 25.5 updated\n",
      "Time 26.0 updated\n",
      "Time 26.5 updated\n",
      "Time 27.0 updated\n",
      "Time 27.5 updated\n",
      "Time 28.0 updated\n",
      "Time 28.5 updated\n",
      "Time 29.0 updated\n",
      "Time 29.5 updated\n",
      "Time 30.0 updated\n",
      "Time 30.5 updated\n",
      "Time 31.0 updated\n",
      "Time 31.5 updated\n",
      "Time 32.0 updated\n",
      "Time 32.5 updated\n",
      "Time 33.0 updated\n",
      "Time 33.5 updated\n",
      "Time 34.0 updated\n",
      "Time 34.5 updated\n",
      "Time 35.0 updated\n",
      "Time 35.5 updated\n",
      "Time 36.0 updated\n",
      "Time 36.5 updated\n",
      "Time 37.0 updated\n",
      "Time 37.5 updated\n",
      "Time 38.0 updated\n",
      "Time 38.5 updated\n",
      "Time 39.0 updated\n",
      "Time 39.5 updated\n",
      "Time 40.0 updated\n",
      "Time 40.5 updated\n",
      "Time 41.0 updated\n",
      "Time 41.5 updated\n",
      "Time 42.0 updated\n",
      "Time 42.5 updated\n",
      "Time 43.0 updated\n",
      "Time 43.5 updated\n",
      "Time 44.0 updated\n",
      "Time 44.5 updated\n",
      "Time 45.0 updated\n",
      "Time 45.5 updated\n",
      "Time 46.0 updated\n",
      "Time 46.5 updated\n",
      "Time 47.0 updated\n",
      "Time 47.5 updated\n",
      "Time 48.0 updated\n",
      "Time 48.5 updated\n",
      "Time 49.0 updated\n",
      "Time 49.5 updated\n",
      "Time 50.0 updated\n",
      "Time 50.5 updated\n",
      "Time 51.0 updated\n",
      "Time 51.5 updated\n",
      "Time 52.0 updated\n",
      "Time 52.5 updated\n",
      "Time 53.0 updated\n",
      "Time 53.5 updated\n",
      "Time 54.0 updated\n",
      "Time 54.5 updated\n",
      "Time 55.0 updated\n",
      "Time 55.5 updated\n",
      "Time 56.0 updated\n",
      "Time 56.5 updated\n",
      "Time 57.0 updated\n",
      "Time 57.5 updated\n",
      "Time 58.0 updated\n",
      "Time 58.5 updated\n",
      "Time 59.0 updated\n",
      "Time 59.5 updated\n",
      "Time 60.0 updated\n",
      "Time 60.5 updated\n",
      "Time 61.0 updated\n",
      "Time 61.5 updated\n",
      "Time 62.0 updated\n",
      "Time 62.5 updated\n",
      "Time 63.0 updated\n",
      "Time 63.5 updated\n",
      "Time 64.0 updated\n",
      "Time 64.5 updated\n",
      "Time 65.0 updated\n",
      "Time 65.5 updated\n",
      "Time 66.0 updated\n",
      "Time 66.5 updated\n",
      "Time 67.0 updated\n",
      "Time 67.5 updated\n",
      "Time 68.0 updated\n",
      "Time 68.5 updated\n",
      "Time 69.0 updated\n",
      "Time 69.5 updated\n",
      "Time 70.0 updated\n",
      "Time 70.5 updated\n",
      "Time 71.0 updated\n",
      "Time 71.5 updated\n",
      "Time 72.0 updated\n",
      "Time 72.5 updated\n",
      "Time 73.0 updated\n",
      "Time 73.5 updated\n",
      "Time 74.0 updated\n",
      "Time 74.5 updated\n",
      "Time 75.0 updated\n",
      "Time 75.5 updated\n",
      "Time 76.0 updated\n",
      "Time 76.5 updated\n",
      "Time 77.0 updated\n",
      "Time 77.5 updated\n",
      "Time 78.0 updated\n",
      "Time 78.5 updated\n",
      "Time 79.0 updated\n",
      "Time 79.5 updated\n",
      "Time 80.0 updated\n",
      "Time 80.5 updated\n",
      "Time 81.0 updated\n",
      "Time 81.5 updated\n",
      "Time 82.0 updated\n",
      "Time 82.5 updated\n",
      "Time 83.0 updated\n",
      "Time 83.5 updated\n",
      "Time 84.0 updated\n",
      "Time 84.5 updated\n",
      "Time 85.0 updated\n",
      "Time 85.5 updated\n",
      "Time 86.0 updated\n",
      "Time 86.5 updated\n",
      "Time 87.0 updated\n",
      "Time 87.5 updated\n",
      "Time 88.0 updated\n",
      "Time 88.5 updated\n",
      "Time 89.0 updated\n",
      "Time 89.5 updated\n",
      "Time 90.0 updated\n",
      "Time 90.5 updated\n",
      "Time 91.0 updated\n",
      "Time 91.5 updated\n",
      "Time 92.0 updated\n",
      "Time 92.5 updated\n",
      "Time 93.0 updated\n",
      "Time 93.5 updated\n",
      "Time 94.0 updated\n",
      "Time 94.5 updated\n",
      "Time 95.0 updated\n",
      "Time 95.5 updated\n",
      "Time 96.0 updated\n",
      "Time 96.5 updated\n",
      "Time 97.0 updated\n",
      "Time 97.5 updated\n",
      "Time 98.0 updated\n",
      "Time 98.5 updated\n",
      "Time 99.0 updated\n",
      "Time 99.5 updated\n",
      "Time 100.0 updated\n",
      "Time 100.5 updated\n",
      "Time 101.0 updated\n",
      "Time 101.5 updated\n",
      "Time 102.0 updated\n",
      "Time 102.5 updated\n",
      "Time 103.0 updated\n",
      "Time 103.5 updated\n",
      "Time 104.0 updated\n",
      "Time 104.5 updated\n",
      "Time 105.0 updated\n",
      "Time 105.5 updated\n",
      "Time 106.0 updated\n",
      "Time 106.5 updated\n",
      "Time 107.0 updated\n",
      "Time 107.5 updated\n",
      "Time 108.0 updated\n",
      "Time 108.5 updated\n",
      "Time 109.0 updated\n",
      "Time 109.5 updated\n",
      "Time 110.0 updated\n",
      "Time 110.5 updated\n",
      "Time 111.0 updated\n",
      "Time 111.5 updated\n",
      "Time 112.0 updated\n",
      "Time 112.5 updated\n",
      "Time 113.0 updated\n",
      "Time 113.5 updated\n",
      "Time 114.0 updated\n",
      "Time 114.5 updated\n",
      "Time 115.0 updated\n",
      "Time 115.5 updated\n",
      "Time 116.0 updated\n",
      "Time 116.5 updated\n",
      "Time 117.0 updated\n",
      "Time 117.5 updated\n",
      "Time 118.0 updated\n",
      "Time 118.5 updated\n",
      "Time 119.0 updated\n",
      "Time 119.5 updated\n",
      "Time 120.0 updated\n",
      "Time 120.5 updated\n",
      "Time 121.0 updated\n",
      "Time 121.5 updated\n",
      "Time 122.0 updated\n",
      "Time 122.5 updated\n",
      "Time 123.0 updated\n",
      "Time 123.5 updated\n",
      "Time 124.0 updated\n",
      "Time 124.5 updated\n",
      "Time 125.0 updated\n",
      "Time 125.5 updated\n",
      "Time 126.0 updated\n",
      "Time 126.5 updated\n",
      "Time 127.0 updated\n",
      "Time 127.5 updated\n",
      "Time 128.0 updated\n",
      "Time 128.5 updated\n",
      "Time 129.0 updated\n",
      "Time 129.5 updated\n",
      "Time 130.0 updated\n",
      "Time 130.5 updated\n",
      "Time 131.0 updated\n",
      "Time 131.5 updated\n",
      "Time 132.0 updated\n",
      "Time 132.5 updated\n",
      "Time 133.0 updated\n",
      "Time 133.5 updated\n",
      "Time 134.0 updated\n",
      "Time 134.5 updated\n",
      "Time 135.0 updated\n",
      "Time 135.5 updated\n",
      "Time 136.0 updated\n",
      "Time 136.5 updated\n",
      "Time 137.0 updated\n",
      "Time 137.5 updated\n",
      "Time 138.0 updated\n",
      "Time 138.5 updated\n",
      "Time 139.0 updated\n",
      "Time 139.5 updated\n",
      "Time 140.0 updated\n",
      "Time 140.5 updated\n",
      "Time 141.0 updated\n",
      "Time 141.5 updated\n",
      "Time 142.0 updated\n",
      "Time 142.5 updated\n",
      "Time 143.0 updated\n",
      "Time 143.5 updated\n",
      "Time 144.0 updated\n",
      "Time 144.5 updated\n",
      "Time 145.0 updated\n",
      "Time 145.5 updated\n",
      "Time 146.0 updated\n",
      "Time 146.5 updated\n",
      "Time 147.0 updated\n",
      "Time 147.5 updated\n",
      "Time 148.0 updated\n",
      "Time 148.5 updated\n",
      "Time 149.0 updated\n",
      "Time 149.5 updated\n",
      "Time 150.0 updated\n",
      "Time 150.5 updated\n",
      "Time 151.0 updated\n",
      "Time 151.5 updated\n",
      "Time 152.0 updated\n",
      "Time 152.5 updated\n",
      "Time 153.0 updated\n",
      "Time 153.5 updated\n",
      "Time 154.0 updated\n",
      "Time 154.5 updated\n",
      "Time 155.0 updated\n",
      "Time 155.5 updated\n",
      "Time 156.0 updated\n",
      "Time 156.5 updated\n",
      "Time 157.0 updated\n",
      "Time 157.5 updated\n",
      "Time 158.0 updated\n",
      "Time 158.5 updated\n",
      "Time 159.0 updated\n",
      "Time 159.5 updated\n",
      "Time 160.0 updated\n",
      "Time 160.5 updated\n",
      "Time 161.0 updated\n",
      "Time 161.5 updated\n",
      "Time 162.0 updated\n",
      "Time 162.5 updated\n",
      "Time 163.0 updated\n",
      "Time 163.5 updated\n",
      "Time 164.0 updated\n",
      "Time 164.5 updated\n",
      "Time 165.0 updated\n",
      "Time 165.5 updated\n",
      "Time 166.0 updated\n",
      "Time 166.5 updated\n",
      "Time 167.0 updated\n",
      "Time 167.5 updated\n",
      "Time 168.0 updated\n",
      "Time 168.5 updated\n",
      "Time 169.0 updated\n",
      "Time 169.5 updated\n",
      "Time 170.0 updated\n",
      "Time 170.5 updated\n",
      "Time 171.0 updated\n",
      "Time 171.5 updated\n",
      "Time 172.0 updated\n",
      "Time 172.5 updated\n",
      "Time 173.0 updated\n",
      "Time 173.5 updated\n",
      "Time 174.0 updated\n",
      "Time 174.5 updated\n",
      "Time 175.0 updated\n",
      "Time 175.5 updated\n",
      "Time 176.0 updated\n",
      "Time 176.5 updated\n",
      "Time 177.0 updated\n",
      "Time 177.5 updated\n",
      "Time 178.0 updated\n",
      "Time 178.5 updated\n",
      "Time 179.0 updated\n",
      "Time 179.5 updated\n",
      "Time 180.0 updated\n",
      "Time 180.5 updated\n",
      "Time 181.0 updated\n",
      "Time 181.5 updated\n",
      "Time 182.0 updated\n",
      "Time 182.5 updated\n",
      "Time 183.0 updated\n",
      "Time 183.5 updated\n",
      "Time 184.0 updated\n",
      "Time 184.5 updated\n",
      "Time 185.0 updated\n",
      "Time 185.5 updated\n",
      "Time 186.0 updated\n",
      "Time 186.5 updated\n",
      "Time 187.0 updated\n",
      "Time 187.5 updated\n",
      "Time 188.0 updated\n",
      "Time 188.5 updated\n",
      "Time 189.0 updated\n",
      "Time 189.5 updated\n",
      "Time 190.0 updated\n",
      "Time 190.5 updated\n",
      "Time 191.0 updated\n",
      "Time 191.5 updated\n",
      "Time 192.0 updated\n",
      "Time 192.5 updated\n",
      "Time 193.0 updated\n",
      "Time 193.5 updated\n",
      "Time 194.0 updated\n",
      "Time 194.5 updated\n",
      "Time 195.0 updated\n",
      "Time 195.5 updated\n",
      "Time 196.0 updated\n",
      "Time 196.5 updated\n",
      "Time 197.0 updated\n",
      "Time 197.5 updated\n",
      "Time 198.0 updated\n",
      "Time 198.5 updated\n",
      "Time 199.0 updated\n",
      "Time 199.5 updated\n",
      "Time 200.0 updated\n",
      "Time 200.5 updated\n",
      "Time 201.0 updated\n",
      "Time 201.5 updated\n",
      "Time 202.0 updated\n",
      "Time 202.5 updated\n",
      "Time 203.0 updated\n",
      "Time 203.5 updated\n",
      "Time 204.0 updated\n",
      "Time 204.5 updated\n",
      "Time 205.0 updated\n",
      "Time 205.5 updated\n",
      "Time 206.0 updated\n",
      "Time 206.5 updated\n",
      "Time 207.0 updated\n",
      "Time 207.5 updated\n",
      "Time 208.0 updated\n",
      "Time 208.5 updated\n",
      "Time 209.0 updated\n",
      "Time 209.5 updated\n",
      "Time 210.0 updated\n",
      "Time 210.5 updated\n",
      "Time 211.0 updated\n",
      "Time 211.5 updated\n",
      "Time 212.0 updated\n",
      "Time 212.5 updated\n",
      "Time 213.0 updated\n",
      "Time 213.5 updated\n",
      "Time 214.0 updated\n",
      "Time 214.5 updated\n",
      "Time 215.0 updated\n",
      "Time 215.5 updated\n",
      "Time 216.0 updated\n",
      "Time 216.5 updated\n",
      "Time 217.0 updated\n",
      "Time 217.5 updated\n",
      "Time 218.0 updated\n",
      "Time 218.5 updated\n",
      "Time 219.0 updated\n",
      "Time 219.5 updated\n",
      "Time 220.0 updated\n",
      "Time 220.5 updated\n",
      "Time 221.0 updated\n",
      "Time 221.5 updated\n",
      "Time 222.0 updated\n",
      "Time 222.5 updated\n",
      "Time 223.0 updated\n",
      "Time 223.5 updated\n",
      "Time 224.0 updated\n",
      "Time 224.5 updated\n",
      "Time 225.0 updated\n",
      "Time 225.5 updated\n",
      "Time 226.0 updated\n",
      "Time 226.5 updated\n",
      "Time 227.0 updated\n",
      "Time 227.5 updated\n",
      "Time 228.0 updated\n",
      "Time 228.5 updated\n",
      "Time 229.0 updated\n",
      "Time 229.5 updated\n",
      "Time 230.0 updated\n",
      "Time 230.5 updated\n",
      "Time 231.0 updated\n",
      "Time 231.5 updated\n",
      "Time 232.0 updated\n",
      "Time 232.5 updated\n",
      "Time 233.0 updated\n",
      "Time 233.5 updated\n",
      "Time 234.0 updated\n",
      "Time 234.5 updated\n",
      "Time 235.0 updated\n",
      "Time 235.5 updated\n",
      "Time 236.0 updated\n",
      "Time 236.5 updated\n",
      "Time 237.0 updated\n",
      "Time 237.5 updated\n",
      "Time 238.0 updated\n",
      "Time 238.5 updated\n",
      "Time 239.0 updated\n",
      "Time 239.5 updated\n",
      "Time 240.0 updated\n",
      "Time 240.5 updated\n",
      "Time 241.0 updated\n",
      "Time 241.5 updated\n",
      "Time 242.0 updated\n",
      "Time 242.5 updated\n",
      "Time 243.0 updated\n",
      "Time 243.5 updated\n",
      "Time 244.0 updated\n",
      "Time 244.5 updated\n",
      "Time 245.0 updated\n",
      "Time 245.5 updated\n",
      "Time 246.0 updated\n",
      "Time 246.5 updated\n",
      "Time 247.0 updated\n",
      "Time 247.5 updated\n",
      "Time 248.0 updated\n",
      "Time 248.5 updated\n",
      "Time 249.0 updated\n",
      "Time 249.5 updated\n",
      "Time 250.0 updated\n",
      "Time 250.5 updated\n",
      "Time 251.0 updated\n",
      "Time 251.5 updated\n",
      "Time 252.0 updated\n",
      "Time 252.5 updated\n",
      "Time 253.0 updated\n",
      "Time 253.5 updated\n",
      "Time 254.0 updated\n",
      "Time 254.5 updated\n",
      "Time 255.0 updated\n",
      "Time 255.5 updated\n",
      "Time 256.0 updated\n",
      "Time 256.5 updated\n",
      "Time 257.0 updated\n",
      "Time 257.5 updated\n",
      "Time 258.0 updated\n",
      "Time 258.5 updated\n",
      "Time 259.0 updated\n",
      "Time 259.5 updated\n",
      "Time 260.0 updated\n",
      "Time 260.5 updated\n",
      "Time 261.0 updated\n",
      "Time 261.5 updated\n",
      "Time 262.0 updated\n",
      "Time 262.5 updated\n",
      "Time 263.0 updated\n",
      "Time 263.5 updated\n",
      "Time 264.0 updated\n",
      "Time 264.5 updated\n",
      "Time 265.0 updated\n",
      "Time 265.5 updated\n",
      "Time 266.0 updated\n",
      "Time 266.5 updated\n",
      "Time 267.0 updated\n",
      "Time 267.5 updated\n",
      "Time 268.0 updated\n",
      "Time 268.5 updated\n",
      "Time 269.0 updated\n",
      "Time 269.5 updated\n",
      "Time 270.0 updated\n",
      "Time 270.5 updated\n",
      "Time 271.0 updated\n",
      "Time 271.5 updated\n",
      "Time 272.0 updated\n",
      "Time 272.5 updated\n",
      "Time 273.0 updated\n",
      "Time 273.5 updated\n",
      "Time 274.0 updated\n",
      "Time 274.5 updated\n",
      "Time 275.0 updated\n",
      "Time 275.5 updated\n",
      "Time 276.0 updated\n",
      "Time 276.5 updated\n",
      "Time 277.0 updated\n",
      "Time 277.5 updated\n",
      "Time 278.0 updated\n",
      "Time 278.5 updated\n",
      "Time 279.0 updated\n",
      "Time 279.5 updated\n",
      "Time 280.0 updated\n",
      "Time 280.5 updated\n",
      "Time 281.0 updated\n",
      "Time 281.5 updated\n",
      "Time 282.0 updated\n",
      "Time 282.5 updated\n",
      "Time 283.0 updated\n",
      "Time 283.5 updated\n",
      "Time 284.0 updated\n",
      "Time 284.5 updated\n",
      "Time 285.0 updated\n",
      "Time 285.5 updated\n",
      "Time 286.0 updated\n",
      "Time 286.5 updated\n",
      "Time 287.0 updated\n",
      "Time 287.5 updated\n",
      "Time 288.0 updated\n",
      "Time 288.5 updated\n",
      "Time 289.0 updated\n",
      "Time 289.5 updated\n",
      "Time 290.0 updated\n",
      "Time 290.5 updated\n",
      "Time 291.0 updated\n",
      "Time 291.5 updated\n",
      "Time 292.0 updated\n",
      "Time 292.5 updated\n",
      "Time 293.0 updated\n",
      "Time 293.5 updated\n",
      "Time 294.0 updated\n",
      "Time 294.5 updated\n",
      "Time 295.0 updated\n",
      "Time 295.5 updated\n",
      "Time 296.0 updated\n",
      "Time 296.5 updated\n",
      "Time 297.0 updated\n",
      "Time 297.5 updated\n",
      "Time 298.0 updated\n",
      "Time 298.5 updated\n",
      "Time 299.0 updated\n",
      "Time 299.5 updated\n",
      "Time 300.0 updated\n",
      "Time 300.5 updated\n",
      "Time 301.0 updated\n",
      "Time 301.5 updated\n",
      "Time 302.0 updated\n",
      "Time 302.5 updated\n",
      "Time 303.0 updated\n",
      "Time 303.5 updated\n",
      "Time 304.0 updated\n",
      "Time 304.5 updated\n",
      "Time 305.0 updated\n",
      "Time 305.5 updated\n",
      "Time 306.0 updated\n",
      "Time 306.5 updated\n",
      "Time 307.0 updated\n",
      "Time 307.5 updated\n",
      "Time 308.0 updated\n",
      "Time 308.5 updated\n",
      "Time 309.0 updated\n",
      "Time 309.5 updated\n",
      "Time 310.0 updated\n",
      "Time 310.5 updated\n",
      "Time 311.0 updated\n",
      "Time 311.5 updated\n",
      "Time 312.0 updated\n",
      "Time 312.5 updated\n",
      "Time 313.0 updated\n",
      "Time 313.5 updated\n",
      "Time 314.0 updated\n",
      "Time 314.5 updated\n",
      "Time 315.0 updated\n",
      "Time 315.5 updated\n",
      "Time 316.0 updated\n",
      "Time 316.5 updated\n",
      "Time 317.0 updated\n",
      "Time 317.5 updated\n",
      "Time 318.0 updated\n",
      "Time 318.5 updated\n",
      "Time 319.0 updated\n",
      "Time 319.5 updated\n",
      "Time 320.0 updated\n",
      "Time 320.5 updated\n",
      "Time 321.0 updated\n",
      "Time 321.5 updated\n",
      "Time 322.0 updated\n",
      "Time 322.5 updated\n",
      "Time 323.0 updated\n",
      "Time 323.5 updated\n",
      "Time 324.0 updated\n",
      "Time 324.5 updated\n",
      "Time 325.0 updated\n",
      "Time 325.5 updated\n",
      "Time 326.0 updated\n",
      "Time 326.5 updated\n",
      "Time 327.0 updated\n",
      "Time 327.5 updated\n",
      "Time 328.0 updated\n",
      "Time 328.5 updated\n",
      "Time 329.0 updated\n",
      "Time 329.5 updated\n",
      "Time 330.0 updated\n",
      "Time 330.5 updated\n",
      "Time 331.0 updated\n",
      "Time 331.5 updated\n",
      "Time 332.0 updated\n",
      "Time 332.5 updated\n",
      "Time 333.0 updated\n",
      "Time 333.5 updated\n",
      "Time 334.0 updated\n",
      "Time 334.5 updated\n",
      "Time 335.0 updated\n",
      "Time 335.5 updated\n",
      "Time 336.0 updated\n",
      "Time 336.5 updated\n",
      "Time 337.0 updated\n",
      "Time 337.5 updated\n",
      "Time 338.0 updated\n",
      "Time 338.5 updated\n",
      "Time 339.0 updated\n",
      "Time 339.5 updated\n",
      "Time 340.0 updated\n",
      "Time 340.5 updated\n",
      "Time 341.0 updated\n",
      "Time 341.5 updated\n",
      "Time 342.0 updated\n",
      "Time 342.5 updated\n",
      "Time 343.0 updated\n",
      "Time 343.5 updated\n",
      "Time 344.0 updated\n",
      "Time 344.5 updated\n",
      "Time 345.0 updated\n",
      "Time 345.5 updated\n",
      "Time 346.0 updated\n",
      "Time 346.5 updated\n",
      "Time 347.0 updated\n",
      "Time 347.5 updated\n",
      "Time 348.0 updated\n",
      "Time 348.5 updated\n",
      "Time 349.0 updated\n",
      "Time 349.5 updated\n",
      "Time 350.0 updated\n",
      "Time 350.5 updated\n",
      "Time 351.0 updated\n",
      "Time 351.5 updated\n",
      "Time 352.0 updated\n",
      "Time 352.5 updated\n",
      "Time 353.0 updated\n",
      "Time 353.5 updated\n",
      "Time 354.0 updated\n",
      "Time 354.5 updated\n",
      "Time 355.0 updated\n",
      "Time 355.5 updated\n",
      "Time 356.0 updated\n",
      "Time 356.5 updated\n",
      "Time 357.0 updated\n",
      "Time 357.5 updated\n",
      "Time 358.0 updated\n",
      "Time 358.5 updated\n",
      "Time 359.0 updated\n",
      "Time 359.5 updated\n",
      "Time 360.0 updated\n",
      "Time 360.5 updated\n",
      "Time 361.0 updated\n",
      "Time 361.5 updated\n",
      "Time 362.0 updated\n",
      "Time 362.5 updated\n",
      "Time 363.0 updated\n",
      "Time 363.5 updated\n",
      "Time 364.0 updated\n",
      "Time 364.5 updated\n",
      "Time 365.0 updated\n",
      "Time 365.5 updated\n",
      "Time 366.0 updated\n",
      "Time 366.5 updated\n",
      "Time 367.0 updated\n",
      "Time 367.5 updated\n",
      "Time 368.0 updated\n",
      "Time 368.5 updated\n",
      "Time 369.0 updated\n",
      "Time 369.5 updated\n",
      "Time 370.0 updated\n",
      "Time 370.5 updated\n",
      "Time 371.0 updated\n",
      "Time 371.5 updated\n",
      "Time 372.0 updated\n",
      "Time 372.5 updated\n",
      "Time 373.0 updated\n",
      "Time 373.5 updated\n",
      "Time 374.0 updated\n",
      "Time 374.5 updated\n",
      "Time 375.0 updated\n",
      "Time 375.5 updated\n",
      "Time 376.0 updated\n",
      "Time 376.5 updated\n",
      "Time 377.0 updated\n",
      "Time 377.5 updated\n",
      "Time 378.0 updated\n",
      "Time 378.5 updated\n",
      "Time 379.0 updated\n",
      "Time 379.5 updated\n",
      "Time 380.0 updated\n",
      "Time 380.5 updated\n",
      "Time 381.0 updated\n",
      "Time 381.5 updated\n",
      "Time 382.0 updated\n",
      "Time 382.5 updated\n",
      "Time 383.0 updated\n",
      "Time 383.5 updated\n",
      "Time 384.0 updated\n",
      "Time 384.5 updated\n",
      "Time 385.0 updated\n",
      "Time 385.5 updated\n",
      "Time 386.0 updated\n",
      "Time 386.5 updated\n",
      "Time 387.0 updated\n",
      "Time 387.5 updated\n",
      "Time 388.0 updated\n",
      "Time 388.5 updated\n",
      "Time 389.0 updated\n",
      "Time 389.5 updated\n",
      "Time 390.0 updated\n",
      "Time 390.5 updated\n",
      "Time 391.0 updated\n",
      "Time 391.5 updated\n",
      "Time 392.0 updated\n",
      "Time 392.5 updated\n",
      "Time 393.0 updated\n",
      "Time 393.5 updated\n",
      "Time 394.0 updated\n",
      "Time 394.5 updated\n",
      "Time 395.0 updated\n",
      "Time 395.5 updated\n",
      "Time 396.0 updated\n",
      "Time 396.5 updated\n",
      "Time 397.0 updated\n",
      "Time 397.5 updated\n",
      "Time 398.0 updated\n",
      "Time 398.5 updated\n",
      "Time 399.0 updated\n",
      "Time 399.5 updated\n",
      "Time 400.0 updated\n",
      "Time 400.5 updated\n",
      "Time 401.0 updated\n",
      "Time 401.5 updated\n",
      "Time 402.0 updated\n",
      "Time 402.5 updated\n",
      "Time 403.0 updated\n",
      "Time 403.5 updated\n",
      "Time 404.0 updated\n",
      "Time 404.5 updated\n",
      "Time 405.0 updated\n",
      "Time 405.5 updated\n",
      "Time 406.0 updated\n",
      "Time 406.5 updated\n",
      "Time 407.0 updated\n",
      "Time 407.5 updated\n",
      "Time 408.0 updated\n",
      "Time 408.5 updated\n",
      "Time 409.0 updated\n",
      "Time 409.5 updated\n",
      "Time 410.0 updated\n",
      "Time 410.5 updated\n",
      "Time 411.0 updated\n",
      "Time 411.5 updated\n",
      "Time 412.0 updated\n",
      "Time 412.5 updated\n",
      "Time 413.0 updated\n",
      "Time 413.5 updated\n",
      "Time 414.0 updated\n",
      "Time 414.5 updated\n",
      "Time 415.0 updated\n",
      "Time 415.5 updated\n",
      "Time 416.0 updated\n",
      "Time 416.5 updated\n",
      "Time 417.0 updated\n",
      "Time 417.5 updated\n",
      "Time 418.0 updated\n",
      "Time 418.5 updated\n",
      "Time 419.0 updated\n",
      "Time 419.5 updated\n",
      "Time 420.0 updated\n",
      "Time 420.5 updated\n",
      "Time 421.0 updated\n",
      "Time 421.5 updated\n",
      "Time 422.0 updated\n",
      "Time 422.5 updated\n",
      "Time 423.0 updated\n",
      "Time 423.5 updated\n",
      "Time 424.0 updated\n",
      "Time 424.5 updated\n",
      "Time 425.0 updated\n",
      "Time 425.5 updated\n",
      "Time 426.0 updated\n",
      "Time 426.5 updated\n",
      "Time 427.0 updated\n",
      "Time 427.5 updated\n",
      "Time 428.0 updated\n",
      "Time 428.5 updated\n",
      "Time 429.0 updated\n",
      "Time 429.5 updated\n",
      "Time 430.0 updated\n",
      "Time 430.5 updated\n",
      "Time 431.0 updated\n",
      "Time 431.5 updated\n",
      "Time 432.0 updated\n",
      "Time 432.5 updated\n",
      "Time 433.0 updated\n",
      "Time 433.5 updated\n",
      "Time 434.0 updated\n",
      "Time 434.5 updated\n",
      "Time 435.0 updated\n",
      "Time 435.5 updated\n",
      "Time 436.0 updated\n",
      "Time 436.5 updated\n",
      "Time 437.0 updated\n",
      "Time 437.5 updated\n",
      "Time 438.0 updated\n",
      "Time 438.5 updated\n",
      "Time 439.0 updated\n",
      "Time 439.5 updated\n",
      "Time 440.0 updated\n",
      "Time 440.5 updated\n",
      "Time 441.0 updated\n",
      "Time 441.5 updated\n",
      "Time 442.0 updated\n",
      "Time 442.5 updated\n",
      "Time 443.0 updated\n",
      "Time 443.5 updated\n",
      "Time 444.0 updated\n",
      "Time 444.5 updated\n",
      "Time 445.0 updated\n",
      "Time 445.5 updated\n",
      "Time 446.0 updated\n",
      "Time 446.5 updated\n",
      "Time 447.0 updated\n",
      "Time 447.5 updated\n",
      "Time 448.0 updated\n",
      "Time 448.5 updated\n",
      "Time 449.0 updated\n",
      "Time 449.5 updated\n",
      "Time 450.0 updated\n",
      "Time 450.5 updated\n",
      "Time 451.0 updated\n",
      "Time 451.5 updated\n",
      "Time 452.0 updated\n",
      "Time 452.5 updated\n",
      "Time 453.0 updated\n",
      "Time 453.5 updated\n",
      "Time 454.0 updated\n",
      "Time 454.5 updated\n",
      "Time 455.0 updated\n",
      "Time 455.5 updated\n",
      "Time 456.0 updated\n",
      "Time 456.5 updated\n",
      "Time 457.0 updated\n",
      "Time 457.5 updated\n",
      "Time 458.0 updated\n",
      "Time 458.5 updated\n",
      "Time 459.0 updated\n",
      "Time 459.5 updated\n",
      "Time 460.0 updated\n",
      "Time 460.5 updated\n",
      "Time 461.0 updated\n",
      "Time 461.5 updated\n",
      "Time 462.0 updated\n",
      "Time 462.5 updated\n",
      "Time 463.0 updated\n",
      "Time 463.5 updated\n",
      "Time 464.0 updated\n",
      "Time 464.5 updated\n",
      "Time 465.0 updated\n",
      "Time 465.5 updated\n",
      "Time 466.0 updated\n",
      "Time 466.5 updated\n",
      "Time 467.0 updated\n",
      "Time 467.5 updated\n",
      "Time 468.0 updated\n",
      "Time 468.5 updated\n",
      "Time 469.0 updated\n",
      "Time 469.5 updated\n",
      "Time 470.0 updated\n",
      "Time 470.5 updated\n",
      "Time 471.0 updated\n",
      "Time 471.5 updated\n",
      "Time 472.0 updated\n",
      "Time 472.5 updated\n",
      "Time 473.0 updated\n",
      "Time 473.5 updated\n",
      "Time 474.0 updated\n",
      "Time 474.5 updated\n",
      "Time 475.0 updated\n",
      "Time 475.5 updated\n",
      "Time 476.0 updated\n",
      "Time 476.5 updated\n",
      "Time 477.0 updated\n",
      "Time 477.5 updated\n",
      "Time 478.0 updated\n",
      "Time 478.5 updated\n",
      "Time 479.0 updated\n",
      "Time 479.5 updated\n",
      "Time 480.0 updated\n",
      "Time 480.5 updated\n",
      "Time 481.0 updated\n",
      "Time 481.5 updated\n",
      "Time 482.0 updated\n",
      "Time 482.5 updated\n",
      "Time 483.0 updated\n",
      "Time 483.5 updated\n",
      "Time 484.0 updated\n",
      "Time 484.5 updated\n",
      "Time 485.0 updated\n",
      "Time 485.5 updated\n",
      "Time 486.0 updated\n",
      "Time 486.5 updated\n",
      "Time 487.0 updated\n",
      "Time 487.5 updated\n",
      "Time 488.0 updated\n",
      "Time 488.5 updated\n",
      "Time 489.0 updated\n",
      "Time 489.5 updated\n",
      "Time 490.0 updated\n",
      "Time 490.5 updated\n",
      "Time 491.0 updated\n",
      "Time 491.5 updated\n",
      "Time 492.0 updated\n",
      "Time 492.5 updated\n",
      "Time 493.0 updated\n",
      "Time 493.5 updated\n",
      "Time 494.0 updated\n",
      "Time 494.5 updated\n",
      "Time 495.0 updated\n",
      "Time 495.5 updated\n",
      "Time 496.0 updated\n",
      "Time 496.5 updated\n",
      "Time 497.0 updated\n",
      "Time 497.5 updated\n",
      "Time 498.0 updated\n",
      "Time 498.5 updated\n",
      "Time 499.0 updated\n",
      "Time 499.5 updated\n",
      "Time 500.0 updated\n",
      "Time 500.5 updated\n",
      "Time 501.0 updated\n",
      "Time 501.5 updated\n",
      "Time 502.0 updated\n",
      "Time 502.5 updated\n",
      "Time 503.0 updated\n",
      "Time 503.5 updated\n",
      "Time 504.0 updated\n",
      "Time 504.5 updated\n",
      "Time 505.0 updated\n",
      "Time 505.5 updated\n",
      "Time 506.0 updated\n",
      "Time 506.5 updated\n",
      "Time 507.0 updated\n",
      "Time 507.5 updated\n",
      "Time 508.0 updated\n",
      "Time 508.5 updated\n",
      "Time 509.0 updated\n",
      "Time 509.5 updated\n",
      "Time 510.0 updated\n",
      "Time 510.5 updated\n",
      "Time 511.0 updated\n",
      "Time 511.5 updated\n",
      "Time 512.0 updated\n",
      "Time 512.5 updated\n",
      "Time 513.0 updated\n",
      "Time 513.5 updated\n",
      "Time 514.0 updated\n",
      "Time 514.5 updated\n",
      "Time 515.0 updated\n",
      "Time 515.5 updated\n",
      "Time 516.0 updated\n",
      "Time 516.5 updated\n",
      "Time 517.0 updated\n",
      "Time 517.5 updated\n",
      "Time 518.0 updated\n",
      "Time 518.5 updated\n",
      "Time 519.0 updated\n",
      "Time 519.5 updated\n",
      "Time 520.0 updated\n",
      "Time 520.5 updated\n",
      "Time 521.0 updated\n",
      "Time 521.5 updated\n",
      "Time 522.0 updated\n",
      "Time 522.5 updated\n",
      "Time 523.0 updated\n",
      "Time 523.5 updated\n",
      "Time 524.0 updated\n",
      "Time 524.5 updated\n",
      "Time 525.0 updated\n",
      "Time 525.5 updated\n",
      "Time 526.0 updated\n",
      "Time 526.5 updated\n",
      "Time 527.0 updated\n",
      "Time 527.5 updated\n",
      "Time 528.0 updated\n",
      "Time 528.5 updated\n",
      "Time 529.0 updated\n",
      "Time 529.5 updated\n",
      "Time 530.0 updated\n",
      "Time 530.5 updated\n",
      "Time 531.0 updated\n",
      "Time 531.5 updated\n",
      "Time 532.0 updated\n",
      "Time 532.5 updated\n",
      "Time 533.0 updated\n",
      "Time 533.5 updated\n",
      "Time 534.0 updated\n",
      "Time 534.5 updated\n",
      "Time 535.0 updated\n",
      "Time 535.5 updated\n",
      "Time 536.0 updated\n",
      "Time 536.5 updated\n",
      "Time 537.0 updated\n",
      "Time 537.5 updated\n",
      "Time 538.0 updated\n",
      "Time 538.5 updated\n",
      "Time 539.0 updated\n",
      "Time 539.5 updated\n",
      "Time 540.0 updated\n",
      "Time 540.5 updated\n",
      "Time 541.0 updated\n",
      "Time 541.5 updated\n",
      "Time 542.0 updated\n",
      "Time 542.5 updated\n",
      "Time 543.0 updated\n",
      "Time 543.5 updated\n",
      "Time 544.0 updated\n",
      "Time 544.5 updated\n",
      "Time 545.0 updated\n",
      "Time 545.5 updated\n",
      "Time 546.0 updated\n",
      "Time 546.5 updated\n",
      "Time 547.0 updated\n",
      "Time 547.5 updated\n",
      "Time 548.0 updated\n",
      "Time 548.5 updated\n",
      "Time 549.0 updated\n",
      "Time 549.5 updated\n",
      "Time 550.0 updated\n",
      "Time 550.5 updated\n",
      "Time 551.0 updated\n",
      "Time 551.5 updated\n",
      "Time 552.0 updated\n",
      "Time 552.5 updated\n",
      "Time 553.0 updated\n",
      "Time 553.5 updated\n",
      "Time 554.0 updated\n",
      "Time 554.5 updated\n",
      "Time 555.0 updated\n",
      "Time 555.5 updated\n",
      "Time 556.0 updated\n",
      "Time 556.5 updated\n",
      "Time 557.0 updated\n",
      "Time 557.5 updated\n",
      "Time 558.0 updated\n",
      "Time 558.5 updated\n",
      "Time 559.0 updated\n",
      "Time 559.5 updated\n",
      "Time 560.0 updated\n",
      "Time 560.5 updated\n",
      "Time 561.0 updated\n",
      "Time 561.5 updated\n",
      "Time 562.0 updated\n",
      "Time 562.5 updated\n",
      "Time 563.0 updated\n",
      "Time 563.5 updated\n",
      "Time 564.0 updated\n",
      "Time 564.5 updated\n",
      "Time 565.0 updated\n",
      "Time 565.5 updated\n",
      "Time 566.0 updated\n",
      "Time 566.5 updated\n",
      "Time 567.0 updated\n",
      "Time 567.5 updated\n",
      "Time 568.0 updated\n",
      "Time 568.5 updated\n",
      "Time 569.0 updated\n",
      "Time 569.5 updated\n",
      "Time 570.0 updated\n",
      "Time 570.5 updated\n",
      "Time 571.0 updated\n",
      "Time 571.5 updated\n",
      "Time 572.0 updated\n",
      "Time 572.5 updated\n",
      "Time 573.0 updated\n",
      "Time 573.5 updated\n",
      "Time 574.0 updated\n",
      "Time 574.5 updated\n",
      "Time 575.0 updated\n",
      "Time 575.5 updated\n",
      "Time 576.0 updated\n",
      "Time 576.5 updated\n",
      "Time 577.0 updated\n",
      "Time 577.5 updated\n",
      "Time 578.0 updated\n",
      "Time 578.5 updated\n",
      "Time 579.0 updated\n",
      "Time 579.5 updated\n",
      "Time 580.0 updated\n",
      "Time 580.5 updated\n",
      "Time 581.0 updated\n",
      "Time 581.5 updated\n",
      "Time 582.0 updated\n",
      "Time 582.5 updated\n",
      "Time 583.0 updated\n",
      "Time 583.5 updated\n",
      "Time 584.0 updated\n",
      "Time 584.5 updated\n",
      "Time 585.0 updated\n",
      "Time 585.5 updated\n",
      "Time 586.0 updated\n",
      "Time 586.5 updated\n",
      "Time 587.0 updated\n",
      "Time 587.5 updated\n",
      "Time 588.0 updated\n",
      "Time 588.5 updated\n",
      "Time 589.0 updated\n",
      "Time 589.5 updated\n",
      "Time 590.0 updated\n",
      "Time 590.5 updated\n",
      "Time 591.0 updated\n",
      "Time 591.5 updated\n",
      "Time 592.0 updated\n",
      "Time 592.5 updated\n",
      "Time 593.0 updated\n",
      "Time 593.5 updated\n",
      "Time 594.0 updated\n",
      "Time 594.5 updated\n",
      "Time 595.0 updated\n",
      "Time 595.5 updated\n",
      "Time 596.0 updated\n",
      "Time 596.5 updated\n",
      "Time 597.0 updated\n",
      "Time 597.5 updated\n",
      "Time 598.0 updated\n",
      "Time 598.5 updated\n",
      "Time 599.0 updated\n",
      "Time 599.5 updated\n",
      "Time 600.0 updated\n",
      "Time 600.5 updated\n",
      "Time 601.0 updated\n",
      "Time 601.5 updated\n",
      "Time 602.0 updated\n",
      "Time 602.5 updated\n",
      "Time 603.0 updated\n",
      "Time 603.5 updated\n",
      "Time 604.0 updated\n",
      "Time 604.5 updated\n",
      "Time 605.0 updated\n",
      "Time 605.5 updated\n",
      "Time 606.0 updated\n",
      "Time 606.5 updated\n",
      "Time 607.0 updated\n",
      "Time 607.5 updated\n",
      "Time 608.0 updated\n",
      "Time 608.5 updated\n",
      "Time 609.0 updated\n",
      "Time 609.5 updated\n",
      "Time 610.0 updated\n",
      "Time 610.5 updated\n",
      "Time 611.0 updated\n",
      "Time 611.5 updated\n",
      "Time 612.0 updated\n",
      "Time 612.5 updated\n",
      "Time 613.0 updated\n",
      "Time 613.5 updated\n",
      "Time 614.0 updated\n",
      "Time 614.5 updated\n",
      "Time 615.0 updated\n",
      "Time 615.5 updated\n",
      "Time 616.0 updated\n",
      "Time 616.5 updated\n",
      "Time 617.0 updated\n",
      "Time 617.5 updated\n",
      "Time 618.0 updated\n",
      "Time 618.5 updated\n",
      "Time 619.0 updated\n",
      "Time 619.5 updated\n",
      "Time 620.0 updated\n",
      "Time 620.5 updated\n",
      "Time 621.0 updated\n",
      "Time 621.5 updated\n",
      "Time 622.0 updated\n",
      "Time 622.5 updated\n",
      "Time 623.0 updated\n",
      "Time 623.5 updated\n",
      "Time 624.0 updated\n",
      "Time 624.5 updated\n",
      "Time 625.0 updated\n",
      "Time 625.5 updated\n",
      "Time 626.0 updated\n",
      "Time 626.5 updated\n",
      "Time 627.0 updated\n",
      "Time 627.5 updated\n",
      "Time 628.0 updated\n",
      "Time 628.5 updated\n",
      "Time 629.0 updated\n",
      "Time 629.5 updated\n",
      "Time 630.0 updated\n",
      "Time 630.5 updated\n",
      "Time 631.0 updated\n",
      "Time 631.5 updated\n",
      "Time 632.0 updated\n",
      "Time 632.5 updated\n",
      "Time 633.0 updated\n",
      "Time 633.5 updated\n",
      "Time 634.0 updated\n",
      "Time 634.5 updated\n",
      "Time 635.0 updated\n",
      "Time 635.5 updated\n",
      "Time 636.0 updated\n",
      "Time 636.5 updated\n",
      "Time 637.0 updated\n",
      "Time 637.5 updated\n",
      "Time 638.0 updated\n",
      "Time 638.5 updated\n",
      "Time 639.0 updated\n",
      "Time 639.5 updated\n",
      "Time 640.0 updated\n",
      "Time 640.5 updated\n",
      "Time 641.0 updated\n",
      "Time 641.5 updated\n",
      "Time 642.0 updated\n",
      "Time 642.5 updated\n",
      "Time 643.0 updated\n",
      "Time 643.5 updated\n",
      "Time 644.0 updated\n",
      "Time 644.5 updated\n",
      "Time 645.0 updated\n",
      "Time 645.5 updated\n",
      "Time 646.0 updated\n",
      "Time 646.5 updated\n",
      "Time 647.0 updated\n",
      "Time 647.5 updated\n",
      "Time 648.0 updated\n",
      "Time 648.5 updated\n",
      "Time 649.0 updated\n",
      "Time 649.5 updated\n",
      "Time 650.0 updated\n",
      "Time 650.5 updated\n",
      "Time 651.0 updated\n",
      "Time 651.5 updated\n",
      "Time 652.0 updated\n",
      "Time 652.5 updated\n",
      "Time 653.0 updated\n",
      "Time 653.5 updated\n",
      "Time 654.0 updated\n",
      "Time 654.5 updated\n",
      "Time 655.0 updated\n",
      "Time 655.5 updated\n",
      "Time 656.0 updated\n",
      "Time 656.5 updated\n",
      "Time 657.0 updated\n",
      "Time 657.5 updated\n",
      "Time 658.0 updated\n",
      "Time 658.5 updated\n",
      "Time 659.0 updated\n",
      "Time 659.5 updated\n",
      "Time 660.0 updated\n",
      "Time 660.5 updated\n",
      "Time 661.0 updated\n",
      "Time 661.5 updated\n",
      "Time 662.0 updated\n",
      "Time 662.5 updated\n",
      "Time 663.0 updated\n",
      "Time 663.5 updated\n",
      "Time 664.0 updated\n",
      "Time 664.5 updated\n",
      "Time 665.0 updated\n",
      "Time 665.5 updated\n",
      "Time 666.0 updated\n",
      "Time 666.5 updated\n",
      "Time 667.0 updated\n",
      "Time 667.5 updated\n",
      "Time 668.0 updated\n",
      "Time 668.5 updated\n",
      "Time 669.0 updated\n",
      "Time 669.5 updated\n",
      "Time 670.0 updated\n",
      "Time 670.5 updated\n",
      "Time 671.0 updated\n",
      "Time 671.5 updated\n",
      "Time 672.0 updated\n",
      "Time 672.5 updated\n",
      "Time 673.0 updated\n",
      "Time 673.5 updated\n",
      "Time 674.0 updated\n",
      "Time 674.5 updated\n",
      "Time 675.0 updated\n",
      "Time 675.5 updated\n",
      "Time 676.0 updated\n",
      "Time 676.5 updated\n",
      "Time 677.0 updated\n",
      "Time 677.5 updated\n",
      "Time 678.0 updated\n",
      "Time 678.5 updated\n",
      "Time 679.0 updated\n",
      "Time 679.5 updated\n",
      "Time 680.0 updated\n",
      "Time 680.5 updated\n",
      "Time 681.0 updated\n",
      "Time 681.5 updated\n",
      "Time 682.0 updated\n",
      "Time 682.5 updated\n",
      "Time 683.0 updated\n",
      "Time 683.5 updated\n",
      "Time 684.0 updated\n",
      "Time 684.5 updated\n",
      "Time 685.0 updated\n",
      "Time 685.5 updated\n",
      "Time 686.0 updated\n",
      "Time 686.5 updated\n",
      "Time 687.0 updated\n",
      "Time 687.5 updated\n",
      "Time 688.0 updated\n",
      "Time 688.5 updated\n",
      "Time 689.0 updated\n",
      "Time 689.5 updated\n",
      "Time 690.0 updated\n",
      "Time 690.5 updated\n",
      "Time 691.0 updated\n",
      "Time 691.5 updated\n",
      "Time 692.0 updated\n",
      "Time 692.5 updated\n",
      "Time 693.0 updated\n",
      "Time 693.5 updated\n",
      "Time 694.0 updated\n",
      "Time 694.5 updated\n",
      "Time 695.0 updated\n",
      "Time 695.5 updated\n",
      "Time 696.0 updated\n",
      "Time 696.5 updated\n",
      "Time 697.0 updated\n",
      "Time 697.5 updated\n",
      "Time 698.0 updated\n",
      "Time 698.5 updated\n",
      "Time 699.0 updated\n",
      "Time 699.5 updated\n",
      "Time 700.0 updated\n",
      "Time 700.5 updated\n",
      "Time 701.0 updated\n",
      "Time 701.5 updated\n",
      "Time 702.0 updated\n",
      "Time 702.5 updated\n",
      "Time 703.0 updated\n",
      "Time 703.5 updated\n",
      "Time 704.0 updated\n",
      "Time 704.5 updated\n",
      "Time 705.0 updated\n",
      "Time 705.5 updated\n",
      "Time 706.0 updated\n",
      "Time 706.5 updated\n",
      "Time 707.0 updated\n",
      "Time 707.5 updated\n",
      "Time 708.0 updated\n",
      "Time 708.5 updated\n",
      "Time 709.0 updated\n",
      "Time 709.5 updated\n",
      "Time 710.0 updated\n",
      "Time 710.5 updated\n",
      "Time 711.0 updated\n",
      "Time 711.5 updated\n",
      "Time 712.0 updated\n",
      "Time 712.5 updated\n",
      "Time 713.0 updated\n",
      "Time 713.5 updated\n",
      "Time 714.0 updated\n",
      "Time 714.5 updated\n",
      "Time 715.0 updated\n",
      "Time 715.5 updated\n",
      "Time 716.0 updated\n",
      "Time 716.5 updated\n",
      "Time 717.0 updated\n",
      "Time 717.5 updated\n",
      "Time 718.0 updated\n",
      "Time 718.5 updated\n",
      "Time 719.0 updated\n",
      "Time 719.5 updated\n",
      "Time 720.0 updated\n",
      "Time 720.5 updated\n",
      "Time 721.0 updated\n",
      "Time 721.5 updated\n",
      "Time 722.0 updated\n",
      "Time 722.5 updated\n",
      "Time 723.0 updated\n",
      "Time 723.5 updated\n",
      "Time 724.0 updated\n",
      "Time 724.5 updated\n",
      "Time 725.0 updated\n",
      "Time 725.5 updated\n",
      "Time 726.0 updated\n",
      "Time 726.5 updated\n",
      "Time 727.0 updated\n",
      "Time 727.5 updated\n",
      "Time 728.0 updated\n",
      "Time 728.5 updated\n",
      "Time 729.0 updated\n",
      "Time 729.5 updated\n",
      "Time 730.0 updated\n",
      "Time 730.5 updated\n",
      "Time 731.0 updated\n",
      "Time 731.5 updated\n",
      "Time 732.0 updated\n",
      "Time 732.5 updated\n",
      "Time 733.0 updated\n",
      "Time 733.5 updated\n",
      "Time 734.0 updated\n",
      "Time 734.5 updated\n",
      "Time 735.0 updated\n",
      "Time 735.5 updated\n",
      "Time 736.0 updated\n",
      "Time 736.5 updated\n",
      "Time 737.0 updated\n",
      "Time 737.5 updated\n",
      "Time 738.0 updated\n",
      "Time 738.5 updated\n",
      "Time 739.0 updated\n",
      "Time 739.5 updated\n",
      "Time 740.0 updated\n",
      "Time 740.5 updated\n",
      "Time 741.0 updated\n",
      "Time 741.5 updated\n",
      "Time 742.0 updated\n",
      "Time 742.5 updated\n",
      "Time 743.0 updated\n",
      "Time 743.5 updated\n",
      "Time 744.0 updated\n",
      "Time 744.5 updated\n",
      "Time 745.0 updated\n",
      "Time 745.5 updated\n",
      "Time 746.0 updated\n",
      "Time 746.5 updated\n",
      "Time 747.0 updated\n",
      "Time 747.5 updated\n",
      "Time 748.0 updated\n",
      "Time 748.5 updated\n",
      "Time 749.0 updated\n",
      "Time 749.5 updated\n",
      "Time 750.0 updated\n",
      "Time 750.5 updated\n",
      "Time 751.0 updated\n",
      "Time 751.5 updated\n",
      "Time 752.0 updated\n",
      "Time 752.5 updated\n",
      "Time 753.0 updated\n",
      "Time 753.5 updated\n",
      "Time 754.0 updated\n",
      "Time 754.5 updated\n",
      "Time 755.0 updated\n",
      "Time 755.5 updated\n",
      "Time 756.0 updated\n",
      "Time 756.5 updated\n",
      "Time 757.0 updated\n",
      "Time 757.5 updated\n",
      "Time 758.0 updated\n",
      "Time 758.5 updated\n",
      "Time 759.0 updated\n",
      "Time 759.5 updated\n",
      "Time 760.0 updated\n",
      "Time 760.5 updated\n",
      "Time 761.0 updated\n",
      "Time 761.5 updated\n",
      "Time 762.0 updated\n",
      "Time 762.5 updated\n",
      "Time 763.0 updated\n",
      "Time 763.5 updated\n",
      "Time 764.0 updated\n",
      "Time 764.5 updated\n",
      "Time 765.0 updated\n",
      "Time 765.5 updated\n",
      "Time 766.0 updated\n",
      "Time 766.5 updated\n",
      "Time 767.0 updated\n",
      "Time 767.5 updated\n",
      "Time 768.0 updated\n",
      "Time 768.5 updated\n",
      "Time 769.0 updated\n",
      "Time 769.5 updated\n",
      "Time 770.0 updated\n",
      "Time 770.5 updated\n",
      "Time 771.0 updated\n",
      "Time 771.5 updated\n",
      "Time 772.0 updated\n",
      "Time 772.5 updated\n",
      "Time 773.0 updated\n",
      "Time 773.5 updated\n",
      "Time 774.0 updated\n",
      "Time 774.5 updated\n",
      "Time 775.0 updated\n",
      "Time 775.5 updated\n",
      "Time 776.0 updated\n",
      "Time 776.5 updated\n",
      "Time 777.0 updated\n",
      "Time 777.5 updated\n",
      "Time 778.0 updated\n",
      "Time 778.5 updated\n",
      "Time 779.0 updated\n",
      "Time 779.5 updated\n",
      "Time 780.0 updated\n",
      "Time 780.5 updated\n",
      "Time 781.0 updated\n",
      "Time 781.5 updated\n",
      "Time 782.0 updated\n",
      "Time 782.5 updated\n",
      "Time 783.0 updated\n",
      "Time 783.5 updated\n",
      "Time 784.0 updated\n",
      "Time 784.5 updated\n",
      "Time 785.0 updated\n",
      "Time 785.5 updated\n",
      "Time 786.0 updated\n",
      "Time 786.5 updated\n",
      "Time 787.0 updated\n",
      "Time 787.5 updated\n",
      "Time 788.0 updated\n",
      "Time 788.5 updated\n",
      "Time 789.0 updated\n",
      "Time 789.5 updated\n",
      "Time 790.0 updated\n",
      "Time 790.5 updated\n",
      "Time 791.0 updated\n",
      "Time 791.5 updated\n",
      "Time 792.0 updated\n",
      "Time 792.5 updated\n",
      "Time 793.0 updated\n",
      "Time 793.5 updated\n",
      "Time 794.0 updated\n",
      "Time 794.5 updated\n",
      "Time 795.0 updated\n",
      "Time 795.5 updated\n",
      "Time 796.0 updated\n",
      "Time 796.5 updated\n",
      "Time 797.0 updated\n",
      "Time 797.5 updated\n",
      "Time 798.0 updated\n",
      "Time 798.5 updated\n",
      "Time 799.0 updated\n",
      "Time 799.5 updated\n",
      "Time 800.0 updated\n",
      "Time 800.5 updated\n",
      "Time 801.0 updated\n",
      "Time 801.5 updated\n",
      "Time 802.0 updated\n",
      "Time 802.5 updated\n",
      "Time 803.0 updated\n",
      "Time 803.5 updated\n",
      "Time 804.0 updated\n",
      "Time 804.5 updated\n",
      "Time 805.0 updated\n",
      "Time 805.5 updated\n",
      "Time 806.0 updated\n",
      "Time 806.5 updated\n",
      "Time 807.0 updated\n",
      "Time 807.5 updated\n",
      "Time 808.0 updated\n",
      "Time 808.5 updated\n",
      "Time 809.0 updated\n",
      "Time 809.5 updated\n",
      "Time 810.0 updated\n",
      "Time 810.5 updated\n",
      "Time 811.0 updated\n",
      "Time 811.5 updated\n",
      "Time 812.0 updated\n",
      "Time 812.5 updated\n",
      "Time 813.0 updated\n",
      "Time 813.5 updated\n",
      "Time 814.0 updated\n",
      "Time 814.5 updated\n",
      "Time 815.0 updated\n",
      "Time 815.5 updated\n",
      "Time 816.0 updated\n",
      "Time 816.5 updated\n",
      "Time 817.0 updated\n",
      "Time 817.5 updated\n",
      "Time 818.0 updated\n",
      "Time 818.5 updated\n",
      "Time 819.0 updated\n",
      "Time 819.5 updated\n",
      "Time 820.0 updated\n",
      "Time 820.5 updated\n",
      "Time 821.0 updated\n",
      "Time 821.5 updated\n",
      "Time 822.0 updated\n",
      "Time 822.5 updated\n",
      "Time 823.0 updated\n",
      "Time 823.5 updated\n",
      "Time 824.0 updated\n",
      "Time 824.5 updated\n",
      "Time 825.0 updated\n",
      "Time 825.5 updated\n",
      "Time 826.0 updated\n",
      "Time 826.5 updated\n",
      "Time 827.0 updated\n",
      "Time 827.5 updated\n",
      "Time 828.0 updated\n",
      "Time 828.5 updated\n",
      "Time 829.0 updated\n",
      "Time 829.5 updated\n",
      "Time 830.0 updated\n",
      "Time 830.5 updated\n",
      "Time 831.0 updated\n",
      "Time 831.5 updated\n",
      "Time 832.0 updated\n",
      "Time 832.5 updated\n",
      "Time 833.0 updated\n",
      "Time 833.5 updated\n",
      "Time 834.0 updated\n",
      "Time 834.5 updated\n",
      "Time 835.0 updated\n",
      "Time 835.5 updated\n",
      "Time 836.0 updated\n",
      "Time 836.5 updated\n",
      "Time 837.0 updated\n",
      "Time 837.5 updated\n",
      "Time 838.0 updated\n",
      "Time 838.5 updated\n",
      "Time 839.0 updated\n",
      "Time 839.5 updated\n",
      "Time 840.0 updated\n",
      "Time 840.5 updated\n",
      "Time 841.0 updated\n",
      "Time 841.5 updated\n",
      "Time 842.0 updated\n",
      "Time 842.5 updated\n",
      "Time 843.0 updated\n",
      "Time 843.5 updated\n",
      "Time 844.0 updated\n",
      "Time 844.5 updated\n",
      "Time 845.0 updated\n",
      "Time 845.5 updated\n",
      "Time 846.0 updated\n",
      "Time 846.5 updated\n",
      "Time 847.0 updated\n",
      "Time 847.5 updated\n",
      "Time 848.0 updated\n",
      "Time 848.5 updated\n",
      "Time 849.0 updated\n",
      "Time 849.5 updated\n",
      "Time 850.0 updated\n",
      "Time 850.5 updated\n",
      "Time 851.0 updated\n",
      "Time 851.5 updated\n",
      "Time 852.0 updated\n",
      "Time 852.5 updated\n",
      "Time 853.0 updated\n",
      "Time 853.5 updated\n",
      "Time 854.0 updated\n",
      "Time 854.5 updated\n",
      "Time 855.0 updated\n",
      "Time 855.5 updated\n",
      "Time 856.0 updated\n",
      "Time 856.5 updated\n",
      "Time 857.0 updated\n",
      "Time 857.5 updated\n",
      "Time 858.0 updated\n",
      "Time 858.5 updated\n",
      "Time 859.0 updated\n",
      "Time 859.5 updated\n",
      "Time 860.0 updated\n",
      "Time 860.5 updated\n",
      "Time 861.0 updated\n",
      "Time 861.5 updated\n",
      "Time 862.0 updated\n",
      "Time 862.5 updated\n",
      "Time 863.0 updated\n",
      "Time 863.5 updated\n",
      "Time 864.0 updated\n",
      "Time 864.5 updated\n",
      "Time 865.0 updated\n",
      "Time 865.5 updated\n",
      "Time 866.0 updated\n",
      "Time 866.5 updated\n",
      "Time 867.0 updated\n",
      "Time 867.5 updated\n",
      "Time 868.0 updated\n",
      "Time 868.5 updated\n",
      "Time 869.0 updated\n",
      "Time 869.5 updated\n",
      "Time 870.0 updated\n",
      "Time 870.5 updated\n",
      "Time 871.0 updated\n",
      "Time 871.5 updated\n",
      "Time 872.0 updated\n",
      "Time 872.5 updated\n",
      "Time 873.0 updated\n",
      "Time 873.5 updated\n",
      "Time 874.0 updated\n",
      "Time 874.5 updated\n",
      "Time 875.0 updated\n",
      "Time 875.5 updated\n",
      "Time 876.0 updated\n",
      "Time 876.5 updated\n",
      "Time 877.0 updated\n",
      "Time 877.5 updated\n",
      "Time 878.0 updated\n",
      "Time 878.5 updated\n",
      "Time 879.0 updated\n",
      "Time 879.5 updated\n",
      "Time 880.0 updated\n",
      "Time 880.5 updated\n",
      "Time 881.0 updated\n",
      "Time 881.5 updated\n",
      "Time 882.0 updated\n",
      "Time 882.5 updated\n",
      "Time 883.0 updated\n",
      "Time 883.5 updated\n",
      "Time 884.0 updated\n",
      "Time 884.5 updated\n",
      "Time 885.0 updated\n",
      "Time 885.5 updated\n",
      "Time 886.0 updated\n",
      "Time 886.5 updated\n",
      "Time 887.0 updated\n",
      "Time 887.5 updated\n",
      "Time 888.0 updated\n",
      "Time 888.5 updated\n",
      "Time 889.0 updated\n",
      "Time 889.5 updated\n",
      "Time 890.0 updated\n",
      "Time 890.5 updated\n",
      "Time 891.0 updated\n",
      "Time 891.5 updated\n",
      "Time 892.0 updated\n",
      "Time 892.5 updated\n",
      "Time 893.0 updated\n",
      "Time 893.5 updated\n",
      "Time 894.0 updated\n",
      "Time 894.5 updated\n",
      "Time 895.0 updated\n",
      "Time 895.5 updated\n",
      "Time 896.0 updated\n",
      "Time 896.5 updated\n",
      "Time 897.0 updated\n",
      "Time 897.5 updated\n",
      "Time 898.0 updated\n",
      "Time 898.5 updated\n",
      "Time 899.0 updated\n",
      "Time 899.5 updated\n",
      "Time 900.0 updated\n",
      "Time 900.5 updated\n",
      "Time 901.0 updated\n",
      "Time 901.5 updated\n",
      "Time 902.0 updated\n",
      "Time 902.5 updated\n",
      "Time 903.0 updated\n",
      "Time 903.5 updated\n",
      "Time 904.0 updated\n",
      "Time 904.5 updated\n",
      "Time 905.0 updated\n",
      "Time 905.5 updated\n",
      "Time 906.0 updated\n",
      "Time 906.5 updated\n",
      "Time 907.0 updated\n",
      "Time 907.5 updated\n",
      "Time 908.0 updated\n",
      "Time 908.5 updated\n",
      "Time 909.0 updated\n",
      "Time 909.5 updated\n",
      "Time 910.0 updated\n",
      "Time 910.5 updated\n",
      "Time 911.0 updated\n",
      "Time 911.5 updated\n",
      "Time 912.0 updated\n",
      "Time 912.5 updated\n",
      "Time 913.0 updated\n",
      "Time 913.5 updated\n",
      "Time 914.0 updated\n",
      "Time 914.5 updated\n",
      "Time 915.0 updated\n",
      "Time 915.5 updated\n",
      "Time 916.0 updated\n",
      "Time 916.5 updated\n",
      "Time 917.0 updated\n",
      "Time 917.5 updated\n",
      "Time 918.0 updated\n",
      "Time 918.5 updated\n",
      "Time 919.0 updated\n",
      "Time 919.5 updated\n",
      "Time 920.0 updated\n",
      "Time 920.5 updated\n",
      "Time 921.0 updated\n",
      "Time 921.5 updated\n",
      "Time 922.0 updated\n",
      "Time 922.5 updated\n",
      "Time 923.0 updated\n",
      "Time 923.5 updated\n",
      "Time 924.0 updated\n",
      "Time 924.5 updated\n",
      "Time 925.0 updated\n",
      "Time 925.5 updated\n",
      "Time 926.0 updated\n",
      "Time 926.5 updated\n",
      "Time 927.0 updated\n",
      "Time 927.5 updated\n",
      "Time 928.0 updated\n",
      "Time 928.5 updated\n",
      "Time 929.0 updated\n",
      "Time 929.5 updated\n",
      "Time 930.0 updated\n",
      "Time 930.5 updated\n",
      "Time 931.0 updated\n",
      "Time 931.5 updated\n",
      "Time 932.0 updated\n",
      "Time 932.5 updated\n",
      "Time 933.0 updated\n",
      "Time 933.5 updated\n",
      "Time 934.0 updated\n",
      "Time 934.5 updated\n",
      "Time 935.0 updated\n",
      "Time 935.5 updated\n",
      "Time 936.0 updated\n",
      "Time 936.5 updated\n",
      "Time 937.0 updated\n",
      "Time 937.5 updated\n",
      "Time 938.0 updated\n",
      "Time 938.5 updated\n",
      "Time 939.0 updated\n",
      "Time 939.5 updated\n",
      "Time 940.0 updated\n",
      "Time 940.5 updated\n",
      "Time 941.0 updated\n",
      "Time 941.5 updated\n",
      "Time 942.0 updated\n",
      "Time 942.5 updated\n",
      "Time 943.0 updated\n",
      "Time 943.5 updated\n",
      "Time 944.0 updated\n",
      "Time 944.5 updated\n",
      "Time 945.0 updated\n",
      "Time 945.5 updated\n",
      "Time 946.0 updated\n",
      "Time 946.5 updated\n",
      "Time 947.0 updated\n",
      "Time 947.5 updated\n",
      "Time 948.0 updated\n",
      "Time 948.5 updated\n",
      "Time 949.0 updated\n",
      "Time 949.5 updated\n",
      "Time 950.0 updated\n",
      "Time 950.5 updated\n",
      "Time 951.0 updated\n",
      "Time 951.5 updated\n",
      "Time 952.0 updated\n",
      "Time 952.5 updated\n",
      "Time 953.0 updated\n",
      "Time 953.5 updated\n",
      "Time 954.0 updated\n",
      "Time 954.5 updated\n",
      "Time 955.0 updated\n",
      "Time 955.5 updated\n",
      "Time 956.0 updated\n",
      "Time 956.5 updated\n",
      "Time 957.0 updated\n",
      "Time 957.5 updated\n",
      "Time 958.0 updated\n",
      "Time 958.5 updated\n",
      "Time 959.0 updated\n",
      "Time 959.5 updated\n",
      "Time 960.0 updated\n",
      "Time 960.5 updated\n",
      "Time 961.0 updated\n",
      "Time 961.5 updated\n",
      "Time 962.0 updated\n",
      "Time 962.5 updated\n",
      "Time 963.0 updated\n",
      "Time 963.5 updated\n",
      "Time 964.0 updated\n",
      "Time 964.5 updated\n",
      "Time 965.0 updated\n",
      "Time 965.5 updated\n",
      "Time 966.0 updated\n",
      "Time 966.5 updated\n",
      "Time 967.0 updated\n",
      "Time 967.5 updated\n",
      "Time 968.0 updated\n",
      "Time 968.5 updated\n",
      "Time 969.0 updated\n",
      "Time 969.5 updated\n",
      "Time 970.0 updated\n",
      "Time 970.5 updated\n",
      "Time 971.0 updated\n",
      "Time 971.5 updated\n",
      "Time 972.0 updated\n",
      "Time 972.5 updated\n",
      "Time 973.0 updated\n",
      "Time 973.5 updated\n",
      "Time 974.0 updated\n",
      "Time 974.5 updated\n",
      "Time 975.0 updated\n",
      "Time 975.5 updated\n",
      "Time 976.0 updated\n",
      "Time 976.5 updated\n",
      "Time 977.0 updated\n",
      "Time 977.5 updated\n",
      "Time 978.0 updated\n",
      "Time 978.5 updated\n",
      "Time 979.0 updated\n",
      "Time 979.5 updated\n",
      "Time 980.0 updated\n",
      "Time 980.5 updated\n",
      "Time 981.0 updated\n",
      "Time 981.5 updated\n",
      "Time 982.0 updated\n",
      "Time 982.5 updated\n",
      "Time 983.0 updated\n",
      "Time 983.5 updated\n",
      "Time 984.0 updated\n",
      "Time 984.5 updated\n",
      "Time 985.0 updated\n",
      "Time 985.5 updated\n",
      "Time 986.0 updated\n",
      "Time 986.5 updated\n",
      "Time 987.0 updated\n",
      "Time 987.5 updated\n",
      "Time 988.0 updated\n",
      "Time 988.5 updated\n",
      "Time 989.0 updated\n",
      "Time 989.5 updated\n",
      "Time 990.0 updated\n",
      "Time 990.5 updated\n",
      "Time 991.0 updated\n",
      "Time 991.5 updated\n",
      "Time 992.0 updated\n",
      "Time 992.5 updated\n",
      "Time 993.0 updated\n",
      "Time 993.5 updated\n",
      "Time 994.0 updated\n",
      "Time 994.5 updated\n",
      "Time 995.0 updated\n",
      "Time 995.5 updated\n",
      "Time 996.0 updated\n",
      "Time 996.5 updated\n",
      "Time 997.0 updated\n",
      "Time 997.5 updated\n",
      "Time 998.0 updated\n",
      "Time 998.5 updated\n",
      "Time 999.0 updated\n",
      "Time 999.5 updated\n",
      "Time 1000.0 updated\n",
      "Time 1000.5 updated\n",
      "Time 1001.0 updated\n",
      "Time 1001.5 updated\n",
      "Time 1002.0 updated\n",
      "Time 1002.5 updated\n",
      "Time 1003.0 updated\n",
      "Time 1003.5 updated\n",
      "Time 1004.0 updated\n",
      "Time 1004.5 updated\n",
      "Time 1005.0 updated\n",
      "Time 1005.5 updated\n",
      "Time 1006.0 updated\n",
      "Time 1006.5 updated\n",
      "Time 1007.0 updated\n",
      "Time 1007.5 updated\n",
      "Time 1008.0 updated\n",
      "Time 1008.5 updated\n",
      "Time 1009.0 updated\n",
      "Time 1009.5 updated\n",
      "Time 1010.0 updated\n",
      "Time 1010.5 updated\n",
      "Time 1011.0 updated\n",
      "Time 1011.5 updated\n",
      "Time 1012.0 updated\n",
      "Time 1012.5 updated\n",
      "Time 1013.0 updated\n",
      "Time 1013.5 updated\n",
      "Time 1014.0 updated\n",
      "Time 1014.5 updated\n",
      "Time 1015.0 updated\n",
      "Time 1015.5 updated\n",
      "Time 1016.0 updated\n",
      "Time 1016.5 updated\n",
      "Time 1017.0 updated\n",
      "Time 1017.5 updated\n",
      "Time 1018.0 updated\n",
      "Time 1018.5 updated\n",
      "Time 1019.0 updated\n",
      "Time 1019.5 updated\n",
      "Time 1020.0 updated\n",
      "Time 1020.5 updated\n",
      "Time 1021.0 updated\n",
      "Time 1021.5 updated\n",
      "Time 1022.0 updated\n",
      "Time 1022.5 updated\n",
      "Time 1023.0 updated\n",
      "Time 1023.5 updated\n",
      "Time 1024.0 updated\n",
      "Time 1024.5 updated\n",
      "Time 1025.0 updated\n",
      "Time 1025.5 updated\n",
      "Time 1026.0 updated\n",
      "Time 1026.5 updated\n",
      "Time 1027.0 updated\n",
      "Time 1027.5 updated\n",
      "Time 1028.0 updated\n",
      "Time 1028.5 updated\n",
      "Time 1029.0 updated\n",
      "Time 1029.5 updated\n",
      "Time 1030.0 updated\n",
      "Time 1030.5 updated\n",
      "Time 1031.0 updated\n",
      "Time 1031.5 updated\n",
      "Time 1032.0 updated\n",
      "Time 1032.5 updated\n",
      "Time 1033.0 updated\n",
      "Time 1033.5 updated\n",
      "Time 1034.0 updated\n",
      "Time 1034.5 updated\n",
      "Time 1035.0 updated\n",
      "Time 1035.5 updated\n",
      "Time 1036.0 updated\n",
      "Time 1036.5 updated\n",
      "Time 1037.0 updated\n",
      "Time 1037.5 updated\n",
      "Time 1038.0 updated\n",
      "Time 1038.5 updated\n",
      "Time 1039.0 updated\n",
      "Time 1039.5 updated\n",
      "Time 1040.0 updated\n",
      "Time 1040.5 updated\n",
      "Time 1041.0 updated\n",
      "Time 1041.5 updated\n",
      "Time 1042.0 updated\n",
      "Time 1042.5 updated\n",
      "Time 1043.0 updated\n",
      "Time 1043.5 updated\n",
      "Time 1044.0 updated\n",
      "Time 1044.5 updated\n",
      "Time 1045.0 updated\n",
      "Time 1045.5 updated\n",
      "Time 1046.0 updated\n",
      "Time 1046.5 updated\n",
      "Time 1047.0 updated\n",
      "Time 1047.5 updated\n",
      "Time 1048.0 updated\n",
      "Time 1048.5 updated\n",
      "Time 1049.0 updated\n",
      "Time 1049.5 updated\n",
      "Time 1050.0 updated\n",
      "Time 1050.5 updated\n",
      "Time 1051.0 updated\n",
      "Time 1051.5 updated\n",
      "Time 1052.0 updated\n",
      "Time 1052.5 updated\n",
      "Time 1053.0 updated\n",
      "Time 1053.5 updated\n",
      "Time 1054.0 updated\n",
      "Time 1054.5 updated\n",
      "Time 1055.0 updated\n",
      "Time 1055.5 updated\n",
      "Time 1056.0 updated\n",
      "Time 1056.5 updated\n",
      "Time 1057.0 updated\n",
      "Time 1057.5 updated\n",
      "Time 1058.0 updated\n",
      "Time 1058.5 updated\n",
      "Time 1059.0 updated\n",
      "Time 1059.5 updated\n",
      "Time 1060.0 updated\n",
      "Time 1060.5 updated\n",
      "Time 1061.0 updated\n",
      "Time 1061.5 updated\n",
      "Time 1062.0 updated\n",
      "Time 1062.5 updated\n",
      "Time 1063.0 updated\n",
      "Time 1063.5 updated\n",
      "Time 1064.0 updated\n",
      "Time 1064.5 updated\n",
      "Time 1065.0 updated\n",
      "Time 1065.5 updated\n",
      "Time 1066.0 updated\n",
      "Time 1066.5 updated\n",
      "Time 1067.0 updated\n",
      "Time 1067.5 updated\n",
      "Time 1068.0 updated\n",
      "Time 1068.5 updated\n",
      "Time 1069.0 updated\n",
      "Time 1069.5 updated\n",
      "Time 1070.0 updated\n",
      "Time 1070.5 updated\n",
      "Time 1071.0 updated\n",
      "Time 1071.5 updated\n",
      "Time 1072.0 updated\n",
      "Time 1072.5 updated\n",
      "Time 1073.0 updated\n",
      "Time 1073.5 updated\n",
      "Time 1074.0 updated\n",
      "Time 1074.5 updated\n",
      "Time 1075.0 updated\n",
      "Time 1075.5 updated\n",
      "Time 1076.0 updated\n",
      "Time 1076.5 updated\n",
      "Time 1077.0 updated\n",
      "Time 1077.5 updated\n",
      "Time 1078.0 updated\n",
      "Time 1078.5 updated\n",
      "Time 1079.0 updated\n",
      "Time 1079.5 updated\n",
      "Time 1080.0 updated\n",
      "Time 1080.5 updated\n",
      "Time 1081.0 updated\n",
      "Time 1081.5 updated\n",
      "Time 1082.0 updated\n",
      "Time 1082.5 updated\n",
      "Time 1083.0 updated\n",
      "Time 1083.5 updated\n",
      "Time 1084.0 updated\n",
      "Time 1084.5 updated\n",
      "Time 1085.0 updated\n",
      "Time 1085.5 updated\n",
      "Time 1086.0 updated\n",
      "Time 1086.5 updated\n",
      "Time 1087.0 updated\n",
      "Time 1087.5 updated\n",
      "Time 1088.0 updated\n",
      "Time 1088.5 updated\n",
      "Time 1089.0 updated\n",
      "Time 1089.5 updated\n",
      "Time 1090.0 updated\n",
      "Time 1090.5 updated\n",
      "Time 1091.0 updated\n",
      "Time 1091.5 updated\n",
      "Time 1092.0 updated\n",
      "Time 1092.5 updated\n",
      "Time 1093.0 updated\n",
      "Time 1093.5 updated\n",
      "Time 1094.0 updated\n",
      "Time 1094.5 updated\n",
      "Time 1095.0 updated\n",
      "Time 1095.5 updated\n",
      "Time 1096.0 updated\n",
      "Time 1096.5 updated\n",
      "Time 1097.0 updated\n",
      "Time 1097.5 updated\n",
      "Time 1098.0 updated\n",
      "Time 1098.5 updated\n",
      "Time 1099.0 updated\n",
      "Time 1099.5 updated\n",
      "Time 1100.0 updated\n",
      "Time 1100.5 updated\n",
      "Time 1101.0 updated\n",
      "Time 1101.5 updated\n",
      "Time 1102.0 updated\n",
      "Time 1102.5 updated\n",
      "Time 1103.0 updated\n",
      "Time 1103.5 updated\n",
      "Time 1104.0 updated\n",
      "Time 1104.5 updated\n",
      "Time 1105.0 updated\n",
      "Time 1105.5 updated\n",
      "Time 1106.0 updated\n",
      "Time 1106.5 updated\n",
      "Time 1107.0 updated\n",
      "Time 1107.5 updated\n",
      "Time 1108.0 updated\n",
      "Time 1108.5 updated\n",
      "Time 1109.0 updated\n",
      "Time 1109.5 updated\n",
      "Time 1110.0 updated\n",
      "Time 1110.5 updated\n",
      "Time 1111.0 updated\n",
      "Time 1111.5 updated\n",
      "Time 1112.0 updated\n",
      "Time 1112.5 updated\n",
      "Time 1113.0 updated\n",
      "Time 1113.5 updated\n",
      "Time 1114.0 updated\n",
      "Time 1114.5 updated\n",
      "Time 1115.0 updated\n",
      "Time 1115.5 updated\n",
      "Time 1116.0 updated\n",
      "Time 1116.5 updated\n",
      "Time 1117.0 updated\n",
      "Time 1117.5 updated\n",
      "Time 1118.0 updated\n",
      "Time 1118.5 updated\n",
      "Time 1119.0 updated\n",
      "Time 1119.5 updated\n",
      "Time 1120.0 updated\n",
      "Time 1120.5 updated\n",
      "Time 1121.0 updated\n",
      "Time 1121.5 updated\n",
      "Time 1122.0 updated\n",
      "Time 1122.5 updated\n",
      "Time 1123.0 updated\n",
      "Time 1123.5 updated\n",
      "Time 1124.0 updated\n",
      "Time 1124.5 updated\n",
      "Time 1125.0 updated\n",
      "Time 1125.5 updated\n",
      "Time 1126.0 updated\n",
      "Time 1126.5 updated\n",
      "Time 1127.0 updated\n",
      "Time 1127.5 updated\n",
      "Time 1128.0 updated\n",
      "Time 1128.5 updated\n",
      "Time 1129.0 updated\n",
      "Time 1129.5 updated\n",
      "Time 1130.0 updated\n",
      "Time 1130.5 updated\n",
      "Time 1131.0 updated\n",
      "Time 1131.5 updated\n",
      "Time 1132.0 updated\n",
      "Time 1132.5 updated\n",
      "Time 1133.0 updated\n",
      "Time 1133.5 updated\n",
      "Time 1134.0 updated\n",
      "Time 1134.5 updated\n",
      "Time 1135.0 updated\n",
      "Time 1135.5 updated\n",
      "Time 1136.0 updated\n",
      "Time 1136.5 updated\n",
      "Time 1137.0 updated\n",
      "Time 1137.5 updated\n",
      "Time 1138.0 updated\n",
      "Time 1138.5 updated\n",
      "Time 1139.0 updated\n",
      "Time 1139.5 updated\n",
      "Time 1140.0 updated\n",
      "Time 1140.5 updated\n",
      "Time 1141.0 updated\n",
      "Time 1141.5 updated\n",
      "Time 1142.0 updated\n",
      "Time 1142.5 updated\n",
      "Time 1143.0 updated\n",
      "Time 1143.5 updated\n",
      "Time 1144.0 updated\n",
      "Time 1144.5 updated\n",
      "Time 1145.0 updated\n",
      "Time 1145.5 updated\n",
      "Time 1146.0 updated\n",
      "Time 1146.5 updated\n",
      "Time 1147.0 updated\n",
      "Time 1147.5 updated\n",
      "Time 1148.0 updated\n",
      "Time 1148.5 updated\n",
      "Time 1149.0 updated\n",
      "Time 1149.5 updated\n",
      "Time 1150.0 updated\n",
      "Time 1150.5 updated\n",
      "Time 1151.0 updated\n",
      "Time 1151.5 updated\n",
      "Time 1152.0 updated\n",
      "Time 1152.5 updated\n",
      "Time 1153.0 updated\n",
      "Time 1153.5 updated\n",
      "Time 1154.0 updated\n",
      "Time 1154.5 updated\n",
      "Time 1155.0 updated\n",
      "Time 1155.5 updated\n",
      "Time 1156.0 updated\n",
      "Time 1156.5 updated\n",
      "Time 1157.0 updated\n",
      "Time 1157.5 updated\n",
      "Time 1158.0 updated\n",
      "Time 1158.5 updated\n",
      "Time 1159.0 updated\n",
      "Time 1159.5 updated\n",
      "Time 1160.0 updated\n",
      "Time 1160.5 updated\n",
      "Time 1161.0 updated\n",
      "Time 1161.5 updated\n",
      "Time 1162.0 updated\n",
      "Time 1162.5 updated\n",
      "Time 1163.0 updated\n",
      "Time 1163.5 updated\n",
      "Time 1164.0 updated\n",
      "Time 1164.5 updated\n",
      "Time 1165.0 updated\n",
      "Time 1165.5 updated\n",
      "Time 1166.0 updated\n",
      "Time 1166.5 updated\n",
      "Time 1167.0 updated\n",
      "Time 1167.5 updated\n",
      "Time 1168.0 updated\n",
      "Time 1168.5 updated\n",
      "Time 1169.0 updated\n",
      "Time 1169.5 updated\n",
      "Time 1170.0 updated\n",
      "Time 1170.5 updated\n",
      "Time 1171.0 updated\n",
      "Time 1171.5 updated\n",
      "Time 1172.0 updated\n",
      "Time 1172.5 updated\n",
      "Time 1173.0 updated\n",
      "Time 1173.5 updated\n",
      "Time 1174.0 updated\n",
      "Time 1174.5 updated\n",
      "Time 1175.0 updated\n",
      "Time 1175.5 updated\n",
      "Time 1176.0 updated\n",
      "Time 1176.5 updated\n",
      "Time 1177.0 updated\n",
      "Time 1177.5 updated\n",
      "Time 1178.0 updated\n",
      "Time 1178.5 updated\n",
      "Time 1179.0 updated\n",
      "Time 1179.5 updated\n",
      "Time 1180.0 updated\n",
      "Time 1180.5 updated\n",
      "Time 1181.0 updated\n",
      "Time 1181.5 updated\n",
      "Time 1182.0 updated\n",
      "Time 1182.5 updated\n",
      "Time 1183.0 updated\n",
      "Time 1183.5 updated\n",
      "Time 1184.0 updated\n",
      "Time 1184.5 updated\n",
      "Time 1185.0 updated\n",
      "Time 1185.5 updated\n",
      "Time 1186.0 updated\n",
      "Time 1186.5 updated\n",
      "Time 1187.0 updated\n",
      "Time 1187.5 updated\n",
      "Time 1188.0 updated\n",
      "Time 1188.5 updated\n",
      "Time 1189.0 updated\n",
      "Time 1189.5 updated\n",
      "Time 1190.0 updated\n",
      "Time 1190.5 updated\n",
      "Time 1191.0 updated\n",
      "Time 1191.5 updated\n",
      "Time 1192.0 updated\n",
      "Time 1192.5 updated\n",
      "Time 1193.0 updated\n",
      "Time 1193.5 updated\n",
      "Time 1194.0 updated\n",
      "Time 1194.5 updated\n",
      "Time 1195.0 updated\n",
      "Time 1195.5 updated\n",
      "Time 1196.0 updated\n",
      "Time 1196.5 updated\n",
      "Time 1197.0 updated\n",
      "Time 1197.5 updated\n",
      "Time 1198.0 updated\n",
      "Time 1198.5 updated\n",
      "Time 1199.0 updated\n",
      "Time 1199.5 updated\n",
      "Time 1200.0 updated\n",
      "Time 1200.5 updated\n",
      "Time 1201.0 updated\n",
      "Time 1201.5 updated\n",
      "Time 1202.0 updated\n",
      "Time 1202.5 updated\n",
      "Time 1203.0 updated\n",
      "Time 1203.5 updated\n",
      "Time 1204.0 updated\n",
      "Time 1204.5 updated\n",
      "Time 1205.0 updated\n",
      "Time 1205.5 updated\n",
      "Time 1206.0 updated\n",
      "Time 1206.5 updated\n",
      "Time 1207.0 updated\n",
      "Time 1207.5 updated\n",
      "Time 1208.0 updated\n",
      "Time 1208.5 updated\n",
      "Time 1209.0 updated\n",
      "Time 1209.5 updated\n",
      "Time 1210.0 updated\n",
      "Time 1210.5 updated\n",
      "Time 1211.0 updated\n",
      "Time 1211.5 updated\n",
      "Time 1212.0 updated\n",
      "Time 1212.5 updated\n",
      "Time 1213.0 updated\n",
      "Time 1213.5 updated\n",
      "Time 1214.0 updated\n",
      "Time 1214.5 updated\n",
      "Time 1215.0 updated\n",
      "Time 1215.5 updated\n",
      "Time 1216.0 updated\n",
      "Time 1216.5 updated\n",
      "Time 1217.0 updated\n",
      "Time 1217.5 updated\n",
      "Time 1218.0 updated\n",
      "Time 1218.5 updated\n",
      "Time 1219.0 updated\n",
      "Time 1219.5 updated\n",
      "Time 1220.0 updated\n",
      "Time 1220.5 updated\n",
      "Time 1221.0 updated\n",
      "Time 1221.5 updated\n",
      "Time 1222.0 updated\n",
      "Time 1222.5 updated\n",
      "Time 1223.0 updated\n",
      "Time 1223.5 updated\n",
      "Time 1224.0 updated\n",
      "Time 1224.5 updated\n",
      "Time 1225.0 updated\n",
      "Time 1225.5 updated\n",
      "Time 1226.0 updated\n",
      "Time 1226.5 updated\n",
      "Time 1227.0 updated\n",
      "Time 1227.5 updated\n",
      "Time 1228.0 updated\n",
      "Time 1228.5 updated\n",
      "Time 1229.0 updated\n",
      "Time 1229.5 updated\n",
      "Time 1230.0 updated\n",
      "Time 1230.5 updated\n",
      "Time 1231.0 updated\n",
      "Time 1231.5 updated\n",
      "Time 1232.0 updated\n",
      "Time 1232.5 updated\n",
      "Time 1233.0 updated\n",
      "Time 1233.5 updated\n",
      "Time 1234.0 updated\n",
      "Time 1234.5 updated\n",
      "Time 1235.0 updated\n",
      "Time 1235.5 updated\n",
      "Time 1236.0 updated\n",
      "Time 1236.5 updated\n",
      "Time 1237.0 updated\n",
      "Time 1237.5 updated\n",
      "Time 1238.0 updated\n",
      "Time 1238.5 updated\n",
      "Time 1239.0 updated\n",
      "Time 1239.5 updated\n",
      "Time 1240.0 updated\n",
      "Time 1240.5 updated\n",
      "Time 1241.0 updated\n",
      "Time 1241.5 updated\n",
      "Time 1242.0 updated\n",
      "Time 1242.5 updated\n",
      "Time 1243.0 updated\n",
      "Time 1243.5 updated\n",
      "Time 1244.0 updated\n",
      "Time 1244.5 updated\n",
      "Time 1245.0 updated\n",
      "Time 1245.5 updated\n",
      "Time 1246.0 updated\n",
      "Time 1246.5 updated\n",
      "Time 1247.0 updated\n",
      "Time 1247.5 updated\n",
      "Time 1248.0 updated\n",
      "Time 1248.5 updated\n",
      "Time 1249.0 updated\n",
      "Time 1249.5 updated\n",
      "Time 1250.0 updated\n",
      "Time 1250.5 updated\n",
      "Time 1251.0 updated\n",
      "Time 1251.5 updated\n",
      "Time 1252.0 updated\n",
      "Time 1252.5 updated\n",
      "Time 1253.0 updated\n",
      "Time 1253.5 updated\n",
      "Time 1254.0 updated\n",
      "Time 1254.5 updated\n",
      "Time 1255.0 updated\n",
      "Time 1255.5 updated\n",
      "Time 1256.0 updated\n",
      "Time 1256.5 updated\n",
      "Time 1257.0 updated\n",
      "Time 1257.5 updated\n",
      "Time 1258.0 updated\n",
      "Time 1258.5 updated\n",
      "Time 1259.0 updated\n",
      "Time 1259.5 updated\n",
      "Time 1260.0 updated\n",
      "Time 1260.5 updated\n",
      "Time 1261.0 updated\n",
      "Time 1261.5 updated\n",
      "Time 1262.0 updated\n",
      "Time 1262.5 updated\n",
      "Time 1263.0 updated\n",
      "Time 1263.5 updated\n",
      "Time 1264.0 updated\n",
      "Time 1264.5 updated\n",
      "Time 1265.0 updated\n",
      "Time 1265.5 updated\n",
      "Time 1266.0 updated\n",
      "Time 1266.5 updated\n",
      "Time 1267.0 updated\n",
      "Time 1267.5 updated\n",
      "Time 1268.0 updated\n",
      "Time 1268.5 updated\n",
      "Time 1269.0 updated\n",
      "Time 1269.5 updated\n",
      "Time 1270.0 updated\n",
      "Time 1270.5 updated\n",
      "Time 1271.0 updated\n",
      "Time 1271.5 updated\n",
      "Time 1272.0 updated\n",
      "Time 1272.5 updated\n",
      "Time 1273.0 updated\n",
      "Time 1273.5 updated\n",
      "Time 1274.0 updated\n",
      "Time 1274.5 updated\n",
      "Time 1275.0 updated\n",
      "Time 1275.5 updated\n",
      "Time 1276.0 updated\n",
      "Time 1276.5 updated\n",
      "Time 1277.0 updated\n",
      "Time 1277.5 updated\n",
      "Time 1278.0 updated\n",
      "Time 1278.5 updated\n",
      "Time 1279.0 updated\n",
      "Time 1279.5 updated\n",
      "Time 1280.0 updated\n",
      "Time 1280.5 updated\n",
      "Time 1281.0 updated\n",
      "Time 1281.5 updated\n",
      "Time 1282.0 updated\n",
      "Time 1282.5 updated\n",
      "Time 1283.0 updated\n",
      "Time 1283.5 updated\n",
      "Time 1284.0 updated\n",
      "Time 1284.5 updated\n",
      "Time 1285.0 updated\n",
      "Time 1285.5 updated\n",
      "Time 1286.0 updated\n",
      "Time 1286.5 updated\n",
      "Time 1287.0 updated\n",
      "Time 1287.5 updated\n",
      "Time 1288.0 updated\n",
      "Time 1288.5 updated\n",
      "Time 1289.0 updated\n",
      "Time 1289.5 updated\n",
      "Time 1290.0 updated\n",
      "Time 1290.5 updated\n",
      "Time 1291.0 updated\n",
      "Time 1291.5 updated\n",
      "Time 1292.0 updated\n",
      "Time 1292.5 updated\n",
      "Time 1293.0 updated\n",
      "Time 1293.5 updated\n",
      "Time 1294.0 updated\n",
      "Time 1294.5 updated\n",
      "Time 1295.0 updated\n",
      "Time 1295.5 updated\n",
      "Time 1296.0 updated\n",
      "Time 1296.5 updated\n",
      "Time 1297.0 updated\n",
      "Time 1297.5 updated\n",
      "Time 1298.0 updated\n",
      "Time 1298.5 updated\n",
      "Time 1299.0 updated\n",
      "Time 1299.5 updated\n",
      "Time 1300.0 updated\n",
      "Time 1300.5 updated\n",
      "Time 1301.0 updated\n",
      "Time 1301.5 updated\n",
      "Time 1302.0 updated\n",
      "Time 1302.5 updated\n",
      "Time 1303.0 updated\n",
      "Time 1303.5 updated\n",
      "Time 1304.0 updated\n",
      "Time 1304.5 updated\n",
      "Time 1305.0 updated\n",
      "Time 1305.5 updated\n",
      "Time 1306.0 updated\n",
      "Time 1306.5 updated\n",
      "Time 1307.0 updated\n",
      "Time 1307.5 updated\n",
      "Time 1308.0 updated\n",
      "Time 1308.5 updated\n",
      "Time 1309.0 updated\n",
      "Time 1309.5 updated\n",
      "Time 1310.0 updated\n",
      "Time 1310.5 updated\n",
      "Time 1311.0 updated\n",
      "Time 1311.5 updated\n",
      "Time 1312.0 updated\n",
      "Time 1312.5 updated\n",
      "Time 1313.0 updated\n",
      "Time 1313.5 updated\n",
      "Time 1314.0 updated\n",
      "Time 1314.5 updated\n",
      "Time 1315.0 updated\n",
      "Time 1315.5 updated\n",
      "Time 1316.0 updated\n",
      "Time 1316.5 updated\n",
      "Time 1317.0 updated\n",
      "Time 1317.5 updated\n",
      "Time 1318.0 updated\n",
      "Time 1318.5 updated\n",
      "Time 1319.0 updated\n",
      "Time 1319.5 updated\n",
      "Time 1320.0 updated\n",
      "Time 1320.5 updated\n",
      "Time 1321.0 updated\n",
      "Time 1321.5 updated\n",
      "Time 1322.0 updated\n",
      "Time 1322.5 updated\n",
      "Time 1323.0 updated\n",
      "Time 1323.5 updated\n",
      "Time 1324.0 updated\n",
      "Time 1324.5 updated\n",
      "Time 1325.0 updated\n",
      "Time 1325.5 updated\n",
      "Time 1326.0 updated\n",
      "Time 1326.5 updated\n",
      "Time 1327.0 updated\n",
      "Time 1327.5 updated\n",
      "Time 1328.0 updated\n",
      "Time 1328.5 updated\n",
      "Time 1329.0 updated\n",
      "Time 1329.5 updated\n",
      "Time 1330.0 updated\n",
      "Time 1330.5 updated\n",
      "Time 1331.0 updated\n",
      "Time 1331.5 updated\n",
      "Time 1332.0 updated\n",
      "Time 1332.5 updated\n",
      "Time 1333.0 updated\n",
      "Time 1333.5 updated\n",
      "Time 1334.0 updated\n",
      "Time 1334.5 updated\n",
      "Time 1335.0 updated\n",
      "Time 1335.5 updated\n",
      "Time 1336.0 updated\n",
      "Time 1336.5 updated\n",
      "Time 1337.0 updated\n",
      "Time 1337.5 updated\n",
      "Time 1338.0 updated\n",
      "Time 1338.5 updated\n",
      "Time 1339.0 updated\n",
      "Time 1339.5 updated\n",
      "Time 1340.0 updated\n",
      "Time 1340.5 updated\n",
      "Time 1341.0 updated\n",
      "Time 1341.5 updated\n",
      "Time 1342.0 updated\n",
      "Time 1342.5 updated\n",
      "Time 1343.0 updated\n",
      "Time 1343.5 updated\n",
      "Time 1344.0 updated\n",
      "Time 1344.5 updated\n",
      "Time 1345.0 updated\n",
      "Time 1345.5 updated\n",
      "Time 1346.0 updated\n",
      "Time 1346.5 updated\n",
      "Time 1347.0 updated\n",
      "Time 1347.5 updated\n",
      "Time 1348.0 updated\n",
      "Time 1348.5 updated\n",
      "Time 1349.0 updated\n",
      "Time 1349.5 updated\n",
      "Time 1350.0 updated\n",
      "Time 1350.5 updated\n",
      "Time 1351.0 updated\n",
      "Time 1351.5 updated\n",
      "Time 1352.0 updated\n",
      "Time 1352.5 updated\n",
      "Time 1353.0 updated\n",
      "Time 1353.5 updated\n",
      "Time 1354.0 updated\n",
      "Time 1354.5 updated\n",
      "Time 1355.0 updated\n",
      "Time 1355.5 updated\n",
      "Time 1356.0 updated\n",
      "Time 1356.5 updated\n",
      "Time 1357.0 updated\n",
      "Time 1357.5 updated\n",
      "Time 1358.0 updated\n",
      "Time 1358.5 updated\n",
      "Time 1359.0 updated\n",
      "Time 1359.5 updated\n",
      "Time 1360.0 updated\n",
      "Time 1360.5 updated\n",
      "Time 1361.0 updated\n",
      "Time 1361.5 updated\n",
      "Time 1362.0 updated\n",
      "Time 1362.5 updated\n",
      "Time 1363.0 updated\n",
      "Time 1363.5 updated\n",
      "Time 1364.0 updated\n",
      "Time 1364.5 updated\n",
      "Time 1365.0 updated\n",
      "Time 1365.5 updated\n",
      "Time 1366.0 updated\n",
      "Time 1366.5 updated\n",
      "Time 1367.0 updated\n",
      "Time 1367.5 updated\n",
      "Time 1368.0 updated\n",
      "Time 1368.5 updated\n",
      "Time 1369.0 updated\n",
      "Time 1369.5 updated\n",
      "Time 1370.0 updated\n",
      "Time 1370.5 updated\n",
      "Time 1371.0 updated\n",
      "Time 1371.5 updated\n",
      "Time 1372.0 updated\n",
      "Time 1372.5 updated\n",
      "Time 1373.0 updated\n",
      "Time 1373.5 updated\n",
      "Time 1374.0 updated\n",
      "Time 1374.5 updated\n",
      "Time 1375.0 updated\n",
      "Time 1375.5 updated\n",
      "Time 1376.0 updated\n",
      "Time 1376.5 updated\n",
      "Time 1377.0 updated\n",
      "Time 1377.5 updated\n",
      "Time 1378.0 updated\n",
      "Time 1378.5 updated\n",
      "Time 1379.0 updated\n",
      "Time 1379.5 updated\n",
      "Time 1380.0 updated\n",
      "Time 1380.5 updated\n",
      "Time 1381.0 updated\n",
      "Time 1381.5 updated\n",
      "Time 1382.0 updated\n",
      "Time 1382.5 updated\n",
      "Time 1383.0 updated\n",
      "Time 1383.5 updated\n",
      "Time 1384.0 updated\n",
      "Time 1384.5 updated\n",
      "Time 1385.0 updated\n",
      "Time 1385.5 updated\n",
      "Time 1386.0 updated\n",
      "Time 1386.5 updated\n",
      "Time 1387.0 updated\n",
      "Time 1387.5 updated\n",
      "Time 1388.0 updated\n",
      "Time 1388.5 updated\n",
      "Time 1389.0 updated\n",
      "Time 1389.5 updated\n",
      "Time 1390.0 updated\n",
      "Time 1390.5 updated\n",
      "Time 1391.0 updated\n",
      "Time 1391.5 updated\n",
      "Time 1392.0 updated\n",
      "Time 1392.5 updated\n",
      "Time 1393.0 updated\n",
      "Time 1393.5 updated\n",
      "Time 1394.0 updated\n",
      "Time 1394.5 updated\n",
      "Time 1395.0 updated\n",
      "Time 1395.5 updated\n",
      "Time 1396.0 updated\n",
      "Time 1396.5 updated\n",
      "Time 1397.0 updated\n",
      "Time 1397.5 updated\n",
      "Time 1398.0 updated\n",
      "Time 1398.5 updated\n",
      "Time 1399.0 updated\n",
      "Time 1399.5 updated\n",
      "Time 1400.0 updated\n",
      "Time 1400.5 updated\n"
     ]
    }
   ],
   "source": [
    "diff_sample_1, norm_ls_1 = generation_detailed(model_350, 50, n, 0.5, 4 * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6999ac8b-9223-4789-bb2e-4e9b3b102ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time_points.detach().cpu().numpy(), norm_ls_1, label=\"Diffusion samples\")\n",
    "plt.plot(time_points.detach().cpu().numpy(), norm_ls, label=\"Brownian motion plug-in\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "77e3bdd5-74c5-4f0b-aa2b-739840e97b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def KL_div_from_noise(model, n, time_step, terminal_time, num_samples=50, device=device):\n",
    "    t = 0\n",
    "    \n",
    "    # Initialize sum\n",
    "    s = 0\n",
    "    \n",
    "    while t <= terminal_time:\n",
    "        if t == 0:\n",
    "            t += time_step\n",
    "\n",
    "        else:\n",
    "            # Generate scaled noise\n",
    "            noise_scaled = torch.randn((num_samples, n, n), device=device) * (t ** (-1/2))\n",
    "            noise_scaled_symm = (1/2) * (noise_scaled + noise_scaled.transpose(-1, -2))\n",
    "            noise_scaled_symm_expand = noise_scaled_symm.unsqueeze(-1)\n",
    "\n",
    "            # Create time array\n",
    "            time_array = torch.ones((num_samples, 1, 1), device=device) * t\n",
    "\n",
    "            # Generate random embedding\n",
    "            # Generate random node embedding\n",
    "            x_init = torch.randn((num_samples, n), device=device)\n",
    "            x_init = x_init / x_init.norm(dim=1, keepdim=True)\n",
    "            x_init_expand = x_init.unsqueeze(-1)\n",
    "\n",
    "            # Get output\n",
    "            out = model.predict(x_init_expand, noise_scaled_symm_expand, time_array)\n",
    "\n",
    "            # Get squared Frobenius norm\n",
    "            norms_sq = torch.norm(out, p=\"fro\", dim=(1, 2)) ** 2\n",
    "\n",
    "            # Get average\n",
    "            norms_sq_avg = torch.mean(norms_sq)\n",
    "\n",
    "            # Store value\n",
    "            s += norms_sq_avg * time_step\n",
    "\n",
    "            # Increment t\n",
    "            t += time_step\n",
    "\n",
    "\n",
    "        # Print update\n",
    "        print(\"Time {} updated!\".format(t))\n",
    "\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93738f0-91bd-436a-bdf2-8836d2eb171a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_trials = 20\n",
    "kl_ls = []\n",
    "\n",
    "for _ in range(50):\n",
    "    s = KL_div_from_noise(model_350, n, 0.5, 4 * n, num_samples=15)\n",
    "    kl_ls.append(s.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3bfef205-97ef-4567-8d92-92456a8f6bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.4595903158187866,\n",
       " 1.4530484676361084,\n",
       " 1.4480549097061157,\n",
       " 1.438002347946167,\n",
       " 1.4529987573623657,\n",
       " 1.4341686964035034,\n",
       " 1.446015477180481,\n",
       " 1.4843459129333496,\n",
       " 1.4721624851226807,\n",
       " 1.4255131483078003,\n",
       " 1.4573787450790405,\n",
       " 1.517512321472168,\n",
       " 1.4398068189620972,\n",
       " 1.4785327911376953,\n",
       " 1.4464600086212158,\n",
       " 1.3937294483184814]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b2d60340-2244-453d-be20-a9e63a938a0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(kl_ls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7c7e36ec-dbbe-4dcd-8d4b-833d46e879d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.0007102656988022815)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.var(kl_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09358ba4-fc77-43bc-9578-498357713ba9",
   "metadata": {},
   "source": [
    "# Fine-tune the model even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9568832-f24d-402b-a653-e6f8516872a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Batch: 0, Loss: 0.15763039886951447\n",
      "Iteration 0, Batch: 1, Loss: 0.1665785163640976\n",
      "Iteration 0, Batch: 2, Loss: 0.18368816375732422\n",
      "Iteration 0, Batch: 3, Loss: 0.18865308165550232\n",
      "Iteration 0, Batch: 4, Loss: 0.1843554824590683\n",
      "Iteration 0, Batch: 5, Loss: 0.1501157134771347\n",
      "Iteration 0, Batch: 6, Loss: 0.1963508129119873\n",
      "Iteration 0, Batch: 7, Loss: 0.1396457701921463\n",
      "Iteration 0, Batch: 8, Loss: 0.21478545665740967\n",
      "Iteration 0, Batch: 9, Loss: 0.14103132486343384\n",
      "Iteration 0, Batch: 10, Loss: 0.2045811414718628\n",
      "Iteration 0, Batch: 11, Loss: 0.13645170629024506\n",
      "Iteration 0, Batch: 12, Loss: 0.21651893854141235\n",
      "Iteration 0, Batch: 13, Loss: 0.2312806248664856\n",
      "Iteration 0, Batch: 14, Loss: 0.20139442384243011\n",
      "Iteration 0, Batch: 15, Loss: 0.172939270734787\n",
      "Iteration 0, Batch: 16, Loss: 0.15155178308486938\n",
      "Iteration 0, Batch: 17, Loss: 0.19920718669891357\n",
      "Iteration 0, Batch: 18, Loss: 0.18708361685276031\n",
      "Iteration 0, Batch: 19, Loss: 0.1766873449087143\n",
      "Iteration 0, Batch: 20, Loss: 0.1961853802204132\n",
      "Iteration 0, Batch: 21, Loss: 0.16728472709655762\n",
      "Iteration 0, Batch: 22, Loss: 0.19130054116249084\n",
      "Iteration 0, Batch: 23, Loss: 0.16198429465293884\n",
      "Iteration 0, Batch: 24, Loss: 0.14169782400131226\n",
      "Iteration 0, Batch: 25, Loss: 0.15888775885105133\n",
      "Iteration 0, Batch: 26, Loss: 0.1848362684249878\n",
      "Iteration 0, Batch: 27, Loss: 0.14963126182556152\n",
      "Iteration 0, Batch: 28, Loss: 0.1622617095708847\n",
      "Iteration 0, Batch: 29, Loss: 0.16196209192276\n",
      "Iteration 0, Batch: 30, Loss: 0.1807129681110382\n",
      "Iteration 0, Batch: 31, Loss: 0.19474342465400696\n",
      "Iteration 0, Batch: 32, Loss: 0.1779550164937973\n",
      "Iteration 0, Batch: 33, Loss: 0.1931685209274292\n",
      "Iteration 0, Batch: 34, Loss: 0.18731209635734558\n",
      "Iteration 0, Batch: 35, Loss: 0.1379556953907013\n",
      "Iteration 0, Batch: 36, Loss: 0.15547499060630798\n",
      "Iteration 0, Batch: 37, Loss: 0.199273481965065\n",
      "Iteration 0, Batch: 38, Loss: 0.15366660058498383\n",
      "Iteration 0, Batch: 39, Loss: 0.1593528389930725\n",
      "Iteration 0, Batch: 40, Loss: 0.18327133357524872\n",
      "Iteration 0, Batch: 41, Loss: 0.20901285111904144\n",
      "Iteration 0, Batch: 42, Loss: 0.15980088710784912\n",
      "Iteration 0, Batch: 43, Loss: 0.1670503318309784\n",
      "Iteration 0, Batch: 44, Loss: 0.17999888956546783\n",
      "Iteration 0, Batch: 45, Loss: 0.18709783256053925\n",
      "Iteration 0, Batch: 46, Loss: 0.20845244824886322\n",
      "Iteration 0, Batch: 47, Loss: 0.1393292248249054\n",
      "Iteration 0, Batch: 48, Loss: 0.16750682890415192\n",
      "Iteration 0, Batch: 49, Loss: 0.15971536934375763\n",
      "Iteration 1, Batch: 0, Loss: 0.22109848260879517\n",
      "Iteration 1, Batch: 1, Loss: 0.23956191539764404\n",
      "Iteration 1, Batch: 2, Loss: 0.16198086738586426\n",
      "Iteration 1, Batch: 3, Loss: 0.1873682141304016\n",
      "Iteration 1, Batch: 4, Loss: 0.19577784836292267\n",
      "Iteration 1, Batch: 5, Loss: 0.14141976833343506\n",
      "Iteration 1, Batch: 6, Loss: 0.20623943209648132\n",
      "Iteration 1, Batch: 7, Loss: 0.1393698900938034\n",
      "Iteration 1, Batch: 8, Loss: 0.19154967367649078\n",
      "Iteration 1, Batch: 9, Loss: 0.17413680255413055\n",
      "Iteration 1, Batch: 10, Loss: 0.1712951958179474\n",
      "Iteration 1, Batch: 11, Loss: 0.19502541422843933\n",
      "Iteration 1, Batch: 12, Loss: 0.16995856165885925\n",
      "Iteration 1, Batch: 13, Loss: 0.20099368691444397\n",
      "Iteration 1, Batch: 14, Loss: 0.19232231378555298\n",
      "Iteration 1, Batch: 15, Loss: 0.1766345351934433\n",
      "Iteration 1, Batch: 16, Loss: 0.12320196628570557\n",
      "Iteration 1, Batch: 17, Loss: 0.16071021556854248\n",
      "Iteration 1, Batch: 18, Loss: 0.12991800904273987\n",
      "Iteration 1, Batch: 19, Loss: 0.1601780652999878\n",
      "Iteration 1, Batch: 20, Loss: 0.17746463418006897\n",
      "Iteration 1, Batch: 21, Loss: 0.16703204810619354\n",
      "Iteration 1, Batch: 22, Loss: 0.14650177955627441\n",
      "Iteration 1, Batch: 23, Loss: 0.2091791331768036\n",
      "Iteration 1, Batch: 24, Loss: 0.19452452659606934\n",
      "Iteration 1, Batch: 25, Loss: 0.20515185594558716\n",
      "Iteration 1, Batch: 26, Loss: 0.15380088984966278\n",
      "Iteration 1, Batch: 27, Loss: 0.17791293561458588\n",
      "Iteration 1, Batch: 28, Loss: 0.2221790850162506\n",
      "Iteration 1, Batch: 29, Loss: 0.17407579720020294\n",
      "Iteration 1, Batch: 30, Loss: 0.16262026131153107\n",
      "Iteration 1, Batch: 31, Loss: 0.17144761979579926\n",
      "Iteration 1, Batch: 32, Loss: 0.19506308436393738\n",
      "Iteration 1, Batch: 33, Loss: 0.18635742366313934\n",
      "Iteration 1, Batch: 34, Loss: 0.21373003721237183\n",
      "Iteration 1, Batch: 35, Loss: 0.14558684825897217\n",
      "Iteration 1, Batch: 36, Loss: 0.18430139124393463\n",
      "Iteration 1, Batch: 37, Loss: 0.19648341834545135\n",
      "Iteration 1, Batch: 38, Loss: 0.17414429783821106\n",
      "Iteration 1, Batch: 39, Loss: 0.13099542260169983\n",
      "Iteration 1, Batch: 40, Loss: 0.1955736130475998\n",
      "Iteration 1, Batch: 41, Loss: 0.16295425593852997\n",
      "Iteration 1, Batch: 42, Loss: 0.20166262984275818\n",
      "Iteration 1, Batch: 43, Loss: 0.17762866616249084\n",
      "Iteration 1, Batch: 44, Loss: 0.17261344194412231\n",
      "Iteration 1, Batch: 45, Loss: 0.18908238410949707\n",
      "Iteration 1, Batch: 46, Loss: 0.17243503034114838\n",
      "Iteration 1, Batch: 47, Loss: 0.1481473445892334\n",
      "Iteration 1, Batch: 48, Loss: 0.16317829489707947\n",
      "Iteration 1, Batch: 49, Loss: 0.2035266011953354\n",
      "Iteration 2, Batch: 0, Loss: 0.21695616841316223\n",
      "Iteration 2, Batch: 1, Loss: 0.17610226571559906\n",
      "Iteration 2, Batch: 2, Loss: 0.1773526966571808\n",
      "Iteration 2, Batch: 3, Loss: 0.11211743950843811\n",
      "Iteration 2, Batch: 4, Loss: 0.15170349180698395\n",
      "Iteration 2, Batch: 5, Loss: 0.18809981644153595\n",
      "Iteration 2, Batch: 6, Loss: 0.14733068645000458\n",
      "Iteration 2, Batch: 7, Loss: 0.17650394141674042\n",
      "Iteration 2, Batch: 8, Loss: 0.17129917442798615\n",
      "Iteration 2, Batch: 9, Loss: 0.2096584588289261\n",
      "Iteration 2, Batch: 10, Loss: 0.16385741531848907\n",
      "Iteration 2, Batch: 11, Loss: 0.19074805080890656\n",
      "Iteration 2, Batch: 12, Loss: 0.13802684843540192\n",
      "Iteration 2, Batch: 13, Loss: 0.18088878691196442\n",
      "Iteration 2, Batch: 14, Loss: 0.18003474175930023\n",
      "Iteration 2, Batch: 15, Loss: 0.16059160232543945\n",
      "Iteration 2, Batch: 16, Loss: 0.1810540407896042\n",
      "Iteration 2, Batch: 17, Loss: 0.1721060574054718\n",
      "Iteration 2, Batch: 18, Loss: 0.22338277101516724\n",
      "Iteration 2, Batch: 19, Loss: 0.13514971733093262\n",
      "Iteration 2, Batch: 20, Loss: 0.21059036254882812\n",
      "Iteration 2, Batch: 21, Loss: 0.17036941647529602\n",
      "Iteration 2, Batch: 22, Loss: 0.21131041646003723\n",
      "Iteration 2, Batch: 23, Loss: 0.2370978146791458\n",
      "Iteration 2, Batch: 24, Loss: 0.19307978451251984\n",
      "Iteration 2, Batch: 25, Loss: 0.1788218766450882\n",
      "Iteration 2, Batch: 26, Loss: 0.20101214945316315\n",
      "Iteration 2, Batch: 27, Loss: 0.18748725950717926\n",
      "Iteration 2, Batch: 28, Loss: 0.18811951577663422\n",
      "Iteration 2, Batch: 29, Loss: 0.2187296599149704\n",
      "Iteration 2, Batch: 30, Loss: 0.20289266109466553\n",
      "Iteration 2, Batch: 31, Loss: 0.1357741355895996\n",
      "Iteration 2, Batch: 32, Loss: 0.17350122332572937\n",
      "Iteration 2, Batch: 33, Loss: 0.17405591905117035\n",
      "Iteration 2, Batch: 34, Loss: 0.19114916026592255\n",
      "Iteration 2, Batch: 35, Loss: 0.16372765600681305\n",
      "Iteration 2, Batch: 36, Loss: 0.1497778743505478\n",
      "Iteration 2, Batch: 37, Loss: 0.1502140462398529\n",
      "Iteration 2, Batch: 38, Loss: 0.15588544309139252\n",
      "Iteration 2, Batch: 39, Loss: 0.21149688959121704\n",
      "Iteration 2, Batch: 40, Loss: 0.15725219249725342\n",
      "Iteration 2, Batch: 41, Loss: 0.21148623526096344\n",
      "Iteration 2, Batch: 42, Loss: 0.14279156923294067\n",
      "Iteration 2, Batch: 43, Loss: 0.17269599437713623\n",
      "Iteration 2, Batch: 44, Loss: 0.1422441303730011\n",
      "Iteration 2, Batch: 45, Loss: 0.19769330322742462\n",
      "Iteration 2, Batch: 46, Loss: 0.1850738227367401\n",
      "Iteration 2, Batch: 47, Loss: 0.20121124386787415\n",
      "Iteration 2, Batch: 48, Loss: 0.15510514378547668\n",
      "Iteration 2, Batch: 49, Loss: 0.19580207765102386\n",
      "Iteration 3, Batch: 0, Loss: 0.19065573811531067\n",
      "Iteration 3, Batch: 1, Loss: 0.19679076969623566\n",
      "Iteration 3, Batch: 2, Loss: 0.17016834020614624\n",
      "Iteration 3, Batch: 3, Loss: 0.1564333289861679\n",
      "Iteration 3, Batch: 4, Loss: 0.17844220995903015\n",
      "Iteration 3, Batch: 5, Loss: 0.17253923416137695\n",
      "Iteration 3, Batch: 6, Loss: 0.1347295492887497\n",
      "Iteration 3, Batch: 7, Loss: 0.1684696525335312\n",
      "Iteration 3, Batch: 8, Loss: 0.20454183220863342\n",
      "Iteration 3, Batch: 9, Loss: 0.21228110790252686\n",
      "Iteration 3, Batch: 10, Loss: 0.1385953575372696\n",
      "Iteration 3, Batch: 11, Loss: 0.15913628041744232\n",
      "Iteration 3, Batch: 12, Loss: 0.23780901730060577\n",
      "Iteration 3, Batch: 13, Loss: 0.18319819867610931\n",
      "Iteration 3, Batch: 14, Loss: 0.20495592057704926\n",
      "Iteration 3, Batch: 15, Loss: 0.17227615416049957\n",
      "Iteration 3, Batch: 16, Loss: 0.13827833533287048\n",
      "Iteration 3, Batch: 17, Loss: 0.20178039371967316\n",
      "Iteration 3, Batch: 18, Loss: 0.21896691620349884\n",
      "Iteration 3, Batch: 19, Loss: 0.19084733724594116\n",
      "Iteration 3, Batch: 20, Loss: 0.19901855289936066\n",
      "Iteration 3, Batch: 21, Loss: 0.1551453322172165\n",
      "Iteration 3, Batch: 22, Loss: 0.1531626433134079\n",
      "Iteration 3, Batch: 23, Loss: 0.18772979080677032\n",
      "Iteration 3, Batch: 24, Loss: 0.1782425194978714\n",
      "Iteration 3, Batch: 25, Loss: 0.1982310265302658\n",
      "Iteration 3, Batch: 26, Loss: 0.16455215215682983\n",
      "Iteration 3, Batch: 27, Loss: 0.19188284873962402\n",
      "Iteration 3, Batch: 28, Loss: 0.21807974576950073\n",
      "Iteration 3, Batch: 29, Loss: 0.17434436082839966\n",
      "Iteration 3, Batch: 30, Loss: 0.14521034061908722\n",
      "Iteration 3, Batch: 31, Loss: 0.13640178740024567\n",
      "Iteration 3, Batch: 32, Loss: 0.16416522860527039\n",
      "Iteration 3, Batch: 33, Loss: 0.15931230783462524\n",
      "Iteration 3, Batch: 34, Loss: 0.19008231163024902\n",
      "Iteration 3, Batch: 35, Loss: 0.21984153985977173\n",
      "Iteration 3, Batch: 36, Loss: 0.1532803177833557\n",
      "Iteration 3, Batch: 37, Loss: 0.14376553893089294\n",
      "Iteration 3, Batch: 38, Loss: 0.24270068109035492\n",
      "Iteration 3, Batch: 39, Loss: 0.24792583286762238\n",
      "Iteration 3, Batch: 40, Loss: 0.16126646101474762\n",
      "Iteration 3, Batch: 41, Loss: 0.19092971086502075\n",
      "Iteration 3, Batch: 42, Loss: 0.1392894983291626\n",
      "Iteration 3, Batch: 43, Loss: 0.14283323287963867\n",
      "Iteration 3, Batch: 44, Loss: 0.18296794593334198\n",
      "Iteration 3, Batch: 45, Loss: 0.08759522438049316\n",
      "Iteration 3, Batch: 46, Loss: 0.1811172068119049\n",
      "Iteration 3, Batch: 47, Loss: 0.16443368792533875\n",
      "Iteration 3, Batch: 48, Loss: 0.16480067372322083\n",
      "Iteration 3, Batch: 49, Loss: 0.14741267263889313\n",
      "Iteration 4, Batch: 0, Loss: 0.19768409430980682\n",
      "Iteration 4, Batch: 1, Loss: 0.19144251942634583\n",
      "Iteration 4, Batch: 2, Loss: 0.15845884382724762\n",
      "Iteration 4, Batch: 3, Loss: 0.1637640744447708\n",
      "Iteration 4, Batch: 4, Loss: 0.19764234125614166\n",
      "Iteration 4, Batch: 5, Loss: 0.17745965719223022\n",
      "Iteration 4, Batch: 6, Loss: 0.19542546570301056\n",
      "Iteration 4, Batch: 7, Loss: 0.13499140739440918\n",
      "Iteration 4, Batch: 8, Loss: 0.19186517596244812\n",
      "Iteration 4, Batch: 9, Loss: 0.2207295298576355\n",
      "Iteration 4, Batch: 10, Loss: 0.18500836193561554\n",
      "Iteration 4, Batch: 11, Loss: 0.14979229867458344\n",
      "Iteration 4, Batch: 12, Loss: 0.21603557467460632\n",
      "Iteration 4, Batch: 13, Loss: 0.17069658637046814\n",
      "Iteration 4, Batch: 14, Loss: 0.1978319138288498\n",
      "Iteration 4, Batch: 15, Loss: 0.16314825415611267\n",
      "Iteration 4, Batch: 16, Loss: 0.16850516200065613\n",
      "Iteration 4, Batch: 17, Loss: 0.18721868097782135\n",
      "Iteration 4, Batch: 18, Loss: 0.18466708064079285\n",
      "Iteration 4, Batch: 19, Loss: 0.1689467579126358\n",
      "Iteration 4, Batch: 20, Loss: 0.17417052388191223\n",
      "Iteration 4, Batch: 21, Loss: 0.17592208087444305\n",
      "Iteration 4, Batch: 22, Loss: 0.12061353772878647\n",
      "Iteration 4, Batch: 23, Loss: 0.14712747931480408\n",
      "Iteration 4, Batch: 24, Loss: 0.15672828257083893\n",
      "Iteration 4, Batch: 25, Loss: 0.17001105844974518\n",
      "Iteration 4, Batch: 26, Loss: 0.13842956721782684\n",
      "Iteration 4, Batch: 27, Loss: 0.15866103768348694\n",
      "Iteration 4, Batch: 28, Loss: 0.1898500770330429\n",
      "Iteration 4, Batch: 29, Loss: 0.22745589911937714\n",
      "Iteration 4, Batch: 30, Loss: 0.1866632103919983\n",
      "Iteration 4, Batch: 31, Loss: 0.18940070271492004\n",
      "Iteration 4, Batch: 32, Loss: 0.14183156192302704\n",
      "Iteration 4, Batch: 33, Loss: 0.1722957193851471\n",
      "Iteration 4, Batch: 34, Loss: 0.2178874909877777\n",
      "Iteration 4, Batch: 35, Loss: 0.18062175810337067\n",
      "Iteration 4, Batch: 36, Loss: 0.14918698370456696\n",
      "Iteration 4, Batch: 37, Loss: 0.22309687733650208\n",
      "Iteration 4, Batch: 38, Loss: 0.18645079433918\n",
      "Iteration 4, Batch: 39, Loss: 0.148915097117424\n",
      "Iteration 4, Batch: 40, Loss: 0.1704789698123932\n",
      "Iteration 4, Batch: 41, Loss: 0.18696574866771698\n",
      "Iteration 4, Batch: 42, Loss: 0.18926875293254852\n",
      "Iteration 4, Batch: 43, Loss: 0.14351609349250793\n",
      "Iteration 4, Batch: 44, Loss: 0.21981267631053925\n",
      "Iteration 4, Batch: 45, Loss: 0.17048776149749756\n",
      "Iteration 4, Batch: 46, Loss: 0.1569555252790451\n",
      "Iteration 4, Batch: 47, Loss: 0.198016956448555\n",
      "Iteration 4, Batch: 48, Loss: 0.13749614357948303\n",
      "Iteration 4, Batch: 49, Loss: 0.19912086427211761\n",
      "Iteration 5, Batch: 0, Loss: 0.1803455352783203\n",
      "Iteration 5, Batch: 1, Loss: 0.21055522561073303\n",
      "Iteration 5, Batch: 2, Loss: 0.18476004898548126\n",
      "Iteration 5, Batch: 3, Loss: 0.16063067317008972\n",
      "Iteration 5, Batch: 4, Loss: 0.20680636167526245\n",
      "Iteration 5, Batch: 5, Loss: 0.12347529828548431\n",
      "Iteration 5, Batch: 6, Loss: 0.19983674585819244\n",
      "Iteration 5, Batch: 7, Loss: 0.23574256896972656\n",
      "Iteration 5, Batch: 8, Loss: 0.1510026454925537\n",
      "Iteration 5, Batch: 9, Loss: 0.20599189400672913\n",
      "Iteration 5, Batch: 10, Loss: 0.19685542583465576\n",
      "Iteration 5, Batch: 11, Loss: 0.183078795671463\n",
      "Iteration 5, Batch: 12, Loss: 0.16308119893074036\n",
      "Iteration 5, Batch: 13, Loss: 0.18753889203071594\n",
      "Iteration 5, Batch: 14, Loss: 0.22597400844097137\n",
      "Iteration 5, Batch: 15, Loss: 0.18966801464557648\n",
      "Iteration 5, Batch: 16, Loss: 0.16451947391033173\n",
      "Iteration 5, Batch: 17, Loss: 0.1687265783548355\n",
      "Iteration 5, Batch: 18, Loss: 0.2083842009305954\n",
      "Iteration 5, Batch: 19, Loss: 0.18467757105827332\n",
      "Iteration 5, Batch: 20, Loss: 0.1715995818376541\n",
      "Iteration 5, Batch: 21, Loss: 0.2022630125284195\n",
      "Iteration 5, Batch: 22, Loss: 0.18310725688934326\n",
      "Iteration 5, Batch: 23, Loss: 0.17767418920993805\n",
      "Iteration 5, Batch: 24, Loss: 0.16504910588264465\n",
      "Iteration 5, Batch: 25, Loss: 0.18006300926208496\n",
      "Iteration 5, Batch: 26, Loss: 0.1400081366300583\n",
      "Iteration 5, Batch: 27, Loss: 0.1788746565580368\n",
      "Iteration 5, Batch: 28, Loss: 0.16782459616661072\n",
      "Iteration 5, Batch: 29, Loss: 0.15751831233501434\n",
      "Iteration 5, Batch: 30, Loss: 0.1763632893562317\n",
      "Iteration 5, Batch: 31, Loss: 0.2049647867679596\n",
      "Iteration 5, Batch: 32, Loss: 0.2083568572998047\n",
      "Iteration 5, Batch: 33, Loss: 0.14074161648750305\n",
      "Iteration 5, Batch: 34, Loss: 0.12923955917358398\n",
      "Iteration 5, Batch: 35, Loss: 0.1473911553621292\n",
      "Iteration 5, Batch: 36, Loss: 0.18394988775253296\n",
      "Iteration 5, Batch: 37, Loss: 0.17049716413021088\n",
      "Iteration 5, Batch: 38, Loss: 0.15455253422260284\n",
      "Iteration 5, Batch: 39, Loss: 0.20590583980083466\n",
      "Iteration 5, Batch: 40, Loss: 0.14830382168293\n",
      "Iteration 5, Batch: 41, Loss: 0.15576426684856415\n",
      "Iteration 5, Batch: 42, Loss: 0.16356566548347473\n",
      "Iteration 5, Batch: 43, Loss: 0.2274441123008728\n",
      "Iteration 5, Batch: 44, Loss: 0.21749022603034973\n",
      "Iteration 5, Batch: 45, Loss: 0.16239407658576965\n",
      "Iteration 5, Batch: 46, Loss: 0.16449473798274994\n",
      "Iteration 5, Batch: 47, Loss: 0.18417266011238098\n",
      "Iteration 5, Batch: 48, Loss: 0.16536694765090942\n",
      "Iteration 5, Batch: 49, Loss: 0.10635993629693985\n",
      "Iteration 6, Batch: 0, Loss: 0.2100389450788498\n",
      "Iteration 6, Batch: 1, Loss: 0.15041327476501465\n",
      "Iteration 6, Batch: 2, Loss: 0.21781378984451294\n",
      "Iteration 6, Batch: 3, Loss: 0.18042393028736115\n",
      "Iteration 6, Batch: 4, Loss: 0.17256174981594086\n",
      "Iteration 6, Batch: 5, Loss: 0.18671096861362457\n",
      "Iteration 6, Batch: 6, Loss: 0.168512225151062\n",
      "Iteration 6, Batch: 7, Loss: 0.20598672330379486\n",
      "Iteration 6, Batch: 8, Loss: 0.1863696277141571\n",
      "Iteration 6, Batch: 9, Loss: 0.155949205160141\n",
      "Iteration 6, Batch: 10, Loss: 0.1787184476852417\n",
      "Iteration 6, Batch: 11, Loss: 0.23010645806789398\n",
      "Iteration 6, Batch: 12, Loss: 0.16705897450447083\n",
      "Iteration 6, Batch: 13, Loss: 0.16860732436180115\n",
      "Iteration 6, Batch: 14, Loss: 0.1640506088733673\n",
      "Iteration 6, Batch: 15, Loss: 0.17478615045547485\n",
      "Iteration 6, Batch: 16, Loss: 0.1629093736410141\n",
      "Iteration 6, Batch: 17, Loss: 0.17300288379192352\n",
      "Iteration 6, Batch: 18, Loss: 0.17293010652065277\n",
      "Iteration 6, Batch: 19, Loss: 0.15731120109558105\n",
      "Iteration 6, Batch: 20, Loss: 0.20360957086086273\n",
      "Iteration 6, Batch: 21, Loss: 0.1638495773077011\n",
      "Iteration 6, Batch: 22, Loss: 0.11192192137241364\n",
      "Iteration 6, Batch: 23, Loss: 0.1725614368915558\n",
      "Iteration 6, Batch: 24, Loss: 0.17723232507705688\n",
      "Iteration 6, Batch: 25, Loss: 0.20531581342220306\n",
      "Iteration 6, Batch: 26, Loss: 0.15595698356628418\n",
      "Iteration 6, Batch: 27, Loss: 0.1683240830898285\n",
      "Iteration 6, Batch: 28, Loss: 0.19234223663806915\n",
      "Iteration 6, Batch: 29, Loss: 0.12704595923423767\n",
      "Iteration 6, Batch: 30, Loss: 0.20882882177829742\n",
      "Iteration 6, Batch: 31, Loss: 0.19988539814949036\n",
      "Iteration 6, Batch: 32, Loss: 0.17906689643859863\n",
      "Iteration 6, Batch: 33, Loss: 0.1920790821313858\n",
      "Iteration 6, Batch: 34, Loss: 0.1412966549396515\n",
      "Iteration 6, Batch: 35, Loss: 0.17754188179969788\n",
      "Iteration 6, Batch: 36, Loss: 0.1518421769142151\n",
      "Iteration 6, Batch: 37, Loss: 0.1635797917842865\n",
      "Iteration 6, Batch: 38, Loss: 0.15497760474681854\n",
      "Iteration 6, Batch: 39, Loss: 0.21775294840335846\n",
      "Iteration 6, Batch: 40, Loss: 0.17512495815753937\n",
      "Iteration 6, Batch: 41, Loss: 0.14649230241775513\n",
      "Iteration 6, Batch: 42, Loss: 0.16167797148227692\n",
      "Iteration 6, Batch: 43, Loss: 0.1657857894897461\n",
      "Iteration 6, Batch: 44, Loss: 0.20294444262981415\n",
      "Iteration 6, Batch: 45, Loss: 0.1656346619129181\n",
      "Iteration 6, Batch: 46, Loss: 0.1461557298898697\n",
      "Iteration 6, Batch: 47, Loss: 0.21856583654880524\n",
      "Iteration 6, Batch: 48, Loss: 0.1827310472726822\n",
      "Iteration 6, Batch: 49, Loss: 0.15845634043216705\n",
      "Iteration 7, Batch: 0, Loss: 0.1569323092699051\n",
      "Iteration 7, Batch: 1, Loss: 0.15446029603481293\n",
      "Iteration 7, Batch: 2, Loss: 0.21169863641262054\n",
      "Iteration 7, Batch: 3, Loss: 0.21477633714675903\n"
     ]
    }
   ],
   "source": [
    "# Specify parameters\n",
    "N = 10000\n",
    "n = 350\n",
    "k = 20\n",
    "\n",
    "# Create dataset for training\n",
    "train_data = sample_data(N, n, k)\n",
    "train_dataset = SubmatrixDataset(train_data)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=200, shuffle=True)\n",
    "\n",
    "# Specify parameters\n",
    "num_epochs = 500\n",
    "\n",
    "# Make an array of time points\n",
    "time_array = torch.linspace(0.5, 700, 1400, device=device)\n",
    "\n",
    "# Store optimal model\n",
    "model = deepcopy(model_350)\n",
    "min_loss = float(\"inf\")\n",
    "model.to(device)\n",
    "model_opt = deepcopy(model)\n",
    "\n",
    "# Specify the optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "\n",
    "for it in range(num_epochs):\n",
    "    counter = 0\n",
    "    loss_total = 0\n",
    "    \n",
    "    # Iterate through batches\n",
    "    for batch in train_dataloader:\n",
    "        # Get loss from model\n",
    "        loss = mc_loss_batch_simul(model, batch, time_array, n, k, num_steps=None, time_threshold=50, p_bad=0.05)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    \n",
    "        # Backpropagate\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Iteration {}, Batch: {}, Loss: {}\".format(it, counter, loss.item()))\n",
    "\n",
    "        # Update counter\n",
    "        counter += 1\n",
    "        loss_total += loss.item()\n",
    "\n",
    "    # Update best model\n",
    "    with torch.no_grad():\n",
    "        if loss_total < min_loss:\n",
    "            min_loss = loss_total\n",
    "            model_opt = deepcopy(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusions",
   "language": "python",
   "name": "diffusions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
